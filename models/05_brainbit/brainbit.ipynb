{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625468ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import mne\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from scipy.signal import detrend\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# CONFIG\n",
    "# ---------------------------------------------------\n",
    "PROJ = Path(\"..\").resolve().parent\n",
    "MODEL_BASE   = PROJ / \"models\" / \"04_hybrid\" / \"ModeloW\" / \"nb2_h4\" / \"nb2_h4\"\n",
    "BRAINBIT_DIR = PROJ / \"data\" / \"brainbit\"\n",
    "BRAINBIT_FILE = BRAINBIT_DIR / \"brainbit_MI_LR_preproc_final_for_model.npz\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Usando modelos en:\", MODEL_BASE)\n",
    "print(\"Usando BrainBit npz:\", BRAINBIT_FILE)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# SWITCHES GLOBALES\n",
    "# ---------------------------------------------------\n",
    "# TTA shifts (los mismos del entrenamiento)\n",
    "TTA_SHIFTS_S = [-0.075, -0.05, -0.025, 0.0, 0.025, 0.05, 0.075]\n",
    "\n",
    "# Notch opcional para BrainBit\n",
    "BB_APPLY_NOTCH  = True        # pon True si quieres aplicar notch\n",
    "BB_NOTCH_FREQS  = [60.0]      # frecuencias de notch, p.ej. [50.0] o [50.0, 100.0]\n",
    "\n",
    "# Z-score opcional (normalizaciÃ³n global por canal)\n",
    "BB_APPLY_ZSCORE = False        # True â†’ aplica z-score, False â†’ no\n",
    "\n",
    "# TTA opcional (tanto en global como en FT)\n",
    "BB_USE_TTA      = True       # True â†’ usa TTA, False â†’ inferencia directa\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 0) Data augmentation (igual que en entrenamiento)\n",
    "# ---------------------------------------------------\n",
    "def augment_batch(xb,\n",
    "                  p_jitter=0.35,\n",
    "                  p_noise=0.35,\n",
    "                  p_chdrop=0.15,\n",
    "                  max_jitter_frac=0.03,\n",
    "                  noise_std=0.03,\n",
    "                  max_chdrop=1):\n",
    "    \"\"\"\n",
    "    xb: tensor (B, C, T)\n",
    "    \"\"\"\n",
    "    B, C, T = xb.shape\n",
    "\n",
    "    # Jitter temporal (shift aleatorio)\n",
    "    if np.random.rand() < p_jitter:\n",
    "        max_shift = int(max(1, T * max_jitter_frac))\n",
    "        shifts = torch.randint(\n",
    "            low=-max_shift,\n",
    "            high=max_shift + 1,\n",
    "            size=(B,),\n",
    "            device=xb.device\n",
    "        )\n",
    "        for i in range(B):\n",
    "            xb[i] = torch.roll(xb[i],\n",
    "                               shifts=int(shifts[i].item()),\n",
    "                               dims=-1)\n",
    "\n",
    "    # Ruido gaussiano\n",
    "    if np.random.rand() < p_noise:\n",
    "        xb = xb + noise_std * torch.randn_like(xb)\n",
    "\n",
    "    # Channel dropout\n",
    "    if np.random.rand() < p_chdrop and max_chdrop > 0:\n",
    "        k = min(max_chdrop, C)\n",
    "        for i in range(B):\n",
    "            idx = torch.randperm(C, device=xb.device)[:k]\n",
    "            xb[i, idx, :] = 0.0\n",
    "\n",
    "    return xb\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1) Cargar 6s (1â€“7 s) desde el NPZ preprocesado\n",
    "# ---------------------------------------------------\n",
    "def load_brainbit_6s(path_npz):\n",
    "    \"\"\"\n",
    "    Extrae los 6 s centrales del NPZ preprocesado:\n",
    "    - Entrada = 8 s (1280 muestras a 160 Hz)\n",
    "    - Salida  = 6 s (960 muestras)\n",
    "    Formato final: (N, 8, 960).\n",
    "    Ventana temporal: 1â€“7 s  (1s antes + 4s MI + 1s despuÃ©s).\n",
    "    \"\"\"\n",
    "    d = np.load(path_npz, allow_pickle=True)\n",
    "\n",
    "    X = d[\"X\"]            # (N, 1280, 8) o (N, 8, 1280)\n",
    "    y = d[\"y\"].astype(int)\n",
    "    fs = int(d[\"fs\"])     # 160 Hz\n",
    "\n",
    "    print(\"\\n====== CARGA BRAINBIT ======\")\n",
    "    print(\"X original:\", X.shape)\n",
    "    print(\"fs =\", fs)\n",
    "\n",
    "    # Asegurar formato (N, C, T)\n",
    "    if X.shape[1] == 8:          # (N, 8, T)\n",
    "        pass\n",
    "    elif X.shape[2] == 8:        # (N, T, 8) â†’ convertir\n",
    "        X = np.transpose(X, (0, 2, 1))\n",
    "    else:\n",
    "        raise ValueError(\"X debe tener 8 canales en alguna dimensiÃ³n.\")\n",
    "\n",
    "    N, C, T = X.shape\n",
    "    assert T == 1280, \"Se esperaban 1280 muestras (8s a 160Hz).\"\n",
    "\n",
    "    # Ventana 1.0s â†’ 7.0s => 6s => 960 muestras\n",
    "    idx_start = int(1.0 * fs)     # 160\n",
    "    idx_end   = int(7.0 * fs)     # 1120\n",
    "    X6 = X[:, :, idx_start:idx_end]    # (N, 8, 960)\n",
    "    print(\"Ventana 6s:\", X6.shape)\n",
    "\n",
    "    # Detrend por canal\n",
    "    for i in range(N):\n",
    "        X6[i] = detrend(X6[i], axis=1, type=\"linear\")\n",
    "\n",
    "    # -----------------------------------------------\n",
    "    # Notch opcional (ej. 60 Hz)\n",
    "    # -----------------------------------------------\n",
    "    if BB_APPLY_NOTCH and BB_NOTCH_FREQS:\n",
    "        print(f\"Aplicando notch en BrainBit: freqs={BB_NOTCH_FREQS} Hz ...\")\n",
    "        # mne.filter.notch_filter exige float64\n",
    "        X_flat = X6.reshape(N * C, -1).astype(np.float64)\n",
    "\n",
    "        X_flat = mne.filter.notch_filter(\n",
    "            X_flat,\n",
    "            Fs=fs,\n",
    "            freqs=BB_NOTCH_FREQS,\n",
    "            verbose=\"ERROR\"\n",
    "        )\n",
    "\n",
    "        # volver a float32 y re-ensamblar\n",
    "        X6 = X_flat.astype(np.float32).reshape(N, C, -1)\n",
    "\n",
    "    # -----------------------------------------------\n",
    "    # Z-score opcional global por canal\n",
    "    # -----------------------------------------------\n",
    "    if BB_APPLY_ZSCORE:\n",
    "        print(\"Aplicando z-score global por canal...\")\n",
    "        mean = X6.mean(axis=(0, 2), keepdims=True)\n",
    "        std  = X6.std(axis=(0, 2), keepdims=True) + 1e-6\n",
    "        X6 = (X6 - mean) / std\n",
    "\n",
    "    print(\"Etiquetas Ãºnicas:\", np.unique(y))\n",
    "    return X6.astype(np.float32), y, fs\n",
    "\n",
    "\n",
    "# Carga final\n",
    "X_bb, y_bb, fs_bb = load_brainbit_6s(BRAINBIT_FILE)\n",
    "print(\"Shape final BrainBit para el modelo:\", X_bb.shape, y_bb.shape)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2) TTA por time-shift (igual filosofÃ­a que en entrenamiento)\n",
    "# ---------------------------------------------------\n",
    "def time_shift_tta_logits(model, X, sfreq, shifts_s, device):\n",
    "    \"\"\"\n",
    "    Aplica TTA por desplazamientos temporales sobre TODA la matriz X:\n",
    "    X: (N, C, T)\n",
    "    Devuelve logits promedio: (N, n_classes)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    T = X.shape[-1]\n",
    "    out = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(X.shape[0]):\n",
    "            x0 = X[i]      # (C, T)\n",
    "            acc = []\n",
    "\n",
    "            for sh in shifts_s:\n",
    "                shift = int(round(sh * sfreq))\n",
    "                if shift == 0:\n",
    "                    x = x0\n",
    "                elif shift > 0:\n",
    "                    # shift hacia la derecha: recorta al inicio y rellena al final\n",
    "                    x = np.pad(x0[:, shift:], ((0, 0), (0, shift)), mode=\"edge\")[:, :T]\n",
    "                else:\n",
    "                    # shift < 0: hacia la izquierda, recorta al final y rellena al inicio\n",
    "                    shift = -shift\n",
    "                    x = np.pad(x0[:, :-shift], ((0, 0), (shift, 0)), mode=\"edge\")[:, :T]\n",
    "\n",
    "                xb = torch.tensor(x[None, ...],\n",
    "                                  dtype=torch.float32,\n",
    "                                  device=device)\n",
    "                logit = model(xb).detach().cpu().numpy()[0]\n",
    "                acc.append(logit)\n",
    "\n",
    "            acc = np.mean(np.stack(acc, axis=0), axis=0)\n",
    "            out.append(acc)\n",
    "\n",
    "    return np.stack(out, axis=0)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3) Cargar modelo global de un fold\n",
    "# ---------------------------------------------------\n",
    "def load_global_model(fold: int):\n",
    "    model_path = MODEL_BASE / f\"fold{fold}\" / f\"model_global_fold{fold}_nb2_h4.pth\"\n",
    "    if not model_path.exists():\n",
    "        raise FileNotFoundError(f\"No se encontrÃ³ el modelo: {model_path}\")\n",
    "\n",
    "    # AsegÃºrate de tener EEGCNNTransformer definido / importado\n",
    "    model = EEGCNNTransformer(\n",
    "        n_ch=8, n_cls=2,\n",
    "        d_model=128, n_heads=4, n_layers=1,\n",
    "        p_drop=0.2, p_drop_encoder=0.1,\n",
    "        n_dw_blocks=2, capture_attn=False\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    state = torch.load(model_path, map_location=DEVICE)\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "    print(f\"âœ” Modelo fold{fold} cargado desde:\\n{model_path}\")\n",
    "    return model\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 4) EvaluaciÃ³n GLOBAL (TTA opcional, sin fine-tuning)\n",
    "# ---------------------------------------------------\n",
    "def eval_global(model, X, y, fs):\n",
    "    \"\"\"\n",
    "    Eval GLOBAL:\n",
    "      - Si BB_USE_TTA=True â†’ TTA por time-shift\n",
    "      - Si BB_USE_TTA=False â†’ inferencia directa\n",
    "    \"\"\"\n",
    "    if BB_USE_TTA:\n",
    "        logits = time_shift_tta_logits(model, X, fs, TTA_SHIFTS_S, DEVICE)\n",
    "        print(\">> GLOBAL (con TTA)\")\n",
    "    else:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_t = torch.tensor(X, dtype=torch.float32, device=DEVICE)\n",
    "            logits = model(X_t).cpu().numpy()\n",
    "        print(\">> GLOBAL (sin TTA)\")\n",
    "\n",
    "    y_pred = logits.argmax(axis=1)\n",
    "\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    f1m = f1_score(y, y_pred, average=\"macro\")\n",
    "\n",
    "    print(f\"ACC = {acc:.3f} | F1_macro = {f1m:.3f}\")\n",
    "    print(classification_report(y, y_pred,\n",
    "                                target_names=[\"left\", \"right\"],\n",
    "                                digits=3))\n",
    "    print(\"Matriz de confusiÃ³n:\\n\", confusion_matrix(y, y_pred, labels=[0, 1]))\n",
    "    return acc, f1m, y_pred\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5) Fine-tuning ligero (solo head) + augmentation + TTA opcional\n",
    "# ---------------------------------------------------\n",
    "def eval_with_ft(model, X, y, fs, epochs=10, batch=8, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Fine-tuning sobre TODO BrainBit:\n",
    "    - Data augmentation en cada batch (augment_batch)\n",
    "    - EvaluaciÃ³n final:\n",
    "        * Si BB_USE_TTA=True â†’ con TTA\n",
    "        * Si BB_USE_TTA=False â†’ directa\n",
    "    \"\"\"\n",
    "    assert y.min() >= 0 and y.max() <= 1, \"Etiquetas fuera de rango [0,1]\"\n",
    "\n",
    "    X_t = torch.tensor(X, dtype=torch.float32)\n",
    "    y_t = torch.tensor(y, dtype=torch.long)\n",
    "    ds = torch.utils.data.TensorDataset(X_t, y_t)\n",
    "    ld = torch.utils.data.DataLoader(ds, batch_size=batch, shuffle=True)\n",
    "\n",
    "    # Congelar backbone\n",
    "    for name, p in model.named_parameters():\n",
    "        if not name.startswith(\"head\"):\n",
    "            p.requires_grad = False\n",
    "\n",
    "    opt = torch.optim.Adam(model.head.parameters(), lr=lr, weight_decay=1e-3)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for ep in range(1, epochs + 1):\n",
    "        tot = 0.0\n",
    "        for xb, yb in ld:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "\n",
    "            # ðŸ”¥ AUGMENTATION:\n",
    "            xb = augment_batch(xb)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            tot += float(loss.item()) * len(yb)\n",
    "\n",
    "        print(f\"  FT epoch {ep:02d} | loss={tot/len(ds):.4f}\")\n",
    "\n",
    "    # Evaluar despuÃ©s de FT\n",
    "    model.eval()\n",
    "\n",
    "    if BB_USE_TTA:\n",
    "        logits_ft = time_shift_tta_logits(model, X, fs, TTA_SHIFTS_S, DEVICE)\n",
    "        print(\">> FINE-TUNED (con augmentation + TTA)\")\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            X_t = torch.tensor(X, dtype=torch.float32, device=DEVICE)\n",
    "            logits_ft = model(X_t).cpu().numpy()\n",
    "        print(\">> FINE-TUNED (con augmentation, sin TTA)\")\n",
    "\n",
    "    y_pred = logits_ft.argmax(axis=1)\n",
    "\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    f1m = f1_score(y, y_pred, average=\"macro\")\n",
    "\n",
    "    print(f\"ACC = {acc:.3f} | F1_macro = {f1m:.3f}\")\n",
    "    print(classification_report(y, y_pred,\n",
    "                                target_names=[\"left\", \"right\"],\n",
    "                                digits=3))\n",
    "    print(\"Matriz de confusiÃ³n:\\n\", confusion_matrix(y, y_pred, labels=[0, 1]))\n",
    "    return acc, f1m, y_pred\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 6) Probar los 5 folds sobre BrainBit\n",
    "# ---------------------------------------------------\n",
    "results = []\n",
    "\n",
    "for fold in range(1, 6):\n",
    "    print(\"\\n====================================\")\n",
    "    print(f\" Fold {fold}\")\n",
    "    print(\"====================================\")\n",
    "\n",
    "    model = load_global_model(fold)\n",
    "\n",
    "    # GLOBAL (con/sin TTA segÃºn flag)\n",
    "    acc_g, f1_g, pred_g = eval_global(model, X_bb, y_bb, fs_bb)\n",
    "\n",
    "    # FINE-TUNING (con augmentation + TTA opcional)\n",
    "    acc_ft, f1_ft, pred_ft = eval_with_ft(model, X_bb, y_bb,\n",
    "                                          fs_bb,\n",
    "                                          epochs=10,\n",
    "                                          batch=8,\n",
    "                                          lr=1e-3)\n",
    "\n",
    "    results.append(dict(\n",
    "        fold=fold,\n",
    "        acc_global=acc_g, f1_global=f1_g,\n",
    "        acc_ft=acc_ft,   f1_ft=f1_ft\n",
    "    ))\n",
    "\n",
    "print(\"\\n======== RESUMEN ========\")\n",
    "for r in results:\n",
    "    print(f\"Fold {r['fold']}: Global={r['acc_global']:.3f} | FT={r['acc_ft']:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
