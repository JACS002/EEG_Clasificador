{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b059e153",
   "metadata": {},
   "source": [
    "# 4 Clases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ada67f0",
   "metadata": {},
   "source": [
    "### Configuración Inicial y Reproducibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af5d61ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "import re, json, random, time, copy\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import mne\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc55c53",
   "metadata": {},
   "source": [
    "### Configuración de Reproducibilidad\n",
    "\n",
    "Esta sección establece las semillas y configuraciones necesarias para garantizar resultados reproducibles en el entrenamiento del modelo para clasificación de 4 clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba4becca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# REPRODUCIBILIDAD\n",
    "# =========================\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    \"\"\"\n",
    "    Establece semillas para todas las bibliotecas de aleatoriedad\n",
    "    para garantizar reproducibilidad completa.\n",
    "    \"\"\"\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def seed_worker(worker_id: int):\n",
    "    \"\"\"\n",
    "    Función para inicializar workers de DataLoader con semilla.\n",
    "    \"\"\"\n",
    "    worker_seed = RANDOM_STATE + worker_id\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "seed_everything(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56edc85f",
   "metadata": {},
   "source": [
    "### Configuración de Hiperparámetros para 4 Clases\n",
    "\n",
    "Definición de todos los hiperparámetros del modelo y preprocesamiento de datos EEG para clasificación de 4 clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58792ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Usando dispositivo: cuda\n",
      " INICIANDO EXPERIMENTO CON CNN+Transformer (K-Fold por sujeto como EEGNet)\n",
      " Configuración: 4c, 8 canales, 6s | EPOCHS=60, BATCH=64, LR=0.001 | ZSCORE_PER_EPOCH=False\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "FOLDS_JSON = PROJ / 'models' / '00_folds' / 'Kfold5.json'\n",
    "\n",
    "# Directorio de salida para 4 clases (MODIFICACIÓN)\n",
    "OUTPUT_DIR = Path(\"4clases\")\n",
    "\n",
    "# Hiperparámetros de entrenamiento\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 64\n",
    "BASE_LR = 1e-3\n",
    "WARMUP_EPOCHS = 8\n",
    "PATIENCE = 12\n",
    "\n",
    "# Split de validación por fold (por sujetos)\n",
    "VAL_SUBJECT_FRAC = 0.18\n",
    "VAL_STRAT_SUBJECT = True\n",
    "\n",
    "# Prepro global\n",
    "RESAMPLE_HZ = None\n",
    "DO_NOTCH = True\n",
    "DO_BANDPASS = False\n",
    "BP_LO, BP_HI = 4.0, 38.0\n",
    "DO_CAR = False\n",
    "ZSCORE_PER_EPOCH = False\n",
    "\n",
    "# Parámetros del modelo\n",
    "D_MODEL = 128\n",
    "N_HEADS = 4\n",
    "N_LAYERS = 2\n",
    "P_DROP = 0.2\n",
    "P_DROP_ENCODER = 0.3\n",
    "\n",
    "# Ventana temporal\n",
    "TMIN, TMAX = -1.0, 5.0\n",
    "\n",
    "# TTA / SUBWINDOW en TEST\n",
    "SW_MODE = 'tta'   # 'none'|'subwin'|'tta'\n",
    "SW_ENABLE = True\n",
    "TTA_SHIFTS_S = [-0.075, -0.05, -0.025, 0.0, 0.025, 0.05, 0.075]\n",
    "SW_LEN, SW_STRIDE = 4.5, 1.5\n",
    "COMBINE_TTA_AND_SUBWIN = False\n",
    "\n",
    "# Sampler balanceado\n",
    "USE_WEIGHTED_SAMPLER = True\n",
    "\n",
    "# EMA (solo GLOBAL). En FT se desactiva.\n",
    "USE_EMA = True\n",
    "EMA_DECAY = 0.9995\n",
    "\n",
    "# Fine-tuning (por sujeto)\n",
    "FT_N_FOLDS = 4\n",
    "FT_FREEZE_EPOCHS = 8           # etapa 1 (congelar backbone)\n",
    "FT_UNFREEZE_EPOCHS = 8         # etapa 2 (descongelar)\n",
    "FT_PATIENCE = 6                # early stopping en FT\n",
    "FT_BATCH = 64\n",
    "FT_LR_HEAD = 1e-3              # cabeza (5x aprox del backbone)\n",
    "FT_LR_BACKBONE = 2e-4          # backbone\n",
    "FT_WD = 1e-3                   # weight decay menor en FT\n",
    "FT_AUG = dict(p_jitter=0.25, p_noise=0.25, p_chdrop=0.10,\n",
    "              max_jitter_frac=0.02, noise_std=0.02, max_chdrop=1)\n",
    "FT_ALPHA_BOOST_FISTS = 1.25\n",
    "FT_ALPHA_BOOST_FEET  = 1.05\n",
    "\n",
    "# Configuración de sujetos y canales\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','FCz']\n",
    "CLASS_NAMES = ['left', 'right', 'both_fists', 'both_feet']\n",
    "\n",
    "# Runs específicos para cada tipo de imaginación\n",
    "IMAGERY_RUNS_LR = {4, 8, 12}\n",
    "IMAGERY_RUNS_BF = {6, 10, 14}\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\" Usando dispositivo: {DEVICE}\")\n",
    "print(\" INICIANDO EXPERIMENTO CON CNN+Transformer (K-Fold por sujeto como EEGNet)\")\n",
    "print(f\" Configuración: 4c, 8 canales, 6s | EPOCHS={EPOCHS}, BATCH={BATCH_SIZE}, LR={BASE_LR} | ZSCORE_PER_EPOCH={ZSCORE_PER_EPOCH}\")\n",
    "\n",
    "# Tag y FLOPs aproximados para la tabla final\n",
    "MODEL_TAG = \"nb4_h2_4c\"\n",
    "MODEL_FLOPS_G = 0.03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc39341b",
   "metadata": {},
   "source": [
    "### Funciones de Utilidad para I/O\n",
    "\n",
    "Funciones para manejar la carga y normalización de datos EEG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9320751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# UTILIDADES I/O\n",
    "# =========================\n",
    "def normalize_ch_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Normaliza nombres de canales eliminando caracteres especiales.\n",
    "    \"\"\"\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', name)\n",
    "    return s.upper()\n",
    "\n",
    "NORMALIZED_TARGETS = [normalize_ch_name(c) for c in EXPECTED_8]\n",
    "\n",
    "def pick_8_channels(raw: mne.io.BaseRaw) -> mne.io.BaseRaw:\n",
    "    \"\"\"\n",
    "    Selecciona los 8 canales motores específicos del dataset.\n",
    "    \"\"\"\n",
    "    chs = raw.info['ch_names']\n",
    "    norm_map = {normalize_ch_name(ch): ch for ch in chs}\n",
    "    picked = []\n",
    "    for target_norm, target_orig in zip(NORMALIZED_TARGETS, EXPECTED_8):\n",
    "        if target_norm in norm_map:\n",
    "            picked.append(norm_map[target_norm])\n",
    "        else:\n",
    "            raise RuntimeError(f\"Canal requerido '{target_orig}' no encontrado. Disponibles: {chs}\")\n",
    "    return raw.pick(picks=picked)\n",
    "\n",
    "def list_subject_imagery_edfs(subject_id: str) -> list:\n",
    "    \"\"\"\n",
    "    Lista archivos EDF de imaginación motora para un sujeto.\n",
    "    \"\"\"\n",
    "    subj_dir = DATA_RAW / subject_id\n",
    "    edfs = []\n",
    "    for r in [4, 6, 8, 10, 12, 14]:\n",
    "        edfs.extend(glob(str(subj_dir / f\"{subject_id}R{r:02d}.edf\")))\n",
    "    return sorted(edfs)\n",
    "\n",
    "def subject_id_to_int(s: str) -> int:\n",
    "    \"\"\"\n",
    "    Convierte ID de sujeto de string a entero.\n",
    "    \"\"\"\n",
    "    m = re.match(r'[Ss](\\d+)', s)\n",
    "    return int(m.group(1)) if m else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b6ceff",
   "metadata": {},
   "source": [
    "### Funciones de Medición de Latencia y P-Values\n",
    "\n",
    "Herramientas para medir rendimiento computacional y análisis estadístico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18f84db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Medición de latencia (ms por batch)\n",
    "# =========================\n",
    "def estimate_latency_ms(model, example_batch, device, n_warmup=10, n_iters=50):\n",
    "    \"\"\"\n",
    "    Estima la latencia media (ms) de un forward pass para un batch dado.\n",
    "    example_batch: numpy array (B, C, T)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    xb = torch.tensor(example_batch, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Warmup (para estabilizar GPU/cuDNN)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_warmup):\n",
    "            _ = model(xb)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize(device)\n",
    "\n",
    "    # Medición\n",
    "    t0 = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_iters):\n",
    "            _ = model(xb)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize(device)\n",
    "\n",
    "    dt = (time.perf_counter() - t0) / max(1, n_iters)\n",
    "    return dt * 1000.0  # ms\n",
    "\n",
    "\n",
    "def compute_channel_pvalues_4c(X, y, n_classes=4):\n",
    "    \"\"\"\n",
    "    P-values por canal para diferencia entre 4 clases.\n",
    "    Usa potencia media (X^2) promediada en el tiempo como feature.\n",
    "    X: (N, C, T)\n",
    "    y: (N,) con labels 0..3\n",
    "    Devuelve: vector (C,) con p-values (ANOVA 1-vía).\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    y = np.asarray(y, dtype=int)\n",
    "\n",
    "    if X.ndim != 3:\n",
    "        raise ValueError(\"X debe ser (N, C, T)\")\n",
    "\n",
    "    N, C, T = X.shape\n",
    "    # feature = potencia media por trial y canal\n",
    "    feats = (X ** 2).mean(axis=2)   # (N, C)\n",
    "\n",
    "    pvals = np.ones(C, dtype=float)\n",
    "\n",
    "    for ch in range(C):\n",
    "        grupos = [feats[y == cls, ch] for cls in range(n_classes)\n",
    "                  if np.any(y == cls)]\n",
    "        # si hay menos de 2 grupos con muestras, el test no tiene sentido\n",
    "        if len(grupos) < 2:\n",
    "            pvals[ch] = 1.0\n",
    "            continue\n",
    "        _, p = stats.f_oneway(*grupos)\n",
    "        pvals[ch] = p\n",
    "\n",
    "    return pvals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2a7402",
   "metadata": {},
   "source": [
    "### Funciones de Saliency y Topomaps\n",
    "\n",
    "Herramientas para visualizar importancia de canales y crear mapas topográficos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21a30dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Channel saliency (Grad*Input agregado por canal) + topomap\n",
    "# =========================\n",
    "def compute_channel_saliency(\n",
    "    model,\n",
    "    X,              # numpy (N, C, T) estandarizado (test)\n",
    "    device,\n",
    "    max_trials=256,\n",
    "    batch_size=32,\n",
    "):\n",
    "    \"\"\"\n",
    "    Importancia por canal usando |Grad*Input|, promediado\n",
    "    sobre tiempo y sobre un subconjunto de trials.\n",
    "    Devuelve vector (C,) normalizado [0,1].\n",
    "\n",
    "    Esta es la magnitud detrás del topomap\n",
    "    \"Channel importance (saliency)\" que usaste en 2 clases.\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=np.float32)\n",
    "    N, C, T = X.shape\n",
    "    if N == 0:\n",
    "        raise ValueError(\"X vacío en compute_channel_saliency\")\n",
    "\n",
    "    n_use = min(max_trials, N)\n",
    "    rng = np.random.RandomState(RANDOM_STATE + 777)\n",
    "    idx = rng.choice(np.arange(N), size=n_use, replace=False)\n",
    "\n",
    "    model.eval()\n",
    "    saliency_sum = np.zeros(C, dtype=np.float64)\n",
    "    n_batches = 0\n",
    "\n",
    "    for start in range(0, n_use, batch_size):\n",
    "        batch_idx = idx[start:start+batch_size]\n",
    "        xb_np = X[batch_idx]   # (B, C, T)\n",
    "\n",
    "        xb = torch.tensor(xb_np, dtype=torch.float32, device=device)\n",
    "        xb.requires_grad_(True)\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)                      # (B, n_cls)\n",
    "        preds = logits.argmax(dim=1)           # clase objetivo = predicha\n",
    "\n",
    "        # score = suma de logits de la clase predicha\n",
    "        score = logits[torch.arange(len(preds), device=device), preds].sum()\n",
    "        score.backward()\n",
    "\n",
    "        grad = xb.grad.detach()                # (B, C, T)\n",
    "        contrib = (grad * xb).abs().sum(dim=-1)        # (B, C)\n",
    "        saliency_batch = contrib.mean(dim=0).detach().cpu().numpy()  # (C,)\n",
    "\n",
    "\n",
    "        saliency_sum += saliency_batch\n",
    "        n_batches += 1\n",
    "\n",
    "    saliency = saliency_sum / max(1, n_batches)\n",
    "    # normalizar a [0,1] para el topomap\n",
    "    saliency = saliency - saliency.min()\n",
    "    maxv = saliency.max()\n",
    "    if maxv > 0:\n",
    "        saliency = saliency / maxv\n",
    "    return saliency.astype(np.float32)\n",
    "\n",
    "\n",
    "def save_channel_saliency_topomap(\n",
    "    model,\n",
    "    X_te_std,\n",
    "    sfreq_used,\n",
    "    fold,\n",
    "    run_tag,\n",
    "    output_dir: Path = OUTPUT_DIR\n",
    "):\n",
    "    \"\"\"\n",
    "    Calcula y guarda topomap de saliency por canal.\n",
    "    MODIFICACIÓN: Guarda en directorio de 4 clases.\n",
    "    \"\"\"\n",
    "    # Crear directorio del fold\n",
    "    fold_dir = output_dir / f\"fold{fold}\"\n",
    "    fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    saliency = compute_channel_saliency(\n",
    "        model=model,\n",
    "        X=X_te_std,\n",
    "        device=DEVICE,\n",
    "        max_trials=256,\n",
    "        batch_size=32,\n",
    "    )  # (C,)\n",
    "\n",
    "    info_topo = make_mne_info_8ch(sfreq_used)\n",
    "\n",
    "    out_path = fold_dir / f\"topomap_saliency_fold{fold}_{run_tag}.png\"\n",
    "    title = f\"Channel importance (saliency) — Fold {fold} [{run_tag}]\"\n",
    "\n",
    "    plot_topomap_from_values(\n",
    "        saliency,\n",
    "        info_topo,\n",
    "        title=title,\n",
    "        out_path=out_path,\n",
    "        cmap=\"Reds\",\n",
    "        cbar_label=\"Normalized saliency\",\n",
    "    )\n",
    "    print(f\"↳ Topomap saliency guardado: {out_path}\")\n",
    "    return saliency\n",
    "\n",
    "\n",
    "\n",
    "def make_mne_info_8ch(sfreq: float):\n",
    "    \"\"\"\n",
    "    Crea estructura de información de MNE para los 8 canales.\n",
    "    \"\"\"\n",
    "    info = mne.create_info(\n",
    "        ch_names=EXPECTED_8,\n",
    "        sfreq=float(sfreq),\n",
    "        ch_types=\"eeg\"\n",
    "    )\n",
    "    try:\n",
    "        montage = mne.channels.make_standard_montage(\"standard_1020\")\n",
    "        info.set_montage(montage)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return info\n",
    "\n",
    "\n",
    "def plot_topomap_from_values(values, info, title: str, out_path: str,\n",
    "                             cmap=\"viridis\", cbar_label=\"Value\",\n",
    "                             vmin=None, vmax=None):\n",
    "    \"\"\"\n",
    "    Dibuja y guarda un topomap dado un vector de valores por canal.\n",
    "    MODIFICACIÓN: Acepta Path o string para out_path.\n",
    "    \"\"\"\n",
    "    values = np.asarray(values, dtype=float)\n",
    "    fig, ax = plt.subplots(figsize=(4.2, 4.0))\n",
    "\n",
    "    im, _ = mne.viz.plot_topomap(\n",
    "        values, info, axes=ax, show=False, cmap=cmap\n",
    "    )\n",
    "\n",
    "    if (vmin is not None) or (vmax is not None):\n",
    "        im.set_clim(vmin=vmin, vmax=vmax)\n",
    "\n",
    "    ax.set_title(title, fontsize=10)\n",
    "    cbar = plt.colorbar(im, ax=ax, shrink=0.7)\n",
    "    cbar.set_label(cbar_label, fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(str(out_path), dpi=160, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"↳ Topomap guardado: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0426b20f",
   "metadata": {},
   "source": [
    "### Carga y Preprocesamiento de Datos\n",
    "\n",
    "Funciones para cargar y preprocesar datos EEG de sujetos individuales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fbea37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_subject_epochs(subject_id: str, resample_hz: int, do_notch: bool,\n",
    "                        do_bandpass: bool, do_car: bool, bp_lo: float, bp_hi: float):\n",
    "    \"\"\"\n",
    "    Carga y preprocesa épocas EEG para un sujeto específico.\n",
    "    Maneja las 4 clases: left (0), right (1), both_fists (2), both_feet (3).\n",
    "    \"\"\"\n",
    "    edfs = list_subject_imagery_edfs(subject_id)\n",
    "    if len(edfs) == 0:\n",
    "        return np.empty((0,8,1), dtype=np.float32), np.empty((0,), dtype=int), None\n",
    "\n",
    "    X_list, y_list, sfreq_list = [], [], []\n",
    "    for edf_path in edfs:\n",
    "        m = re.search(r\"R(\\d{2})\", Path(edf_path).name)\n",
    "        run = int(m.group(1)) if m else -1\n",
    "\n",
    "        raw = mne.io.read_raw_edf(edf_path, preload=True, verbose='ERROR')\n",
    "        raw = pick_8_channels(raw)\n",
    "\n",
    "        if do_notch:\n",
    "            raw.notch_filter(freqs=[60.0], picks='all', verbose='ERROR')\n",
    "        if do_bandpass:\n",
    "            raw.filter(l_freq=bp_lo, h_freq=bp_hi, picks='all', verbose='ERROR')\n",
    "        if do_car:\n",
    "            raw.set_eeg_reference('average', projection=False, verbose='ERROR')\n",
    "        if resample_hz is not None and resample_hz > 0:\n",
    "            raw.resample(resample_hz)\n",
    "\n",
    "        sfreq = raw.info['sfreq']\n",
    "        events, event_id = mne.events_from_annotations(raw, verbose='ERROR')\n",
    "        keep = {k: v for k, v in event_id.items() if k in {'T1', 'T2'}}\n",
    "        if len(keep) == 0:\n",
    "            continue\n",
    "\n",
    "        epochs = mne.Epochs(raw, events=events, event_id=keep, tmin=TMIN, tmax=TMAX,\n",
    "                            baseline=None, preload=True, verbose='ERROR')\n",
    "        X = epochs.get_data()\n",
    "\n",
    "        if ZSCORE_PER_EPOCH:\n",
    "            X = X.astype(np.float32)\n",
    "            eps = 1e-6\n",
    "            mu = X.mean(axis=2, keepdims=True)\n",
    "            sd = X.std(axis=2, keepdims=True) + eps\n",
    "            X = (X - mu) / sd\n",
    "\n",
    "        ev_codes = epochs.events[:, 2]\n",
    "        inv = {v: k for k, v in keep.items()}\n",
    "        y_run = []\n",
    "        for code in ev_codes:\n",
    "            lab = inv[code]\n",
    "            if run in IMAGERY_RUNS_LR:\n",
    "                y_run.append(0 if lab == 'T1' else 1)\n",
    "            elif run in IMAGERY_RUNS_BF:\n",
    "                y_run.append(2 if lab == 'T1' else 3)\n",
    "            else:\n",
    "                y_run.append(-1)\n",
    "        y_run = np.array(y_run, dtype=int)\n",
    "        mask = y_run >= 0\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        X_list.append(X[mask])\n",
    "        y_list.append(y_run[mask])\n",
    "        sfreq_list.append(sfreq)\n",
    "\n",
    "    if len(X_list) == 0:\n",
    "        return np.empty((0,8,1), dtype=np.float32), np.empty((0,), dtype=int), None\n",
    "\n",
    "    X_all = np.concatenate(X_list, axis=0).astype(np.float32)\n",
    "    y_all = np.concatenate(y_list, axis=0).astype(int)\n",
    "\n",
    "    if len(set([int(round(s)) for s in sfreq_list])) != 1:\n",
    "        raise RuntimeError(f\"Sampling rates inconsistentes: {sfreq_list}\")\n",
    "\n",
    "    return X_all, y_all, sfreq_list[0]\n",
    "\n",
    "def load_fold_subjects(folds_json: Path, fold: int):\n",
    "    \"\"\"\n",
    "    Carga sujetos de train y test para un fold específico.\n",
    "    \"\"\"\n",
    "    with open(folds_json, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    for item in data.get('folds', []):\n",
    "        if int(item.get('fold', -1)) == int(fold):\n",
    "            return list(item.get('train', [])), list(item.get('test', []))\n",
    "    raise ValueError(f\"Fold {fold} not found in {folds_json}\")\n",
    "\n",
    "def standardize_per_channel(train_X, other_X):\n",
    "    \"\"\"\n",
    "    Estandariza datos por canal usando estadísticas del conjunto de entrenamiento.\n",
    "    \"\"\"\n",
    "    C = train_X.shape[1]\n",
    "    train_X = train_X.astype(np.float32)\n",
    "    other_X = other_X.astype(np.float32)\n",
    "    for c in range(C):\n",
    "        mu = train_X[:, c, :].mean()\n",
    "        sd = train_X[:, c, :].std()\n",
    "        sd = sd if sd > 1e-6 else 1.0\n",
    "        train_X[:, c, :] = (train_X[:, c, :] - mu) / sd\n",
    "        other_X[:, c, :] = (other_X[:, c, :] - mu) / sd\n",
    "    return train_X, other_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e55fc14",
   "metadata": {},
   "source": [
    "### Funciones de Visualización Grad-CAM++ para 4 Clases\n",
    "\n",
    "Herramientas para visualizar activaciones del modelo mediante Grad-CAM++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d8d877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cam_grid_4classes(\n",
    "    X_te_std, y_te, model_to_probe,\n",
    "    sfreq_eff, fold, run_tag,\n",
    "    channel_mode=\"mean\", output_dir: Path = OUTPUT_DIR\n",
    "):\n",
    "    \"\"\"\n",
    "    Genera:\n",
    "      1) Un grid EEG + Grad-CAM++ con 4 filas, una trial correcta por clase.\n",
    "      2) Un gráfico separado con el Grad-CAM++ 1D de esas 4 clases.\n",
    "\n",
    "    MODIFICACIÓN: Guarda en directorio de 4 clases.\n",
    "    \"\"\"\n",
    "    # Crear directorio del fold\n",
    "    fold_dir = output_dir / f\"fold{fold}\"\n",
    "    fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    model_to_probe.eval()\n",
    "    rng = np.random.RandomState(RANDOM_STATE + 2000 + fold)\n",
    "\n",
    "    # 1) Predicciones en TODO el test\n",
    "    with torch.no_grad():\n",
    "        logits_chunks = []\n",
    "        for i in range(0, len(X_te_std), 64):\n",
    "            xb = torch.tensor(X_te_std[i:i+64],\n",
    "                              dtype=torch.float32,\n",
    "                              device=DEVICE)\n",
    "            logits_chunks.append(model_to_probe(xb).cpu().numpy())\n",
    "        logits = np.concatenate(logits_chunks, axis=0)\n",
    "    preds = logits.argmax(axis=1)\n",
    "\n",
    "    class_to_idx = {}\n",
    "    for c in range(4):\n",
    "        # correctos de esa clase\n",
    "        mask_corr = (y_te == c) & (preds == c)\n",
    "        idxs = np.where(mask_corr)[0]\n",
    "        if len(idxs) == 0:\n",
    "            # fallback: cualquier ejemplo de esa clase\n",
    "            idxs = np.where(y_te == c)[0]\n",
    "            if len(idxs) == 0:\n",
    "                continue\n",
    "        class_to_idx[c] = int(rng.choice(idxs, size=1)[0])\n",
    "\n",
    "    if not class_to_idx:\n",
    "        print(\"[WARN] No hay ejemplos disponibles para ninguna clase; no se genera grid 4c.\")\n",
    "        return\n",
    "\n",
    "    # =======================\n",
    "    # (A) EEG + Grad-CAM++ (4 filas)\n",
    "    # =======================\n",
    "    n_rows = len(class_to_idx)\n",
    "    fig, axes = plt.subplots(\n",
    "        n_rows, 1,\n",
    "        figsize=(10, 2.8 * n_rows),\n",
    "        sharex=True\n",
    "    )\n",
    "    if n_rows == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    cams_per_class = {}\n",
    "\n",
    "    for row, c in enumerate(sorted(class_to_idx.keys())):\n",
    "        ax = axes[row]\n",
    "        idx = class_to_idx[c]\n",
    "\n",
    "        x_epoch = X_te_std[idx]  # (C, T)\n",
    "        xb = torch.tensor(x_epoch[None, ...],\n",
    "                          dtype=torch.float32,\n",
    "                          device=DEVICE)\n",
    "\n",
    "        # Grad-CAM++ targeteado a la clase c\n",
    "        cam_vec = gradcampp_1d(model_to_probe, xb, target_class=c)\n",
    "        cams_per_class[c] = cam_vec\n",
    "\n",
    "        title = f\"Correct example — class: {CLASS_NAMES[c].replace('_', ' ')}\"\n",
    "        _plot_overlay_one(\n",
    "            ax,\n",
    "            x_epoch,\n",
    "            cam_vec,\n",
    "            sfreq=sfreq_eff,\n",
    "            tmin=TMIN,\n",
    "            title=title,\n",
    "            channel_mode=channel_mode,\n",
    "            cls_idx=c\n",
    "        )\n",
    "\n",
    "    axes[-1].set_xlabel(\"Time (s)\")\n",
    "    plt.suptitle(\n",
    "        f\"EEG + Grad-CAM++ (1 ejemplo por clase) — Fold {fold}  [{run_tag}]\",\n",
    "        y=1.02,\n",
    "        fontsize=12\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    out_png = fold_dir / f\"eeg_cam_grid4_perclass_fold{fold}_{run_tag}.png\"\n",
    "    plt.savefig(out_png, dpi=140, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"↳ Grid 4 clases (EEG+CAM) guardado: {out_png}\")\n",
    "\n",
    "    # =======================\n",
    "    # (B) CAM 1D para las 4 clases\n",
    "    # =======================\n",
    "    fig2, axes2 = plt.subplots(\n",
    "        n_rows, 1,\n",
    "        figsize=(6, 2.0 * n_rows),\n",
    "        sharex=True\n",
    "    )\n",
    "    if n_rows == 1:\n",
    "        axes2 = [axes2]\n",
    "\n",
    "    for ax2, c in zip(axes2, sorted(cams_per_class.keys())):\n",
    "        cam_vec = cams_per_class[c]\n",
    "        ax2.plot(cam_vec)\n",
    "        ax2.set_ylabel(\"CAM\")\n",
    "        ax2.set_ylim(0, 1.05)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.set_title(f\"Grad-CAM++ conv_t — class {CLASS_NAMES[c].replace('_', ' ')}\")\n",
    "\n",
    "    axes2[-1].set_xlabel(\"Time (conv feature index)\")\n",
    "    plt.tight_layout()\n",
    "    out_png2 = fold_dir / f\"gradcampp_perclass_fold{fold}_{run_tag}.png\"\n",
    "    plt.savefig(out_png2, dpi=140, bbox_inches=\"tight\")\n",
    "    plt.close(fig2)\n",
    "    print(f\"↳ Grad-CAM++ 1D por clase guardado: {out_png2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c888cfb",
   "metadata": {},
   "source": [
    "### Arquitectura del Modelo: CNN + Transformer para 4 Clases\n",
    "\n",
    "Implementación del modelo híbrido CNN-Transformer para clasificación de 4 clases de EEG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3417e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# MODELO (GroupNorm en conv) + atención expuesta\n",
    "# =========================\n",
    "def make_gn(num_channels, num_groups=8):\n",
    "    \"\"\"\n",
    "    Crea capa GroupNorm adaptativa que garantiza divisibilidad.\n",
    "    \"\"\"\n",
    "    g = min(num_groups, num_channels)\n",
    "    while num_channels % g != 0 and g > 1:\n",
    "        g -= 1\n",
    "    return nn.GroupNorm(g, num_channels)\n",
    "\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolución depthwise separable para reducción de parámetros.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, k, s=1, p=0, p_drop=0.2):\n",
    "        super().__init__()\n",
    "        self.dw = nn.Conv1d(in_ch, in_ch, kernel_size=k, stride=s,\n",
    "                            padding=p, groups=in_ch, bias=False)\n",
    "        self.pw = nn.Conv1d(in_ch, out_ch, kernel_size=1, bias=False)\n",
    "        self.norm = make_gn(out_ch)\n",
    "        self.act = nn.ELU()\n",
    "        self.dropout = nn.Dropout(p=p_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dw(x)\n",
    "        x = self.pw(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class _CustomEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Capa personalizada del encoder Transformer con atención multi-cabeza.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout, batch_first, norm_first):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(\n",
    "            d_model, nhead, dropout=dropout, batch_first=batch_first\n",
    "        )\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.activation = nn.GELU()\n",
    "        self.norm_first = norm_first\n",
    "\n",
    "    def forward(self, src):\n",
    "        sa_out, attn_weights = self.self_attn(\n",
    "            src, src, src, need_weights=True, average_attn_weights=False\n",
    "        )  # attn: [B, heads, T, T]\n",
    "        src = self.norm1(src + self.dropout1(sa_out))\n",
    "        ff = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = self.norm2(src + self.dropout2(ff))\n",
    "        return src, attn_weights\n",
    "\n",
    "class _CustomEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder Transformer compuesto por múltiples capas.\n",
    "    \"\"\"\n",
    "    def __init__(self, layer, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, src):\n",
    "        attn_list = []\n",
    "        out = src\n",
    "        for lyr in self.layers:\n",
    "            out, attn = lyr(out)\n",
    "            attn_list.append(attn)\n",
    "        return out, attn_list\n",
    "\n",
    "class EEGCNNTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo híbrido CNN-Transformer para clasificación de 4 clases de señales EEG.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_ch=8, n_cls=4, d_model=128, n_heads=4, n_layers=2,\n",
    "                 p_drop=0.2, p_drop_encoder=0.3, capture_attn: bool = True):\n",
    "        super().__init__()\n",
    "        self.capture_attn = capture_attn\n",
    "        self._last_attn = None\n",
    "        self.pos_encoding = None\n",
    "\n",
    "        # Stem convolucional inicial\n",
    "        self.conv_t = nn.Sequential(\n",
    "            nn.Conv1d(n_ch, 32, kernel_size=129, stride=2, padding=64, bias=False),\n",
    "            make_gn(32),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=p_drop),\n",
    "            DepthwiseSeparableConv(32, 64, k=31, s=2, p=15, p_drop=p_drop),\n",
    "            DepthwiseSeparableConv(64, 128, k=15, s=2, p=7,  p_drop=p_drop),\n",
    "        )\n",
    "        self.proj = nn.Conv1d(128, d_model, kernel_size=1, bias=False)\n",
    "        self.dropout = nn.Dropout(p=p_drop_encoder)\n",
    "\n",
    "        # Encoder Transformer\n",
    "        enc_layer = _CustomEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=2*d_model,\n",
    "            dropout=0.1, batch_first=True, norm_first=False\n",
    "        )\n",
    "        self.encoder = _CustomEncoder(enc_layer, num_layers=n_layers)\n",
    "\n",
    "        # Token CLS y head de clasificación\n",
    "        self.cls = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        nn.init.normal_(self.cls, std=0.02)\n",
    "        self.head = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, n_cls))\n",
    "\n",
    "    def _positional_encoding(self, L, d):\n",
    "        \"\"\"\n",
    "        Codificación posicional sinusoidal para secuencias.\n",
    "        \"\"\"\n",
    "        pos = torch.arange(0, L, dtype=torch.float32).unsqueeze(1)\n",
    "        i   = torch.arange(0, d, dtype=torch.float32).unsqueeze(0)\n",
    "        angle = pos / torch.pow(10000, (2 * (i//2)) / d)\n",
    "        pe = torch.zeros(L, d, dtype=torch.float32)\n",
    "        pe[:, 0::2] = torch.sin(angle[:, 0::2])\n",
    "        pe[:, 1::2] = torch.cos(angle[:, 1::2])\n",
    "        return pe\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        \"\"\"\n",
    "        Devuelve el embedding CLS antes de la cabeza, y captura las atenciones.\n",
    "        x: (B, C, T)\n",
    "        \"\"\"\n",
    "        z = self.conv_t(x)           # (B, 128, T')\n",
    "        z = self.proj(z)             # (B, d_model, T')\n",
    "        z = self.dropout(z)\n",
    "        z = z.transpose(1, 2)        # (B, T', d_model)\n",
    "        B, L, D = z.shape\n",
    "        if (self.pos_encoding is None) or \\\n",
    "           (self.pos_encoding.shape[0] != L) or \\\n",
    "           (self.pos_encoding.shape[1] != D):\n",
    "            self.pos_encoding = self._positional_encoding(L, D).to(z.device)\n",
    "        z = z + self.pos_encoding[None, :, :]\n",
    "        cls_tok = self.cls.expand(B, -1, -1)\n",
    "        z = torch.cat([cls_tok, z], dim=1)  # (B, 1+L, D)\n",
    "\n",
    "        z, attn_list = self.encoder(z)\n",
    "        if self.capture_attn:\n",
    "            self._last_attn = attn_list\n",
    "\n",
    "        cls = z[:, 0, :]             # (B, d_model)\n",
    "        return cls\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward completo del modelo.\n",
    "        \"\"\"\n",
    "        cls = self.forward_features(x)\n",
    "        return self.head(cls)\n",
    "\n",
    "    def get_last_attention_maps(self):\n",
    "        \"\"\"\n",
    "        Retorna mapas de atención de la última pasada forward.\n",
    "        \"\"\"\n",
    "        return self._last_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bf3895",
   "metadata": {},
   "source": [
    "### Funciones de Pérdida y Aumentación de Datos\n",
    "\n",
    "Implementación de Focal Loss y técnicas de aumentación de datos para EEG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7776600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# FOCAL LOSS\n",
    "# =========================\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss para manejar desbalance de clases en 4 clases.\n",
    "    Reduce la contribución de ejemplos fáciles y enfoca en los difíciles.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha: torch.Tensor, gamma: float = 1.5, reduction: str = 'mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha / alpha.sum()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        logp = nn.functional.log_softmax(logits, dim=-1)      # (B,C)\n",
    "        p = logp.exp()\n",
    "        idx = torch.arange(target.shape[0], device=logits.device)\n",
    "        pt = p[idx, target]\n",
    "        logpt = logp[idx, target]\n",
    "        at = self.alpha[target]\n",
    "        loss = - at * ((1 - pt) ** self.gamma) * logpt\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        if self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        return loss\n",
    "\n",
    "# =========================\n",
    "# AUGMENTS\n",
    "# =========================\n",
    "def augment_batch(\n",
    "    xb,\n",
    "    p_jitter=0.35, p_noise=0.35, p_chdrop=0.15,\n",
    "    max_jitter_frac=0.03, noise_std=0.03, max_chdrop=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Aplica aumentación de datos a un batch de señales EEG.\n",
    "    Técnicas: jitter temporal, ruido gaussiano, drop de canales.\n",
    "    \"\"\"\n",
    "    B, C, T = xb.shape\n",
    "    if np.random.rand() < p_jitter:\n",
    "        max_shift = int(max(1, T*max_jitter_frac))\n",
    "        shifts = torch.randint(low=-max_shift, high=max_shift+1,\n",
    "                               size=(B,), device=xb.device)\n",
    "        for i in range(B):\n",
    "            xb[i] = torch.roll(xb[i], shifts=int(shifts[i].item()), dims=-1)\n",
    "    if np.random.rand() < p_noise:\n",
    "        xb = xb + noise_std*torch.randn_like(xb)\n",
    "    if np.random.rand() < p_chdrop and max_chdrop > 0:\n",
    "        k = min(max_chdrop, C)\n",
    "        for i in range(B):\n",
    "            idx = torch.randperm(C, device=xb.device)[:k]\n",
    "            xb[i, idx, :] = 0.0\n",
    "    return xb\n",
    "\n",
    "# Versión suave para FT\n",
    "def augment_batch_ft(xb):\n",
    "    \"\"\"\n",
    "    Aplica aumentación específica para fine-tuning.\n",
    "    \"\"\"\n",
    "    return augment_batch(xb, **FT_AUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d70989",
   "metadata": {},
   "source": [
    "### EMA y Técnicas de Inferencia\n",
    "\n",
    "Implementación de Exponential Moving Average y técnicas de inferencia robusta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3811eddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# EMA de pesos (solo global)\n",
    "# =========================\n",
    "class ModelEMA:\n",
    "    \"\"\"\n",
    "    Exponential Moving Average para estabilizar el entrenamiento.\n",
    "    Mantiene una versión suavizada de los pesos del modelo.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: nn.Module, decay: float = 0.9995, device=None):\n",
    "        self.ema = self._clone(model).to(\n",
    "            device if device is not None else next(model.parameters()).device\n",
    "        )\n",
    "        self.decay = decay\n",
    "        self._updates = 0\n",
    "        self.update(model, force=True)\n",
    "\n",
    "    def _clone(self, model):\n",
    "        ema = type(model)(**{})  # usa los defaults del constructor\n",
    "        ema.load_state_dict(model.state_dict())\n",
    "        for p in ema.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        return ema\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model: nn.Module, force: bool = False):\n",
    "        d = self.decay\n",
    "        if self._updates < 1000:\n",
    "            d = (self._updates / 1000.0) * self.decay\n",
    "        msd = model.state_dict()\n",
    "        esd = self.ema.state_dict()\n",
    "        for k in esd.keys():\n",
    "            if esd[k].dtype.is_floating_point:\n",
    "                esd[k].mul_(d).add_(msd[k].detach(), alpha=1.0 - d)\n",
    "            else:\n",
    "                esd[k] = msd[k]\n",
    "        self._updates += 1\n",
    "\n",
    "# =========================\n",
    "# INFERENCIA TTA / SUBWINDOW\n",
    "# =========================\n",
    "def subwindow_logits(model, X, sfreq, sw_len, sw_stride, device):\n",
    "    \"\"\"\n",
    "    Inferencia usando subventanas para capturar información temporal.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    wl = int(round(sw_len * sfreq))\n",
    "    st = int(round(sw_stride * sfreq))\n",
    "    wl = max(1, min(wl, X.shape[-1]))\n",
    "    st = max(1, st)\n",
    "    out = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(X.shape[0]):\n",
    "            x = X[i]\n",
    "            acc = []\n",
    "            for s in range(0, max(1, X.shape[-1]-wl+1), st):\n",
    "                seg = x[:, s:s+wl]\n",
    "                if seg.shape[-1] < wl:\n",
    "                    pad = wl - seg.shape[-1]\n",
    "                    seg = np.pad(seg, ((0,0),(0,pad)), mode='edge')\n",
    "                xb = torch.tensor(seg[None, ...], dtype=torch.float32, device=device)\n",
    "                logit = model(xb).detach().cpu().numpy()[0]\n",
    "                acc.append(logit)\n",
    "            acc = np.mean(np.stack(acc, axis=0), axis=0) if len(acc) \\\n",
    "                  else np.zeros(4, dtype=np.float32)\n",
    "            out.append(acc)\n",
    "    return np.stack(out, axis=0)\n",
    "\n",
    "def time_shift_tta_logits(model, X, sfreq, shifts_s, device):\n",
    "    \"\"\"\n",
    "    Test Time Augmentation usando desplazamientos temporales.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    T = X.shape[-1]\n",
    "    out = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(X.shape[0]):\n",
    "            x0 = X[i]\n",
    "            acc = []\n",
    "            for sh in shifts_s:\n",
    "                shift = int(round(sh * sfreq))\n",
    "                if shift == 0:\n",
    "                    x = x0\n",
    "                elif shift > 0:\n",
    "                    x = np.pad(x0[:, shift:], ((0,0),(0,shift)), mode='edge')[:, :T]\n",
    "                else:\n",
    "                    shift = -shift\n",
    "                    x = np.pad(x0[:, :-shift], ((0,0),(shift,0)), mode='edge')[:, :T]\n",
    "                xb = torch.tensor(x[None, ...], dtype=torch.float32, device=device)\n",
    "                logit = model(xb).detach().cpu().numpy()[0]\n",
    "                acc.append(logit)\n",
    "            out.append(np.mean(np.stack(acc, axis=0), axis=0))\n",
    "    return np.stack(out, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d7261",
   "metadata": {},
   "source": [
    "### Herramientas de Visualización y Métricas\n",
    "\n",
    "Funciones para calcular métricas y visualizar resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2fca371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Helper: matriz de confusión sin colorbar, con texto\n",
    "# =========================\n",
    "def plot_confusion_with_text(cm, class_names, title, out_path, cmap='Blues'):\n",
    "    \"\"\"\n",
    "    Dibuja una matriz de confusión sin colorbar y con el conteo\n",
    "    en cada celda.\n",
    "    MODIFICACIÓN: Acepta Path para out_path.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(4.8, 4.2))\n",
    "\n",
    "    im = ax.imshow(cm, cmap=cmap)\n",
    "\n",
    "    # Ejes y labels\n",
    "    tick_labels = [c.replace('_', '\\n') for c in class_names]\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True label\")\n",
    "    ax.set_xticks(range(len(class_names)))\n",
    "    ax.set_yticks(range(len(class_names)))\n",
    "    ax.set_xticklabels(tick_labels)\n",
    "    ax.set_yticklabels(tick_labels)\n",
    "\n",
    "    # Números dentro de cada celda\n",
    "    max_val = cm.max() if cm.size else 1\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            val = cm[i, j]\n",
    "            color = 'white' if val > max_val * 0.5 else 'black'\n",
    "            ax.text(j, i, str(val),\n",
    "                    ha='center', va='center',\n",
    "                    color=color, fontsize=11, fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(out_path), dpi=140)\n",
    "    plt.close(fig)\n",
    "\n",
    "# =========================\n",
    "# Utilidades splits estratificados por sujeto\n",
    "# =========================\n",
    "def build_subject_label_map(subject_ids):\n",
    "    \"\"\"\n",
    "    Construye mapa de etiquetas dominantes por sujeto para estratificación.\n",
    "    \"\"\"\n",
    "    y_dom_list = []\n",
    "    for sid in subject_ids:\n",
    "        Xs, ys, _ = load_subject_epochs(\n",
    "            sid, RESAMPLE_HZ, DO_NOTCH, DO_BANDPASS, DO_CAR, BP_LO, BP_HI\n",
    "        )\n",
    "        if len(ys) == 0:\n",
    "            y_dom_list.append(-1)\n",
    "            continue\n",
    "        binc = np.bincount(ys, minlength=4)\n",
    "        y_dom = int(np.argmax(binc))\n",
    "        y_dom_list.append(y_dom)\n",
    "    return np.array(y_dom_list, dtype=int)\n",
    "\n",
    "# =========================\n",
    "# MÉTRICAS COMPLETAS\n",
    "# =========================\n",
    "def compute_metrics_multiclass(y_true, y_pred, n_classes=4):\n",
    "    \"\"\"\n",
    "    Devuelve Acc, F1 macro, F1 weighted, Prec macro, Rec macro,\n",
    "    Spec macro y Sens (macro recall), además de la matriz de confusión.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "    prec_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    rec_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(n_classes)))\n",
    "    spec_per_class = []\n",
    "    for c in range(n_classes):\n",
    "        TP = cm[c, c]\n",
    "        FP = cm[:, c].sum() - TP\n",
    "        FN = cm[c, :].sum() - TP\n",
    "        TN = cm.sum() - (TP + FP + FN)\n",
    "        spec = TN / (TN + FP + 1e-12)\n",
    "        spec_per_class.append(spec)\n",
    "    spec_macro = float(np.mean(spec_per_class))\n",
    "\n",
    "    metrics = dict(\n",
    "        acc=acc,\n",
    "        f1_macro=f1_macro,\n",
    "        f1_weighted=f1_weighted,\n",
    "        prec_macro=prec_macro,\n",
    "        rec_macro=rec_macro,\n",
    "        spec_macro=spec_macro,\n",
    "        sens_macro=rec_macro,  # sensibilidad = recall macro\n",
    "        cm=cm,\n",
    "    )\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56f6b10",
   "metadata": {},
   "source": [
    "### Herramientas de Interpretabilidad: Grad-CAM++\n",
    "\n",
    "Funciones para analizar y visualizar el comportamiento del modelo mediante Grad-CAM++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c020d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Interpretabilidad: Grad-CAM++ 1D sobre conv_t\n",
    "# =========================\n",
    "def gradcampp_1d(model, x, target_class=None):\n",
    "    \"\"\"\n",
    "    Grad-CAM++ 1D sobre la capa conv_t.\n",
    "    x: tensor (1, C, T)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    feats = {}\n",
    "    grads = {}\n",
    "\n",
    "    def f_hook(m, inp, out):\n",
    "        feats['f'] = out.detach()\n",
    "\n",
    "    def b_hook(m, gin, gout):\n",
    "        grads['g'] = gout[0].detach()\n",
    "\n",
    "    handle_f = model.conv_t.register_forward_hook(f_hook)\n",
    "    handle_b = model.conv_t.register_full_backward_hook(b_hook)\n",
    "\n",
    "    logits = model(x)\n",
    "    if target_class is None:\n",
    "        cls = int(logits.argmax(1).item())\n",
    "    else:\n",
    "        cls = int(target_class)\n",
    "\n",
    "    score = logits[:, cls].sum()\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    score.backward(retain_graph=True)\n",
    "\n",
    "    F = feats['f'][0]  # (C', T')\n",
    "    G = grads['g'][0]  # (C', T')\n",
    "    eps = 1e-8\n",
    "    alpha_num = (G**2)\n",
    "    alpha_den = 2*(G**2) + (F*G**3).sum(dim=-1, keepdims=True) + eps\n",
    "    alpha = alpha_num / alpha_den\n",
    "    weights = (alpha.clamp(min=0) * G.clamp(min=0)).sum(dim=-1)  # (C',)\n",
    "\n",
    "    cam = (weights[:, None] * F).sum(dim=0)   # (T',)\n",
    "    cam = torch.relu(cam)\n",
    "    cam = (cam - cam.min()) / (cam.max() - cam.min() + eps)\n",
    "\n",
    "    handle_f.remove()\n",
    "    handle_b.remove()\n",
    "    return cam.cpu().numpy()\n",
    "\n",
    "def _upsample_cam_to_signal(cam_vec, T):\n",
    "    \"\"\"\n",
    "    Interpola el vector CAM a la longitud temporal original.\n",
    "    \"\"\"\n",
    "    x_src = np.linspace(0.0, 1.0, num=len(cam_vec))\n",
    "    x_tgt = np.linspace(0.0, 1.0, num=T)\n",
    "    return np.interp(x_tgt, x_src, cam_vec)\n",
    "\n",
    "def _plot_overlay_one(ax, x_epoch, cam_vec, sfreq, tmin, title,\n",
    "                      channel_mode=\"mean\", cls_idx=None):\n",
    "    \"\"\"\n",
    "    Overlay de EEG + Grad-CAM++ 1D.\n",
    "    cls_idx: 0/1/2/3 para colorear según clase.\n",
    "    \"\"\"\n",
    "    C, T = x_epoch.shape\n",
    "    cam_T = _upsample_cam_to_signal(cam_vec, T)\n",
    "    cam_T = (cam_T - cam_T.min()) / (cam_T.max() - cam_T.min() + 1e-8)\n",
    "\n",
    "    if channel_mode == \"mean\":\n",
    "        sig = x_epoch.mean(axis=0)\n",
    "        sig_label = \"mean(8ch)\"\n",
    "    elif isinstance(channel_mode, int):\n",
    "        sig = x_epoch[channel_mode]; sig_label = f\"ch#{channel_mode}\"\n",
    "    else:\n",
    "        if channel_mode in EXPECTED_8:\n",
    "            sig = x_epoch[EXPECTED_8.index(channel_mode)]\n",
    "            sig_label = channel_mode\n",
    "        else:\n",
    "            sig = x_epoch.mean(axis=0); sig_label = \"mean(8ch)\"\n",
    "\n",
    "    t = tmin + np.arange(T) / float(sfreq)\n",
    "    thr = np.quantile(cam_T, 0.80)\n",
    "    mask = cam_T >= thr\n",
    "\n",
    "    # Colores por clase (4 clases)\n",
    "    color_map = {\n",
    "        0: \"green\",    # left\n",
    "        1: \"blue\",     # right\n",
    "        2: \"orange\",   # both_fists\n",
    "        3: \"purple\",   # both_feet\n",
    "    }\n",
    "    sig_color = color_map.get(cls_idx, \"black\")\n",
    "\n",
    "    ax.plot(t, sig, label=f\"EEG ({sig_label})\", color=sig_color)\n",
    "\n",
    "    ax.fill_between(\n",
    "        t,\n",
    "        sig.min(),\n",
    "        sig.max(),\n",
    "        where=mask,\n",
    "        alpha=0.25,\n",
    "        color=\"red\",\n",
    "        label=\"Alta importancia (Grad-CAM++)\"\n",
    "    )\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(\"EEG (z)\")\n",
    "    ax.grid(True, alpha=0.25)\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(t, cam_T, linestyle=\"--\", alpha=0.6, label=\"CAM (norm.)\")\n",
    "    ax2.set_ylim(0, 1.05); ax2.set_ylabel(\"CAM\")\n",
    "    h1,l1 = ax.get_legend_handles_labels(); h2,l2 = ax2.get_legend_handles_labels()\n",
    "    ax.legend(h1+h2, l1+l2, loc=\"upper left\")\n",
    "\n",
    "def save_cam_grid_for_5_subjects(X_te_std, y_te, sub_te, model_to_probe,\n",
    "                                 sfreq_eff, fold, run_tag,\n",
    "                                 n_subjects=5, channel_mode=\"mean\",\n",
    "                                 output_dir: Path = OUTPUT_DIR):\n",
    "    \"\"\"\n",
    "    Genera un grid de Grad-CAM++ en TEST usando SOLO ejemplos correctamente clasificados.\n",
    "    MODIFICACIÓN: Guarda en directorio de 4 clases.\n",
    "    \"\"\"\n",
    "    # Crear directorio del fold\n",
    "    fold_dir = output_dir / f\"fold{fold}\"\n",
    "    fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    model_to_probe.eval()\n",
    "    rng = np.random.RandomState(RANDOM_STATE + 1000 + fold)\n",
    "\n",
    "    # 1) Predicciones en TODO el test\n",
    "    with torch.no_grad():\n",
    "        logits_chunks = []\n",
    "        for i in range(0, len(X_te_std), 64):\n",
    "            xb = torch.tensor(X_te_std[i:i+64], dtype=torch.float32, device=DEVICE)\n",
    "            logits_chunks.append(model_to_probe(xb).cpu().numpy())\n",
    "        logits = np.concatenate(logits_chunks, axis=0)\n",
    "    preds = logits.argmax(axis=1)\n",
    "    correct_mask = (preds == y_te)\n",
    "\n",
    "    if correct_mask.sum() == 0:\n",
    "        print(\"[WARN] No hay ejemplos correctamente clasificados en TEST; no se genera grid.\")\n",
    "        return\n",
    "\n",
    "    X_corr   = X_te_std[correct_mask]\n",
    "    y_corr   = y_te[correct_mask]\n",
    "    sub_corr = sub_te[correct_mask]\n",
    "\n",
    "    uniq_subjects = np.unique(sub_corr)\n",
    "    if len(uniq_subjects) == 0:\n",
    "        print(\"[WARN] No hay sujetos con aciertos; no se genera grid.\")\n",
    "        return\n",
    "\n",
    "    sel_subjects = rng.choice(uniq_subjects,\n",
    "                              size=min(n_subjects, len(uniq_subjects)),\n",
    "                              replace=False)\n",
    "    print(f\"[INFO] Grid con {len(sel_subjects)} sujetos (solo aciertos).\")\n",
    "\n",
    "    fig, axes = plt.subplots(len(sel_subjects), 1,\n",
    "                             figsize=(10, 2.8*len(sel_subjects)), sharex=True)\n",
    "    if len(sel_subjects) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, s in zip(axes, sel_subjects):\n",
    "        idxs = np.where(sub_corr == s)[0]\n",
    "        if len(idxs) == 0:\n",
    "            ax.set_visible(False)\n",
    "            continue\n",
    "\n",
    "        i_local = int(rng.choice(idxs, size=1)[0])\n",
    "\n",
    "        x_epoch = X_corr[i_local]  # (C,T)\n",
    "        true_cls = int(y_corr[i_local])\n",
    "\n",
    "        xb = torch.tensor(x_epoch[None, ...], dtype=torch.float32, device=DEVICE)\n",
    "        with torch.no_grad():\n",
    "            pred_cls = int(model_to_probe(xb).argmax(1).item())\n",
    "\n",
    "        if pred_cls != true_cls:\n",
    "            continue\n",
    "\n",
    "        cam_vec = gradcampp_1d(model_to_probe, xb, target_class=pred_cls)\n",
    "        title = f\"S{int(s):03d}  |  correct ({CLASS_NAMES[true_cls]})\"\n",
    "        _plot_overlay_one(\n",
    "            ax,\n",
    "            x_epoch,\n",
    "            cam_vec,\n",
    "            sfreq=sfreq_eff,\n",
    "            tmin=TMIN,\n",
    "            title=title,\n",
    "            channel_mode=channel_mode,\n",
    "            cls_idx=true_cls\n",
    "        )\n",
    "\n",
    "    axes[-1].set_xlabel(\"Time (s)\")\n",
    "    plt.suptitle(f\"EEG + Grad-CAM++ (correct) — Fold {fold}  [{run_tag}]\",\n",
    "                 y=1.02, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    out_png = fold_dir / f\"eeg_cam_grid5_correct_fold{fold}_{run_tag}.png\"\n",
    "    plt.savefig(out_png, dpi=140, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"↳ Grid (solo CORRECTOS) guardado: {out_png}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64af57dc",
   "metadata": {},
   "source": [
    "### Función Principal de Entrenamiento para 4 Clases\n",
    "\n",
    "Función que entrena un fold completo del modelo para clasificación de 4 clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "373e3290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# TRAIN/EVAL GLOBAL POR FOLD\n",
    "# =========================\n",
    "def train_one_fold(fold: int, device):\n",
    "    \"\"\"\n",
    "    Entrena y evalúa el modelo para un fold específico (4 clases).\n",
    "    \n",
    "    Args:\n",
    "        fold: Número del fold (1-5)\n",
    "        device: Dispositivo para entrenamiento (CPU/GPU)\n",
    "    \n",
    "    Returns:\n",
    "        Tupla con métricas y resultados del fold\n",
    "    \"\"\"\n",
    "    def load_fold_subjects_local(folds_json: Path, fold: int):\n",
    "        return load_fold_subjects(folds_json, fold)\n",
    "\n",
    "    # Crear directorio de salida para este fold\n",
    "    fold_dir = OUTPUT_DIR / f\"fold{fold}\"\n",
    "    fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    train_sub, test_sub = load_fold_subjects_local(FOLDS_JSON, fold)\n",
    "    train_sub = [s for s in train_sub if subject_id_to_int(s) not in EXCLUDE_SUBJECTS]\n",
    "    test_sub  = [s for s in test_sub  if subject_id_to_int(s) not in EXCLUDE_SUBJECTS]\n",
    "\n",
    "    rng = np.random.RandomState(RANDOM_STATE + fold)\n",
    "    tr_subjects = sorted(train_sub)\n",
    "\n",
    "    if VAL_STRAT_SUBJECT and len(tr_subjects) > 1:\n",
    "        y_dom = build_subject_label_map(tr_subjects)\n",
    "        if np.any(y_dom < 0):\n",
    "            mask = y_dom >= 0\n",
    "            moda = int(np.bincount(y_dom[mask]).argmax()) if mask.sum() > 0 else 0\n",
    "            y_dom[~mask] = moda\n",
    "        n_val_subj = max(1, int(round(len(tr_subjects) * VAL_SUBJECT_FRAC)))\n",
    "        sss = StratifiedShuffleSplit(\n",
    "            n_splits=1, test_size=n_val_subj,\n",
    "            random_state=RANDOM_STATE + fold\n",
    "        )\n",
    "        idx = np.arange(len(tr_subjects))\n",
    "        _, val_idx = next(sss.split(idx, y_dom))\n",
    "        val_subjects = sorted([tr_subjects[i] for i in val_idx])\n",
    "        train_subjects = [s for s in tr_subjects if s not in val_subjects]\n",
    "    else:\n",
    "        tr_subjects_shuf = tr_subjects.copy()\n",
    "        rng.shuffle(tr_subjects_shuf)\n",
    "        n_val_subj = max(1, int(round(len(tr_subjects_shuf) * VAL_SUBJECT_FRAC)))\n",
    "        val_subjects = sorted(tr_subjects_shuf[:n_val_subj])\n",
    "        train_subjects = sorted(tr_subjects_shuf[n_val_subj:])\n",
    "\n",
    "    # Carga TRAIN/VAL/TEST\n",
    "    X_tr_list, y_tr_list, sub_tr_list = [], [], []\n",
    "    X_val_list, y_val_list, sub_val_list = [], [], []\n",
    "    X_te_list, y_te_list, sub_te_list = [], [], []\n",
    "    sfreq = None\n",
    "\n",
    "    for sid in tqdm(train_subjects, desc=f\"Cargando train fold{fold}\"):\n",
    "        Xs, ys, sf = load_subject_epochs(\n",
    "            sid, RESAMPLE_HZ, DO_NOTCH, DO_BANDPASS, DO_CAR, BP_LO, BP_HI\n",
    "        )\n",
    "        if len(ys) == 0:\n",
    "            continue\n",
    "        X_tr_list.append(Xs)\n",
    "        y_tr_list.append(ys)\n",
    "        sub_tr_list.append(np.full_like(ys, fill_value=subject_id_to_int(sid)))\n",
    "        sfreq = sf if sfreq is None else sfreq\n",
    "\n",
    "    for sid in tqdm(val_subjects, desc=f\"Cargando val fold{fold}\"):\n",
    "        Xs, ys, sf = load_subject_epochs(\n",
    "            sid, RESAMPLE_HZ, DO_NOTCH, DO_BANDPASS, DO_CAR, BP_LO, BP_HI\n",
    "        )\n",
    "        if len(ys) == 0:\n",
    "            continue\n",
    "        X_val_list.append(Xs)\n",
    "        y_val_list.append(ys)\n",
    "        sub_val_list.append(np.full_like(ys, fill_value=subject_id_to_int(sid)))\n",
    "        sfreq = sf if sfreq is None else sfreq\n",
    "\n",
    "    for sid in tqdm(test_sub, desc=f\"Cargando test fold{fold}\"):\n",
    "        Xs, ys, sf = load_subject_epochs(\n",
    "            sid, RESAMPLE_HZ, DO_NOTCH, DO_BANDPASS, DO_CAR, BP_LO, BP_HI\n",
    "        )\n",
    "        if len(ys) == 0:\n",
    "            continue\n",
    "        X_te_list.append(Xs)\n",
    "        y_te_list.append(ys)\n",
    "        sub_te_list.append(np.full_like(ys, fill_value=subject_id_to_int(sid)))\n",
    "        sfreq = sf if sfreq is None else sfreq\n",
    "\n",
    "    # Concatenar\n",
    "    X_tr = np.concatenate(X_tr_list, axis=0)\n",
    "    y_tr = np.concatenate(y_tr_list, axis=0)\n",
    "    sub_tr = np.concatenate(sub_tr_list, axis=0)\n",
    "    X_val = np.concatenate(X_val_list, axis=0)\n",
    "    y_val = np.concatenate(y_val_list, axis=0)\n",
    "    sub_val = np.concatenate(sub_val_list, axis=0)\n",
    "    X_te = np.concatenate(X_te_list, axis=0)\n",
    "    y_te = np.concatenate(y_te_list, axis=0)\n",
    "    sub_te = np.concatenate(sub_te_list, axis=0)\n",
    "\n",
    "    print(f\"[Fold {fold}/5] Entrenando modelo global... \"\n",
    "          f\"(n_train={len(y_tr)} | n_val={len(y_val)} | n_test={len(y_te)})\")\n",
    "\n",
    "    # Normalización por canal (fit en TRAIN y aplicar a VAL/TEST)\n",
    "    if ZSCORE_PER_EPOCH:\n",
    "        X_tr_std, X_val_std, X_te_std = X_tr, X_val, X_te\n",
    "    else:\n",
    "        X_tr_std, X_val_std = standardize_per_channel(X_tr, X_val)\n",
    "        _,        X_te_std  = standardize_per_channel(X_tr, X_te)\n",
    "\n",
    "    # Datasets\n",
    "    tr_ds  = TensorDataset(torch.tensor(X_tr_std),\n",
    "                           torch.tensor(y_tr).long(),\n",
    "                           torch.tensor(sub_tr).long())\n",
    "    val_ds = TensorDataset(torch.tensor(X_val_std),\n",
    "                           torch.tensor(y_val).long(),\n",
    "                           torch.tensor(sub_val).long())\n",
    "    te_ds  = TensorDataset(torch.tensor(X_te_std),\n",
    "                           torch.tensor(y_te).long(),\n",
    "                           torch.tensor(sub_te).long())\n",
    "\n",
    "    # Weighted sampler templado\n",
    "    def make_weighted_sampler(dataset: TensorDataset):\n",
    "        _Xb, yb, sb = dataset.tensors\n",
    "        yb_np = yb.numpy()\n",
    "        sb_np = sb.numpy()\n",
    "        uniq_s, cnt_s = np.unique(sb_np, return_counts=True)\n",
    "        map_s = {s: c for s, c in zip(uniq_s, cnt_s)}\n",
    "        key = sb_np.astype(np.int64) * 10 + yb_np.astype(np.int64)\n",
    "        uniq_k, cnt_k = np.unique(key, return_counts=True)\n",
    "        map_k = {k: c for k, c in zip(uniq_k, cnt_k)}\n",
    "        a, b = 0.8, 1.0\n",
    "        w = []\n",
    "        for s, y in zip(sb_np, yb_np):\n",
    "            k = int(s)*10 + int(y)\n",
    "            ws = float(map_s[int(s)])\n",
    "            wk = float(map_k[k])\n",
    "            w.append((ws ** (-a)) * (wk ** (-b)))\n",
    "        w = np.array(w, dtype=np.float64)\n",
    "        w = w / (w.mean() + 1e-12)\n",
    "        sampler = WeightedRandomSampler(\n",
    "            weights=torch.tensor(w, dtype=torch.double),\n",
    "            num_samples=len(yb_np), replacement=True\n",
    "        )\n",
    "        return sampler\n",
    "\n",
    "    if USE_WEIGHTED_SAMPLER:\n",
    "        tr_sampler = make_weighted_sampler(tr_ds)\n",
    "        tr_ld  = DataLoader(tr_ds, batch_size=BATCH_SIZE,\n",
    "                            sampler=tr_sampler, drop_last=False,\n",
    "                            worker_init_fn=seed_worker)\n",
    "    else:\n",
    "        tr_ld  = DataLoader(tr_ds, batch_size=BATCH_SIZE,\n",
    "                            shuffle=True,  drop_last=False,\n",
    "                            worker_init_fn=seed_worker)\n",
    "\n",
    "    val_ld = DataLoader(val_ds, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, drop_last=False,\n",
    "                        worker_init_fn=seed_worker)\n",
    "    te_ld  = DataLoader(te_ds,  batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, drop_last=False,\n",
    "                        worker_init_fn=seed_worker)\n",
    "\n",
    "    # Modelo\n",
    "    model = EEGCNNTransformer(\n",
    "        n_ch=8, n_cls=4, d_model=D_MODEL, n_heads=N_HEADS,\n",
    "        n_layers=N_LAYERS, p_drop=P_DROP, p_drop_encoder=P_DROP_ENCODER,\n",
    "        capture_attn=True\n",
    "    ).to(device)\n",
    "\n",
    "    # Optimizador + Focal Loss\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=1e-2)\n",
    "    class_counts = np.bincount(y_tr, minlength=4).astype(np.float32)\n",
    "    inv = class_counts.sum() / (4.0 * np.maximum(class_counts, 1.0))\n",
    "    alpha = torch.tensor(inv, dtype=torch.float32, device=device)\n",
    "    alpha_mean = alpha.mean().item()\n",
    "    alpha[2] = 1.25 * alpha_mean  # both_fists\n",
    "    alpha[3] = 1.05 * alpha_mean  # both_feet\n",
    "    crit = FocalLoss(alpha=alpha, gamma=1.5, reduction='mean')\n",
    "\n",
    "    # LR scheduler Warmup+Cosine\n",
    "    from torch.optim.lr_scheduler import LambdaLR\n",
    "    total_epochs = EPOCHS\n",
    "    warmup_epochs = max(1, int(WARMUP_EPOCHS))\n",
    "    min_factor = 0.1\n",
    "\n",
    "    def lr_lambda(current_epoch):\n",
    "        if current_epoch < warmup_epochs:\n",
    "            return (current_epoch + 1) / warmup_epochs\n",
    "        progress = (current_epoch - warmup_epochs) / max(1, (total_epochs - warmup_epochs))\n",
    "        progress = min(1.0, max(0.0, progress))\n",
    "        return min_factor + 0.5 * (1.0 - min_factor) * (1.0 + np.cos(np.pi * progress))\n",
    "\n",
    "    scheduler = LambdaLR(opt, lr_lambda=lr_lambda)\n",
    "\n",
    "    # EMA (solo global)\n",
    "    if USE_EMA:\n",
    "        ema = ModelEMA(model, decay=EMA_DECAY, device=device)\n",
    "    else:\n",
    "        ema = None\n",
    "\n",
    "    # Entrenamiento global con early stopping por F1 macro\n",
    "    best_f1, best_state, wait = 0.0, None, 0\n",
    "    hist = {\"ep\": [], \"tr_loss\": [], \"val_loss\": [],\n",
    "            \"tr_acc\": [], \"val_acc\": [], \"val_f1m\": [], \"lr\": []}\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "    t_train0 = time.perf_counter()\n",
    "\n",
    "    def evaluate_on(loader, use_ema=True):\n",
    "        mdl = ema.ema if (ema is not None and use_ema) else model\n",
    "        mdl.eval()\n",
    "        preds, gts = [], []\n",
    "        total_loss, n_total = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _sb in loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                logits = mdl(xb)\n",
    "                loss = crit(logits, yb)\n",
    "                total_loss += loss.item() * len(yb)\n",
    "                n_total += len(yb)\n",
    "                p = logits.argmax(dim=1).cpu().numpy()\n",
    "                preds.append(p)\n",
    "                gts.append(yb.cpu().numpy())\n",
    "        preds = np.concatenate(preds)\n",
    "        gts = np.concatenate(gts)\n",
    "        acc = accuracy_score(gts, preds)\n",
    "        f1m = f1_score(gts, preds, average='macro')\n",
    "        val_loss = total_loss / max(1, n_total)\n",
    "        return val_loss, acc, f1m\n",
    "\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        tr_loss, n_seen, tr_correct = 0.0, 0, 0\n",
    "        for xb, yb, _sb in tr_ld:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            xb = augment_batch(xb)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            opt.step()\n",
    "            if ema is not None:\n",
    "                ema.update(model)\n",
    "\n",
    "            tr_loss += loss.item() * len(yb)\n",
    "            n_seen += len(yb)\n",
    "            tr_correct += (logits.argmax(1) == yb).sum().item()\n",
    "\n",
    "        tr_loss /= max(1, n_seen)\n",
    "        tr_acc = tr_correct / max(1, n_seen)\n",
    "\n",
    "        val_loss, acc_val, f1m = evaluate_on(val_ld, use_ema=True)\n",
    "\n",
    "        hist[\"ep\"].append(ep)\n",
    "        hist[\"tr_loss\"].append(tr_loss)\n",
    "        hist[\"val_loss\"].append(val_loss)\n",
    "        hist[\"tr_acc\"].append(tr_acc)\n",
    "        hist[\"val_acc\"].append(acc_val)\n",
    "        hist[\"val_f1m\"].append(f1m)\n",
    "        hist[\"lr\"].append(scheduler.get_last_lr()[0])\n",
    "\n",
    "        print(f\"  Época {ep:3d} | train_loss={tr_loss:.4f} | \"\n",
    "              f\"train_acc={tr_acc:.4f} | val_loss={val_loss:.4f} | \"\n",
    "              f\"val_acc={acc_val:.4f} | val_f1m={f1m:.4f} | \"\n",
    "              f\"LR={scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        improved = f1m > best_f1 + 1e-4\n",
    "        if improved:\n",
    "            best_f1 = f1m\n",
    "            ref_model = ema.ema if ema is not None else model\n",
    "            best_state = {k: v.detach().cpu() for k, v in ref_model.state_dict().items()}\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "\n",
    "        scheduler.step()\n",
    "        if wait >= PATIENCE:\n",
    "            print(f\"  Early stopping en época {ep} (mejor val_f1m={best_f1:.4f})\")\n",
    "            break\n",
    "\n",
    "    train_time = time.perf_counter() - t_train0\n",
    "    if torch.cuda.is_available():\n",
    "        train_mem = torch.cuda.max_memory_allocated(device) / (1024**2)\n",
    "    else:\n",
    "        train_mem = float('nan')\n",
    "\n",
    "    # Cargamos los mejores pesos tanto en ema.ema (si existe) como en model,\n",
    "    # y reactivamos gradients en model para usarlo en Grad-CAM++\n",
    "    if best_state is not None:\n",
    "        if ema is not None:\n",
    "            ema.ema.load_state_dict(best_state)\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad_(True)\n",
    "\n",
    "    # ---- Guardar curva (pérdidas) ----\n",
    "    fig = plt.figure(figsize=(8, 4.5))\n",
    "    ax1 = plt.gca()\n",
    "    ax1.plot(hist[\"ep\"], hist[\"tr_loss\"], label=\"train_loss\")\n",
    "    ax1.plot(hist[\"ep\"], hist[\"val_loss\"], label=\"val_loss\")\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_title(f\"Fold {fold} — Training Curve (4 classes)\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    out_png = fold_dir / f\"training_curve_fold{fold}_4c.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=140)\n",
    "    plt.close(fig)\n",
    "    print(f\"↳ Curva de entrenamiento guardada: {out_png}\")\n",
    "\n",
    "    # ---- Evaluación final en TEST (global) ----\n",
    "    eval_model = ema.ema if (ema is not None) else model\n",
    "    eval_model.eval()\n",
    "\n",
    "    sfreq_used = RESAMPLE_HZ\n",
    "    if sfreq_used is None:\n",
    "        sfreq_used = int(round(X_te_std.shape[-1] / (TMAX - TMIN)))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "    t_test0 = time.perf_counter()\n",
    "\n",
    "    if (not SW_ENABLE) or SW_MODE == 'none':\n",
    "        preds, gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _sb in te_ld:\n",
    "                xb = xb.to(device)\n",
    "                p = eval_model(xb).argmax(dim=1).cpu().numpy()\n",
    "                preds.append(p)\n",
    "                gts.append(yb.numpy())\n",
    "        preds = np.concatenate(preds)\n",
    "        gts = np.concatenate(gts)\n",
    "    elif SW_MODE in ('subwin', 'tta'):\n",
    "        logits_tta = None\n",
    "        logits_sw  = None\n",
    "        if SW_MODE == 'subwin':\n",
    "            logits_sw = subwindow_logits(eval_model, X_te_std, sfreq_used,\n",
    "                                         SW_LEN, SW_STRIDE, device)\n",
    "        elif SW_MODE == 'tta':\n",
    "            logits_tta = time_shift_tta_logits(eval_model, X_te_std,\n",
    "                                               sfreq_used, TTA_SHIFTS_S, device)\n",
    "\n",
    "        if COMBINE_TTA_AND_SUBWIN:\n",
    "            if logits_tta is None:\n",
    "                logits_tta = time_shift_tta_logits(eval_model, X_te_std,\n",
    "                                                   sfreq_used, TTA_SHIFTS_S, device)\n",
    "            if logits_sw is None:\n",
    "                logits_sw  = subwindow_logits(eval_model, X_te_std, sfreq_used,\n",
    "                                              SW_LEN, SW_STRIDE, device)\n",
    "            logits = 0.5 * logits_tta + 0.5 * logits_sw\n",
    "        else:\n",
    "            logits = logits_tta if logits_tta is not None else logits_sw\n",
    "\n",
    "        preds = logits.argmax(axis=1)\n",
    "        gts = y_te\n",
    "    else:\n",
    "        raise ValueError(f\"SW_MODE desconocido: {SW_MODE}\")\n",
    "\n",
    "    test_time = time.perf_counter() - t_test0\n",
    "    if torch.cuda.is_available():\n",
    "        test_mem = torch.cuda.max_memory_allocated(device) / (1024**2)\n",
    "    else:\n",
    "        test_mem = float('nan')\n",
    "\n",
    "    metrics_global = compute_metrics_multiclass(gts, preds, n_classes=4)\n",
    "    acc = metrics_global[\"acc\"]\n",
    "    f1m = metrics_global[\"f1_macro\"]\n",
    "\n",
    "    print(f\"[Fold {fold}/5] Global acc={acc:.4f} | f1_macro={f1m:.4f}\\n\")\n",
    "    print(classification_report(\n",
    "        gts, preds,\n",
    "        target_names=[c.replace('_', ' ') for c in CLASS_NAMES],\n",
    "        digits=4\n",
    "    ))\n",
    "    print(\"Confusion matrix (rows=true, cols=pred):\")\n",
    "    cm = metrics_global[\"cm\"]\n",
    "    print(cm)\n",
    "    print(\n",
    "        f\"precision_macro={metrics_global['prec_macro']:.4f} | \"\n",
    "        f\"recall_macro={metrics_global['rec_macro']:.4f} | \"\n",
    "        f\"specificity_macro={metrics_global['spec_macro']:.4f} | \"\n",
    "        f\"sensitivity_macro={metrics_global['sens_macro']:.4f} | \"\n",
    "        f\"f1_weighted={metrics_global['f1_weighted']:.4f}\"\n",
    "    )\n",
    "\n",
    "    # matriz de confusión global (imagen)\n",
    "    cm_path = fold_dir / f\"confusion_global_fold{fold}_4c.png\"\n",
    "    plot_confusion_with_text(\n",
    "        cm=cm,\n",
    "        class_names=CLASS_NAMES,\n",
    "        title=f\"Confusion — Fold {fold} Global (4 classes)\",\n",
    "        out_path=cm_path,\n",
    "        cmap='Blues'\n",
    "    )\n",
    "    print(f\"↳ Matriz de confusión global guardada: {cm_path}\")\n",
    "\n",
    "    # ===== Topomap de saliency (channel importance) =====\n",
    "    saliency_fold = None\n",
    "    try:\n",
    "        saliency_fold = save_channel_saliency_topomap(\n",
    "            model=eval_model,\n",
    "            X_te_std=X_te_std,\n",
    "            sfreq_used=sfreq_used,\n",
    "            fold=fold,\n",
    "            run_tag=MODEL_TAG,\n",
    "            output_dir=OUTPUT_DIR\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Topomap saliency no generado: {e}\")\n",
    "\n",
    "    # ===== Topomap de p-values 4 clases (TASK EFFECT) =====\n",
    "    try:\n",
    "        pvals_4c = compute_channel_pvalues_4c(X_te_std, y_te, n_classes=4)\n",
    "        pvals_log = -np.log10(pvals_4c + 1e-12)  # comparable a los de 2c\n",
    "\n",
    "        info_topo = make_mne_info_8ch(sfreq_used)\n",
    "        topo_p_png = fold_dir / f\"topomap_pvalues_4c_fold{fold}_{MODEL_TAG}.png\"\n",
    "        plot_topomap_from_values(\n",
    "            pvals_log,\n",
    "            info_topo,\n",
    "            title=f\"-log10(p) task effect (4 classes) — Fold {fold} [{MODEL_TAG}]\",\n",
    "            out_path=topo_p_png,\n",
    "            cmap=\"viridis\",\n",
    "            cbar_label=\"-log10(p)\"\n",
    "        )\n",
    "        print(f\"↳ Topomap p-values 4c guardado: {topo_p_png}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Topomap p-values 4c no generado: {e}\")\n",
    "\n",
    "    # ---- t-SNE 2D de embeddings CLS (GLOBAL) ----\n",
    "    print(\"↳ Calculando t-SNE 2D de CLS embeddings (global)...\")\n",
    "    eval_model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_te_tensor = torch.tensor(X_te_std, dtype=torch.float32, device=device)\n",
    "        cls_emb = eval_model.forward_features(X_te_tensor).cpu().numpy()\n",
    "\n",
    "    tsne = TSNE(\n",
    "        n_components=2,\n",
    "        random_state=RANDOM_STATE + fold,\n",
    "        perplexity=30,\n",
    "        init='random',\n",
    "        learning_rate='auto'\n",
    "    )\n",
    "    z = tsne.fit_transform(cls_emb)\n",
    "\n",
    "    fig = plt.figure(figsize=(5.5, 5.0))\n",
    "    for cls_idx, cls_name in enumerate(CLASS_NAMES):\n",
    "        mask = (gts == cls_idx)\n",
    "        plt.scatter(\n",
    "            z[mask, 0], z[mask, 1],\n",
    "            s=10, alpha=0.7, label=cls_name.replace('_', ' ')\n",
    "        )\n",
    "    plt.xlabel(\"t-SNE dimension 1\")\n",
    "    plt.ylabel(\"t-SNE dimension 2\")\n",
    "    plt.title(f\"t-SNE of CLS embeddings — Fold {fold} Global (4 classes)\")\n",
    "    plt.legend(markerscale=2, fontsize=8)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    tsne_path = fold_dir / f\"tsne_global_fold{fold}_4c.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(tsne_path, dpi=140)\n",
    "    plt.close(fig)\n",
    "    print(f\"↳ t-SNE global guardado: {tsne_path}\")\n",
    "\n",
    "    # =========================\n",
    "    # Interpretabilidad (attention + Grad-CAM++)\n",
    "    # =========================\n",
    "    try:\n",
    "        # Usamos eval_model (EMA) para atención y t-SNE,\n",
    "        # y model (con grad) para Grad-CAM++\n",
    "        base_model = eval_model   # para attention maps\n",
    "        cam_model  = model        # para Grad-CAM++\n",
    "\n",
    "        # Attention maps (primera capa, media en heads)\n",
    "        base_model.eval()\n",
    "        with torch.no_grad():\n",
    "            xb_val, yb_val, _ = next(iter(val_ld))\n",
    "        xb_val = xb_val[:8].to(device)\n",
    "        _ = base_model(xb_val)\n",
    "        attn_list = base_model.get_last_attention_maps()\n",
    "        if attn_list and len(attn_list) > 0:\n",
    "            # attn_list[0]: [B, heads, T, T]\n",
    "            A = attn_list[-1].mean(dim=1)[0].detach().cpu().numpy() # (T, T) del primer ejemplo\n",
    "            plt.figure(figsize=(5, 4))\n",
    "            plt.imshow(A, aspect='auto')\n",
    "            plt.title(f\"Attention (last layer, mean heads) — Fold {fold}\")\n",
    "            plt.colorbar()\n",
    "            plt.tight_layout()\n",
    "            attn_png = fold_dir / f\"attention_map_fold{fold}_4c.png\"\n",
    "            plt.savefig(attn_png, dpi=140)\n",
    "            plt.close()\n",
    "            print(f\"↳ Mapa de atención guardado: {attn_png}\")\n",
    "\n",
    "        # =========================\n",
    "        # Grad-CAM++: 4 clases\n",
    "        # =========================\n",
    "        sfreq_eff = sfreq_used\n",
    "        save_cam_grid_4classes(\n",
    "            X_te_std=X_te_std,\n",
    "            y_te=y_te,\n",
    "            model_to_probe=cam_model,\n",
    "            sfreq_eff=sfreq_eff,\n",
    "            fold=fold,\n",
    "            run_tag=MODEL_TAG,\n",
    "            channel_mode=\"mean\",\n",
    "            output_dir=OUTPUT_DIR\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Interpretabilidad no generada: {e}\")\n",
    "\n",
    "    # =========================\n",
    "    # FINE-TUNING PROGRESIVO POR SUJETO (4-fold CV interno)\n",
    "    # =========================\n",
    "    subjects = np.unique(sub_te)\n",
    "    subj_to_idx = {s: np.where(sub_te == s)[0] for s in subjects}\n",
    "\n",
    "    def make_ft_optimizer(model):\n",
    "        head_params = list(model.head.parameters())\n",
    "        backbone_params = [p for n, p in model.named_parameters()\n",
    "                           if not n.startswith('head.')]\n",
    "        return torch.optim.AdamW([\n",
    "            {'params': backbone_params, 'lr': FT_LR_BACKBONE},\n",
    "            {'params': head_params,     'lr': FT_LR_HEAD}\n",
    "        ], weight_decay=FT_WD)\n",
    "\n",
    "    def freeze_backbone(model, freeze=True):\n",
    "        for n, p in model.named_parameters():\n",
    "            if not n.startswith('head.'):\n",
    "                p.requires_grad_(not freeze)\n",
    "\n",
    "    def ft_make_criterion_from_counts(counts):\n",
    "        inv = counts.sum() / (4.0 * np.maximum(counts.astype(np.float32), 1.0))\n",
    "        a = torch.tensor(inv, dtype=torch.float32, device=device)\n",
    "        am = a.mean().item()\n",
    "        a[2] = FT_ALPHA_BOOST_FISTS * am\n",
    "        a[3] = FT_ALPHA_BOOST_FEET  * am\n",
    "        return FocalLoss(alpha=a, gamma=1.5, reduction='mean')\n",
    "\n",
    "    def evaluate_tensor(model_t, X, y):\n",
    "        model_t.eval()\n",
    "        with torch.no_grad():\n",
    "            xb = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "            p = model_t(xb).argmax(1).cpu().numpy()\n",
    "        return accuracy_score(y, p), f1_score(y, p, average='macro'), p\n",
    "\n",
    "    all_true, all_pred = [], []\n",
    "    base_state = (ema.ema if (ema is not None) else model).state_dict()\n",
    "\n",
    "    for s in subjects:\n",
    "        idx = subj_to_idx[s]\n",
    "        Xs, ys = X_te_std[idx], y_te[idx]\n",
    "\n",
    "        if len(np.unique(ys)) < 2 or len(ys) < 20:\n",
    "            eval_model_local = EEGCNNTransformer(\n",
    "                n_ch=8, n_cls=4, d_model=D_MODEL, n_heads=N_HEADS,\n",
    "                n_layers=N_LAYERS, p_drop=P_DROP, p_drop_encoder=P_DROP_ENCODER,\n",
    "                capture_attn=False\n",
    "            ).to(device)\n",
    "            eval_model_local.load_state_dict(base_state)\n",
    "            acc_s, f1_s, pred_s = evaluate_tensor(eval_model_local, Xs, ys)\n",
    "            all_true.append(ys)\n",
    "            all_pred.append(pred_s)\n",
    "            continue\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=FT_N_FOLDS, shuffle=True,\n",
    "                              random_state=RANDOM_STATE + fold + int(s))\n",
    "        for tr_idx, te_idx in skf.split(Xs, ys):\n",
    "            Xtr, ytr = Xs[tr_idx].copy(), ys[tr_idx].copy()\n",
    "            Xte_s, yte_s = Xs[te_idx].copy(), ys[te_idx].copy()\n",
    "\n",
    "            Xtr_std, Xte_std = standardize_per_channel(Xtr, Xte_s)\n",
    "\n",
    "            ft_model = EEGCNNTransformer(\n",
    "                n_ch=8, n_cls=4, d_model=D_MODEL, n_heads=N_HEADS,\n",
    "                n_layers=N_LAYERS, p_drop=P_DROP, p_drop_encoder=P_DROP_ENCODER,\n",
    "                capture_attn=False\n",
    "            ).to(device)\n",
    "            ft_model.load_state_dict(base_state)\n",
    "\n",
    "            opt_ft = make_ft_optimizer(ft_model)\n",
    "            alpha_counts = np.bincount(ytr, minlength=4)\n",
    "            ft_crit = ft_make_criterion_from_counts(alpha_counts)\n",
    "\n",
    "            ds_tr = TensorDataset(torch.tensor(Xtr_std),\n",
    "                                  torch.tensor(ytr).long())\n",
    "            ds_te = TensorDataset(torch.tensor(Xte_std),\n",
    "                                  torch.tensor(yte_s).long())\n",
    "            ld_tr = DataLoader(ds_tr, batch_size=FT_BATCH,\n",
    "                               shuffle=True,  drop_last=False,\n",
    "                               worker_init_fn=seed_worker)\n",
    "            ld_te = DataLoader(ds_te, batch_size=FT_BATCH,\n",
    "                               shuffle=False, drop_last=False,\n",
    "                               worker_init_fn=seed_worker)\n",
    "\n",
    "            # ETAPA 1: freeze backbone\n",
    "            freeze_backbone(ft_model, True)\n",
    "            best_f1_s, best_state_s, wait_s = -1, None, 0\n",
    "            for ep in range(1, FT_FREEZE_EPOCHS+1):\n",
    "                ft_model.train()\n",
    "                for xb, yb in ld_tr:\n",
    "                    xb = xb.to(device)\n",
    "                    yb = yb.to(device)\n",
    "                    xb = augment_batch_ft(xb)\n",
    "                    opt_ft.zero_grad()\n",
    "                    logits = ft_model(xb)\n",
    "                    loss = ft_crit(logits, yb)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(ft_model.parameters(), 1.0)\n",
    "                    opt_ft.step()\n",
    "\n",
    "                acc_s, f1_s = evaluate_tensor(ft_model, Xte_std, yte_s)[:2]\n",
    "                improved = f1_s > best_f1_s + 1e-4\n",
    "                if improved:\n",
    "                    best_f1_s = f1_s\n",
    "                    best_state_s = {k: v.detach().cpu()\n",
    "                                    for k, v in ft_model.state_dict().items()}\n",
    "                    wait_s = 0\n",
    "                else:\n",
    "                    wait_s += 1\n",
    "                if wait_s >= FT_PATIENCE:\n",
    "                    break\n",
    "\n",
    "            if best_state_s is not None:\n",
    "                ft_model.load_state_dict(best_state_s)\n",
    "\n",
    "            # ETAPA 2: unfreeze con early stopping\n",
    "            freeze_backbone(ft_model, False)\n",
    "            best_f1_s2, best_state_s2, wait_s2 = -1, None, 0\n",
    "            for ep in range(1, FT_UNFREEZE_EPOCHS+1):\n",
    "                ft_model.train()\n",
    "                for xb, yb in ld_tr:\n",
    "                    xb = xb.to(device)\n",
    "                    yb = yb.to(device)\n",
    "                    xb = augment_batch_ft(xb)\n",
    "                    opt_ft.zero_grad()\n",
    "                    logits = ft_model(xb)\n",
    "                    loss = ft_crit(logits, yb)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(ft_model.parameters(), 1.0)\n",
    "                    opt_ft.step()\n",
    "\n",
    "                acc_s, f1_s = evaluate_tensor(ft_model, Xte_std, yte_s)[:2]\n",
    "                improved = f1_s > best_f1_s2 + 1e-4\n",
    "                if improved:\n",
    "                    best_f1_s2 = f1_s\n",
    "                    best_state_s2 = {k: v.detach().cpu()\n",
    "                                     for k, v in ft_model.state_dict().items()}\n",
    "                    wait_s2 = 0\n",
    "                else:\n",
    "                    wait_s2 += 1\n",
    "                if wait_s2 >= FT_PATIENCE:\n",
    "                    break\n",
    "\n",
    "            if best_state_s2 is not None:\n",
    "                ft_model.load_state_dict(best_state_s2)\n",
    "\n",
    "            acc_s, f1_s, pred_s = evaluate_tensor(ft_model, Xte_std, yte_s)\n",
    "            all_true.append(yte_s)\n",
    "            all_pred.append(pred_s)\n",
    "\n",
    "    all_true = np.concatenate(all_true)\n",
    "    all_pred = np.concatenate(all_pred)\n",
    "    metrics_ft = compute_metrics_multiclass(all_true, all_pred, n_classes=4)\n",
    "    ft_acc = metrics_ft[\"acc\"]\n",
    "    ft_f1m = metrics_ft[\"f1_macro\"]\n",
    "\n",
    "    print(f\"  Fine-tuning PROGRESIVO (por sujeto, {FT_N_FOLDS}-fold CV) \"\n",
    "          f\"acc={ft_acc:.4f} | f1_macro={ft_f1m:.4f}\")\n",
    "    print(f\"  Δ(FT-Global) = {ft_acc - acc:+.4f}\")\n",
    "    cm_ft = metrics_ft[\"cm\"]\n",
    "    print(\"Confusion matrix FT (rows=true, cols=pred):\")\n",
    "    print(cm_ft)\n",
    "    print(\n",
    "        f\"precision_macro={metrics_ft['prec_macro']:.4f} | \"\n",
    "        f\"recall_macro={metrics_ft['rec_macro']:.4f} | \"\n",
    "        f\"specificity_macro={metrics_ft['spec_macro']:.4f} | \"\n",
    "        f\"sensitivity_macro={metrics_ft['sens_macro']:.4f} | \"\n",
    "        f\"f1_weighted={metrics_ft['f1_weighted']:.4f}\"\n",
    "    )\n",
    "\n",
    "    cm_ft_path = fold_dir / f\"confusion_ft_fold{fold}_4c.png\"\n",
    "    plot_confusion_with_text(\n",
    "        cm=cm_ft,\n",
    "        class_names=CLASS_NAMES,\n",
    "        title=f\"Confusion — Fold {fold} FT (4 classes)\",\n",
    "        out_path=cm_ft_path,\n",
    "        cmap='Greens'\n",
    "    )\n",
    "    print(f\"↳ Matriz de confusión FT guardada: {cm_ft_path}\")\n",
    "\n",
    "    # =========================\n",
    "    # Métricas de consumo (params + latencia)\n",
    "    # =========================\n",
    "    params_total = sum(p.numel() for p in model.parameters())\n",
    "    params_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    params_m = params_total / 1e6\n",
    "\n",
    "    # Latencia por batch (B=8) usando el modelo global (EMA si existe)\n",
    "    latency_batch_ms = float('nan')\n",
    "    example_batch = None\n",
    "    if len(X_te_std) >= 8:\n",
    "        example_batch = X_te_std[:8]\n",
    "    elif len(X_te_std) > 0:\n",
    "        example_batch = X_te_std   # batch más pequeño si no hay 8 trials\n",
    "\n",
    "    if example_batch is not None:\n",
    "        latency_batch_ms = estimate_latency_ms(eval_model, example_batch, device)\n",
    "\n",
    "    print(f\"[Consumo] params_total={params_total:,} | \"\n",
    "          f\"params_trainable={params_trainable:,} | \"\n",
    "          f\"FLOPs~{MODEL_FLOPS_G*1e3:.1f}M | \"\n",
    "          f\"latency/batch={latency_batch_ms:.2f} ms (B=8)\")\n",
    "\n",
    "    return (\n",
    "        metrics_global,\n",
    "        metrics_ft,\n",
    "        train_time,\n",
    "        train_mem,\n",
    "        test_time,\n",
    "        test_mem,\n",
    "        params_m,\n",
    "        latency_batch_ms,\n",
    "        pvals_4c,    \n",
    "        sfreq_used,  \n",
    "        saliency_fold,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9b1cf3",
   "metadata": {},
   "source": [
    "### Loop Principal y Resumen Final\n",
    "\n",
    "Función principal para ejecutar los 5 folds y generar resumen final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c9c1223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold1: 100%|██████████| 67/67 [00:07<00:00,  8.38it/s]\n",
      "Cargando val fold1: 100%|██████████| 15/15 [00:01<00:00,  8.31it/s]\n",
      "Cargando test fold1: 100%|██████████| 21/21 [00:02<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1/5] Entrenando modelo global... (n_train=5628 | n_val=1260 | n_test=1764)\n",
      "  Época   1 | train_loss=0.2329 | train_acc=0.2608 | val_loss=0.2246 | val_acc=0.2754 | val_f1m=0.2302 | LR=0.000125\n",
      "  Época   2 | train_loss=0.2114 | train_acc=0.3360 | val_loss=0.2184 | val_acc=0.3794 | val_f1m=0.3413 | LR=0.000250\n",
      "  Época   3 | train_loss=0.1843 | train_acc=0.4723 | val_loss=0.2042 | val_acc=0.4357 | val_f1m=0.4333 | LR=0.000375\n",
      "  Época   4 | train_loss=0.1765 | train_acc=0.4913 | val_loss=0.1951 | val_acc=0.4619 | val_f1m=0.4583 | LR=0.000500\n",
      "  Época   5 | train_loss=0.1711 | train_acc=0.5217 | val_loss=0.2070 | val_acc=0.4659 | val_f1m=0.4611 | LR=0.000625\n",
      "  Época   6 | train_loss=0.1676 | train_acc=0.5323 | val_loss=0.2075 | val_acc=0.4460 | val_f1m=0.4437 | LR=0.000750\n",
      "  Época   7 | train_loss=0.1631 | train_acc=0.5430 | val_loss=0.1967 | val_acc=0.4619 | val_f1m=0.4530 | LR=0.000875\n",
      "  Época   8 | train_loss=0.1649 | train_acc=0.5300 | val_loss=0.1947 | val_acc=0.4635 | val_f1m=0.4654 | LR=0.001000\n",
      "  Época   9 | train_loss=0.1667 | train_acc=0.5210 | val_loss=0.2004 | val_acc=0.4603 | val_f1m=0.4634 | LR=0.001000\n",
      "  Época  10 | train_loss=0.1592 | train_acc=0.5430 | val_loss=0.1978 | val_acc=0.4659 | val_f1m=0.4654 | LR=0.000999\n",
      "  Época  11 | train_loss=0.1575 | train_acc=0.5494 | val_loss=0.1997 | val_acc=0.4683 | val_f1m=0.4679 | LR=0.000997\n",
      "  Época  12 | train_loss=0.1544 | train_acc=0.5537 | val_loss=0.1999 | val_acc=0.4690 | val_f1m=0.4678 | LR=0.000993\n",
      "  Época  13 | train_loss=0.1506 | train_acc=0.5734 | val_loss=0.2000 | val_acc=0.4706 | val_f1m=0.4694 | LR=0.000987\n",
      "  Época  14 | train_loss=0.1520 | train_acc=0.5734 | val_loss=0.1998 | val_acc=0.4730 | val_f1m=0.4716 | LR=0.000980\n",
      "  Época  15 | train_loss=0.1534 | train_acc=0.5634 | val_loss=0.1999 | val_acc=0.4714 | val_f1m=0.4700 | LR=0.000971\n",
      "  Época  16 | train_loss=0.1474 | train_acc=0.5732 | val_loss=0.2000 | val_acc=0.4706 | val_f1m=0.4690 | LR=0.000960\n",
      "  Época  17 | train_loss=0.1450 | train_acc=0.5935 | val_loss=0.2003 | val_acc=0.4714 | val_f1m=0.4698 | LR=0.000948\n",
      "  Época  18 | train_loss=0.1448 | train_acc=0.5814 | val_loss=0.2006 | val_acc=0.4722 | val_f1m=0.4706 | LR=0.000935\n",
      "  Época  19 | train_loss=0.1453 | train_acc=0.5848 | val_loss=0.2008 | val_acc=0.4738 | val_f1m=0.4718 | LR=0.000920\n",
      "  Época  20 | train_loss=0.1441 | train_acc=0.5846 | val_loss=0.2011 | val_acc=0.4722 | val_f1m=0.4701 | LR=0.000904\n",
      "  Época  21 | train_loss=0.1458 | train_acc=0.5714 | val_loss=0.2013 | val_acc=0.4698 | val_f1m=0.4677 | LR=0.000887\n",
      "  Época  22 | train_loss=0.1442 | train_acc=0.5846 | val_loss=0.2014 | val_acc=0.4698 | val_f1m=0.4679 | LR=0.000868\n",
      "  Época  23 | train_loss=0.1417 | train_acc=0.5890 | val_loss=0.2016 | val_acc=0.4722 | val_f1m=0.4701 | LR=0.000848\n",
      "  Época  24 | train_loss=0.1407 | train_acc=0.6047 | val_loss=0.2020 | val_acc=0.4714 | val_f1m=0.4693 | LR=0.000828\n",
      "  Época  25 | train_loss=0.1382 | train_acc=0.6031 | val_loss=0.2025 | val_acc=0.4714 | val_f1m=0.4697 | LR=0.000806\n",
      "  Época  26 | train_loss=0.1369 | train_acc=0.6118 | val_loss=0.2029 | val_acc=0.4714 | val_f1m=0.4698 | LR=0.000783\n",
      "  Época  27 | train_loss=0.1321 | train_acc=0.6199 | val_loss=0.2033 | val_acc=0.4683 | val_f1m=0.4667 | LR=0.000759\n",
      "  Época  28 | train_loss=0.1356 | train_acc=0.6132 | val_loss=0.2037 | val_acc=0.4690 | val_f1m=0.4679 | LR=0.000735\n",
      "  Época  29 | train_loss=0.1293 | train_acc=0.6278 | val_loss=0.2043 | val_acc=0.4714 | val_f1m=0.4710 | LR=0.000710\n",
      "  Época  30 | train_loss=0.1282 | train_acc=0.6297 | val_loss=0.2049 | val_acc=0.4690 | val_f1m=0.4686 | LR=0.000684\n",
      "  Época  31 | train_loss=0.1255 | train_acc=0.6393 | val_loss=0.2056 | val_acc=0.4698 | val_f1m=0.4693 | LR=0.000658\n",
      "  Early stopping en época 31 (mejor val_f1m=0.4718)\n",
      "↳ Curva de entrenamiento guardada: 4clases/fold1/training_curve_fold1_4c.png\n",
      "[Fold 1/5] Global acc=0.5034 | f1_macro=0.5065\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.6263    0.5397    0.5798       441\n",
      "       right     0.5640    0.5692    0.5666       441\n",
      "  both fists     0.3812    0.4694    0.4207       441\n",
      "   both feet     0.4848    0.4354    0.4588       441\n",
      "\n",
      "    accuracy                         0.5034      1764\n",
      "   macro avg     0.5141    0.5034    0.5065      1764\n",
      "weighted avg     0.5141    0.5034    0.5065      1764\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[238  37 112  54]\n",
      " [ 25 251 105  60]\n",
      " [ 62  82 207  90]\n",
      " [ 55  75 119 192]]\n",
      "precision_macro=0.5141 | recall_macro=0.5034 | specificity_macro=0.8345 | sensitivity_macro=0.5034 | f1_weighted=0.5065\n",
      "↳ Matriz de confusión global guardada: 4clases/fold1/confusion_global_fold1_4c.png\n",
      "↳ Topomap guardado: 4clases/fold1/topomap_saliency_fold1_nb4_h2_4c.png\n",
      "↳ Topomap saliency guardado: 4clases/fold1/topomap_saliency_fold1_nb4_h2_4c.png\n",
      "↳ Topomap guardado: 4clases/fold1/topomap_pvalues_4c_fold1_nb4_h2_4c.png\n",
      "↳ Topomap p-values 4c guardado: 4clases/fold1/topomap_pvalues_4c_fold1_nb4_h2_4c.png\n",
      "↳ Calculando t-SNE 2D de CLS embeddings (global)...\n",
      "↳ t-SNE global guardado: 4clases/fold1/tsne_global_fold1_4c.png\n",
      "↳ Mapa de atención guardado: 4clases/fold1/attention_map_fold1_4c.png\n",
      "↳ Grid 4 clases (EEG+CAM) guardado: 4clases/fold1/eeg_cam_grid4_perclass_fold1_nb4_h2_4c.png\n",
      "↳ Grad-CAM++ 1D por clase guardado: 4clases/fold1/gradcampp_perclass_fold1_nb4_h2_4c.png\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5703 | f1_macro=0.5721\n",
      "  Δ(FT-Global) = +0.0669\n",
      "Confusion matrix FT (rows=true, cols=pred):\n",
      "[[274  36  82  49]\n",
      " [ 28 262  93  58]\n",
      " [ 54  62 244  81]\n",
      " [ 60  56  99 226]]\n",
      "precision_macro=0.5763 | recall_macro=0.5703 | specificity_macro=0.8568 | sensitivity_macro=0.5703 | f1_weighted=0.5721\n",
      "↳ Matriz de confusión FT guardada: 4clases/fold1/confusion_ft_fold1_4c.png\n",
      "[Consumo] params_total=327,908 | params_trainable=327,908 | FLOPs~30.0M | latency/batch=1.80 ms (B=8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold2: 100%|██████████| 67/67 [00:08<00:00,  8.37it/s]\n",
      "Cargando val fold2: 100%|██████████| 15/15 [00:01<00:00,  8.37it/s]\n",
      "Cargando test fold2: 100%|██████████| 21/21 [00:02<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 2/5] Entrenando modelo global... (n_train=5628 | n_val=1260 | n_test=1764)\n",
      "  Época   1 | train_loss=0.2279 | train_acc=0.2656 | val_loss=0.2207 | val_acc=0.2857 | val_f1m=0.1819 | LR=0.000125\n",
      "  Época   2 | train_loss=0.2113 | train_acc=0.3451 | val_loss=0.1998 | val_acc=0.4302 | val_f1m=0.4000 | LR=0.000250\n",
      "  Época   3 | train_loss=0.1895 | train_acc=0.4517 | val_loss=0.1896 | val_acc=0.4468 | val_f1m=0.4368 | LR=0.000375\n",
      "  Época   4 | train_loss=0.1805 | train_acc=0.4732 | val_loss=0.1890 | val_acc=0.4659 | val_f1m=0.4670 | LR=0.000500\n",
      "  Época   5 | train_loss=0.1812 | train_acc=0.4794 | val_loss=0.1880 | val_acc=0.4675 | val_f1m=0.4608 | LR=0.000625\n",
      "  Época   6 | train_loss=0.1738 | train_acc=0.5039 | val_loss=0.1961 | val_acc=0.4571 | val_f1m=0.4350 | LR=0.000750\n",
      "  Época   7 | train_loss=0.1758 | train_acc=0.4941 | val_loss=0.1879 | val_acc=0.4762 | val_f1m=0.4731 | LR=0.000875\n",
      "  Época   8 | train_loss=0.1692 | train_acc=0.5105 | val_loss=0.1832 | val_acc=0.4952 | val_f1m=0.4964 | LR=0.001000\n",
      "  Época   9 | train_loss=0.1721 | train_acc=0.5069 | val_loss=0.1875 | val_acc=0.4857 | val_f1m=0.4873 | LR=0.001000\n",
      "  Época  10 | train_loss=0.1664 | train_acc=0.5187 | val_loss=0.1822 | val_acc=0.4865 | val_f1m=0.4897 | LR=0.000999\n",
      "  Época  11 | train_loss=0.1666 | train_acc=0.5105 | val_loss=0.1799 | val_acc=0.4905 | val_f1m=0.4914 | LR=0.000997\n",
      "  Época  12 | train_loss=0.1649 | train_acc=0.5313 | val_loss=0.1791 | val_acc=0.4881 | val_f1m=0.4901 | LR=0.000993\n",
      "  Época  13 | train_loss=0.1614 | train_acc=0.5416 | val_loss=0.1790 | val_acc=0.4913 | val_f1m=0.4934 | LR=0.000987\n",
      "  Época  14 | train_loss=0.1621 | train_acc=0.5327 | val_loss=0.1789 | val_acc=0.4960 | val_f1m=0.4984 | LR=0.000980\n",
      "  Época  15 | train_loss=0.1604 | train_acc=0.5410 | val_loss=0.1788 | val_acc=0.4960 | val_f1m=0.4983 | LR=0.000971\n",
      "  Época  16 | train_loss=0.1608 | train_acc=0.5396 | val_loss=0.1786 | val_acc=0.4944 | val_f1m=0.4967 | LR=0.000960\n",
      "  Época  17 | train_loss=0.1563 | train_acc=0.5496 | val_loss=0.1784 | val_acc=0.4984 | val_f1m=0.5006 | LR=0.000948\n",
      "  Época  18 | train_loss=0.1527 | train_acc=0.5569 | val_loss=0.1784 | val_acc=0.4984 | val_f1m=0.5002 | LR=0.000935\n",
      "  Época  19 | train_loss=0.1524 | train_acc=0.5590 | val_loss=0.1784 | val_acc=0.4992 | val_f1m=0.5008 | LR=0.000920\n",
      "  Época  20 | train_loss=0.1510 | train_acc=0.5679 | val_loss=0.1783 | val_acc=0.5048 | val_f1m=0.5061 | LR=0.000904\n",
      "  Época  21 | train_loss=0.1481 | train_acc=0.5675 | val_loss=0.1783 | val_acc=0.5056 | val_f1m=0.5069 | LR=0.000887\n",
      "  Época  22 | train_loss=0.1502 | train_acc=0.5631 | val_loss=0.1784 | val_acc=0.5048 | val_f1m=0.5060 | LR=0.000868\n",
      "  Época  23 | train_loss=0.1479 | train_acc=0.5657 | val_loss=0.1786 | val_acc=0.5056 | val_f1m=0.5067 | LR=0.000848\n",
      "  Época  24 | train_loss=0.1456 | train_acc=0.5766 | val_loss=0.1787 | val_acc=0.5103 | val_f1m=0.5114 | LR=0.000828\n",
      "  Época  25 | train_loss=0.1443 | train_acc=0.5780 | val_loss=0.1789 | val_acc=0.5095 | val_f1m=0.5104 | LR=0.000806\n",
      "  Época  26 | train_loss=0.1412 | train_acc=0.5848 | val_loss=0.1791 | val_acc=0.5135 | val_f1m=0.5143 | LR=0.000783\n",
      "  Época  27 | train_loss=0.1442 | train_acc=0.5817 | val_loss=0.1793 | val_acc=0.5103 | val_f1m=0.5107 | LR=0.000759\n",
      "  Época  28 | train_loss=0.1366 | train_acc=0.6093 | val_loss=0.1797 | val_acc=0.5087 | val_f1m=0.5092 | LR=0.000735\n",
      "  Época  29 | train_loss=0.1416 | train_acc=0.5814 | val_loss=0.1799 | val_acc=0.5135 | val_f1m=0.5140 | LR=0.000710\n",
      "  Época  30 | train_loss=0.1359 | train_acc=0.5968 | val_loss=0.1803 | val_acc=0.5151 | val_f1m=0.5152 | LR=0.000684\n",
      "  Época  31 | train_loss=0.1360 | train_acc=0.5986 | val_loss=0.1807 | val_acc=0.5151 | val_f1m=0.5150 | LR=0.000658\n",
      "  Época  32 | train_loss=0.1334 | train_acc=0.6063 | val_loss=0.1812 | val_acc=0.5167 | val_f1m=0.5165 | LR=0.000631\n",
      "  Época  33 | train_loss=0.1296 | train_acc=0.6118 | val_loss=0.1817 | val_acc=0.5190 | val_f1m=0.5187 | LR=0.000604\n",
      "  Época  34 | train_loss=0.1248 | train_acc=0.6166 | val_loss=0.1823 | val_acc=0.5175 | val_f1m=0.5174 | LR=0.000577\n",
      "  Época  35 | train_loss=0.1232 | train_acc=0.6281 | val_loss=0.1831 | val_acc=0.5119 | val_f1m=0.5117 | LR=0.000550\n",
      "  Época  36 | train_loss=0.1253 | train_acc=0.6230 | val_loss=0.1838 | val_acc=0.5095 | val_f1m=0.5092 | LR=0.000523\n",
      "  Época  37 | train_loss=0.1272 | train_acc=0.6086 | val_loss=0.1846 | val_acc=0.5056 | val_f1m=0.5050 | LR=0.000496\n",
      "  Época  38 | train_loss=0.1161 | train_acc=0.6519 | val_loss=0.1855 | val_acc=0.5063 | val_f1m=0.5058 | LR=0.000469\n",
      "  Época  39 | train_loss=0.1148 | train_acc=0.6610 | val_loss=0.1865 | val_acc=0.5056 | val_f1m=0.5047 | LR=0.000442\n",
      "  Época  40 | train_loss=0.1106 | train_acc=0.6572 | val_loss=0.1874 | val_acc=0.5024 | val_f1m=0.5016 | LR=0.000416\n",
      "  Época  41 | train_loss=0.1150 | train_acc=0.6491 | val_loss=0.1884 | val_acc=0.5024 | val_f1m=0.5019 | LR=0.000390\n",
      "  Época  42 | train_loss=0.1050 | train_acc=0.6773 | val_loss=0.1895 | val_acc=0.5040 | val_f1m=0.5034 | LR=0.000365\n",
      "  Época  43 | train_loss=0.1085 | train_acc=0.6716 | val_loss=0.1906 | val_acc=0.5016 | val_f1m=0.5010 | LR=0.000341\n",
      "  Época  44 | train_loss=0.1023 | train_acc=0.6857 | val_loss=0.1918 | val_acc=0.5016 | val_f1m=0.5007 | LR=0.000317\n",
      "  Época  45 | train_loss=0.0936 | train_acc=0.7066 | val_loss=0.1933 | val_acc=0.5008 | val_f1m=0.5000 | LR=0.000294\n",
      "  Early stopping en época 45 (mejor val_f1m=0.5187)\n",
      "↳ Curva de entrenamiento guardada: 4clases/fold2/training_curve_fold2_4c.png\n",
      "[Fold 2/5] Global acc=0.5658 | f1_macro=0.5658\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.6187    0.6440    0.6311       441\n",
      "       right     0.6261    0.6190    0.6226       441\n",
      "  both fists     0.4611    0.5238    0.4904       441\n",
      "   both feet     0.5707    0.4762    0.5192       441\n",
      "\n",
      "    accuracy                         0.5658      1764\n",
      "   macro avg     0.5692    0.5658    0.5658      1764\n",
      "weighted avg     0.5692    0.5658    0.5658      1764\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[284  23  89  45]\n",
      " [ 37 273  80  51]\n",
      " [ 88  60 231  62]\n",
      " [ 50  80 101 210]]\n",
      "precision_macro=0.5692 | recall_macro=0.5658 | specificity_macro=0.8553 | sensitivity_macro=0.5658 | f1_weighted=0.5658\n",
      "↳ Matriz de confusión global guardada: 4clases/fold2/confusion_global_fold2_4c.png\n",
      "↳ Topomap guardado: 4clases/fold2/topomap_saliency_fold2_nb4_h2_4c.png\n",
      "↳ Topomap saliency guardado: 4clases/fold2/topomap_saliency_fold2_nb4_h2_4c.png\n",
      "↳ Topomap guardado: 4clases/fold2/topomap_pvalues_4c_fold2_nb4_h2_4c.png\n",
      "↳ Topomap p-values 4c guardado: 4clases/fold2/topomap_pvalues_4c_fold2_nb4_h2_4c.png\n",
      "↳ Calculando t-SNE 2D de CLS embeddings (global)...\n",
      "↳ t-SNE global guardado: 4clases/fold2/tsne_global_fold2_4c.png\n",
      "↳ Mapa de atención guardado: 4clases/fold2/attention_map_fold2_4c.png\n",
      "↳ Grid 4 clases (EEG+CAM) guardado: 4clases/fold2/eeg_cam_grid4_perclass_fold2_nb4_h2_4c.png\n",
      "↳ Grad-CAM++ 1D por clase guardado: 4clases/fold2/gradcampp_perclass_fold2_nb4_h2_4c.png\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.6378 | f1_macro=0.6393\n",
      "  Δ(FT-Global) = +0.0720\n",
      "Confusion matrix FT (rows=true, cols=pred):\n",
      "[[292  21  91  37]\n",
      " [ 36 296  62  47]\n",
      " [ 56  41 282  62]\n",
      " [ 41  54  91 255]]\n",
      "precision_macro=0.6444 | recall_macro=0.6378 | specificity_macro=0.8793 | sensitivity_macro=0.6378 | f1_weighted=0.6393\n",
      "↳ Matriz de confusión FT guardada: 4clases/fold2/confusion_ft_fold2_4c.png\n",
      "[Consumo] params_total=327,908 | params_trainable=327,908 | FLOPs~30.0M | latency/batch=1.76 ms (B=8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold3: 100%|██████████| 67/67 [00:08<00:00,  8.26it/s]\n",
      "Cargando val fold3: 100%|██████████| 15/15 [00:01<00:00,  8.37it/s]\n",
      "Cargando test fold3: 100%|██████████| 21/21 [00:02<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 3/5] Entrenando modelo global... (n_train=5628 | n_val=1260 | n_test=1764)\n",
      "  Época   1 | train_loss=0.2287 | train_acc=0.2660 | val_loss=0.2213 | val_acc=0.2770 | val_f1m=0.2047 | LR=0.000125\n",
      "  Época   2 | train_loss=0.2158 | train_acc=0.3186 | val_loss=0.1954 | val_acc=0.3897 | val_f1m=0.3571 | LR=0.000250\n",
      "  Época   3 | train_loss=0.1877 | train_acc=0.4550 | val_loss=0.1960 | val_acc=0.4421 | val_f1m=0.4402 | LR=0.000375\n",
      "  Época   4 | train_loss=0.1791 | train_acc=0.4940 | val_loss=0.1838 | val_acc=0.4746 | val_f1m=0.4767 | LR=0.000500\n",
      "  Época   5 | train_loss=0.1769 | train_acc=0.4998 | val_loss=0.1830 | val_acc=0.4817 | val_f1m=0.4795 | LR=0.000625\n",
      "  Época   6 | train_loss=0.1730 | train_acc=0.5130 | val_loss=0.1769 | val_acc=0.4913 | val_f1m=0.4935 | LR=0.000750\n",
      "  Época   7 | train_loss=0.1683 | train_acc=0.5128 | val_loss=0.1804 | val_acc=0.5024 | val_f1m=0.5033 | LR=0.000875\n",
      "  Época   8 | train_loss=0.1674 | train_acc=0.5272 | val_loss=0.1790 | val_acc=0.4992 | val_f1m=0.4957 | LR=0.001000\n",
      "  Época   9 | train_loss=0.1623 | train_acc=0.5378 | val_loss=0.1834 | val_acc=0.4849 | val_f1m=0.4830 | LR=0.001000\n",
      "  Época  10 | train_loss=0.1618 | train_acc=0.5453 | val_loss=0.1781 | val_acc=0.5016 | val_f1m=0.5023 | LR=0.000999\n",
      "  Época  11 | train_loss=0.1603 | train_acc=0.5551 | val_loss=0.1769 | val_acc=0.5095 | val_f1m=0.5113 | LR=0.000997\n",
      "  Época  12 | train_loss=0.1605 | train_acc=0.5444 | val_loss=0.1780 | val_acc=0.4984 | val_f1m=0.4997 | LR=0.000993\n",
      "  Época  13 | train_loss=0.1553 | train_acc=0.5665 | val_loss=0.1779 | val_acc=0.5008 | val_f1m=0.5020 | LR=0.000987\n",
      "  Época  14 | train_loss=0.1578 | train_acc=0.5535 | val_loss=0.1777 | val_acc=0.5000 | val_f1m=0.5015 | LR=0.000980\n",
      "  Época  15 | train_loss=0.1580 | train_acc=0.5595 | val_loss=0.1774 | val_acc=0.5008 | val_f1m=0.5022 | LR=0.000971\n",
      "  Época  16 | train_loss=0.1528 | train_acc=0.5707 | val_loss=0.1773 | val_acc=0.5008 | val_f1m=0.5019 | LR=0.000960\n",
      "  Época  17 | train_loss=0.1515 | train_acc=0.5709 | val_loss=0.1772 | val_acc=0.4984 | val_f1m=0.4991 | LR=0.000948\n",
      "  Época  18 | train_loss=0.1504 | train_acc=0.5759 | val_loss=0.1771 | val_acc=0.5040 | val_f1m=0.5046 | LR=0.000935\n",
      "  Época  19 | train_loss=0.1474 | train_acc=0.5769 | val_loss=0.1770 | val_acc=0.5032 | val_f1m=0.5039 | LR=0.000920\n",
      "  Época  20 | train_loss=0.1447 | train_acc=0.5942 | val_loss=0.1769 | val_acc=0.5056 | val_f1m=0.5060 | LR=0.000904\n",
      "  Época  21 | train_loss=0.1476 | train_acc=0.5730 | val_loss=0.1768 | val_acc=0.5087 | val_f1m=0.5091 | LR=0.000887\n",
      "  Época  22 | train_loss=0.1475 | train_acc=0.5832 | val_loss=0.1767 | val_acc=0.5127 | val_f1m=0.5133 | LR=0.000868\n",
      "  Época  23 | train_loss=0.1447 | train_acc=0.5928 | val_loss=0.1767 | val_acc=0.5151 | val_f1m=0.5158 | LR=0.000848\n",
      "  Época  24 | train_loss=0.1390 | train_acc=0.5995 | val_loss=0.1766 | val_acc=0.5151 | val_f1m=0.5157 | LR=0.000828\n",
      "  Época  25 | train_loss=0.1375 | train_acc=0.6107 | val_loss=0.1767 | val_acc=0.5190 | val_f1m=0.5195 | LR=0.000806\n",
      "  Época  26 | train_loss=0.1424 | train_acc=0.5871 | val_loss=0.1767 | val_acc=0.5206 | val_f1m=0.5212 | LR=0.000783\n",
      "  Época  27 | train_loss=0.1354 | train_acc=0.6135 | val_loss=0.1768 | val_acc=0.5214 | val_f1m=0.5224 | LR=0.000759\n",
      "  Época  28 | train_loss=0.1286 | train_acc=0.6304 | val_loss=0.1771 | val_acc=0.5190 | val_f1m=0.5195 | LR=0.000735\n",
      "  Época  29 | train_loss=0.1329 | train_acc=0.6141 | val_loss=0.1774 | val_acc=0.5151 | val_f1m=0.5154 | LR=0.000710\n",
      "  Época  30 | train_loss=0.1298 | train_acc=0.6158 | val_loss=0.1777 | val_acc=0.5167 | val_f1m=0.5170 | LR=0.000684\n",
      "  Época  31 | train_loss=0.1323 | train_acc=0.6102 | val_loss=0.1779 | val_acc=0.5159 | val_f1m=0.5163 | LR=0.000658\n",
      "  Época  32 | train_loss=0.1273 | train_acc=0.6338 | val_loss=0.1782 | val_acc=0.5135 | val_f1m=0.5140 | LR=0.000631\n",
      "  Época  33 | train_loss=0.1289 | train_acc=0.6214 | val_loss=0.1783 | val_acc=0.5143 | val_f1m=0.5150 | LR=0.000604\n",
      "  Época  34 | train_loss=0.1283 | train_acc=0.6267 | val_loss=0.1787 | val_acc=0.5175 | val_f1m=0.5183 | LR=0.000577\n",
      "  Época  35 | train_loss=0.1263 | train_acc=0.6363 | val_loss=0.1791 | val_acc=0.5198 | val_f1m=0.5207 | LR=0.000550\n",
      "  Época  36 | train_loss=0.1212 | train_acc=0.6388 | val_loss=0.1796 | val_acc=0.5183 | val_f1m=0.5193 | LR=0.000523\n",
      "  Época  37 | train_loss=0.1165 | train_acc=0.6606 | val_loss=0.1802 | val_acc=0.5183 | val_f1m=0.5193 | LR=0.000496\n",
      "  Época  38 | train_loss=0.1164 | train_acc=0.6505 | val_loss=0.1809 | val_acc=0.5190 | val_f1m=0.5200 | LR=0.000469\n",
      "  Época  39 | train_loss=0.1141 | train_acc=0.6693 | val_loss=0.1816 | val_acc=0.5198 | val_f1m=0.5209 | LR=0.000442\n",
      "  Early stopping en época 39 (mejor val_f1m=0.5224)\n",
      "↳ Curva de entrenamiento guardada: 4clases/fold3/training_curve_fold3_4c.png\n",
      "[Fold 3/5] Global acc=0.5125 | f1_macro=0.5143\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.5577    0.5805    0.5689       441\n",
      "       right     0.6294    0.5238    0.5718       441\n",
      "  both fists     0.4309    0.4807    0.4544       441\n",
      "   both feet     0.4596    0.4649    0.4622       441\n",
      "\n",
      "    accuracy                         0.5125      1764\n",
      "   macro avg     0.5194    0.5125    0.5143      1764\n",
      "weighted avg     0.5194    0.5125    0.5143      1764\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[256  29  92  64]\n",
      " [ 38 231  88  84]\n",
      " [ 92  44 212  93]\n",
      " [ 73  63 100 205]]\n",
      "precision_macro=0.5194 | recall_macro=0.5125 | specificity_macro=0.8375 | sensitivity_macro=0.5125 | f1_weighted=0.5143\n",
      "↳ Matriz de confusión global guardada: 4clases/fold3/confusion_global_fold3_4c.png\n",
      "↳ Topomap guardado: 4clases/fold3/topomap_saliency_fold3_nb4_h2_4c.png\n",
      "↳ Topomap saliency guardado: 4clases/fold3/topomap_saliency_fold3_nb4_h2_4c.png\n",
      "↳ Topomap guardado: 4clases/fold3/topomap_pvalues_4c_fold3_nb4_h2_4c.png\n",
      "↳ Topomap p-values 4c guardado: 4clases/fold3/topomap_pvalues_4c_fold3_nb4_h2_4c.png\n",
      "↳ Calculando t-SNE 2D de CLS embeddings (global)...\n",
      "↳ t-SNE global guardado: 4clases/fold3/tsne_global_fold3_4c.png\n",
      "↳ Mapa de atención guardado: 4clases/fold3/attention_map_fold3_4c.png\n",
      "↳ Grid 4 clases (EEG+CAM) guardado: 4clases/fold3/eeg_cam_grid4_perclass_fold3_nb4_h2_4c.png\n",
      "↳ Grad-CAM++ 1D por clase guardado: 4clases/fold3/gradcampp_perclass_fold3_nb4_h2_4c.png\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5692 | f1_macro=0.5690\n",
      "  Δ(FT-Global) = +0.0567\n",
      "Confusion matrix FT (rows=true, cols=pred):\n",
      "[[286  35  76  44]\n",
      " [ 33 270  76  62]\n",
      " [ 84  49 238  70]\n",
      " [ 71  60 100 210]]\n",
      "precision_macro=0.5713 | recall_macro=0.5692 | specificity_macro=0.8564 | sensitivity_macro=0.5692 | f1_weighted=0.5690\n",
      "↳ Matriz de confusión FT guardada: 4clases/fold3/confusion_ft_fold3_4c.png\n",
      "[Consumo] params_total=327,908 | params_trainable=327,908 | FLOPs~30.0M | latency/batch=1.78 ms (B=8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold4: 100%|██████████| 68/68 [00:08<00:00,  8.32it/s]\n",
      "Cargando val fold4: 100%|██████████| 15/15 [00:01<00:00,  8.43it/s]\n",
      "Cargando test fold4: 100%|██████████| 20/20 [00:02<00:00,  8.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 4/5] Entrenando modelo global... (n_train=5712 | n_val=1260 | n_test=1680)\n",
      "  Época   1 | train_loss=0.2275 | train_acc=0.2759 | val_loss=0.2215 | val_acc=0.2778 | val_f1m=0.1869 | LR=0.000125\n",
      "  Época   2 | train_loss=0.2116 | train_acc=0.3568 | val_loss=0.2051 | val_acc=0.4016 | val_f1m=0.3481 | LR=0.000250\n",
      "  Época   3 | train_loss=0.1907 | train_acc=0.4463 | val_loss=0.1958 | val_acc=0.4452 | val_f1m=0.4428 | LR=0.000375\n",
      "  Época   4 | train_loss=0.1829 | train_acc=0.4674 | val_loss=0.1880 | val_acc=0.4865 | val_f1m=0.4875 | LR=0.000500\n",
      "  Época   5 | train_loss=0.1753 | train_acc=0.5007 | val_loss=0.1928 | val_acc=0.4817 | val_f1m=0.4825 | LR=0.000625\n",
      "  Época   6 | train_loss=0.1773 | train_acc=0.4939 | val_loss=0.1888 | val_acc=0.4833 | val_f1m=0.4758 | LR=0.000750\n",
      "  Época   7 | train_loss=0.1763 | train_acc=0.4930 | val_loss=0.1855 | val_acc=0.4976 | val_f1m=0.4941 | LR=0.000875\n",
      "  Época   8 | train_loss=0.1741 | train_acc=0.4995 | val_loss=0.1770 | val_acc=0.5032 | val_f1m=0.5023 | LR=0.001000\n",
      "  Época   9 | train_loss=0.1692 | train_acc=0.5278 | val_loss=0.1819 | val_acc=0.4905 | val_f1m=0.4891 | LR=0.001000\n",
      "  Época  10 | train_loss=0.1690 | train_acc=0.5147 | val_loss=0.1786 | val_acc=0.4944 | val_f1m=0.4984 | LR=0.000999\n",
      "  Época  11 | train_loss=0.1622 | train_acc=0.5368 | val_loss=0.1772 | val_acc=0.5087 | val_f1m=0.5095 | LR=0.000997\n",
      "  Época  12 | train_loss=0.1682 | train_acc=0.5263 | val_loss=0.1771 | val_acc=0.5079 | val_f1m=0.5089 | LR=0.000993\n",
      "  Época  13 | train_loss=0.1617 | train_acc=0.5378 | val_loss=0.1771 | val_acc=0.5063 | val_f1m=0.5070 | LR=0.000987\n",
      "  Época  14 | train_loss=0.1628 | train_acc=0.5448 | val_loss=0.1771 | val_acc=0.5095 | val_f1m=0.5104 | LR=0.000980\n",
      "  Época  15 | train_loss=0.1581 | train_acc=0.5553 | val_loss=0.1770 | val_acc=0.5071 | val_f1m=0.5079 | LR=0.000971\n",
      "  Época  16 | train_loss=0.1577 | train_acc=0.5478 | val_loss=0.1769 | val_acc=0.5127 | val_f1m=0.5132 | LR=0.000960\n",
      "  Época  17 | train_loss=0.1554 | train_acc=0.5578 | val_loss=0.1767 | val_acc=0.5103 | val_f1m=0.5107 | LR=0.000948\n",
      "  Época  18 | train_loss=0.1574 | train_acc=0.5487 | val_loss=0.1765 | val_acc=0.5111 | val_f1m=0.5113 | LR=0.000935\n",
      "  Época  19 | train_loss=0.1579 | train_acc=0.5441 | val_loss=0.1763 | val_acc=0.5111 | val_f1m=0.5115 | LR=0.000920\n",
      "  Época  20 | train_loss=0.1521 | train_acc=0.5749 | val_loss=0.1762 | val_acc=0.5127 | val_f1m=0.5132 | LR=0.000904\n",
      "  Época  21 | train_loss=0.1513 | train_acc=0.5674 | val_loss=0.1761 | val_acc=0.5143 | val_f1m=0.5148 | LR=0.000887\n",
      "  Época  22 | train_loss=0.1500 | train_acc=0.5769 | val_loss=0.1760 | val_acc=0.5119 | val_f1m=0.5127 | LR=0.000868\n",
      "  Época  23 | train_loss=0.1537 | train_acc=0.5607 | val_loss=0.1759 | val_acc=0.5135 | val_f1m=0.5148 | LR=0.000848\n",
      "  Época  24 | train_loss=0.1501 | train_acc=0.5735 | val_loss=0.1759 | val_acc=0.5135 | val_f1m=0.5148 | LR=0.000828\n",
      "  Época  25 | train_loss=0.1477 | train_acc=0.5840 | val_loss=0.1759 | val_acc=0.5143 | val_f1m=0.5157 | LR=0.000806\n",
      "  Época  26 | train_loss=0.1430 | train_acc=0.5884 | val_loss=0.1760 | val_acc=0.5103 | val_f1m=0.5116 | LR=0.000783\n",
      "  Época  27 | train_loss=0.1456 | train_acc=0.5818 | val_loss=0.1762 | val_acc=0.5119 | val_f1m=0.5133 | LR=0.000759\n",
      "  Época  28 | train_loss=0.1408 | train_acc=0.5954 | val_loss=0.1764 | val_acc=0.5119 | val_f1m=0.5133 | LR=0.000735\n",
      "  Época  29 | train_loss=0.1394 | train_acc=0.6005 | val_loss=0.1765 | val_acc=0.5175 | val_f1m=0.5187 | LR=0.000710\n",
      "  Época  30 | train_loss=0.1397 | train_acc=0.6031 | val_loss=0.1767 | val_acc=0.5175 | val_f1m=0.5186 | LR=0.000684\n",
      "  Época  31 | train_loss=0.1377 | train_acc=0.6021 | val_loss=0.1769 | val_acc=0.5167 | val_f1m=0.5178 | LR=0.000658\n",
      "  Época  32 | train_loss=0.1346 | train_acc=0.6169 | val_loss=0.1773 | val_acc=0.5159 | val_f1m=0.5169 | LR=0.000631\n",
      "  Época  33 | train_loss=0.1307 | train_acc=0.6203 | val_loss=0.1777 | val_acc=0.5127 | val_f1m=0.5137 | LR=0.000604\n",
      "  Época  34 | train_loss=0.1308 | train_acc=0.6169 | val_loss=0.1782 | val_acc=0.5127 | val_f1m=0.5137 | LR=0.000577\n",
      "  Época  35 | train_loss=0.1260 | train_acc=0.6301 | val_loss=0.1787 | val_acc=0.5135 | val_f1m=0.5143 | LR=0.000550\n",
      "  Época  36 | train_loss=0.1272 | train_acc=0.6273 | val_loss=0.1794 | val_acc=0.5143 | val_f1m=0.5150 | LR=0.000523\n",
      "  Época  37 | train_loss=0.1275 | train_acc=0.6285 | val_loss=0.1800 | val_acc=0.5143 | val_f1m=0.5148 | LR=0.000496\n",
      "  Época  38 | train_loss=0.1243 | train_acc=0.6385 | val_loss=0.1807 | val_acc=0.5143 | val_f1m=0.5149 | LR=0.000469\n",
      "  Época  39 | train_loss=0.1171 | train_acc=0.6453 | val_loss=0.1814 | val_acc=0.5143 | val_f1m=0.5149 | LR=0.000442\n",
      "  Época  40 | train_loss=0.1181 | train_acc=0.6532 | val_loss=0.1822 | val_acc=0.5127 | val_f1m=0.5134 | LR=0.000416\n",
      "  Época  41 | train_loss=0.1197 | train_acc=0.6434 | val_loss=0.1829 | val_acc=0.5135 | val_f1m=0.5140 | LR=0.000390\n",
      "  Early stopping en época 41 (mejor val_f1m=0.5187)\n",
      "↳ Curva de entrenamiento guardada: 4clases/fold4/training_curve_fold4_4c.png\n",
      "[Fold 4/5] Global acc=0.5476 | f1_macro=0.5469\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.6564    0.6095    0.6321       420\n",
      "       right     0.5911    0.6024    0.5967       420\n",
      "  both fists     0.4508    0.5786    0.5068       420\n",
      "   both feet     0.5201    0.4000    0.4522       420\n",
      "\n",
      "    accuracy                         0.5476      1680\n",
      "   macro avg     0.5546    0.5476    0.5469      1680\n",
      "weighted avg     0.5546    0.5476    0.5469      1680\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[256  24 100  40]\n",
      " [ 26 253  76  65]\n",
      " [ 60  67 243  50]\n",
      " [ 48  84 120 168]]\n",
      "precision_macro=0.5546 | recall_macro=0.5476 | specificity_macro=0.8492 | sensitivity_macro=0.5476 | f1_weighted=0.5469\n",
      "↳ Matriz de confusión global guardada: 4clases/fold4/confusion_global_fold4_4c.png\n",
      "↳ Topomap guardado: 4clases/fold4/topomap_saliency_fold4_nb4_h2_4c.png\n",
      "↳ Topomap saliency guardado: 4clases/fold4/topomap_saliency_fold4_nb4_h2_4c.png\n",
      "↳ Topomap guardado: 4clases/fold4/topomap_pvalues_4c_fold4_nb4_h2_4c.png\n",
      "↳ Topomap p-values 4c guardado: 4clases/fold4/topomap_pvalues_4c_fold4_nb4_h2_4c.png\n",
      "↳ Calculando t-SNE 2D de CLS embeddings (global)...\n",
      "↳ t-SNE global guardado: 4clases/fold4/tsne_global_fold4_4c.png\n",
      "↳ Mapa de atención guardado: 4clases/fold4/attention_map_fold4_4c.png\n",
      "↳ Grid 4 clases (EEG+CAM) guardado: 4clases/fold4/eeg_cam_grid4_perclass_fold4_nb4_h2_4c.png\n",
      "↳ Grad-CAM++ 1D por clase guardado: 4clases/fold4/gradcampp_perclass_fold4_nb4_h2_4c.png\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.6202 | f1_macro=0.6212\n",
      "  Δ(FT-Global) = +0.0726\n",
      "Confusion matrix FT (rows=true, cols=pred):\n",
      "[[275  25  79  41]\n",
      " [ 25 281  60  54]\n",
      " [ 57  49 255  59]\n",
      " [ 49  52  88 231]]\n",
      "precision_macro=0.6242 | recall_macro=0.6202 | specificity_macro=0.8734 | sensitivity_macro=0.6202 | f1_weighted=0.6212\n",
      "↳ Matriz de confusión FT guardada: 4clases/fold4/confusion_ft_fold4_4c.png\n",
      "[Consumo] params_total=327,908 | params_trainable=327,908 | FLOPs~30.0M | latency/batch=1.76 ms (B=8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold5: 100%|██████████| 68/68 [00:08<00:00,  8.34it/s]\n",
      "Cargando val fold5: 100%|██████████| 15/15 [00:01<00:00,  8.45it/s]\n",
      "Cargando test fold5: 100%|██████████| 20/20 [00:02<00:00,  8.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 5/5] Entrenando modelo global... (n_train=5712 | n_val=1260 | n_test=1680)\n",
      "  Época   1 | train_loss=0.2276 | train_acc=0.2670 | val_loss=0.2297 | val_acc=0.2651 | val_f1m=0.1791 | LR=0.000125\n",
      "  Época   2 | train_loss=0.2098 | train_acc=0.3568 | val_loss=0.2146 | val_acc=0.3841 | val_f1m=0.3675 | LR=0.000250\n",
      "  Época   3 | train_loss=0.1882 | train_acc=0.4625 | val_loss=0.2151 | val_acc=0.4159 | val_f1m=0.4107 | LR=0.000375\n",
      "  Época   4 | train_loss=0.1842 | train_acc=0.4692 | val_loss=0.1982 | val_acc=0.4540 | val_f1m=0.4561 | LR=0.000500\n",
      "  Época   5 | train_loss=0.1762 | train_acc=0.5004 | val_loss=0.2008 | val_acc=0.4341 | val_f1m=0.4344 | LR=0.000625\n",
      "  Época   6 | train_loss=0.1755 | train_acc=0.5035 | val_loss=0.1991 | val_acc=0.4437 | val_f1m=0.4371 | LR=0.000750\n",
      "  Época   7 | train_loss=0.1709 | train_acc=0.5060 | val_loss=0.1973 | val_acc=0.4556 | val_f1m=0.4515 | LR=0.000875\n",
      "  Época   8 | train_loss=0.1702 | train_acc=0.5254 | val_loss=0.2072 | val_acc=0.4413 | val_f1m=0.4416 | LR=0.001000\n",
      "  Época   9 | train_loss=0.1680 | train_acc=0.5240 | val_loss=0.2003 | val_acc=0.4556 | val_f1m=0.4575 | LR=0.001000\n",
      "  Época  10 | train_loss=0.1637 | train_acc=0.5254 | val_loss=0.2002 | val_acc=0.4500 | val_f1m=0.4498 | LR=0.000999\n",
      "  Época  11 | train_loss=0.1656 | train_acc=0.5228 | val_loss=0.1968 | val_acc=0.4516 | val_f1m=0.4526 | LR=0.000997\n",
      "  Época  12 | train_loss=0.1639 | train_acc=0.5331 | val_loss=0.1967 | val_acc=0.4532 | val_f1m=0.4540 | LR=0.000993\n",
      "  Época  13 | train_loss=0.1650 | train_acc=0.5254 | val_loss=0.1966 | val_acc=0.4548 | val_f1m=0.4559 | LR=0.000987\n",
      "  Época  14 | train_loss=0.1605 | train_acc=0.5509 | val_loss=0.1965 | val_acc=0.4563 | val_f1m=0.4575 | LR=0.000980\n",
      "  Época  15 | train_loss=0.1616 | train_acc=0.5364 | val_loss=0.1963 | val_acc=0.4571 | val_f1m=0.4583 | LR=0.000971\n",
      "  Época  16 | train_loss=0.1575 | train_acc=0.5432 | val_loss=0.1962 | val_acc=0.4595 | val_f1m=0.4606 | LR=0.000960\n",
      "  Época  17 | train_loss=0.1546 | train_acc=0.5613 | val_loss=0.1963 | val_acc=0.4579 | val_f1m=0.4593 | LR=0.000948\n",
      "  Época  18 | train_loss=0.1584 | train_acc=0.5497 | val_loss=0.1962 | val_acc=0.4603 | val_f1m=0.4616 | LR=0.000935\n",
      "  Época  19 | train_loss=0.1557 | train_acc=0.5611 | val_loss=0.1962 | val_acc=0.4595 | val_f1m=0.4606 | LR=0.000920\n",
      "  Época  20 | train_loss=0.1515 | train_acc=0.5611 | val_loss=0.1961 | val_acc=0.4619 | val_f1m=0.4628 | LR=0.000904\n",
      "  Época  21 | train_loss=0.1503 | train_acc=0.5637 | val_loss=0.1962 | val_acc=0.4659 | val_f1m=0.4664 | LR=0.000887\n",
      "  Época  22 | train_loss=0.1510 | train_acc=0.5636 | val_loss=0.1962 | val_acc=0.4667 | val_f1m=0.4675 | LR=0.000868\n",
      "  Época  23 | train_loss=0.1478 | train_acc=0.5695 | val_loss=0.1963 | val_acc=0.4683 | val_f1m=0.4692 | LR=0.000848\n",
      "  Época  24 | train_loss=0.1459 | train_acc=0.5784 | val_loss=0.1965 | val_acc=0.4690 | val_f1m=0.4701 | LR=0.000828\n",
      "  Época  25 | train_loss=0.1427 | train_acc=0.5912 | val_loss=0.1967 | val_acc=0.4706 | val_f1m=0.4717 | LR=0.000806\n",
      "  Época  26 | train_loss=0.1429 | train_acc=0.5790 | val_loss=0.1970 | val_acc=0.4738 | val_f1m=0.4751 | LR=0.000783\n",
      "  Época  27 | train_loss=0.1415 | train_acc=0.5828 | val_loss=0.1973 | val_acc=0.4706 | val_f1m=0.4718 | LR=0.000759\n",
      "  Época  28 | train_loss=0.1443 | train_acc=0.5760 | val_loss=0.1976 | val_acc=0.4714 | val_f1m=0.4729 | LR=0.000735\n",
      "  Época  29 | train_loss=0.1370 | train_acc=0.6070 | val_loss=0.1978 | val_acc=0.4714 | val_f1m=0.4725 | LR=0.000710\n",
      "  Época  30 | train_loss=0.1355 | train_acc=0.6024 | val_loss=0.1983 | val_acc=0.4730 | val_f1m=0.4742 | LR=0.000684\n",
      "  Época  31 | train_loss=0.1340 | train_acc=0.6049 | val_loss=0.1988 | val_acc=0.4714 | val_f1m=0.4725 | LR=0.000658\n",
      "  Época  32 | train_loss=0.1309 | train_acc=0.6173 | val_loss=0.1993 | val_acc=0.4706 | val_f1m=0.4717 | LR=0.000631\n",
      "  Época  33 | train_loss=0.1344 | train_acc=0.6042 | val_loss=0.1999 | val_acc=0.4706 | val_f1m=0.4711 | LR=0.000604\n",
      "  Época  34 | train_loss=0.1295 | train_acc=0.6154 | val_loss=0.2004 | val_acc=0.4738 | val_f1m=0.4739 | LR=0.000577\n",
      "  Época  35 | train_loss=0.1288 | train_acc=0.6248 | val_loss=0.2012 | val_acc=0.4698 | val_f1m=0.4701 | LR=0.000550\n",
      "  Época  36 | train_loss=0.1222 | train_acc=0.6408 | val_loss=0.2021 | val_acc=0.4714 | val_f1m=0.4716 | LR=0.000523\n",
      "  Época  37 | train_loss=0.1208 | train_acc=0.6430 | val_loss=0.2028 | val_acc=0.4706 | val_f1m=0.4710 | LR=0.000496\n",
      "  Época  38 | train_loss=0.1234 | train_acc=0.6276 | val_loss=0.2036 | val_acc=0.4683 | val_f1m=0.4684 | LR=0.000469\n",
      "  Early stopping en época 38 (mejor val_f1m=0.4751)\n",
      "↳ Curva de entrenamiento guardada: 4clases/fold5/training_curve_fold5_4c.png\n",
      "[Fold 5/5] Global acc=0.5821 | f1_macro=0.5835\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.6728    0.6119    0.6409       420\n",
      "       right     0.6170    0.6214    0.6192       420\n",
      "  both fists     0.4957    0.5429    0.5182       420\n",
      "   both feet     0.5590    0.5524    0.5557       420\n",
      "\n",
      "    accuracy                         0.5821      1680\n",
      "   macro avg     0.5861    0.5821    0.5835      1680\n",
      "weighted avg     0.5861    0.5821    0.5835      1680\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[257  27  84  52]\n",
      " [ 21 261  68  70]\n",
      " [ 63  68 228  61]\n",
      " [ 41  67  80 232]]\n",
      "precision_macro=0.5861 | recall_macro=0.5821 | specificity_macro=0.8607 | sensitivity_macro=0.5821 | f1_weighted=0.5835\n",
      "↳ Matriz de confusión global guardada: 4clases/fold5/confusion_global_fold5_4c.png\n",
      "↳ Topomap guardado: 4clases/fold5/topomap_saliency_fold5_nb4_h2_4c.png\n",
      "↳ Topomap saliency guardado: 4clases/fold5/topomap_saliency_fold5_nb4_h2_4c.png\n",
      "↳ Topomap guardado: 4clases/fold5/topomap_pvalues_4c_fold5_nb4_h2_4c.png\n",
      "↳ Topomap p-values 4c guardado: 4clases/fold5/topomap_pvalues_4c_fold5_nb4_h2_4c.png\n",
      "↳ Calculando t-SNE 2D de CLS embeddings (global)...\n",
      "↳ t-SNE global guardado: 4clases/fold5/tsne_global_fold5_4c.png\n",
      "↳ Mapa de atención guardado: 4clases/fold5/attention_map_fold5_4c.png\n",
      "↳ Grid 4 clases (EEG+CAM) guardado: 4clases/fold5/eeg_cam_grid4_perclass_fold5_nb4_h2_4c.png\n",
      "↳ Grad-CAM++ 1D por clase guardado: 4clases/fold5/gradcampp_perclass_fold5_nb4_h2_4c.png\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.6375 | f1_macro=0.6381\n",
      "  Δ(FT-Global) = +0.0554\n",
      "Confusion matrix FT (rows=true, cols=pred):\n",
      "[[288  25  68  39]\n",
      " [ 24 284  67  45]\n",
      " [ 61  59 249  51]\n",
      " [ 40  58  72 250]]\n",
      "precision_macro=0.6399 | recall_macro=0.6375 | specificity_macro=0.8792 | sensitivity_macro=0.6375 | f1_weighted=0.6381\n",
      "↳ Matriz de confusión FT guardada: 4clases/fold5/confusion_ft_fold5_4c.png\n",
      "[Consumo] params_total=327,908 | params_trainable=327,908 | FLOPs~30.0M | latency/batch=1.77 ms (B=8)\n",
      "\n",
      "============================================================\n",
      "RESULTADOS FINALES (4 clases)\n",
      "============================================================\n",
      "Global folds (ACC): ['0.5034', '0.5658', '0.5125', '0.5476', '0.5821']\n",
      "Global mean ACC:    0.5423\n",
      "Global F1 folds (MACRO): ['0.5065', '0.5658', '0.5143', '0.5469', '0.5835']\n",
      "Global F1 mean (MACRO):  0.5434\n",
      "Global F1 folds (WEIGHTED): ['0.5065', '0.5658', '0.5143', '0.5469', '0.5835']\n",
      "Global F1 mean (WEIGHTED):  0.5434\n",
      "Global PREC mean (MACRO):   0.5487\n",
      "Global REC/SENS mean (MACRO): 0.5423\n",
      "Global SPEC mean (MACRO):   0.8474\n",
      "\n",
      "FT folds (ACC): ['0.5703', '0.6378', '0.5692', '0.6202', '0.6375']\n",
      "FT mean ACC:     0.6070\n",
      "FT F1 folds (MACRO): ['0.5721', '0.6393', '0.5690', '0.6212', '0.6381']\n",
      "FT F1 mean (MACRO):  0.6079\n",
      "FT F1 mean (WEIGHTED):  0.6079\n",
      "FT PREC mean (MACRO):   0.6112\n",
      "FT REC/SENS mean (MACRO): 0.6070\n",
      "FT SPEC mean (MACRO):   0.8690\n",
      "\n",
      "Δ(FT-Global) mean (ACC): +0.0647\n",
      "\n",
      "================ Computational metrics ================\n",
      "Model tag: nb4_h2_4c\n",
      "Phase        time (s)           memory (MB)        FLOPs (g)   params (m)\n",
      "-----------  -----------------  -----------------  ----------  ----------\n",
      "model specs  N/A                N/A                    0.030      0.328\n",
      "train         59.28 ( 6.76)    3279.5 ( 0.4)         N/A        N/A\n",
      "test          25.01 ( 0.56)     150.2 (10.5)         N/A        N/A\n",
      "latency/batch (B=8): 1.77 ms (0.01)\n",
      "↳ Topomap guardado: 4clases/topomap_pvalues_4c_mean_nb4_h2_4c.png\n",
      "↳ Topomap p-values 4c PROMEDIO guardado: 4clases/topomap_pvalues_4c_mean_nb4_h2_4c.png\n",
      "↳ Topomap guardado: 4clases/topomap_saliency_mean_nb4_h2_4c.png\n",
      "↳ Topomap saliency PROMEDIO guardado: 4clases/topomap_saliency_mean_nb4_h2_4c.png\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# LOOP 5 FOLDS + RESUMEN\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Crear directorio principal de 4 clases\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    global_metrics_folds = []\n",
    "    ft_metrics_folds = []\n",
    "    train_times, train_mems = [], []\n",
    "    test_times, test_mems = [], []\n",
    "    params_m_list = []\n",
    "    latencies_batch_ms = []\n",
    "    pvals_folds = []       \n",
    "    sfreq_folds = []    \n",
    "    saliency_folds = []\n",
    "\n",
    "    for fold in range(1, 6):\n",
    "        (mg, mft, t_tr, mem_tr, t_te, mem_te,\n",
    "         params_m, lat_ms, pvals_4c, sfreq_used, saliency_fold) = train_one_fold(fold, DEVICE)\n",
    "\n",
    "        global_metrics_folds.append(mg)\n",
    "        ft_metrics_folds.append(mft)\n",
    "        train_times.append(t_tr)\n",
    "        train_mems.append(mem_tr)\n",
    "        test_times.append(t_te)\n",
    "        test_mems.append(mem_te)\n",
    "        params_m_list.append(params_m)\n",
    "        latencies_batch_ms.append(lat_ms)\n",
    "        pvals_folds.append(pvals_4c)    \n",
    "        sfreq_folds.append(sfreq_used)   \n",
    "        saliency_folds.append(saliency_fold)\n",
    "        continue\n",
    "\n",
    "    def mean_of(metric_list, key):\n",
    "        return float(np.mean([m[key] for m in metric_list]))\n",
    "\n",
    "    print(\"\\n============================================================\")\n",
    "    print(\"RESULTADOS FINALES (4 clases)\")\n",
    "    print(\"============================================================\")\n",
    "\n",
    "    print(\"Global folds (ACC):\", [f\"{m['acc']:.4f}\" for m in global_metrics_folds])\n",
    "    print(\"Global mean ACC:   \", f\"{mean_of(global_metrics_folds, 'acc'):.4f}\")\n",
    "    print(\"Global F1 folds (MACRO):\", [f\"{m['f1_macro']:.4f}\" for m in global_metrics_folds])\n",
    "    print(\"Global F1 mean (MACRO): \", f\"{mean_of(global_metrics_folds, 'f1_macro'):.4f}\")\n",
    "    print(\"Global F1 folds (WEIGHTED):\", [f\"{m['f1_weighted']:.4f}\" for m in global_metrics_folds])\n",
    "    print(\"Global F1 mean (WEIGHTED): \", f\"{mean_of(global_metrics_folds, 'f1_weighted'):.4f}\")\n",
    "    print(\"Global PREC mean (MACRO):  \", f\"{mean_of(global_metrics_folds, 'prec_macro'):.4f}\")\n",
    "    print(\"Global REC/SENS mean (MACRO):\", f\"{mean_of(global_metrics_folds, 'rec_macro'):.4f}\")\n",
    "    print(\"Global SPEC mean (MACRO):  \", f\"{mean_of(global_metrics_folds, 'spec_macro'):.4f}\")\n",
    "\n",
    "    print(\"\\nFT folds (ACC):\", [f\"{m['acc']:.4f}\" for m in ft_metrics_folds])\n",
    "    print(\"FT mean ACC:    \", f\"{mean_of(ft_metrics_folds, 'acc'):.4f}\")\n",
    "    print(\"FT F1 folds (MACRO):\", [f\"{m['f1_macro']:.4f}\" for m in ft_metrics_folds])\n",
    "    print(\"FT F1 mean (MACRO): \", f\"{mean_of(ft_metrics_folds, 'f1_macro'):.4f}\")\n",
    "    print(\"FT F1 mean (WEIGHTED): \", f\"{mean_of(ft_metrics_folds, 'f1_weighted'):.4f}\")\n",
    "    print(\"FT PREC mean (MACRO):  \", f\"{mean_of(ft_metrics_folds, 'prec_macro'):.4f}\")\n",
    "    print(\"FT REC/SENS mean (MACRO):\", f\"{mean_of(ft_metrics_folds, 'rec_macro'):.4f}\")\n",
    "    print(\"FT SPEC mean (MACRO):  \", f\"{mean_of(ft_metrics_folds, 'spec_macro'):.4f}\")\n",
    "\n",
    "    delta_mean = mean_of(ft_metrics_folds, 'acc') - mean_of(global_metrics_folds, 'acc')\n",
    "    print(f\"\\nΔ(FT-Global) mean (ACC): {delta_mean:+.4f}\")\n",
    "\n",
    "    # ==================== Tabla de métricas computacionales ====================\n",
    "    train_time_mean = float(np.mean(train_times))\n",
    "    train_time_std  = float(np.std(train_times))\n",
    "    test_time_mean  = float(np.mean(test_times))\n",
    "    test_time_std   = float(np.std(test_times))\n",
    "\n",
    "    train_mem_mean = float(np.mean(train_mems))\n",
    "    train_mem_std  = float(np.std(train_mems))\n",
    "    test_mem_mean  = float(np.mean(test_mems))\n",
    "    test_mem_std   = float(np.std(test_mems))\n",
    "\n",
    "    params_m_mean = float(np.mean(params_m_list))\n",
    "\n",
    "    lat_mean = float(np.nanmean(latencies_batch_ms))\n",
    "    lat_std  = float(np.nanstd(latencies_batch_ms))\n",
    "\n",
    "    print(\"\\n================ Computational metrics ================\")\n",
    "    print(f\"Model tag: {MODEL_TAG}\")\n",
    "    print(\"Phase        time (s)           memory (MB)        FLOPs (g)   params (m)\")\n",
    "    print(\"-----------  -----------------  -----------------  ----------  ----------\")\n",
    "    print(f\"model specs  N/A                N/A                    {MODEL_FLOPS_G:0.3f}      {params_m_mean:0.3f}\")\n",
    "    print(f\"train        {train_time_mean:6.2f} ({train_time_std:5.2f})    \"\n",
    "          f\"{train_mem_mean:6.1f} ({train_mem_std:4.1f})         N/A        N/A\")\n",
    "    print(f\"test         {test_time_mean:6.2f} ({test_time_std:5.2f})    \"\n",
    "          f\"{test_mem_mean:6.1f} ({test_mem_std:4.1f})         N/A        N/A\")\n",
    "    print(f\"latency/batch (B=8): {lat_mean:.2f} ms ({lat_std:.2f})\")\n",
    "\n",
    "    # ==================== Topomap PROMEDIO 4 clases ====================\n",
    "    try:\n",
    "        # apilamos p-values [fold, canal] y promediamos\n",
    "        pvals_stack = np.stack(pvals_folds, axis=0)      # (5, C)\n",
    "        pvals_mean = pvals_stack.mean(axis=0)            # (C,)\n",
    "\n",
    "        pvals_mean_log = -np.log10(pvals_mean + 1e-12)\n",
    "\n",
    "        # usamos la media de sfreq (debería ser el mismo en todos)\n",
    "        sfreq_mean = float(np.mean(sfreq_folds))\n",
    "        info_topo_mean = make_mne_info_8ch(sfreq_mean)\n",
    "\n",
    "        topo_mean_png = OUTPUT_DIR / f\"topomap_pvalues_4c_mean_{MODEL_TAG}.png\"\n",
    "        plot_topomap_from_values(\n",
    "            pvals_mean_log,\n",
    "            info_topo_mean,\n",
    "            title=f\"-log10(p) task effect (4 classes) — 5-fold mean [{MODEL_TAG}]\",\n",
    "            out_path=topo_mean_png,\n",
    "            cmap=\"viridis\",\n",
    "            cbar_label=\"-log10(p)\"\n",
    "        )\n",
    "        print(f\"↳ Topomap p-values 4c PROMEDIO guardado: {topo_mean_png}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Topomap p-values 4c promedio no generado: {e}\")\n",
    "\n",
    "    # ==================== Topomap SALIENCY PROMEDIO ====================\n",
    "    try:\n",
    "        sal_stack = np.stack(saliency_folds, axis=0)   # (5, C)\n",
    "        sal_mean  = sal_stack.mean(axis=0)             # (C,)\n",
    "\n",
    "        sal_mean = sal_mean - sal_mean.min()\n",
    "        maxv = sal_mean.max()\n",
    "        if maxv > 0:\n",
    "            sal_mean = sal_mean / maxv\n",
    "\n",
    "        sfreq_mean = float(np.mean(sfreq_folds))\n",
    "        info_topo_mean = make_mne_info_8ch(sfreq_mean)\n",
    "\n",
    "        topo_sal_mean_png = OUTPUT_DIR / f\"topomap_saliency_mean_{MODEL_TAG}.png\"\n",
    "        plot_topomap_from_values(\n",
    "            sal_mean,\n",
    "            info_topo_mean,\n",
    "            title=f\"Channel importance (saliency) — 5-fold mean [{MODEL_TAG}]\",\n",
    "            out_path=topo_sal_mean_png,\n",
    "            cmap=\"Reds\",\n",
    "            cbar_label=\"Normalized saliency\",\n",
    "        )\n",
    "        print(f\"↳ Topomap saliency PROMEDIO guardado: {topo_sal_mean_png}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Topomap saliency promedio no generado: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
