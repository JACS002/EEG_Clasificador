{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b99cce37",
   "metadata": {},
   "source": [
    "# FBCSP + LDA (intra-sujeto y cross-sujeto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767badd5",
   "metadata": {},
   "source": [
    "### Bloque 1 — Rutas de salida + utilidades de logging y guardado\n",
    "\n",
    "Qué hace: define las carpetas donde se guardarán figuras, tablas y logs bajo models/fbcsp_lda/. Incluye utilidades para inicializar un logger limpio, guardar matrices de confusión (PNG + CSV) y anexar métricas a CSVs acumulativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "27eaa450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio de datos procesados: /root/Proyecto/EEG_Clasificador/data/processed\n"
     ]
    }
   ],
   "source": [
    "# %% [PATHS & LOGGING] — rutas de salida + helpers para logs/figuras/tablas\n",
    "import sys, logging, warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import mne\n",
    "\n",
    "# Raíz del repo (este notebook está en models/fbcsp_lda/)\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_PROC = PROJ / 'data' / 'processed'\n",
    "OUT_ROOT = PROJ / 'models' / 'fbcsp_lda'\n",
    "FIG_DIR  = OUT_ROOT / 'figures'\n",
    "TAB_DIR  = OUT_ROOT / 'tables'\n",
    "LOG_DIR  = OUT_ROOT / 'logs'\n",
    "for d in (FIG_DIR, TAB_DIR, LOG_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Directorio de datos procesados: {DATA_PROC}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57bbfa1",
   "metadata": {},
   "source": [
    "### Bloque 2 — FBCSP Helpers\n",
    "\n",
    "Qué hace: helpers de modelo. Extraen X/y desde Epochs, aplican Filter-Bank + CSP por bandas y entrenan/escala LDA. Mantiene tus comentarios y añade compatibilidad con versiones de CSP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d10abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [HELPERS — comunes FBCSP/LOSO/Calibración/Logging]\n",
    "# Reúne en un solo bloque:\n",
    "#  - Descubrimiento de sujetos + DROP-only\n",
    "#  - Perillas (_knobs_dict)\n",
    "#  - FBCSP helpers (banco de filtros, picks motores, z-score por época, FBCSP transform)\n",
    "#  - Clasificador (scaler + LDA)\n",
    "#  - Split de calibración\n",
    "#  - Logger\n",
    "\n",
    "# ====== IMPORTS ======\n",
    "import sys, logging, warnings, re\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from mne.decoding import CSP\n",
    "\n",
    "# ====== RUTAS y ARCHIVOS AUXILIARES ======\n",
    "# NOTA: este bloque asume que en un bloque anterior ya definiste:\n",
    "#   PROJ, DATA_PROC, LOG_DIR  (del bloque PATHS & LOGGING original)\n",
    "# Archivo opcional con sujetos a excluir (DROP-only)\n",
    "STRICT_DROP_TXT = PROJ / 'reports' / 'tables' / '02_prepro' / 'subjects_strict_DROP.txt'\n",
    "_re_sid = re.compile(r'^S\\d{3}$')\n",
    "# ====== FBCSP HELPERS ======\n",
    "# Bancos de filtros\n",
    "FB_BANDS_DENSE = [(f, f+2) for f in range(8, 30, 2)]                  # denso 8–30 por pasos de 2 Hz\n",
    "FB_BANDS_CLASSIC = [(8,12), (12,16), (16,20), (20,24), (24,28), (28,30)]\n",
    "DEFAULT_FB_BANDS = FB_BANDS_DENSE\n",
    "\n",
    "# Nº de componentes CSP por sub-banda (típico 6–8 en bancos densos)\n",
    "DEFAULT_N_CSP = 6\n",
    "\n",
    "# LDA con shrinkage automático robusto\n",
    "LDA_PARAMS = dict(solver='lsqr', shrinkage='auto')\n",
    "\n",
    "# Picks motores (si deseas limitar; tus datos ya tienen 8 canales motores)\n",
    "MOTOR_TOKENS = ['C3', 'CZ', 'C4', 'FC3', 'FC4', 'CP3', 'CPZ', 'CP4']\n",
    "\n",
    "def _list_subject_fifs(fif_dir=DATA_PROC, pattern='S???_MI-epo.fif'):\n",
    "    \"\"\"Devuelve lista ordenada de rutas a los FIF de sujetos disponibles.\"\"\"\n",
    "    return sorted(glob(str(fif_dir / pattern)))\n",
    "\n",
    "def _list_available_subjects(fif_dir=DATA_PROC):\n",
    "    \"\"\"IDs únicos SXXX disponibles en el directorio de FIF procesados.\"\"\"\n",
    "    files = _list_subject_fifs(fif_dir)\n",
    "    return sorted({Path(f).stem.split('_')[0] for f in files})\n",
    "\n",
    "def _read_drop_file(path: Path):\n",
    "    \"\"\"Lee archivo con IDs de sujetos a excluir (uno por línea, formato SXXX).\"\"\"\n",
    "    if not path.exists():\n",
    "        return set()\n",
    "    s = set()\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for ln in f:\n",
    "            sid = ln.strip().upper()\n",
    "            if _re_sid.match(sid):\n",
    "                s.add(sid)\n",
    "    return s\n",
    "\n",
    "def _strict_valid_from_drop(avail_ids):\n",
    "    \"\"\"\n",
    "    Aplica DROP-only:\n",
    "      - Lee STRICT_DROP_TXT si existe\n",
    "      - Devuelve (lista válidos, info string)\n",
    "    \"\"\"\n",
    "    drop = _read_drop_file(STRICT_DROP_TXT)\n",
    "    avail = set(avail_ids)\n",
    "    valid = sorted(avail - drop)\n",
    "    info = f\"DROP-only: {len(drop)} en DROP; válidos={len(valid)}/{len(avail)}\"\n",
    "    if not STRICT_DROP_TXT.exists():\n",
    "        info += \" (archivo DROP no encontrado → sin exclusiones)\"\n",
    "    return valid, info\n",
    "\n",
    "# Helper para registrar “perillas” (config) en logs/CSV\n",
    "def _knobs_dict(crop_window, motor_only, zscore_epoch, fb_bands, n_csp):\n",
    "    return dict(\n",
    "        crop_window=crop_window if crop_window is not None else None,\n",
    "        motor_only=bool(motor_only),\n",
    "        zscore_epoch=bool(zscore_epoch),\n",
    "        fb_bands=str(fb_bands),\n",
    "        n_csp=int(n_csp)\n",
    "    )\n",
    "\n",
    "def _epochs_to_Xy(epochs: mne.Epochs):\n",
    "    \"\"\"\n",
    "    Extrae X e y (clases string) desde Epochs respetando event_id.\n",
    "    X: (n_epochs, n_channels, n_times)\n",
    "    y: (n_epochs,) etiquetas string según mapping event_id.\n",
    "    \"\"\"\n",
    "    X = epochs.get_data()\n",
    "    inv = {v: k for k, v in epochs.event_id.items()}  # int->clase\n",
    "    y = np.array([inv[e[-1]] for e in epochs.events], dtype=object)\n",
    "    return X, y\n",
    "\n",
    "def _find_motor_chs(ch_names, tokens=MOTOR_TOKENS):\n",
    "    \"\"\"Devuelve índices de canales que contienen tokens motores (case-insensitive).\"\"\"\n",
    "    up = [c.upper() for c in ch_names]\n",
    "    picks = []\n",
    "    for tok in tokens:\n",
    "        TU = tok.upper()\n",
    "        for i, name in enumerate(up):\n",
    "            if TU in name:\n",
    "                picks.append(i); break\n",
    "    return sorted(set(picks))\n",
    "\n",
    "def _epochwise_zscore(X, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Z-score por época y canal (normaliza a lo largo del tiempo).\n",
    "    X: (n_epochs, n_channels, n_times) → mismo shape.\n",
    "    \"\"\"\n",
    "    mean = X.mean(axis=-1, keepdims=True)\n",
    "    std  = X.std(axis=-1, keepdims=True)\n",
    "    return (X - mean) / (std + eps)\n",
    "\n",
    "def _fit_fb_csp_transform(train_ep: mne.Epochs,\n",
    "                          test_ep: mne.Epochs,\n",
    "                          fb_bands=DEFAULT_FB_BANDS,\n",
    "                          n_csp=DEFAULT_N_CSP,\n",
    "                          motor_only=False,\n",
    "                          zscore_epoch=False,\n",
    "                          crop_window=None):\n",
    "    \"\"\"\n",
    "    Aplica FBCSP con opciones:\n",
    "      - crop_window=(tmin,tmax): recorta épocas\n",
    "      - motor_only=True: usa solo canales motores comunes\n",
    "      - zscore_epoch=True: z-score por época/canal antes de CSP\n",
    "      - fb_bands: lista de sub-bandas [(fmin,fmax), ...]\n",
    "    Devuelve (Xtr_fb, Xte_fb) con features concatenadas por sub-banda.\n",
    "    \"\"\"\n",
    "    tr = train_ep.copy()\n",
    "    te = test_ep.copy()\n",
    "\n",
    "    if crop_window is not None:\n",
    "        tmin, tmax = crop_window\n",
    "        tr.crop(tmin, tmax)\n",
    "        te.crop(tmin, tmax)\n",
    "\n",
    "    if motor_only:\n",
    "        picks = _find_motor_chs(tr.ch_names)\n",
    "        if picks:\n",
    "            tr.pick(picks)\n",
    "            # Alinear canales del test con los del train\n",
    "            te = te.copy().reorder_channels(tr.ch_names)\n",
    "\n",
    "    Xtr_list, Xte_list = [], []\n",
    "    y_tr = tr.events[:, -1]\n",
    "\n",
    "    for (fmin, fmax) in fb_bands:\n",
    "        tr_b = tr.copy().filter(fmin, fmax, picks='eeg', verbose=False)\n",
    "        te_b = te.copy().filter(fmin, fmax, picks='eeg', verbose=False)\n",
    "\n",
    "        Xtr = tr_b.get_data()\n",
    "        Xte = te_b.get_data()\n",
    "\n",
    "        if zscore_epoch:\n",
    "            Xtr = _epochwise_zscore(Xtr)\n",
    "            Xte = _epochwise_zscore(Xte)\n",
    "\n",
    "        try:\n",
    "            csp = CSP(n_components=n_csp, reg='ledoit_wolf', log=True, norm_trace=False)\n",
    "        except TypeError:\n",
    "            # Compatibilidad con MNE antiguos sin 'norm_trace'\n",
    "            csp = CSP(n_components=n_csp, reg='ledoit_wolf', log=True)\n",
    "\n",
    "        Xtr_c = csp.fit_transform(Xtr, y_tr)\n",
    "        Xte_c = csp.transform(Xte)\n",
    "\n",
    "        Xtr_list.append(Xtr_c)\n",
    "        Xte_list.append(Xte_c)\n",
    "\n",
    "    Xtr_fb = np.concatenate(Xtr_list, axis=1)\n",
    "    Xte_fb = np.concatenate(Xte_list,  axis=1)\n",
    "    return Xtr_fb, Xte_fb\n",
    "\n",
    "def _fit_scale_lda(Xtr, ytr, Xte, lda_params=LDA_PARAMS):\n",
    "    \"\"\"\n",
    "    Estandariza features (fit solo en train) y entrena LDA.\n",
    "    Devuelve: (yhat, clf, scaler)\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    Xtr_s = scaler.fit_transform(Xtr)\n",
    "    Xte_s = scaler.transform(Xte)\n",
    "\n",
    "    clf = LDA(**lda_params)\n",
    "    clf.fit(Xtr_s, ytr)\n",
    "    yhat = clf.predict(Xte_s)\n",
    "    return yhat, clf, scaler\n",
    "\n",
    "# ====== CALIBRACIÓN (para LOSO calibrado) ======\n",
    "def _split_calibration(ep_te, k_per_class=5):\n",
    "    \"\"\"\n",
    "    Toma k épocas por clase como calibración y el resto como evaluación.\n",
    "    Si k_per_class <= 0 → (None, ep_te).\n",
    "    \"\"\"\n",
    "    if k_per_class <= 0:\n",
    "        return None, ep_te\n",
    "    labels = ep_te.events[:, -1]\n",
    "    ep_calib_list, ep_eval_list = [], []\n",
    "    for code in np.unique(labels):\n",
    "        idx = np.where(labels == code)[0]\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "        take = min(k_per_class, len(idx))\n",
    "        sel = idx[:take]\n",
    "        rem = idx[take:]\n",
    "        if take > 0:\n",
    "            ep_calib_list.append(ep_te.copy()[sel])\n",
    "        if len(rem) > 0:\n",
    "            ep_eval_list.append(ep_te.copy()[rem])\n",
    "    ep_calib = mne.concatenate_epochs(ep_calib_list, on_mismatch='ignore') if ep_calib_list else None\n",
    "    ep_eval  = mne.concatenate_epochs(ep_eval_list,  on_mismatch='ignore') if ep_eval_list  else ep_te\n",
    "    return ep_calib, ep_eval\n",
    "\n",
    "# ====== LOGGER ======\n",
    "def _init_logger(run_name: str):\n",
    "    \"\"\"\n",
    "    Crea un logger que escribe a consola y a TXT en models/fbcsp_lda/logs/.\n",
    "    Reduce la verbosidad de MNE para que los logs sean legibles.\n",
    "    \"\"\"\n",
    "    ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_path = LOG_DIR / f\"{ts}_{run_name}.txt\"\n",
    "\n",
    "    logger = logging.getLogger(run_name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.handlers.clear()\n",
    "\n",
    "    fmt = logging.Formatter(\"[%(asctime)s] %(levelname)s: %(message)s\", datefmt=\"%H:%M:%S\")\n",
    "    ch = logging.StreamHandler(stream=sys.stdout); ch.setLevel(logging.INFO); ch.setFormatter(fmt)\n",
    "    fh = logging.FileHandler(log_path, encoding=\"utf-8\"); fh.setLevel(logging.INFO); fh.setFormatter(fmt)\n",
    "    logger.addHandler(ch); logger.addHandler(fh)\n",
    "\n",
    "    # Silenciar ruido externo (sin afectar tus prints/logs)\n",
    "    mne.set_log_level(\"ERROR\")\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"mne\")\n",
    "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"mne\")\n",
    "    return logger, log_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1f707b",
   "metadata": {},
   "source": [
    "### Bloque 3 — Inspección de datos\n",
    "\n",
    "Qué hace: muestra rutas y un listado rápido del contenido de data/ y data/processed/ para verificar que los FIF están donde esperamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0adb0212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJ: /root/Proyecto/EEG_Clasificador\n",
      "DATA: /root/Proyecto/EEG_Clasificador/data\n",
      "DATA_PROC: /root/Proyecto/EEG_Clasificador/data/processed\n",
      "\n",
      "Contenido de data (top-level):\n",
      " - processed/\n",
      " - raw/\n",
      "\n",
      "Contenido de data/processed (muestras):\n",
      " - S001_MI-epo.fif\n",
      " - S002_MI-epo.fif\n",
      " - S003_MI-epo.fif\n",
      " - S004_MI-epo.fif\n",
      " - S005_MI-epo.fif\n",
      " - S006_MI-epo.fif\n",
      " - S007_MI-epo.fif\n",
      " - S008_MI-epo.fif\n",
      " - S009_MI-epo.fif\n",
      " - S010_MI-epo.fif\n",
      " - S011_MI-epo.fif\n",
      " - S012_MI-epo.fif\n",
      " - S013_MI-epo.fif\n",
      " - S014_MI-epo.fif\n",
      " - S015_MI-epo.fif\n",
      " - S016_MI-epo.fif\n",
      " - S017_MI-epo.fif\n",
      " - S018_MI-epo.fif\n",
      " - S019_MI-epo.fif\n",
      " - S020_MI-epo.fif\n",
      " - S021_MI-epo.fif\n",
      " - S022_MI-epo.fif\n",
      " - S023_MI-epo.fif\n",
      " - S024_MI-epo.fif\n",
      " - S025_MI-epo.fif\n",
      " - S026_MI-epo.fif\n",
      " - S027_MI-epo.fif\n",
      " - S028_MI-epo.fif\n",
      " - S029_MI-epo.fif\n",
      " - S030_MI-epo.fif\n",
      " - S031_MI-epo.fif\n",
      " - S032_MI-epo.fif\n",
      " - S033_MI-epo.fif\n",
      " - S034_MI-epo.fif\n",
      " - S035_MI-epo.fif\n",
      " - S036_MI-epo.fif\n",
      " - S037_MI-epo.fif\n",
      " - S039_MI-epo.fif\n",
      " - S040_MI-epo.fif\n",
      " - S041_MI-epo.fif\n",
      " - S042_MI-epo.fif\n",
      " - S043_MI-epo.fif\n",
      " - S044_MI-epo.fif\n",
      " - S045_MI-epo.fif\n",
      " - S046_MI-epo.fif\n",
      " - S047_MI-epo.fif\n",
      " - S048_MI-epo.fif\n",
      " - S049_MI-epo.fif\n",
      " - S050_MI-epo.fif\n",
      " - S051_MI-epo.fif\n"
     ]
    }
   ],
   "source": [
    "# %% [Inspect data folders]\n",
    "# Celda añadida automáticamente: muestra rutas y lista contenidos de data y data/processed\n",
    "try:\n",
    "    print(f\"PROJ: {PROJ}\")\n",
    "    print(f\"DATA: {PROJ / 'data'}\")\n",
    "    print(f\"DATA_PROC: {DATA_PROC}\")\n",
    "    print('\\nContenido de data (top-level):')\n",
    "    data_dir = PROJ / 'data'\n",
    "    if data_dir.exists():\n",
    "        for p in sorted(data_dir.iterdir()):\n",
    "            print(f\" - {p.name}{'/' if p.is_dir() else ''}\")\n",
    "    else:\n",
    "        print('  (no existe)')\n",
    "\n",
    "    print('\\nContenido de data/processed (muestras):')\n",
    "    if DATA_PROC.exists():\n",
    "        for p in sorted(DATA_PROC.glob('*'))[:50]:\n",
    "            print(f\" - {p.name}\")\n",
    "    else:\n",
    "        print('  (no existe)')\n",
    "except Exception as e:\n",
    "    print('Error inspeccionando data:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd66c27b",
   "metadata": {},
   "source": [
    "### Bloque 4 — Intra-sujeto (k-Fold CV) con logs + guardado\n",
    "\n",
    "Qué hace: ejecuta CV por sujeto con FBCSP+LDA, imprime métricas limpias, guarda matriz de confusión (PNG/CSV), y escribe métricas en tables/metrics_intra.csv. Además genera un TXT en logs/ con el detalle de la corrida.\n",
    "\n",
    "Añade argumentos crop_window, motor_only, zscore_epoch, fb_bands, n_csp. Guarda lo de siempre (matriz, métricas, log) pero ahora puedes probar rápidamente ventanas/picks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f44cfa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [INTRA — todos los sujetos, con timestamp y artefactos consolidados + fila GLOBAL]\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from math import ceil\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "def _discover_subject_ids(fif_dir=DATA_PROC, pattern='S???_MI-epo.fif'):\n",
    "    files = sorted(glob(str(fif_dir / pattern)))\n",
    "    return [Path(f).stem.split('_')[0] for f in files]\n",
    "\n",
    "\n",
    "def run_intra_all(\n",
    "    fif_dir=DATA_PROC,\n",
    "    k=5,\n",
    "    random_state=42,\n",
    "    crop_window=(0.5, 3.5),\n",
    "    motor_only=True,\n",
    "    zscore_epoch=True,\n",
    "    fb_bands=DEFAULT_FB_BANDS,\n",
    "    n_csp=DEFAULT_N_CSP,\n",
    "    max_subplots_per_fig=12,\n",
    "    n_cols=4,\n",
    "    save_txt_name=None,            # opcional: nombre base TXT (se antepone timestamp)\n",
    "    save_csv_name=None             # opcional: nombre base CSV (se antepone timestamp)\n",
    "):\n",
    "    \"\"\"\n",
    "    Ejecuta INTRA (k-fold) en TODOS los sujetos.\n",
    "    Guarda: 1 log con timestamp, 1 CSV y 1 TXT (ambos con fila GLOBAL),\n",
    "    y mosaicos de matrices de confusión por sujeto (sin archivos por sujeto).\n",
    "    \"\"\"\n",
    "    ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    run_tag = f\"intra_all_{ts}\"\n",
    "\n",
    "    # Log global\n",
    "    logger, log_path = _init_logger(run_name=run_tag)\n",
    "    logger.info(f\"[RUN {run_tag}] Inicio de ejecución INTRA\")\n",
    "    logger.info(f\"Parámetros: k={k}, crop_window={crop_window}, motor_only={motor_only}, \"\n",
    "                f\"zscore_epoch={zscore_epoch}, n_csp={n_csp}, fb_bands={len(fb_bands)}\")\n",
    "\n",
    "    subject_ids = _discover_subject_ids(fif_dir)\n",
    "    if not subject_ids:\n",
    "        print(\"No se encontraron sujetos en\", fif_dir)\n",
    "        return None\n",
    "\n",
    "    logger.info(f\"Sujetos detectados: {subject_ids}\")\n",
    "    print(f\"[INTRA ALL] sujetos detectados: {subject_ids}\")\n",
    "\n",
    "    rows_summary = []\n",
    "    cm_items = []\n",
    "\n",
    "    for subject_id in subject_ids:\n",
    "        fif_path = fif_dir / f\"{subject_id}_MI-epo.fif\"\n",
    "        epochs = mne.read_epochs(fif_path, preload=True, verbose=False)\n",
    "\n",
    "        _, y_str = _epochs_to_Xy(epochs)\n",
    "        le = LabelEncoder(); y = le.fit_transform(y_str)\n",
    "        classes = list(le.classes_)\n",
    "\n",
    "        logger.info(f\"== {subject_id} | n_epochs={len(y)} | clases={classes} | sfreq={epochs.info['sfreq']}\")\n",
    "        skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=random_state)\n",
    "        accs, f1s = [], []\n",
    "        cm_sum = np.zeros((len(classes), len(classes)), dtype=int)\n",
    "\n",
    "        for fold, (tr_idx, te_idx) in enumerate(skf.split(np.zeros(len(y)), y), start=1):\n",
    "            ep_tr = epochs[tr_idx]; ep_te = epochs[te_idx]\n",
    "            with mne.utils.use_log_level(\"ERROR\"):\n",
    "                Xtr_fb, Xte_fb = _fit_fb_csp_transform(\n",
    "                    ep_tr, ep_te,\n",
    "                    fb_bands=fb_bands,\n",
    "                    n_csp=n_csp,\n",
    "                    motor_only=motor_only,\n",
    "                    zscore_epoch=zscore_epoch,\n",
    "                    crop_window=crop_window\n",
    "                )\n",
    "            yhat, clf, scaler = _fit_scale_lda(Xtr_fb, y[tr_idx], Xte_fb)\n",
    "\n",
    "            acc = accuracy_score(y[te_idx], yhat)\n",
    "            f1m = f1_score(y[te_idx], yhat, average='macro')\n",
    "            cm_sum += confusion_matrix(y[te_idx], yhat, labels=np.arange(len(classes)))\n",
    "            accs.append(acc); f1s.append(f1m)\n",
    "            logger.info(f\"[{subject_id} | fold {fold}] acc={acc:.3f} | f1m={f1m:.3f}\")\n",
    "\n",
    "        acc_mu, acc_sd = float(np.mean(accs)), float(np.std(accs))\n",
    "        f1_mu,  f1_sd  = float(np.mean(f1s)),  float(np.std(f1s))\n",
    "\n",
    "        logger.info(f\"[{subject_id}] ACC={acc_mu:.3f}±{acc_sd:.3f} | F1m={f1_mu:.3f}±{f1_sd:.3f}\")\n",
    "        rows_summary.append(dict(\n",
    "            subject=subject_id,\n",
    "            acc_mean=acc_mu,\n",
    "            f1_macro_mean=f1_mu,\n",
    "            k=k,\n",
    "            n_classes=len(classes),\n",
    "            crop=str(crop_window),\n",
    "            motor_only=bool(motor_only),\n",
    "            zscore_epoch=bool(zscore_epoch),\n",
    "            n_csp=int(n_csp),\n",
    "            fb_bands=len(fb_bands)\n",
    "        ))\n",
    "        cm_items.append((subject_id, cm_sum, classes))\n",
    "\n",
    "    # --- CSV/TXT (con fila GLOBAL)\n",
    "    df = pd.DataFrame(rows_summary).sort_values(\"subject\")\n",
    "\n",
    "    acc_mu = float(df['acc_mean'].mean()) if not df.empty else 0.0\n",
    "    acc_sd = float(df['acc_mean'].std(ddof=0)) if not df.empty else 0.0\n",
    "    f1_mu  = float(df['f1_macro_mean'].mean()) if not df.empty else 0.0\n",
    "    f1_sd  = float(df['f1_macro_mean'].std(ddof=0)) if not df.empty else 0.0\n",
    "\n",
    "    df_global = pd.DataFrame([{\n",
    "        'subject': 'GLOBAL',\n",
    "        'acc_mean': acc_mu,\n",
    "        'f1_macro_mean': f1_mu,\n",
    "        'k': k,\n",
    "        'n_classes': int(df['n_classes'].mean()) if 'n_classes' in df.columns and not df.empty else 0,\n",
    "        'crop': str(crop_window),\n",
    "        'motor_only': bool(motor_only),\n",
    "        'zscore_epoch': bool(zscore_epoch),\n",
    "        'n_csp': int(n_csp),\n",
    "        'fb_bands': len(fb_bands)\n",
    "    }])\n",
    "    df_out = pd.concat([df, df_global], ignore_index=True)\n",
    "\n",
    "    out_csv = (TAB_DIR / f\"{ts}_{save_csv_name}\") if save_csv_name else (TAB_DIR / f\"metrics_intra_all_{ts}.csv\")\n",
    "    df_out.to_csv(out_csv, index=False)\n",
    "    logger.info(f\"Resumen CSV guardado → {out_csv}\")\n",
    "    print(\"Resumen INTRA (todos) →\", out_csv)\n",
    "\n",
    "    logger.info(f\"[GLOBAL INTRA] ACC={acc_mu:.3f}±{acc_sd:.3f} | F1m={f1_mu:.3f}±{f1_sd:.3f}\")\n",
    "    print(f\"[GLOBAL INTRA] ACC={acc_mu:.3f}±{acc_sd:.3f} | F1m={f1_mu:.3f}±{f1_sd:.3f}\")\n",
    "\n",
    "    try:\n",
    "        display(df_out)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    out_txt = (LOG_DIR / f\"{ts}_{save_txt_name}\") if save_txt_name else (LOG_DIR / f\"metrics_intra_all_{ts}.txt\")\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"INTRA-SUJETO (k-fold) — Métricas por sujeto (incluye GLOBAL)\\n\")\n",
    "        f.write(f\"Generado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Total filas: {len(df_out)}\\n\\n\")\n",
    "        header = df_out.columns.tolist()\n",
    "        f.write(\" | \".join(header) + \"\\n\")\n",
    "        f.write(\"-\" * 90 + \"\\n\")\n",
    "        for _, row in df_out.iterrows():\n",
    "            vals = []\n",
    "            for kcol in header:\n",
    "                v = row[kcol]\n",
    "                vals.append(f\"{v:.4f}\" if isinstance(v, float) else str(int(v)) if isinstance(v, (np.integer,)) else str(v))\n",
    "            f.write(\" | \".join(vals) + \"\\n\")\n",
    "    logger.info(f\"Métricas TXT guardadas → {out_txt}\")\n",
    "    print(\"Métricas TXT guardadas →\", out_txt)\n",
    "\n",
    "    # --- Mosaicos de matrices de confusión\n",
    "    if cm_items:\n",
    "        n = len(cm_items)\n",
    "        per_fig = max(1, int(max_subplots_per_fig))\n",
    "        n_figs = ceil(n / per_fig)\n",
    "        n_rows_per_fig = lambda count: ceil(count / n_cols)\n",
    "\n",
    "        for fig_idx in range(n_figs):\n",
    "            start = fig_idx * per_fig\n",
    "            end   = min((fig_idx + 1) * per_fig, n)\n",
    "            chunk = cm_items[start:end]\n",
    "            count = len(chunk)\n",
    "            n_rows = n_rows_per_fig(count)\n",
    "\n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(4.5*n_cols, 3.8*n_rows), dpi=140)\n",
    "            axes = np.atleast_2d(axes).flatten()\n",
    "\n",
    "            for ax_i, (sid, cm_sum, classes) in enumerate(chunk):\n",
    "                ax = axes[ax_i]\n",
    "                disp = ConfusionMatrixDisplay(cm_sum, display_labels=classes)\n",
    "                disp.plot(ax=ax, cmap=\"Blues\", colorbar=False, values_format='d')\n",
    "                ax.set_title(f\"{sid}\")\n",
    "                ax.set_xlabel(\"\"); ax.set_ylabel(\"\")\n",
    "            for j in range(ax_i + 1, len(axes)):\n",
    "                axes[j].axis(\"off\")\n",
    "\n",
    "            out_png = FIG_DIR / f\"intra_all_confusions_{ts}_p{fig_idx+1}.png\"\n",
    "            fig.suptitle(f\"Intra — Matrices de confusión (página {fig_idx+1}/{n_figs})\", y=0.995, fontsize=14)\n",
    "            fig.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "            fig.savefig(out_png)\n",
    "            plt.close(fig)\n",
    "            logger.info(f\"Figura consolidada → {out_png}\")\n",
    "            print(\"Figura consolidada →\", out_png)\n",
    "\n",
    "    logger.info(f\"Log global de esta corrida → {log_path}\")\n",
    "    print(f\"Log global → {log_path}\")\n",
    "\n",
    "    return df_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad74f20",
   "metadata": {},
   "source": [
    "### Bloque 5 — Cross-sujeto (LOSO) con logs + guardado\n",
    "\n",
    "Qué hace: para cada sujeto como test, entrena en el resto, calcula métricas y guarda una matriz de confusión por sujeto y una global. También guarda métricas por sujeto en tables/metrics_loso_per_subject.csv y un resumen global en tables/metrics_loso.csv, además de un TXT en logs/\n",
    "\n",
    "- run_loso(..., use_strict=True) hace LOSO clásico sobre el conjunto resuelto (con Strict si está ON).\n",
    "\n",
    "- run_loso_single(test_subject, ...) entrena con todos los demás y prueba sólo en ese sujeto (Strict opcional).\n",
    "\n",
    "- Incluye utilidades de selección Strict y reemplazo para subject_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9122d415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [LOSO — todos los sujetos, con timestamp, fila GLOBAL y calibración opcional]\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def run_loso_all(\n",
    "    fif_dir=DATA_PROC,\n",
    "    use_strict=True,                # True → aplica DROP-only si existe el txt\n",
    "    crop_window=(0.5, 3.5),\n",
    "    motor_only=True,\n",
    "    zscore_epoch=True,\n",
    "    fb_bands=DEFAULT_FB_BANDS,\n",
    "    n_csp=DEFAULT_N_CSP,\n",
    "    max_subplots_per_fig=12,        # sujetos por página en los mosaicos\n",
    "    n_cols=4,                       # columnas por página\n",
    "    save_txt_name=None,             # opcional: nombre base TXT (se antepone timestamp)\n",
    "    save_csv_name=None,             # opcional: nombre base CSV (se antepone timestamp)\n",
    "    calibrate_k_per_class=None      # NUEVO: None/<=0 → sin calibración; >0 → calibración con k por clase\n",
    "):\n",
    "    \"\"\"\n",
    "    LOSO clásico en TODOS los sujetos, con opción de calibración ligera.\n",
    "\n",
    "    - Sin calibración (por defecto): entrena en sujetos train y evalúa todo el sujeto test.\n",
    "    - Con calibración (calibrate_k_per_class > 0): toma k épocas por clase del sujeto test para\n",
    "      recalibrar solo StandardScaler + LDA y evalúa en el resto del sujeto test.\n",
    "\n",
    "    Artefactos:\n",
    "      - 1 log (timestamp), 1 CSV (con fila GLOBAL), 1 TXT (con fila GLOBAL)\n",
    "      - Mosaicos de matrices por sujeto y matriz GLOBAL.\n",
    "    \"\"\"\n",
    "    ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    run_tag = f\"loso_all_{ts}\"\n",
    "\n",
    "    logger, log_path = _init_logger(run_name=run_tag)\n",
    "    knobs = _knobs_dict(crop_window, motor_only, zscore_epoch, fb_bands, n_csp)\n",
    "    knobs['calibrate_k'] = int(calibrate_k_per_class or 0)\n",
    "    logger.info(f\"[RUN {run_tag}] Inicio de ejecución LOSO\")\n",
    "    logger.info(f\"Perillas: {knobs}\")\n",
    "\n",
    "    avail = _list_available_subjects(fif_dir)\n",
    "    if not avail:\n",
    "        logger.error(f\"No hay FIF S???_MI-epo.fif en {fif_dir}\")\n",
    "        return None\n",
    "\n",
    "    if use_strict:\n",
    "        sids, info = _strict_valid_from_drop(avail)\n",
    "        logger.info(f\"Usando todos los válidos ({len(sids)}). {info}\")\n",
    "    else:\n",
    "        sids = avail\n",
    "        logger.info(f\"(strict=OFF) tests → {sids[:10]}{' ...' if len(sids)>10 else ''}\")\n",
    "\n",
    "    if not sids:\n",
    "        logger.warning(\"No hay sujetos para LOSO.\")\n",
    "        return None\n",
    "\n",
    "    ep_map = {sid: mne.read_epochs(str(fif_dir / f\"{sid}_MI-epo.fif\"),\n",
    "                                   preload=True, verbose=False) for sid in sids}\n",
    "\n",
    "    classes_global, cm_global = None, None\n",
    "    rows, cm_items = [], []\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # Bucle LOSO\n",
    "    for s_test, ep_te_full in ep_map.items():\n",
    "        train_ids = [sid for sid in sids if sid != s_test]\n",
    "        if not train_ids:\n",
    "            logger.warning(f\"Sin train para {s_test}.\")\n",
    "            continue\n",
    "\n",
    "        ep_tr = mne.concatenate_epochs([ep_map[s] for s in train_ids], on_mismatch='ignore')\n",
    "        ep_te_full = ep_te_full.copy().reorder_channels(ep_tr.ch_names)\n",
    "\n",
    "        # Etiquetas para train + (test/calib/eval)\n",
    "        _, y_tr_str = _epochs_to_Xy(ep_tr)\n",
    "\n",
    "        # ¿Calibración?\n",
    "        kcal = int(calibrate_k_per_class or 0)\n",
    "        if kcal > 0:\n",
    "            # Divide test en calibración y evaluación\n",
    "            ep_calib, ep_eval = _split_calibration(ep_te_full, k_per_class=kcal)\n",
    "            if (ep_calib is None) or (len(ep_calib) == 0):\n",
    "                logger.info(f\"[{s_test}] Calibración solicitada (k={kcal}), pero no hay muestras válidas → sin calibración.\")\n",
    "                ep_eval = ep_te_full\n",
    "                kcal = 0  # desactiva calibración\n",
    "        else:\n",
    "            ep_calib, ep_eval = None, ep_te_full\n",
    "\n",
    "        # Ajusta codificador con TRAIN + EVAL (y CALIB si existe, aunque no es imprescindible)\n",
    "        _, y_ev_str = _epochs_to_Xy(ep_eval)\n",
    "        if ep_calib is not None and len(ep_calib) > 0:\n",
    "            _, y_ca_str = _epochs_to_Xy(ep_calib)\n",
    "            le = LabelEncoder().fit(np.concatenate([y_tr_str, y_ev_str, y_ca_str]))\n",
    "        else:\n",
    "            le = LabelEncoder().fit(np.concatenate([y_tr_str, y_ev_str]))\n",
    "\n",
    "        y_tr = le.transform(y_tr_str)\n",
    "        y_ev = le.transform(y_ev_str)\n",
    "        classes = list(le.classes_)\n",
    "\n",
    "        if classes_global is None:\n",
    "            classes_global = classes\n",
    "            cm_global = np.zeros((len(classes), len(classes)), dtype=int)\n",
    "\n",
    "        # --- Features para TRAIN + EVAL\n",
    "        with mne.utils.use_log_level(\"ERROR\"):\n",
    "            Xtr_fb, Xev_fb = _fit_fb_csp_transform(\n",
    "                ep_tr, ep_eval,\n",
    "                fb_bands=fb_bands,\n",
    "                n_csp=n_csp,\n",
    "                motor_only=motor_only,\n",
    "                zscore_epoch=zscore_epoch,\n",
    "                crop_window=crop_window\n",
    "            )\n",
    "\n",
    "        if kcal <= 0:\n",
    "            # LOSO clásico\n",
    "            yhat, clf, scaler = _fit_scale_lda(Xtr_fb, y_tr, Xev_fb)\n",
    "        else:\n",
    "            # --- Features para CALIBRACIÓN (mismos filtros CSP)\n",
    "            with mne.utils.use_log_level(\"ERROR\"):\n",
    "                Xtr_fb2, Xca_fb = _fit_fb_csp_transform(\n",
    "                    ep_tr, ep_calib,\n",
    "                    fb_bands=fb_bands,\n",
    "                    n_csp=n_csp,\n",
    "                    motor_only=motor_only,\n",
    "                    zscore_epoch=zscore_epoch,\n",
    "                    crop_window=crop_window\n",
    "                )\n",
    "\n",
    "            _, y_ca_str = _epochs_to_Xy(ep_calib)\n",
    "            y_ca = le.transform(y_ca_str)\n",
    "\n",
    "            # Re-ajuste SOLO de scaler + LDA con TRAIN + CALIB\n",
    "            scaler2 = StandardScaler()\n",
    "            X_join = np.vstack([Xtr_fb2, Xca_fb])\n",
    "            y_join = np.concatenate([y_tr, y_ca])\n",
    "            X_join_s = scaler2.fit_transform(X_join)\n",
    "            Xev_s = scaler2.transform(Xev_fb)\n",
    "\n",
    "            clf2 = LDA(**LDA_PARAMS)\n",
    "            clf2.fit(X_join_s, y_join)\n",
    "            yhat = clf2.predict(Xev_s)\n",
    "\n",
    "        # Métricas por sujeto (en EVAL)\n",
    "        acc = accuracy_score(y_ev, yhat)\n",
    "        f1m = f1_score(y_ev, yhat, average='macro')\n",
    "        cm  = confusion_matrix(y_ev, yhat, labels=np.arange(len(classes)))\n",
    "        cm_global += cm\n",
    "\n",
    "        logger.info(f\"[LOSO] test={s_test} | acc={acc:.3f} | f1m={f1m:.3f} | \"\n",
    "                    f\"n_test={len(y_ev)} | calib_k={kcal}\")\n",
    "\n",
    "        rows.append(dict(\n",
    "            test_subject=s_test,\n",
    "            acc=float(acc),\n",
    "            f1_macro=float(f1m),\n",
    "            n_test=int(len(y_ev)),\n",
    "            crop=str(crop_window),\n",
    "            motor_only=bool(motor_only),\n",
    "            zscore_epoch=bool(zscore_epoch),\n",
    "            n_csp=int(n_csp),\n",
    "            fb_bands=len(fb_bands),\n",
    "            calibrate_k=int(kcal)\n",
    "        ))\n",
    "        cm_items.append((s_test, cm, classes))\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # Fila GLOBAL\n",
    "    acc_mu = float(np.mean([r['acc'] for r in rows])) if rows else 0.0\n",
    "    f1_mu  = float(np.mean([r['f1_macro'] for r in rows])) if rows else 0.0\n",
    "    rows.append(dict(\n",
    "        test_subject=\"GLOBAL\",\n",
    "        acc=acc_mu,\n",
    "        f1_macro=f1_mu,\n",
    "        n_test=int(np.sum([r['n_test'] for r in rows])) if rows else 0,\n",
    "        crop=str(crop_window),\n",
    "        motor_only=bool(motor_only),\n",
    "        zscore_epoch=bool(zscore_epoch),\n",
    "        n_csp=int(n_csp),\n",
    "        fb_bands=len(fb_bands),\n",
    "        calibrate_k=int(calibrate_k_per_class or 0)\n",
    "    ))\n",
    "    logger.info(f\"[GLOBAL LOSO] ACC={acc_mu:.3f} | F1m={f1_mu:.3f}\")\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # CSV único\n",
    "    df_rows = pd.DataFrame(rows).sort_values('test_subject')\n",
    "    out_csv = (TAB_DIR / f\"{ts}_{save_csv_name}\") if save_csv_name else (TAB_DIR / f\"metrics_loso_all_{ts}.csv\")\n",
    "    df_rows.to_csv(out_csv, index=False)\n",
    "    logger.info(f\"CSV consolidado → {out_csv}\")\n",
    "    print(\"CSV consolidado →\", out_csv)\n",
    "    try:\n",
    "        display(df_rows)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # TXT único\n",
    "    out_txt = (LOG_DIR / f\"{ts}_{save_txt_name}\") if save_txt_name else (LOG_DIR / f\"metrics_loso_all_{ts}.txt\")\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"LOSO — Métricas por sujeto (incluye GLOBAL)\\n\")\n",
    "        f.write(f\"Generado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Total filas: {len(df_rows)}\\n\\n\")\n",
    "        header = df_rows.columns.tolist()\n",
    "        f.write(\" | \".join(header) + \"\\n\")\n",
    "        f.write(\"-\" * 90 + \"\\n\")\n",
    "        for _, r in df_rows.iterrows():\n",
    "            vals = []\n",
    "            for k in header:\n",
    "                v = r[k]\n",
    "                vals.append(f\"{v:.4f}\" if isinstance(v, float) else str(int(v)) if isinstance(v, (np.integer,)) else str(v))\n",
    "            f.write(\" | \".join(vals) + \"\\n\")\n",
    "    logger.info(f\"TXT consolidado → {out_txt}\")\n",
    "    print(\"TXT consolidado →\", out_txt)\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # Mosaicos por sujeto\n",
    "    if cm_items:\n",
    "        n = len(cm_items)\n",
    "        per_fig = max(1, int(max_subplots_per_fig))\n",
    "        n_figs = ceil(n / per_fig)\n",
    "        n_cols = max(1, int(n_cols))\n",
    "        n_rows_needed = lambda count: ceil(count / n_cols)\n",
    "\n",
    "        for fig_idx in range(n_figs):\n",
    "            start = fig_idx * per_fig\n",
    "            end   = min((fig_idx + 1) * per_fig, n)\n",
    "            chunk = cm_items[start:end]\n",
    "            count = len(chunk)\n",
    "            n_rows = n_rows_needed(count)\n",
    "\n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(4.5*n_cols, 3.8*n_rows), dpi=140)\n",
    "            axes = np.atleast_2d(axes).flatten()\n",
    "\n",
    "            for ax_i, (sid, cm_sum, classes) in enumerate(chunk):\n",
    "                ax = axes[ax_i]\n",
    "                disp = ConfusionMatrixDisplay(cm_sum, display_labels=classes)\n",
    "                disp.plot(ax=ax, cmap=\"Blues\", colorbar=False, values_format='d')\n",
    "                ax.set_title(f\"{sid}\")\n",
    "                ax.set_xlabel(\"\"); ax.set_ylabel(\"\")\n",
    "            for j in range(ax_i + 1, len(axes)):\n",
    "                axes[j].axis(\"off\")\n",
    "\n",
    "            out_png = FIG_DIR / f\"loso_all_confusions_{ts}_p{fig_idx+1}.png\"\n",
    "            fig.suptitle(f\"LOSO — Matrices de confusión por sujeto (página {fig_idx+1}/{n_figs})\", y=0.995, fontsize=14)\n",
    "            fig.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "            fig.savefig(out_png)\n",
    "            plt.close(fig)\n",
    "            logger.info(f\"Mosaico de confusiones → {out_png}\")\n",
    "            print(\"Mosaico de confusiones →\", out_png)\n",
    "\n",
    "    # Matriz GLOBAL\n",
    "    if len(cm_items) > 0 and \"GLOBAL\" in df_rows[\"test_subject\"].values:\n",
    "        fig, ax = plt.subplots(figsize=(6.5, 5.2), dpi=140)\n",
    "        disp = ConfusionMatrixDisplay(cm_global, display_labels=classes_global)\n",
    "        disp.plot(ax=ax, cmap=\"Blues\", colorbar=True, values_format='d')\n",
    "        ax.set_title(\"LOSO — Matriz de confusión GLOBAL\")\n",
    "        fig.tight_layout()\n",
    "        out_png_glob = FIG_DIR / f\"loso_global_confusion_{ts}.png\"\n",
    "        fig.savefig(out_png_glob)\n",
    "        plt.close(fig)\n",
    "        logger.info(f\"Matriz GLOBAL → {out_png_glob}\")\n",
    "        print(\"Matriz GLOBAL →\", out_png_glob)\n",
    "\n",
    "    # Print/Log global final\n",
    "    logger.info(f\"[GLOBAL LOSO] ACC={acc_mu:.3f} | F1m={f1_mu:.3f}\")\n",
    "    print(f\"[GLOBAL LOSO] ACC={acc_mu:.3f} | F1m={f1_mu:.3f}\")\n",
    "    logger.info(f\"Log global de esta corrida → {log_path}\")\n",
    "    print(f\"Log global → {log_path}\")\n",
    "\n",
    "    return df_rows.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce94a18",
   "metadata": {},
   "source": [
    "### Bloque 7 — Ejemplos de ejecución\n",
    "\n",
    "Qué hace: muestra cómo lanzar los “batch” por defecto (4 intra, 2 LOSO) y cómo pasar propia lista de sujetos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bd06edc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTRA en todos los sujetos\n",
    "# df_intra = run_intra_all(\n",
    "#     k=5,\n",
    "#     random_state=42,\n",
    "#     crop_window=(0.5, 4.5),\n",
    "#     motor_only=False,\n",
    "#     zscore_epoch=True,\n",
    "#     fb_bands=FB_BANDS_DENSE,\n",
    "#     n_csp=4,\n",
    "#     save_txt_name=\"metrics_intra_all.txt\"\n",
    "# )\n",
    "\n",
    "# LOSO clásico en todos los sujetos\n",
    "# df_loso = run_loso_all(\n",
    "#     crop_window=(0.5, 4.5),\n",
    "#     motor_only=False,\n",
    "#     zscore_epoch=False,\n",
    "#     fb_bands=FB_BANDS_DENSE,\n",
    "#     n_csp=4,\n",
    "#     use_strict=False,\n",
    "#     save_txt_name=\"metrics_loso_all.txt\",\n",
    "#     calibrate_k_per_class=5  # None o numero entero > 0 para calibración ligera\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75148a5",
   "metadata": {},
   "source": [
    "### Inter sujeto con Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "64bc106e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN=63 | VAL=16 | TEST(fijo)=24\n",
      "[VAL] ACC=0.340 | F1m=0.340\n",
      "[TEST CALIB-LDA] k=10 | ACC=0.410 | F1m=0.407\n",
      "Confusión TEST → /root/Proyecto/EEG_Clasificador/models/inter_fixedsplit/figures/inter_fbcsp_confusion_test_20251006-062822.png\n",
      "CSV → /root/Proyecto/EEG_Clasificador/models/inter_fixedsplit/tables/20251006-062822_inter_fbcsp_fixedsplit.csv\n",
      "TXT → /root/Proyecto/EEG_Clasificador/models/inter_fixedsplit/logs/20251006-062822_inter_fbcsp_fixedsplit.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-dd63e71a-5d2b-4716-9508-eb3b99b8e558\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mode</th>\n",
       "      <th>acc_val</th>\n",
       "      <th>f1_val</th>\n",
       "      <th>acc_test</th>\n",
       "      <th>f1_test</th>\n",
       "      <th>n_train</th>\n",
       "      <th>n_val</th>\n",
       "      <th>n_test</th>\n",
       "      <th>crop</th>\n",
       "      <th>n_csp</th>\n",
       "      <th>fb_bands</th>\n",
       "      <th>refit_on_trainval_for_test</th>\n",
       "      <th>calibrate_k_per_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inter_fixedsplit_fbcsp</td>\n",
       "      <td>0.340161</td>\n",
       "      <td>0.340162</td>\n",
       "      <td>0.409885</td>\n",
       "      <td>0.406748</td>\n",
       "      <td>63</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>(0.5, 3.0)</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd63e71a-5d2b-4716-9508-eb3b99b8e558')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-dd63e71a-5d2b-4716-9508-eb3b99b8e558 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-dd63e71a-5d2b-4716-9508-eb3b99b8e558');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                     mode   acc_val    f1_val  acc_test   f1_test  n_train  \\\n",
       "0  inter_fixedsplit_fbcsp  0.340161  0.340162  0.409885  0.406748       63   \n",
       "\n",
       "   n_val  n_test        crop  n_csp  fb_bands  refit_on_trainval_for_test  \\\n",
       "0     16      24  (0.5, 3.0)      4         6                        True   \n",
       "\n",
       "   calibrate_k_per_class  \n",
       "0                     10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% [INTER-SUJETO — FBCSP+LDA con TEST fijo, TRAIN/VAL aleatorios y CALIBRACIÓN opcional (LDA-only)]\n",
    "# Requiere:\n",
    "#   - Archivos preprocesados S???_MI-epo.fif en DATA_PROC\n",
    "#   - Paquetes: mne, numpy, pandas, scikit-learn, matplotlib\n",
    "\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from mne.decoding import CSP\n",
    "import mne, os, sys\n",
    "\n",
    "# -------------------- RUTAS --------------------\n",
    "PROJ      = Path('..').resolve().parent     # asumiendo notebook en models/...\n",
    "DATA_PROC = PROJ / 'data' / 'processed'\n",
    "OUT_ROOT  = PROJ / 'models' / 'inter_fixedsplit'\n",
    "FIG_DIR   = OUT_ROOT / 'figures'\n",
    "TAB_DIR   = OUT_ROOT / 'tables'\n",
    "LOG_DIR   = OUT_ROOT / 'logs'\n",
    "for d in (FIG_DIR, TAB_DIR, LOG_DIR): d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------- CONFIG FBCSP --------------------\n",
    "FB_BANDS_DENSE = [(f, f+2) for f in range(8, 30, 2)]  # 8–30 Hz, paso 2 Hz\n",
    "DEFAULT_N_CSP  = 6\n",
    "LDA_PARAMS     = dict(solver='lsqr', shrinkage='auto')  # shrinkage automático ayuda inter-sujeto\n",
    "\n",
    "# -------------------- UTILS --------------------\n",
    "def _discover_subject_ids(fif_dir=DATA_PROC, pattern='S???_MI-epo.fif'):\n",
    "    files = sorted(glob(str(Path(fif_dir) / pattern)))\n",
    "    return [Path(f).stem.split('_')[0] for f in files]\n",
    "\n",
    "def _epochs_to_Xy(epochs: mne.Epochs):\n",
    "    X = epochs.get_data()\n",
    "    inv = {v:k for k,v in epochs.event_id.items()}\n",
    "    y = np.array([inv[e[-1]] for e in epochs.events], dtype=object)\n",
    "    return X, y\n",
    "\n",
    "def _fit_fb_csp_transform(train_ep: mne.Epochs,\n",
    "                          test_ep:  mne.Epochs,\n",
    "                          fb_bands=FB_BANDS_DENSE,\n",
    "                          n_csp=DEFAULT_N_CSP,\n",
    "                          crop_window=None):\n",
    "    \"\"\"\n",
    "    Ajusta CSP por sub-banda en TRAIN y transforma TRAIN/TEST. Devuelve features log-var concatenadas.\n",
    "    \"\"\"\n",
    "    tr = train_ep.copy(); te = test_ep.copy()\n",
    "    if crop_window is not None:\n",
    "        tr.crop(*crop_window); te.crop(*crop_window)\n",
    "    y_tr = tr.events[:, -1]\n",
    "    Xtr_list, Xte_list = [], []\n",
    "    for (fmin, fmax) in fb_bands:\n",
    "        tr_b = tr.copy().filter(fmin, fmax, picks='eeg', verbose=False)\n",
    "        te_b = te.copy().filter(fmin, fmax, picks='eeg', verbose=False)\n",
    "        Xtr = tr_b.get_data(); Xte = te_b.get_data()\n",
    "        try:\n",
    "            csp = CSP(n_components=n_csp, reg='ledoit_wolf', log=True, norm_trace=False)\n",
    "        except TypeError:\n",
    "            csp = CSP(n_components=n_csp, reg='ledoit_wolf', log=True)\n",
    "        Xtr_c = csp.fit_transform(Xtr, y_tr)\n",
    "        Xte_c = csp.transform(Xte)\n",
    "        Xtr_list.append(Xtr_c); Xte_list.append(Xte_c)\n",
    "    return np.concatenate(Xtr_list, axis=1), np.concatenate(Xte_list, axis=1)\n",
    "\n",
    "def _transform_with_trained_fb_csp(train_ep: mne.Epochs,\n",
    "                                   target_ep: mne.Epochs,\n",
    "                                   fb_bands=FB_BANDS_DENSE,\n",
    "                                   n_csp=DEFAULT_N_CSP,\n",
    "                                   crop_window=None):\n",
    "    \"\"\"\n",
    "    Entrena CSP SOLO con 'train_ep' y transforma 'target_ep' con esos mismos filtros.\n",
    "    Útil para calibración LDA-only.\n",
    "    \"\"\"\n",
    "    tr = train_ep.copy(); tg = target_ep.copy()\n",
    "    if crop_window is not None:\n",
    "        tr.crop(*crop_window); tg.crop(*crop_window)\n",
    "    y_tr = tr.events[:, -1]\n",
    "    Xtr_list, Xtg_list = [], []\n",
    "    for (fmin, fmax) in fb_bands:\n",
    "        tr_b = tr.copy().filter(fmin, fmax, picks='eeg', verbose=False)\n",
    "        tg_b = tg.copy().filter(fmin, fmax, picks='eeg', verbose=False)\n",
    "        Xtr = tr_b.get_data(); Xtg = tg_b.get_data()\n",
    "        try:\n",
    "            csp = CSP(n_components=n_csp, reg='ledoit_wolf', log=True, norm_trace=False)\n",
    "        except TypeError:\n",
    "            csp = CSP(n_components=n_csp, reg='ledoit_wolf', log=True)\n",
    "        csp.fit(Xtr, y_tr)\n",
    "        Xtg_c = csp.transform(Xtg)\n",
    "        Xtr_list.append(csp.transform(Xtr))  # para tener base coherente\n",
    "        Xtg_list.append(Xtg_c)\n",
    "    return np.concatenate(Xtr_list, axis=1), np.concatenate(Xtg_list, axis=1)\n",
    "\n",
    "def _fit_scale_lda(Xtr, ytr, Xte):\n",
    "    scaler = StandardScaler()\n",
    "    Xtr_s = scaler.fit_transform(Xtr); Xte_s = scaler.transform(Xte)\n",
    "    clf = LDA(**LDA_PARAMS).fit(Xtr_s, ytr)\n",
    "    return clf.predict(Xte_s), clf, scaler\n",
    "\n",
    "def _save_confusion(cm: np.ndarray, class_names, title: str, out_png: Path):\n",
    "    fig, ax = plt.subplots(figsize=(5.4, 4.6), dpi=140)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=class_names)\n",
    "    disp.plot(ax=ax, cmap=\"Blues\", colorbar=True, values_format='d')\n",
    "    ax.set_title(title); fig.tight_layout(); fig.savefig(out_png); plt.close(fig)\n",
    "\n",
    "def _split_calibration(ep_test: mne.Epochs, k_per_class=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Toma k_per_class épocas por clase del set de TEST como 'calibración' y devuelve:\n",
    "      ep_calib (k por clase) y ep_eval (el resto).\n",
    "    Si k_per_class es 0/None -> (None, ep_test).\n",
    "    \"\"\"\n",
    "    if not k_per_class or k_per_class <= 0:\n",
    "        return None, ep_test\n",
    "    rng = np.random.RandomState(int(random_state))\n",
    "    labels = ep_test.events[:, -1]\n",
    "    ep_calib_list, ep_eval_list = [], []\n",
    "    for code in np.unique(labels):\n",
    "        idx = np.where(labels == code)[0]\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "        rng.shuffle(idx)\n",
    "        take = min(int(k_per_class), len(idx))\n",
    "        sel, rem = idx[:take], idx[take:]\n",
    "        if take > 0: ep_calib_list.append(ep_test.copy()[sel])\n",
    "        if len(rem) > 0: ep_eval_list.append(ep_test.copy()[rem])\n",
    "    ep_calib = mne.concatenate_epochs(ep_calib_list, on_mismatch='ignore') if ep_calib_list else None\n",
    "    ep_eval  = mne.concatenate_epochs(ep_eval_list,  on_mismatch='ignore') if ep_eval_list  else ep_test\n",
    "    return ep_calib, ep_eval\n",
    "\n",
    "# -------------------- TEST fijo (24 sujetos) --------------------\n",
    "FIXED_TEST_SUBJECTS = [\n",
    "    # Buenos (>~0.70)\n",
    "    'S007','S025','S029','S031','S032','S034','S035','S042','S043','S049','S056','S058','S062','S072',\n",
    "    # Medios (~0.50–0.69)\n",
    "    'S001','S010','S013','S017','S019','S030',\n",
    "    # Malos (<~0.45)\n",
    "    'S005','S006','S009','S097'\n",
    "]\n",
    "\n",
    "def _make_split_with_fixed_test(fif_dir=DATA_PROC, fixed_test=(), val_size=16, random_state=42):\n",
    "    all_subjects = _discover_subject_ids(fif_dir)\n",
    "    existing = {p.stem.split('_')[0] for p in Path(fif_dir).glob('S???_MI-epo.fif')}\n",
    "    fixed_test = [s for s in list(fixed_test) if s in existing]\n",
    "    remaining = [s for s in all_subjects if s not in fixed_test]\n",
    "    if len(remaining) <= val_size:\n",
    "        raise ValueError(f\"Insuficientes sujetos para VALIDATION: remaining={len(remaining)} <= val_size={val_size}\")\n",
    "    train_subjects, val_subjects = train_test_split(remaining, test_size=val_size,\n",
    "                                                    shuffle=True, random_state=random_state)\n",
    "    return train_subjects, val_subjects, fixed_test\n",
    "\n",
    "# -------------------- FUNCIÓN PRINCIPAL --------------------\n",
    "def run_inter_fixedsplit_fbcsp(\n",
    "    fif_dir=DATA_PROC,\n",
    "    crop_window=(0.5, 4.5),\n",
    "    fb_bands=FB_BANDS_DENSE,\n",
    "    n_csp=DEFAULT_N_CSP,\n",
    "    val_size=16,\n",
    "    random_state=42,\n",
    "    refit_on_trainval_for_test=True,\n",
    "    # === Calibración (LDA-only) ===\n",
    "    calibrate_k_per_class=None,         # None/0 -> sin calibración; >0 -> usar k por clase del TEST\n",
    "    # === Salidas ===\n",
    "    save_csv_name=\"inter_fbcsp_fixedsplit.csv\",\n",
    "    save_txt_name=\"inter_fbcsp_fixedsplit.txt\"\n",
    "):\n",
    "    ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    train_subs, val_subs, test_subs = _make_split_with_fixed_test(\n",
    "        fif_dir=fif_dir, fixed_test=FIXED_TEST_SUBJECTS,\n",
    "        val_size=val_size, random_state=random_state\n",
    "    )\n",
    "    print(f\"TRAIN={len(train_subs)} | VAL={len(val_subs)} | TEST(fijo)={len(test_subs)}\")\n",
    "\n",
    "    # ----- carga epochs por grupo\n",
    "    ep_tr = mne.concatenate_epochs([mne.read_epochs(str(Path(fif_dir)/f\"{s}_MI-epo.fif\"),\n",
    "                                                    preload=True, verbose=False) for s in train_subs],\n",
    "                                   on_mismatch='ignore')\n",
    "    ep_va = mne.concatenate_epochs([mne.read_epochs(str(Path(fif_dir)/f\"{s}_MI-epo.fif\"),\n",
    "                                                    preload=True, verbose=False) for s in val_subs],\n",
    "                                   on_mismatch='ignore')\n",
    "    ep_te_all = mne.concatenate_epochs([mne.read_epochs(str(Path(fif_dir)/f\"{s}_MI-epo.fif\"),\n",
    "                                                        preload=True, verbose=False) for s in test_subs],\n",
    "                                       on_mismatch='ignore')\n",
    "\n",
    "    # ----- alinear canales\n",
    "    ep_va = ep_va.copy().reorder_channels(ep_tr.ch_names)\n",
    "    ep_te_all = ep_te_all.copy().reorder_channels(ep_tr.ch_names)\n",
    "\n",
    "    # ----- labels globales (para encoder consistente)\n",
    "    _, y_tr_str = _epochs_to_Xy(ep_tr)\n",
    "    _, y_va_str = _epochs_to_Xy(ep_va)\n",
    "    _, y_te_str_all = _epochs_to_Xy(ep_te_all)\n",
    "    le = LabelEncoder().fit(np.concatenate([y_tr_str, y_va_str, y_te_str_all]))\n",
    "    y_tr = le.transform(y_tr_str); y_va = le.transform(y_va_str)\n",
    "    classes = list(le.classes_)\n",
    "\n",
    "    # ----- features FBCSP para VALIDATION (sin calibración)\n",
    "    with mne.utils.use_log_level(\"ERROR\"):\n",
    "        Xtr, Xva = _fit_fb_csp_transform(ep_tr, ep_va, fb_bands=fb_bands, n_csp=n_csp, crop_window=crop_window)\n",
    "        yhat_va, clf, scaler = _fit_scale_lda(Xtr, y_tr, Xva)\n",
    "    y_va_enc = le.transform(y_va_str)\n",
    "    acc_va = accuracy_score(y_va_enc, yhat_va); f1_va = f1_score(y_va_enc, yhat_va, average='macro')\n",
    "    print(f\"[VAL] ACC={acc_va:.3f} | F1m={f1_va:.3f}\")\n",
    "\n",
    "    # ====== TEST FIJO (con o sin calibración LDA-only) ======\n",
    "    ep_calib, ep_eval = _split_calibration(ep_te_all, k_per_class=calibrate_k_per_class, random_state=random_state)\n",
    "    k_eff = 0 if ep_calib is None else len(ep_calib)\n",
    "\n",
    "    # Base para TEST: si quieres, re-ajusta base con TRAIN+VAL antes de calibrar\n",
    "    if refit_on_trainval_for_test:\n",
    "        ep_base = mne.concatenate_epochs([ep_tr, ep_va], on_mismatch='ignore')\n",
    "        _, y_base_str = _epochs_to_Xy(ep_base)\n",
    "        y_base = le.transform(y_base_str)\n",
    "    else:\n",
    "        ep_base = ep_tr\n",
    "        y_base = y_tr\n",
    "\n",
    "    if not calibrate_k_per_class or calibrate_k_per_class <= 0:\n",
    "        # --- SIN calibración ---\n",
    "        with mne.utils.use_log_level(\"ERROR\"):\n",
    "            Xbase, Xte = _fit_fb_csp_transform(ep_base, ep_eval, fb_bands=fb_bands, n_csp=n_csp, crop_window=crop_window)\n",
    "        yhat_te, clf2, scaler2 = _fit_scale_lda(Xbase, y_base, Xte)\n",
    "        _, y_eval_str = _epochs_to_Xy(ep_eval)\n",
    "        y_eval = le.transform(y_eval_str)\n",
    "    else:\n",
    "        # --- CON calibración (LDA-only) ---\n",
    "        with mne.utils.use_log_level(\"ERROR\"):\n",
    "            # 1) Entrena CSP con ep_base, transforma base/calib/eval con esos mismos filtros\n",
    "            Xbase_from_base, Xcal_from_base = _transform_with_trained_fb_csp(\n",
    "                ep_base, ep_calib, fb_bands=fb_bands, n_csp=n_csp, crop_window=crop_window\n",
    "            )\n",
    "            _, Xeval_from_base = _transform_with_trained_fb_csp(\n",
    "                ep_base, ep_eval, fb_bands=fb_bands, n_csp=n_csp, crop_window=crop_window\n",
    "            )\n",
    "        # 2) Reentrena SOLO LDA con (base + calib) y evalúa en eval\n",
    "        _, y_calib_str = _epochs_to_Xy(ep_calib)\n",
    "        y_calib = le.transform(y_calib_str)\n",
    "        X_join = np.vstack([Xbase_from_base, Xcal_from_base])\n",
    "        y_join = np.concatenate([y_base, y_calib])\n",
    "        yhat_te, clf2, scaler2 = _fit_scale_lda(X_join, y_join, Xeval_from_base)\n",
    "        _, y_eval_str = _epochs_to_Xy(ep_eval)\n",
    "        y_eval = le.transform(y_eval_str)\n",
    "\n",
    "    # Métricas TEST\n",
    "    acc_te = accuracy_score(y_eval, yhat_te); f1_te = f1_score(y_eval, yhat_te, average='macro')\n",
    "    print(f\"[TEST {'CALIB-LDA' if k_eff>0 else 'SIN CALIB'}] k={calibrate_k_per_class} | ACC={acc_te:.3f} | F1m={f1_te:.3f}\")\n",
    "\n",
    "    # Matriz de confusión (TEST eval)\n",
    "    cm_te = confusion_matrix(y_eval, yhat_te, labels=np.arange(len(classes)))\n",
    "    cm_png = FIG_DIR / f\"inter_fbcsp_confusion_test_{ts}.png\"\n",
    "    _save_confusion(cm_te, classes, f\"FBCSP — TEST fijo (calib LDA k={calibrate_k_per_class})\", cm_png)\n",
    "    print(\"Confusión TEST →\", cm_png)\n",
    "\n",
    "    # Guardar CSV + TXT\n",
    "    df = pd.DataFrame([dict(\n",
    "        mode=\"inter_fixedsplit_fbcsp\",\n",
    "        acc_val=float(acc_va), f1_val=float(f1_va),\n",
    "        acc_test=float(acc_te), f1_test=float(f1_te),\n",
    "        n_train=len(train_subs), n_val=len(val_subs), n_test=len(test_subs),\n",
    "        crop=str(crop_window), n_csp=int(n_csp), fb_bands=len(fb_bands),\n",
    "        refit_on_trainval_for_test=bool(refit_on_trainval_for_test),\n",
    "        calibrate_k_per_class=int(calibrate_k_per_class or 0)\n",
    "    )])\n",
    "    out_csv = TAB_DIR / f\"{ts}_inter_fbcsp_fixedsplit.csv\"; df.to_csv(out_csv, index=False)\n",
    "    out_txt = LOG_DIR / f\"{ts}_inter_fbcsp_fixedsplit.txt\"\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"INTER-SUJETO (FBCSP) — TEST fijo\\n\")\n",
    "        f.write(f\"Generado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        f.write(df.to_string(index=False))\n",
    "    print(\"CSV →\", out_csv); print(\"TXT →\", out_txt)\n",
    "    return df\n",
    "\n",
    "# -------------------- EJECUCIÓN INMEDIATA --------------------\n",
    "df_fbcsp_inter = run_inter_fixedsplit_fbcsp(\n",
    "    fif_dir=DATA_PROC,\n",
    "    crop_window=(0.5, 3.0),\n",
    "    fb_bands=FB_BANDS_CLASSIC,\n",
    "    n_csp=4,\n",
    "    val_size=16,\n",
    "    random_state=42,\n",
    "    refit_on_trainval_for_test=True,   # re-ajusta con TRAIN+VAL antes del test si True\n",
    "    calibrate_k_per_class=10             # <= pon 0/None si no quieres calibración\n",
    ")\n",
    "try: display(df_fbcsp_inter)\n",
    "except: pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
