{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b99cce37",
   "metadata": {},
   "source": [
    "# FBCSP + LDA (intra-sujeto y cross-sujeto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767badd5",
   "metadata": {},
   "source": [
    "### Bloque 1 — Rutas de salida + utilidades de logging y guardado\n",
    "\n",
    "Qué hace: define las carpetas donde se guardarán figuras, tablas y logs bajo models/fbcsp_lda/. Incluye utilidades para inicializar un logger limpio, guardar matrices de confusión (PNG + CSV) y anexar métricas a CSVs acumulativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27eaa450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio de datos procesados: /root/Proyecto/EEG_Clasificador/data/processed\n"
     ]
    }
   ],
   "source": [
    "# %% [PATHS & LOGGING] — rutas de salida + helpers para logs/figuras/tablas\n",
    "import sys, logging, warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import mne\n",
    "\n",
    "# Raíz del repo (este notebook está en models/fbcsp_lda/)\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_PROC = PROJ / 'data' / 'processed'\n",
    "OUT_ROOT = PROJ / 'models' / 'fbcsp_lda'\n",
    "FIG_DIR  = OUT_ROOT / 'figures'\n",
    "TAB_DIR  = OUT_ROOT / 'tables'\n",
    "LOG_DIR  = OUT_ROOT / 'logs'\n",
    "for d in (FIG_DIR, TAB_DIR, LOG_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Directorio de datos procesados: {DATA_PROC}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57bbfa1",
   "metadata": {},
   "source": [
    "### Bloque 2 — FBCSP Helpers\n",
    "\n",
    "Qué hace: helpers de modelo. Extraen X/y desde Epochs, aplican Filter-Bank + CSP por bandas y entrenan/escala LDA. Mantiene tus comentarios y añade compatibilidad con versiones de CSP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d10abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [HELPERS — comunes FBCSP/LOSO/Calibración/Logging]\n",
    "# Reúne en un solo bloque:\n",
    "#  - Descubrimiento de sujetos + DROP-only\n",
    "#  - Perillas (_knobs_dict)\n",
    "#  - FBCSP helpers (banco de filtros, picks motores, z-score por época, FBCSP transform)\n",
    "#  - Clasificador (scaler + LDA)\n",
    "#  - Split de calibración\n",
    "#  - Logger\n",
    "\n",
    "# ====== IMPORTS ======\n",
    "import sys, logging, warnings, re\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from mne.decoding import CSP\n",
    "\n",
    "# ====== RUTAS y ARCHIVOS AUXILIARES ======\n",
    "# NOTA: este bloque asume que en un bloque anterior ya definiste:\n",
    "#   PROJ, DATA_PROC, LOG_DIR  (del bloque PATHS & LOGGING original)\n",
    "# Archivo opcional con sujetos a excluir (DROP-only)\n",
    "STRICT_DROP_TXT = PROJ / 'reports' / 'tables' / '02_prepro' / 'subjects_strict_DROP.txt'\n",
    "_re_sid = re.compile(r'^S\\d{3}$')\n",
    "# ====== FBCSP HELPERS ======\n",
    "# Bancos de filtros\n",
    "FB_BANDS_DENSE = [(f, f+2) for f in range(8, 30, 2)]                  # denso 8–30 por pasos de 2 Hz\n",
    "FB_BANDS_CLASSIC = [(8,12), (12,16), (16,20), (20,24), (24,28), (28,30)]\n",
    "DEFAULT_FB_BANDS = FB_BANDS_DENSE\n",
    "\n",
    "# Nº de componentes CSP por sub-banda (típico 6–8 en bancos densos)\n",
    "DEFAULT_N_CSP = 6\n",
    "\n",
    "# LDA con shrinkage automático robusto\n",
    "LDA_PARAMS = dict(solver='lsqr', shrinkage='auto')\n",
    "\n",
    "# Picks motores (si deseas limitar; tus datos ya tienen 8 canales motores)\n",
    "MOTOR_TOKENS = ['C3', 'CZ', 'C4', 'FC3', 'FC4', 'CP3', 'CPZ', 'CP4']\n",
    "\n",
    "def _list_subject_fifs(fif_dir=DATA_PROC, pattern='S???_MI-epo.fif'):\n",
    "    \"\"\"Devuelve lista ordenada de rutas a los FIF de sujetos disponibles.\"\"\"\n",
    "    return sorted(glob(str(fif_dir / pattern)))\n",
    "\n",
    "def _list_available_subjects(fif_dir=DATA_PROC):\n",
    "    \"\"\"IDs únicos SXXX disponibles en el directorio de FIF procesados.\"\"\"\n",
    "    files = _list_subject_fifs(fif_dir)\n",
    "    return sorted({Path(f).stem.split('_')[0] for f in files})\n",
    "\n",
    "def _read_drop_file(path: Path):\n",
    "    \"\"\"Lee archivo con IDs de sujetos a excluir (uno por línea, formato SXXX).\"\"\"\n",
    "    if not path.exists():\n",
    "        return set()\n",
    "    s = set()\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for ln in f:\n",
    "            sid = ln.strip().upper()\n",
    "            if _re_sid.match(sid):\n",
    "                s.add(sid)\n",
    "    return s\n",
    "\n",
    "def _strict_valid_from_drop(avail_ids):\n",
    "    \"\"\"\n",
    "    Aplica DROP-only:\n",
    "      - Lee STRICT_DROP_TXT si existe\n",
    "      - Devuelve (lista válidos, info string)\n",
    "    \"\"\"\n",
    "    drop = _read_drop_file(STRICT_DROP_TXT)\n",
    "    avail = set(avail_ids)\n",
    "    valid = sorted(avail - drop)\n",
    "    info = f\"DROP-only: {len(drop)} en DROP; válidos={len(valid)}/{len(avail)}\"\n",
    "    if not STRICT_DROP_TXT.exists():\n",
    "        info += \" (archivo DROP no encontrado → sin exclusiones)\"\n",
    "    return valid, info\n",
    "\n",
    "# Helper para registrar “perillas” (config) en logs/CSV\n",
    "def _knobs_dict(crop_window, motor_only, zscore_epoch, fb_bands, n_csp):\n",
    "    return dict(\n",
    "        crop_window=crop_window if crop_window is not None else None,\n",
    "        motor_only=bool(motor_only),\n",
    "        zscore_epoch=bool(zscore_epoch),\n",
    "        fb_bands=str(fb_bands),\n",
    "        n_csp=int(n_csp)\n",
    "    )\n",
    "\n",
    "def _epochs_to_Xy(epochs: mne.Epochs):\n",
    "    \"\"\"\n",
    "    Extrae X e y (clases string) desde Epochs respetando event_id.\n",
    "    X: (n_epochs, n_channels, n_times)\n",
    "    y: (n_epochs,) etiquetas string según mapping event_id.\n",
    "    \"\"\"\n",
    "    X = epochs.get_data()\n",
    "    inv = {v: k for k, v in epochs.event_id.items()}  # int->clase\n",
    "    y = np.array([inv[e[-1]] for e in epochs.events], dtype=object)\n",
    "    return X, y\n",
    "\n",
    "def _find_motor_chs(ch_names, tokens=MOTOR_TOKENS):\n",
    "    \"\"\"Devuelve índices de canales que contienen tokens motores (case-insensitive).\"\"\"\n",
    "    up = [c.upper() for c in ch_names]\n",
    "    picks = []\n",
    "    for tok in tokens:\n",
    "        TU = tok.upper()\n",
    "        for i, name in enumerate(up):\n",
    "            if TU in name:\n",
    "                picks.append(i); break\n",
    "    return sorted(set(picks))\n",
    "\n",
    "def _epochwise_zscore(X, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Z-score por época y canal (normaliza a lo largo del tiempo).\n",
    "    X: (n_epochs, n_channels, n_times) → mismo shape.\n",
    "    \"\"\"\n",
    "    mean = X.mean(axis=-1, keepdims=True)\n",
    "    std  = X.std(axis=-1, keepdims=True)\n",
    "    return (X - mean) / (std + eps)\n",
    "\n",
    "def _fit_fb_csp_transform(train_ep: mne.Epochs,\n",
    "                          test_ep: mne.Epochs,\n",
    "                          fb_bands=DEFAULT_FB_BANDS,\n",
    "                          n_csp=DEFAULT_N_CSP,\n",
    "                          motor_only=False,\n",
    "                          zscore_epoch=False,\n",
    "                          crop_window=None):\n",
    "    \"\"\"\n",
    "    Aplica FBCSP con opciones:\n",
    "      - crop_window=(tmin,tmax): recorta épocas\n",
    "      - motor_only=True: usa solo canales motores comunes\n",
    "      - zscore_epoch=True: z-score por época/canal antes de CSP\n",
    "      - fb_bands: lista de sub-bandas [(fmin,fmax), ...]\n",
    "    Devuelve (Xtr_fb, Xte_fb) con features concatenadas por sub-banda.\n",
    "    \"\"\"\n",
    "    tr = train_ep.copy()\n",
    "    te = test_ep.copy()\n",
    "\n",
    "    if crop_window is not None:\n",
    "        tmin, tmax = crop_window\n",
    "        tr.crop(tmin, tmax)\n",
    "        te.crop(tmin, tmax)\n",
    "\n",
    "    if motor_only:\n",
    "        picks = _find_motor_chs(tr.ch_names)\n",
    "        if picks:\n",
    "            tr.pick(picks)\n",
    "            # Alinear canales del test con los del train\n",
    "            te = te.copy().reorder_channels(tr.ch_names)\n",
    "\n",
    "    Xtr_list, Xte_list = [], []\n",
    "    y_tr = tr.events[:, -1]\n",
    "\n",
    "    for (fmin, fmax) in fb_bands:\n",
    "        tr_b = tr.copy().filter(fmin, fmax, picks='eeg', verbose=False)\n",
    "        te_b = te.copy().filter(fmin, fmax, picks='eeg', verbose=False)\n",
    "\n",
    "        Xtr = tr_b.get_data()\n",
    "        Xte = te_b.get_data()\n",
    "\n",
    "        if zscore_epoch:\n",
    "            Xtr = _epochwise_zscore(Xtr)\n",
    "            Xte = _epochwise_zscore(Xte)\n",
    "\n",
    "        try:\n",
    "            csp = CSP(n_components=n_csp, reg='ledoit_wolf', log=True, norm_trace=False)\n",
    "        except TypeError:\n",
    "            # Compatibilidad con MNE antiguos sin 'norm_trace'\n",
    "            csp = CSP(n_components=n_csp, reg='ledoit_wolf', log=True)\n",
    "\n",
    "        Xtr_c = csp.fit_transform(Xtr, y_tr)\n",
    "        Xte_c = csp.transform(Xte)\n",
    "\n",
    "        Xtr_list.append(Xtr_c)\n",
    "        Xte_list.append(Xte_c)\n",
    "\n",
    "    Xtr_fb = np.concatenate(Xtr_list, axis=1)\n",
    "    Xte_fb = np.concatenate(Xte_list,  axis=1)\n",
    "    return Xtr_fb, Xte_fb\n",
    "\n",
    "def _fit_scale_lda(Xtr, ytr, Xte, lda_params=LDA_PARAMS):\n",
    "    \"\"\"\n",
    "    Estandariza features (fit solo en train) y entrena LDA.\n",
    "    Devuelve: (yhat, clf, scaler)\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    Xtr_s = scaler.fit_transform(Xtr)\n",
    "    Xte_s = scaler.transform(Xte)\n",
    "\n",
    "    clf = LDA(**lda_params)\n",
    "    clf.fit(Xtr_s, ytr)\n",
    "    yhat = clf.predict(Xte_s)\n",
    "    return yhat, clf, scaler\n",
    "\n",
    "# ====== CALIBRACIÓN (para LOSO calibrado) ======\n",
    "def _split_calibration(ep_te, k_per_class=5):\n",
    "    \"\"\"\n",
    "    Toma k épocas por clase como calibración y el resto como evaluación.\n",
    "    Si k_per_class <= 0 → (None, ep_te).\n",
    "    \"\"\"\n",
    "    if k_per_class <= 0:\n",
    "        return None, ep_te\n",
    "    labels = ep_te.events[:, -1]\n",
    "    ep_calib_list, ep_eval_list = [], []\n",
    "    for code in np.unique(labels):\n",
    "        idx = np.where(labels == code)[0]\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "        take = min(k_per_class, len(idx))\n",
    "        sel = idx[:take]\n",
    "        rem = idx[take:]\n",
    "        if take > 0:\n",
    "            ep_calib_list.append(ep_te.copy()[sel])\n",
    "        if len(rem) > 0:\n",
    "            ep_eval_list.append(ep_te.copy()[rem])\n",
    "    ep_calib = mne.concatenate_epochs(ep_calib_list, on_mismatch='ignore') if ep_calib_list else None\n",
    "    ep_eval  = mne.concatenate_epochs(ep_eval_list,  on_mismatch='ignore') if ep_eval_list  else ep_te\n",
    "    return ep_calib, ep_eval\n",
    "\n",
    "# ====== LOGGER ======\n",
    "def _init_logger(run_name: str):\n",
    "    \"\"\"\n",
    "    Crea un logger que escribe a consola y a TXT en models/fbcsp_lda/logs/.\n",
    "    Reduce la verbosidad de MNE para que los logs sean legibles.\n",
    "    \"\"\"\n",
    "    ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_path = LOG_DIR / f\"{ts}_{run_name}.txt\"\n",
    "\n",
    "    logger = logging.getLogger(run_name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.handlers.clear()\n",
    "\n",
    "    fmt = logging.Formatter(\"[%(asctime)s] %(levelname)s: %(message)s\", datefmt=\"%H:%M:%S\")\n",
    "    ch = logging.StreamHandler(stream=sys.stdout); ch.setLevel(logging.INFO); ch.setFormatter(fmt)\n",
    "    fh = logging.FileHandler(log_path, encoding=\"utf-8\"); fh.setLevel(logging.INFO); fh.setFormatter(fmt)\n",
    "    logger.addHandler(ch); logger.addHandler(fh)\n",
    "\n",
    "    # Silenciar ruido externo (sin afectar tus prints/logs)\n",
    "    mne.set_log_level(\"ERROR\")\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"mne\")\n",
    "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"mne\")\n",
    "    return logger, log_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1f707b",
   "metadata": {},
   "source": [
    "### Bloque 3 — Inspección de datos\n",
    "\n",
    "Qué hace: muestra rutas y un listado rápido del contenido de data/ y data/processed/ para verificar que los FIF están donde esperamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0adb0212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJ: /root/Proyecto/EEG_Clasificador\n",
      "DATA: /root/Proyecto/EEG_Clasificador/data\n",
      "DATA_PROC: /root/Proyecto/EEG_Clasificador/data/processed\n",
      "\n",
      "Contenido de data (top-level):\n",
      " - cache/\n",
      " - processed/\n",
      " - raw/\n",
      "\n",
      "Contenido de data/processed (muestras):\n",
      " - S001_MI-epo.fif\n",
      " - S002_MI-epo.fif\n",
      " - S003_MI-epo.fif\n",
      " - S004_MI-epo.fif\n",
      " - S005_MI-epo.fif\n",
      " - S006_MI-epo.fif\n",
      " - S007_MI-epo.fif\n",
      " - S008_MI-epo.fif\n",
      " - S009_MI-epo.fif\n",
      " - S010_MI-epo.fif\n",
      " - S011_MI-epo.fif\n",
      " - S012_MI-epo.fif\n",
      " - S013_MI-epo.fif\n",
      " - S014_MI-epo.fif\n",
      " - S015_MI-epo.fif\n",
      " - S016_MI-epo.fif\n",
      " - S017_MI-epo.fif\n",
      " - S018_MI-epo.fif\n",
      " - S019_MI-epo.fif\n",
      " - S020_MI-epo.fif\n",
      " - S021_MI-epo.fif\n",
      " - S022_MI-epo.fif\n",
      " - S023_MI-epo.fif\n",
      " - S024_MI-epo.fif\n",
      " - S025_MI-epo.fif\n",
      " - S026_MI-epo.fif\n",
      " - S027_MI-epo.fif\n",
      " - S028_MI-epo.fif\n",
      " - S029_MI-epo.fif\n",
      " - S030_MI-epo.fif\n",
      " - S031_MI-epo.fif\n",
      " - S032_MI-epo.fif\n",
      " - S033_MI-epo.fif\n",
      " - S034_MI-epo.fif\n",
      " - S035_MI-epo.fif\n",
      " - S036_MI-epo.fif\n",
      " - S037_MI-epo.fif\n",
      " - S039_MI-epo.fif\n",
      " - S040_MI-epo.fif\n",
      " - S041_MI-epo.fif\n",
      " - S042_MI-epo.fif\n",
      " - S043_MI-epo.fif\n",
      " - S044_MI-epo.fif\n",
      " - S045_MI-epo.fif\n",
      " - S046_MI-epo.fif\n",
      " - S047_MI-epo.fif\n",
      " - S048_MI-epo.fif\n",
      " - S049_MI-epo.fif\n",
      " - S050_MI-epo.fif\n",
      " - S051_MI-epo.fif\n"
     ]
    }
   ],
   "source": [
    "# %% [Inspect data folders]\n",
    "# Celda añadida automáticamente: muestra rutas y lista contenidos de data y data/processed\n",
    "try:\n",
    "    print(f\"PROJ: {PROJ}\")\n",
    "    print(f\"DATA: {PROJ / 'data'}\")\n",
    "    print(f\"DATA_PROC: {DATA_PROC}\")\n",
    "    print('\\nContenido de data (top-level):')\n",
    "    data_dir = PROJ / 'data'\n",
    "    if data_dir.exists():\n",
    "        for p in sorted(data_dir.iterdir()):\n",
    "            print(f\" - {p.name}{'/' if p.is_dir() else ''}\")\n",
    "    else:\n",
    "        print('  (no existe)')\n",
    "\n",
    "    print('\\nContenido de data/processed (muestras):')\n",
    "    if DATA_PROC.exists():\n",
    "        for p in sorted(DATA_PROC.glob('*'))[:50]:\n",
    "            print(f\" - {p.name}\")\n",
    "    else:\n",
    "        print('  (no existe)')\n",
    "except Exception as e:\n",
    "    print('Error inspeccionando data:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd66c27b",
   "metadata": {},
   "source": [
    "### Bloque 4 — Intra-sujeto (k-Fold CV) con logs + guardado\n",
    "\n",
    "Qué hace: ejecuta CV por sujeto con FBCSP+LDA, imprime métricas limpias, guarda matriz de confusión (PNG/CSV), y escribe métricas en tables/metrics_intra.csv. Además genera un TXT en logs/ con el detalle de la corrida.\n",
    "\n",
    "Añade argumentos crop_window, motor_only, zscore_epoch, fb_bands, n_csp. Guarda lo de siempre (matriz, métricas, log) pero ahora puedes probar rápidamente ventanas/picks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f44cfa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [INTRA — todos los sujetos, con timestamp y artefactos consolidados + fila GLOBAL]\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from math import ceil\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "def _discover_subject_ids(fif_dir=DATA_PROC, pattern='S???_MI-epo.fif'):\n",
    "    files = sorted(glob(str(fif_dir / pattern)))\n",
    "    return [Path(f).stem.split('_')[0] for f in files]\n",
    "\n",
    "\n",
    "def run_intra_all(\n",
    "    fif_dir=DATA_PROC,\n",
    "    k=5,\n",
    "    random_state=42,\n",
    "    crop_window=(0.5, 3.5),\n",
    "    motor_only=True,\n",
    "    zscore_epoch=True,\n",
    "    fb_bands=DEFAULT_FB_BANDS,\n",
    "    n_csp=DEFAULT_N_CSP,\n",
    "    max_subplots_per_fig=12,\n",
    "    n_cols=4,\n",
    "    save_txt_name=None,            # opcional: nombre base TXT (se antepone timestamp)\n",
    "    save_csv_name=None             # opcional: nombre base CSV (se antepone timestamp)\n",
    "):\n",
    "    \"\"\"\n",
    "    Ejecuta INTRA (k-fold) en TODOS los sujetos.\n",
    "    Guarda: 1 log con timestamp, 1 CSV y 1 TXT (ambos con fila GLOBAL),\n",
    "    y mosaicos de matrices de confusión por sujeto (sin archivos por sujeto).\n",
    "    \"\"\"\n",
    "    ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    run_tag = f\"intra_all_{ts}\"\n",
    "\n",
    "    # Log global\n",
    "    logger, log_path = _init_logger(run_name=run_tag)\n",
    "    logger.info(f\"[RUN {run_tag}] Inicio de ejecución INTRA\")\n",
    "    logger.info(f\"Parámetros: k={k}, crop_window={crop_window}, motor_only={motor_only}, \"\n",
    "                f\"zscore_epoch={zscore_epoch}, n_csp={n_csp}, fb_bands={len(fb_bands)}\")\n",
    "\n",
    "    subject_ids = _discover_subject_ids(fif_dir)\n",
    "    if not subject_ids:\n",
    "        print(\"No se encontraron sujetos en\", fif_dir)\n",
    "        return None\n",
    "\n",
    "    logger.info(f\"Sujetos detectados: {subject_ids}\")\n",
    "    print(f\"[INTRA ALL] sujetos detectados: {subject_ids}\")\n",
    "\n",
    "    rows_summary = []\n",
    "    cm_items = []\n",
    "\n",
    "    for subject_id in subject_ids:\n",
    "        fif_path = fif_dir / f\"{subject_id}_MI-epo.fif\"\n",
    "        epochs = mne.read_epochs(fif_path, preload=True, verbose=False)\n",
    "\n",
    "        _, y_str = _epochs_to_Xy(epochs)\n",
    "        le = LabelEncoder(); y = le.fit_transform(y_str)\n",
    "        classes = list(le.classes_)\n",
    "\n",
    "        logger.info(f\"== {subject_id} | n_epochs={len(y)} | clases={classes} | sfreq={epochs.info['sfreq']}\")\n",
    "        skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=random_state)\n",
    "        accs, f1s = [], []\n",
    "        cm_sum = np.zeros((len(classes), len(classes)), dtype=int)\n",
    "\n",
    "        for fold, (tr_idx, te_idx) in enumerate(skf.split(np.zeros(len(y)), y), start=1):\n",
    "            ep_tr = epochs[tr_idx]; ep_te = epochs[te_idx]\n",
    "            with mne.utils.use_log_level(\"ERROR\"):\n",
    "                Xtr_fb, Xte_fb = _fit_fb_csp_transform(\n",
    "                    ep_tr, ep_te,\n",
    "                    fb_bands=fb_bands,\n",
    "                    n_csp=n_csp,\n",
    "                    motor_only=motor_only,\n",
    "                    zscore_epoch=zscore_epoch,\n",
    "                    crop_window=crop_window\n",
    "                )\n",
    "            yhat, clf, scaler = _fit_scale_lda(Xtr_fb, y[tr_idx], Xte_fb)\n",
    "\n",
    "            acc = accuracy_score(y[te_idx], yhat)\n",
    "            f1m = f1_score(y[te_idx], yhat, average='macro')\n",
    "            cm_sum += confusion_matrix(y[te_idx], yhat, labels=np.arange(len(classes)))\n",
    "            accs.append(acc); f1s.append(f1m)\n",
    "            logger.info(f\"[{subject_id} | fold {fold}] acc={acc:.3f} | f1m={f1m:.3f}\")\n",
    "\n",
    "        acc_mu, acc_sd = float(np.mean(accs)), float(np.std(accs))\n",
    "        f1_mu,  f1_sd  = float(np.mean(f1s)),  float(np.std(f1s))\n",
    "\n",
    "        logger.info(f\"[{subject_id}] ACC={acc_mu:.3f}±{acc_sd:.3f} | F1m={f1_mu:.3f}±{f1_sd:.3f}\")\n",
    "        rows_summary.append(dict(\n",
    "            subject=subject_id,\n",
    "            acc_mean=acc_mu,\n",
    "            f1_macro_mean=f1_mu,\n",
    "            k=k,\n",
    "            n_classes=len(classes),\n",
    "            crop=str(crop_window),\n",
    "            motor_only=bool(motor_only),\n",
    "            zscore_epoch=bool(zscore_epoch),\n",
    "            n_csp=int(n_csp),\n",
    "            fb_bands=len(fb_bands)\n",
    "        ))\n",
    "        cm_items.append((subject_id, cm_sum, classes))\n",
    "\n",
    "    # --- CSV/TXT (con fila GLOBAL)\n",
    "    df = pd.DataFrame(rows_summary).sort_values(\"subject\")\n",
    "\n",
    "    acc_mu = float(df['acc_mean'].mean()) if not df.empty else 0.0\n",
    "    acc_sd = float(df['acc_mean'].std(ddof=0)) if not df.empty else 0.0\n",
    "    f1_mu  = float(df['f1_macro_mean'].mean()) if not df.empty else 0.0\n",
    "    f1_sd  = float(df['f1_macro_mean'].std(ddof=0)) if not df.empty else 0.0\n",
    "\n",
    "    df_global = pd.DataFrame([{\n",
    "        'subject': 'GLOBAL',\n",
    "        'acc_mean': acc_mu,\n",
    "        'f1_macro_mean': f1_mu,\n",
    "        'k': k,\n",
    "        'n_classes': int(df['n_classes'].mean()) if 'n_classes' in df.columns and not df.empty else 0,\n",
    "        'crop': str(crop_window),\n",
    "        'motor_only': bool(motor_only),\n",
    "        'zscore_epoch': bool(zscore_epoch),\n",
    "        'n_csp': int(n_csp),\n",
    "        'fb_bands': len(fb_bands)\n",
    "    }])\n",
    "    df_out = pd.concat([df, df_global], ignore_index=True)\n",
    "\n",
    "    out_csv = (TAB_DIR / f\"{ts}_{save_csv_name}\") if save_csv_name else (TAB_DIR / f\"metrics_intra_all_{ts}.csv\")\n",
    "    df_out.to_csv(out_csv, index=False)\n",
    "    logger.info(f\"Resumen CSV guardado → {out_csv}\")\n",
    "    print(\"Resumen INTRA (todos) →\", out_csv)\n",
    "\n",
    "    logger.info(f\"[GLOBAL INTRA] ACC={acc_mu:.3f}±{acc_sd:.3f} | F1m={f1_mu:.3f}±{f1_sd:.3f}\")\n",
    "    print(f\"[GLOBAL INTRA] ACC={acc_mu:.3f}±{acc_sd:.3f} | F1m={f1_mu:.3f}±{f1_sd:.3f}\")\n",
    "\n",
    "    try:\n",
    "        display(df_out)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    out_txt = (LOG_DIR / f\"{ts}_{save_txt_name}\") if save_txt_name else (LOG_DIR / f\"metrics_intra_all_{ts}.txt\")\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"INTRA-SUJETO (k-fold) — Métricas por sujeto (incluye GLOBAL)\\n\")\n",
    "        f.write(f\"Generado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Total filas: {len(df_out)}\\n\\n\")\n",
    "        header = df_out.columns.tolist()\n",
    "        f.write(\" | \".join(header) + \"\\n\")\n",
    "        f.write(\"-\" * 90 + \"\\n\")\n",
    "        for _, row in df_out.iterrows():\n",
    "            vals = []\n",
    "            for kcol in header:\n",
    "                v = row[kcol]\n",
    "                vals.append(f\"{v:.4f}\" if isinstance(v, float) else str(int(v)) if isinstance(v, (np.integer,)) else str(v))\n",
    "            f.write(\" | \".join(vals) + \"\\n\")\n",
    "    logger.info(f\"Métricas TXT guardadas → {out_txt}\")\n",
    "    print(\"Métricas TXT guardadas →\", out_txt)\n",
    "\n",
    "    # --- Mosaicos de matrices de confusión\n",
    "    if cm_items:\n",
    "        n = len(cm_items)\n",
    "        per_fig = max(1, int(max_subplots_per_fig))\n",
    "        n_figs = ceil(n / per_fig)\n",
    "        n_rows_per_fig = lambda count: ceil(count / n_cols)\n",
    "\n",
    "        for fig_idx in range(n_figs):\n",
    "            start = fig_idx * per_fig\n",
    "            end   = min((fig_idx + 1) * per_fig, n)\n",
    "            chunk = cm_items[start:end]\n",
    "            count = len(chunk)\n",
    "            n_rows = n_rows_per_fig(count)\n",
    "\n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(4.5*n_cols, 3.8*n_rows), dpi=140)\n",
    "            axes = np.atleast_2d(axes).flatten()\n",
    "\n",
    "            for ax_i, (sid, cm_sum, classes) in enumerate(chunk):\n",
    "                ax = axes[ax_i]\n",
    "                disp = ConfusionMatrixDisplay(cm_sum, display_labels=classes)\n",
    "                disp.plot(ax=ax, cmap=\"Blues\", colorbar=False, values_format='d')\n",
    "                ax.set_title(f\"{sid}\")\n",
    "                ax.set_xlabel(\"\"); ax.set_ylabel(\"\")\n",
    "            for j in range(ax_i + 1, len(axes)):\n",
    "                axes[j].axis(\"off\")\n",
    "\n",
    "            out_png = FIG_DIR / f\"intra_all_confusions_{ts}_p{fig_idx+1}.png\"\n",
    "            fig.suptitle(f\"Intra — Matrices de confusión (página {fig_idx+1}/{n_figs})\", y=0.995, fontsize=14)\n",
    "            fig.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "            fig.savefig(out_png)\n",
    "            plt.close(fig)\n",
    "            logger.info(f\"Figura consolidada → {out_png}\")\n",
    "            print(\"Figura consolidada →\", out_png)\n",
    "\n",
    "    logger.info(f\"Log global de esta corrida → {log_path}\")\n",
    "    print(f\"Log global → {log_path}\")\n",
    "\n",
    "    return df_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad74f20",
   "metadata": {},
   "source": [
    "### Bloque 5 — Cross-sujeto (LOSO) con logs + guardado\n",
    "\n",
    "Qué hace: para cada sujeto como test, entrena en el resto, calcula métricas y guarda una matriz de confusión por sujeto y una global. También guarda métricas por sujeto en tables/metrics_loso_per_subject.csv y un resumen global en tables/metrics_loso.csv, además de un TXT en logs/\n",
    "\n",
    "- run_loso(..., use_strict=True) hace LOSO clásico sobre el conjunto resuelto (con Strict si está ON).\n",
    "\n",
    "- run_loso_single(test_subject, ...) entrena con todos los demás y prueba sólo en ese sujeto (Strict opcional).\n",
    "\n",
    "- Incluye utilidades de selección Strict y reemplazo para subject_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9122d415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [INTER-SUBJECT CV desde JSON de folds — NO usa archivo DROP; usa todos los sujetos del JSON]\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "from math import ceil\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report,   # <<< NUEVO\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def run_inter_subject_cv_from_json(\n",
    "    fif_dir=DATA_PROC,\n",
    "    folds_json_path=None,\n",
    "    crop_window=(0.5, 3.5),\n",
    "    motor_only=True,\n",
    "    zscore_epoch=True,\n",
    "    fb_bands=DEFAULT_FB_BANDS,\n",
    "    n_csp=DEFAULT_N_CSP,\n",
    "    calibrate_n=None,                  # calibración opcional con sujetos de test\n",
    "    val_ratio_subjects=0.16,           # % sujetos de TRAIN que van a VALID (≈ 13/82 ≈ 0.16)\n",
    "    random_state=42,                   # para reproducibilidad del split por sujeto\n",
    "    max_subplots_per_fig=12,\n",
    "    n_cols=4,\n",
    "    save_csv_name=None,\n",
    "    save_txt_name=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Inter-subject CV usando folds del JSON, con VALIDACIÓN INTERNA POR SUJETOS.\n",
    "    - El split de validación se hace a nivel SUJETO dentro del conjunto de train de cada fold.\n",
    "    - FBCSP/Scaler/LDA se ajustan SOLO con sujetos de train (sin val ni test).\n",
    "    - Se reportan métricas en valid y en test.\n",
    "    - Classification report (TEST) por fold impreso y guardado en TXT.\n",
    "    \"\"\"\n",
    "    ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    run_tag = f\"inter_subject_cv_json_{ts}\"\n",
    "    logger, log_path = _init_logger(run_name=run_tag)\n",
    "    knobs = _knobs_dict(crop_window, motor_only, zscore_epoch, fb_bands, n_csp)\n",
    "    logger.info(f\"[RUN {run_tag}] Inter-Subject CV (con VALID interno por sujetos)\")\n",
    "    logger.info(f\"Perillas: {knobs} | val_ratio_subjects={val_ratio_subjects:.2f} | calibrate_n={calibrate_n}\")\n",
    "\n",
    "    # JSON de folds\n",
    "    if folds_json_path is None:\n",
    "        folds_json_path = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "    folds_json_path = Path(folds_json_path)\n",
    "    if not folds_json_path.exists():\n",
    "        raise FileNotFoundError(f\"No se encontró folds JSON en {folds_json_path}\")\n",
    "\n",
    "    with open(folds_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        payload = json.load(f)\n",
    "\n",
    "    folds = payload.get(\"folds\", [])\n",
    "    subject_ids_json = payload.get(\"subject_ids\", [])\n",
    "    logger.info(f\"Folds cargadas: {len(folds)} | sujetos en JSON: {len(subject_ids_json)}\")\n",
    "\n",
    "    # Cargar epochs por sujeto\n",
    "    ep_map = {}\n",
    "    for sid in subject_ids_json:\n",
    "        fif_path = Path(fif_dir) / f\"{sid}_MI-epo.fif\"\n",
    "        if fif_path.exists():\n",
    "            try:\n",
    "                ep_map[sid] = mne.read_epochs(str(fif_path), preload=True, verbose=False)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error leyendo {fif_path} para {sid}: {e}\")\n",
    "        else:\n",
    "            logger.warning(f\"Falta archivo FIF para {sid}: {fif_path}\")\n",
    "\n",
    "    # Resultados\n",
    "    rows = []\n",
    "    cm_items = []\n",
    "    cm_global = None\n",
    "    classes_global = None\n",
    "    per_fold_reports = []  # <<< NUEVO: guardamos classification_report (TEST) por fold\n",
    "\n",
    "    for f in folds:\n",
    "        fold_i = int(f.get(\"fold\"))\n",
    "        train_sids = [sid for sid in f.get(\"train\", []) if sid in ep_map]\n",
    "        test_sids  = [sid for sid in f.get(\"test\", [])  if sid in ep_map]\n",
    "\n",
    "        logger.info(f\"[Fold {fold_i}] train({len(train_sids)}): {train_sids}\")\n",
    "        logger.info(f\"[Fold {fold_i}] test ({len(test_sids)}): {test_sids}\")\n",
    "\n",
    "        if len(train_sids) == 0 or len(test_sids) == 0:\n",
    "            logger.warning(f\"[Fold {fold_i}] faltan sujetos train/test — saltando fold.\")\n",
    "            continue\n",
    "\n",
    "        # ---------- VALIDACIÓN INTERNA POR SUJETOS ----------\n",
    "        rng = np.random.RandomState(random_state + fold_i)\n",
    "        n_val_subj = max(1, int(round(len(train_sids) * float(val_ratio_subjects))))\n",
    "        val_indices = rng.choice(len(train_sids), size=n_val_subj, replace=False)\n",
    "        val_sids = sorted([train_sids[i] for i in val_indices])\n",
    "        tr_sids  = sorted([sid for sid in train_sids if sid not in set(val_sids)])\n",
    "\n",
    "        logger.info(f\"[Fold {fold_i}] split interno → train_sids={len(tr_sids)}, val_sids={len(val_sids)}\")\n",
    "\n",
    "        # Concatenar epochs por split\n",
    "        ep_tr  = mne.concatenate_epochs([ep_map[sid] for sid in tr_sids],  on_mismatch='ignore')\n",
    "        ep_val = mne.concatenate_epochs([ep_map[sid] for sid in val_sids], on_mismatch='ignore')\n",
    "        ep_te  = mne.concatenate_epochs([ep_map[sid] for sid in test_sids], on_mismatch='ignore')\n",
    "\n",
    "        # Alinear canales\n",
    "        try:\n",
    "            ep_val = ep_val.copy().reorder_channels(ep_tr.ch_names)\n",
    "            ep_te  = ep_te.copy().reorder_channels(ep_tr.ch_names)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"[Fold {fold_i}] reorder_channels: {e}\")\n",
    "\n",
    "        # Etiquetas\n",
    "        _, y_tr_str  = _epochs_to_Xy(ep_tr)\n",
    "        _, y_val_str = _epochs_to_Xy(ep_val)\n",
    "        _, y_te_str  = _epochs_to_Xy(ep_te)\n",
    "\n",
    "        le = LabelEncoder().fit(np.concatenate([y_tr_str, y_val_str, y_te_str]))\n",
    "        y_tr  = le.transform(y_tr_str)\n",
    "        y_val = le.transform(y_val_str)\n",
    "        y_te  = le.transform(y_te_str)\n",
    "        classes = list(le.classes_)\n",
    "\n",
    "        if classes_global is None:\n",
    "            classes_global = classes\n",
    "            cm_global = np.zeros((len(classes), len(classes)), dtype=int)\n",
    "\n",
    "        # ---------- FEATURES FBCSP (ajuste SOLO con TRAIN-SUBJECTS) ----------\n",
    "        with mne.utils.use_log_level(\"ERROR\"):\n",
    "            # Fit en ep_tr, transform en ep_val y ep_te\n",
    "            Xtr_fb, Xval_fb = _fit_fb_csp_transform(\n",
    "                ep_tr, ep_val,\n",
    "                fb_bands=fb_bands,\n",
    "                n_csp=n_csp,\n",
    "                motor_only=motor_only,\n",
    "                zscore_epoch=zscore_epoch,\n",
    "                crop_window=crop_window\n",
    "            )\n",
    "            # Obtener Xte con el mismo ajuste (re-aplicando con ep_tr como \"ajuste\")\n",
    "            _, Xte_fb = _fit_fb_csp_transform(\n",
    "                ep_tr, ep_te,\n",
    "                fb_bands=fb_bands,\n",
    "                n_csp=n_csp,\n",
    "                motor_only=motor_only,\n",
    "                zscore_epoch=zscore_epoch,\n",
    "                crop_window=crop_window\n",
    "            )\n",
    "\n",
    "        # ---------- ENTRENAR CLASIFICADOR SOLO CON TRAIN ----------\n",
    "        # Ajuste scaler/LDA con TRAIN y evaluar en VAL y TEST\n",
    "        yhat_val, clf_val, scaler_val = _fit_scale_lda(Xtr_fb, y_tr, Xval_fb)\n",
    "        acc_val = accuracy_score(y_val, yhat_val)\n",
    "        f1m_val = f1_score(y_val, yhat_val, average='macro')\n",
    "        logger.info(f\"[Fold {fold_i}] VAL   acc={acc_val:.4f} | f1m={f1m_val:.4f} | n_val={len(y_val)}\")\n",
    "\n",
    "        # --------- TEST normal o con calibración opcional ----------\n",
    "        if calibrate_n is None or calibrate_n <= 0:\n",
    "            # usar el mismo clf/scaler ajustado con TRAIN para predecir TEST\n",
    "            yhat_te = clf_val.predict(scaler_val.transform(Xte_fb))\n",
    "        else:\n",
    "            # calibración con n sujetos de TEST (no usa VAL)\n",
    "            n_subjs = min(int(calibrate_n), len(test_sids))\n",
    "            calib_sids = test_sids[:n_subjs]\n",
    "            rest_sids  = test_sids[n_subjs:]\n",
    "            ep_calib = mne.concatenate_epochs([ep_map[sid] for sid in calib_sids], on_mismatch='ignore')\n",
    "            ep_te_rest = mne.concatenate_epochs([ep_map[sid] for sid in rest_sids], on_mismatch='ignore') if rest_sids else ep_calib\n",
    "\n",
    "            # Reordenar y etiquetas\n",
    "            try:\n",
    "                ep_calib   = ep_calib.copy().reorder_channels(ep_tr.ch_names)\n",
    "                ep_te_rest = ep_te_rest.copy().reorder_channels(ep_tr.ch_names)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"[Fold {fold_i}] reorder (calib/test_rest): {e}\")\n",
    "\n",
    "            _, y_calib_str = _epochs_to_Xy(ep_calib)\n",
    "            y_calib = le.transform(y_calib_str)\n",
    "\n",
    "            # Recalcular features con TRAIN+CALIB (ajuste) y transformar TEST-REST\n",
    "            with mne.utils.use_log_level(\"ERROR\"):\n",
    "                ep_train_plus_calib = mne.concatenate_epochs([ep_tr, ep_calib], on_mismatch='ignore')\n",
    "                Xtr_comb, Xte_rest = _fit_fb_csp_transform(\n",
    "                    ep_train_plus_calib, ep_te_rest,\n",
    "                    fb_bands=fb_bands,\n",
    "                    n_csp=n_csp,\n",
    "                    motor_only=motor_only,\n",
    "                    zscore_epoch=zscore_epoch,\n",
    "                    crop_window=crop_window\n",
    "                )\n",
    "\n",
    "            y_tr_comb = np.concatenate([y_tr, y_calib])\n",
    "            yhat_te = _fit_scale_lda(Xtr_comb, y_tr_comb, Xte_rest)[0]\n",
    "            if rest_sids:\n",
    "                _, y_te_rest_str = _epochs_to_Xy(ep_te_rest)\n",
    "                y_te = le.transform(y_te_rest_str)\n",
    "            else:\n",
    "                y_te = y_tr_comb  # caso extremo: calibras todo el test\n",
    "\n",
    "        # ---------- MÉTRICAS TEST ----------\n",
    "        acc = accuracy_score(y_te, yhat_te)\n",
    "        f1m = f1_score(y_te, yhat_te, average='macro')\n",
    "        cm = confusion_matrix(y_te, yhat_te, labels=np.arange(len(classes)))\n",
    "        cm_global += cm\n",
    "\n",
    "        # <<< NUEVO: Classification report (TEST) por fold\n",
    "        cls_rep = classification_report(y_te, yhat_te, target_names=classes, digits=4)\n",
    "        logger.info(f\"[Fold {fold_i}] Classification report (TEST):\\n{cls_rep}\")\n",
    "        print(f\"[Fold {fold_i}] Classification report (TEST):\\n{cls_rep}\")\n",
    "        per_fold_reports.append((fold_i, cls_rep))\n",
    "\n",
    "        logger.info(f\"[Fold {fold_i}] TEST  acc={acc:.4f} | f1m={f1m:.4f} | n_test={len(y_te)}\")\n",
    "        rows.append(dict(\n",
    "            fold=int(fold_i),\n",
    "            train_subjects=\",\".join(tr_sids),\n",
    "            val_subjects=\",\".join(val_sids),\n",
    "            test_subjects=\",\".join(test_sids),\n",
    "            val_acc=float(acc_val),\n",
    "            val_f1_macro=float(f1m_val),\n",
    "            acc=float(acc),\n",
    "            f1_macro=float(f1m),\n",
    "            n_val=int(len(y_val)),\n",
    "            n_test=int(len(y_te))\n",
    "        ))\n",
    "        cm_items.append((f\"fold_{fold_i}\", cm, classes))\n",
    "\n",
    "    # ---------- Consolidados ----------\n",
    "    df_rows = pd.DataFrame(rows).sort_values(\"fold\") if rows else pd.DataFrame()\n",
    "    acc_mu   = float(df_rows['acc'].mean()) if not df_rows.empty else 0.0\n",
    "    f1_mu    = float(df_rows['f1_macro'].mean()) if not df_rows.empty else 0.0\n",
    "    val_mu   = float(df_rows['val_acc'].mean()) if not df_rows.empty else 0.0\n",
    "    valf1_mu = float(df_rows['val_f1_macro'].mean()) if not df_rows.empty else 0.0\n",
    "\n",
    "    if not df_rows.empty:\n",
    "        df_rows = pd.concat([df_rows, pd.DataFrame([{\n",
    "            'fold': 0,\n",
    "            'train_subjects': 'GLOBAL',\n",
    "            'val_subjects': 'GLOBAL',\n",
    "            'test_subjects': 'GLOBAL',\n",
    "            'val_acc': val_mu,\n",
    "            'val_f1_macro': valf1_mu,\n",
    "            'acc': acc_mu,\n",
    "            'f1_macro': f1_mu,\n",
    "            'n_val': int(df_rows['n_val'].sum()),\n",
    "            'n_test': int(df_rows['n_test'].sum())\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "    out_csv = (TAB_DIR / f\"{ts}_{save_csv_name}\") if save_csv_name else (TAB_DIR / f\"metrics_inter_subject_cv_{ts}.csv\")\n",
    "    df_rows.to_csv(out_csv, index=False)\n",
    "    logger.info(f\"CSV consolidado → {out_csv}\")\n",
    "    print(\"CSV consolidado →\", out_csv)\n",
    "\n",
    "    out_txt = (LOG_DIR / f\"{ts}_{save_txt_name}\") if save_txt_name else (LOG_DIR / f\"metrics_inter_subject_cv_{ts}.txt\")\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"INTER-SUBJECT CV (folds JSON) — Con VALID interno por sujetos\\n\")\n",
    "        f.write(f\"Generado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Total filas: {len(df_rows)}\\n\\n\")\n",
    "        if not df_rows.empty:\n",
    "            header = df_rows.columns.tolist()\n",
    "            f.write(\" | \".join(header) + \"\\n\")\n",
    "            f.write(\"-\" * 140 + \"\\n\")\n",
    "            for _, row in df_rows.iterrows():\n",
    "                vals = []\n",
    "                for kcol in header:\n",
    "                    v = row[kcol]\n",
    "                    if isinstance(v, float):\n",
    "                        vals.append(f\"{v:.4f}\")\n",
    "                    elif isinstance(v, (np.integer,)):\n",
    "                        vals.append(str(int(v)))\n",
    "                    else:\n",
    "                        vals.append(str(v))\n",
    "                f.write(\" | \".join(vals) + \"\\n\")\n",
    "    logger.info(f\"TXT consolidado → {out_txt}\")\n",
    "    print(\"TXT consolidado →\", out_txt)\n",
    "\n",
    "    # <<< NUEVO: TXT con todos los classification reports por fold (TEST)\n",
    "    reports_txt = LOG_DIR / f\"classification_reports_by_fold_{ts}.txt\"\n",
    "    with open(reports_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"INTER-SUBJECT CV — Classification reports por fold (TEST)\\n\")\n",
    "        f.write(f\"Generado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        for fold_i, rep in sorted(per_fold_reports, key=lambda x: x[0]):\n",
    "            f.write(f\"[Fold {fold_i}] Classification report (TEST)\\n\")\n",
    "            f.write(rep)\n",
    "            if not rep.endswith(\"\\n\"):\n",
    "                f.write(\"\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\")\n",
    "    logger.info(f\"TXT de classification reports → {reports_txt}\")\n",
    "    print(\"TXT de classification reports →\", reports_txt)\n",
    "\n",
    "    # Mosaicos de confusión por fold\n",
    "    if cm_items:\n",
    "        n = len(cm_items)\n",
    "        per_fig = max(1, int(max_subplots_per_fig))\n",
    "        n_figs = ceil(n / per_fig)\n",
    "        n_rows_per_fig = lambda count: ceil(count / n_cols)\n",
    "        for fig_idx in range(n_figs):\n",
    "            start = fig_idx * per_fig\n",
    "            end   = min((fig_idx + 1) * per_fig, n)\n",
    "            chunk = cm_items[start:end]\n",
    "            count = len(chunk)\n",
    "            n_rows = n_rows_per_fig(count)\n",
    "\n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(4.5*n_cols, 3.8*n_rows), dpi=140)\n",
    "            axes = np.atleast_2d(axes).flatten()\n",
    "            for ax_i, (label, cm_sum, classes) in enumerate(chunk):\n",
    "                ax = axes[ax_i]\n",
    "                disp = ConfusionMatrixDisplay(cm_sum, display_labels=classes)\n",
    "                disp.plot(ax=ax, cmap=\"Blues\", colorbar=False, values_format='d')\n",
    "                ax.set_title(f\"{label}\")\n",
    "                ax.set_xlabel(\"\"); ax.set_ylabel(\"\")\n",
    "            for j in range(ax_i + 1, len(axes)):\n",
    "                axes[j].axis(\"off\")\n",
    "\n",
    "            out_png = FIG_DIR / f\"inter_subject_confusions_{ts}_p{fig_idx+1}.png\"\n",
    "            fig.suptitle(f\"Inter-Subject CV — Matrices de confusión (página {fig_idx+1}/{n_figs})\", y=0.995, fontsize=14)\n",
    "            fig.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "            fig.savefig(out_png)\n",
    "            plt.close(fig)\n",
    "            logger.info(f\"Figura consolidada → {out_png}\")\n",
    "            print(\"Figura consolidada →\", out_png)\n",
    "\n",
    "    # Matriz GLOBAL\n",
    "    if cm_global is not None and classes_global is not None:\n",
    "        fig, ax = plt.subplots(figsize=(6.5, 5.2), dpi=140)\n",
    "        disp = ConfusionMatrixDisplay(cm_global, display_labels=classes_global)\n",
    "        disp.plot(ax=ax, cmap=\"Blues\", colorbar=True, values_format='d')\n",
    "        ax.set_title(\"Inter-Subject CV — Matriz de confusión GLOBAL (sumatoria folds)\")\n",
    "        fig.tight_layout()\n",
    "        out_png_glob = FIG_DIR / f\"inter_subject_global_confusion_{ts}.png\"\n",
    "        fig.savefig(out_png_glob)\n",
    "        plt.close(fig)\n",
    "        logger.info(f\"Matriz GLOBAL → {out_png_glob}\")\n",
    "        print(\"Matriz GLOBAL →\", out_png_glob)\n",
    "\n",
    "    logger.info(f\"[GLOBAL] VAL_acc={val_mu:.3f} | VAL_f1m={valf1_mu:.3f} | TEST_acc={acc_mu:.3f} | TEST_f1m={f1_mu:.3f}\")\n",
    "    print(f\"[GLOBAL] VAL_acc={val_mu:.3f} | VAL_f1m={valf1_mu:.3f} | TEST_acc={acc_mu:.3f} | TEST_f1m={f1_mu:.3f}\")\n",
    "    logger.info(f\"Log global de esta corrida → {log_path}\")\n",
    "    print(f\"Log global → {log_path}\")\n",
    "\n",
    "    return df_rows.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce94a18",
   "metadata": {},
   "source": [
    "### Bloque 7 — Ejemplos de ejecución\n",
    "\n",
    "Qué hace: muestra cómo lanzar los “batch” por defecto (4 intra, 2 LOSO) y cómo pasar propia lista de sujetos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd06edc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:24:52] INFO: [RUN inter_subject_cv_json_20251013-062452] Inter-Subject CV (con VALID interno por sujetos)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[RUN inter_subject_cv_json_20251013-062452] Inter-Subject CV (con VALID interno por sujetos)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:24:52] INFO: Perillas: {'crop_window': (0.5, 3.5), 'motor_only': True, 'zscore_epoch': False, 'fb_bands': '[(8, 10), (10, 12), (12, 14), (14, 16), (16, 18), (18, 20), (20, 22), (22, 24), (24, 26), (26, 28), (28, 30)]', 'n_csp': 4} | val_ratio_subjects=0.16 | calibrate_n=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:Perillas: {'crop_window': (0.5, 3.5), 'motor_only': True, 'zscore_epoch': False, 'fb_bands': '[(8, 10), (10, 12), (12, 14), (14, 16), (16, 18), (18, 20), (20, 22), (22, 24), (24, 26), (26, 28), (28, 30)]', 'n_csp': 4} | val_ratio_subjects=0.16 | calibrate_n=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:24:52] INFO: Folds cargadas: 5 | sujetos en JSON: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:Folds cargadas: 5 | sujetos en JSON: 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:24:52] INFO: [Fold 1] train(82): ['S001', 'S002', 'S004', 'S005', 'S006', 'S007', 'S009', 'S010', 'S011', 'S012', 'S014', 'S015', 'S016', 'S017', 'S019', 'S020', 'S021', 'S022', 'S024', 'S025', 'S026', 'S027', 'S029', 'S030', 'S031', 'S032', 'S034', 'S035', 'S036', 'S037', 'S040', 'S041', 'S042', 'S043', 'S045', 'S046', 'S047', 'S048', 'S050', 'S051', 'S052', 'S053', 'S055', 'S056', 'S057', 'S058', 'S060', 'S061', 'S062', 'S063', 'S065', 'S066', 'S067', 'S068', 'S070', 'S071', 'S072', 'S073', 'S075', 'S076', 'S077', 'S078', 'S080', 'S081', 'S082', 'S083', 'S085', 'S086', 'S087', 'S090', 'S093', 'S094', 'S095', 'S096', 'S098', 'S099', 'S101', 'S102', 'S105', 'S106', 'S107', 'S108']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 1] train(82): ['S001', 'S002', 'S004', 'S005', 'S006', 'S007', 'S009', 'S010', 'S011', 'S012', 'S014', 'S015', 'S016', 'S017', 'S019', 'S020', 'S021', 'S022', 'S024', 'S025', 'S026', 'S027', 'S029', 'S030', 'S031', 'S032', 'S034', 'S035', 'S036', 'S037', 'S040', 'S041', 'S042', 'S043', 'S045', 'S046', 'S047', 'S048', 'S050', 'S051', 'S052', 'S053', 'S055', 'S056', 'S057', 'S058', 'S060', 'S061', 'S062', 'S063', 'S065', 'S066', 'S067', 'S068', 'S070', 'S071', 'S072', 'S073', 'S075', 'S076', 'S077', 'S078', 'S080', 'S081', 'S082', 'S083', 'S085', 'S086', 'S087', 'S090', 'S093', 'S094', 'S095', 'S096', 'S098', 'S099', 'S101', 'S102', 'S105', 'S106', 'S107', 'S108']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:24:52] INFO: [Fold 1] test (21): ['S003', 'S008', 'S013', 'S018', 'S023', 'S028', 'S033', 'S039', 'S044', 'S049', 'S054', 'S059', 'S064', 'S069', 'S074', 'S079', 'S084', 'S091', 'S097', 'S103', 'S109']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 1] test (21): ['S003', 'S008', 'S013', 'S018', 'S023', 'S028', 'S033', 'S039', 'S044', 'S049', 'S054', 'S059', 'S064', 'S069', 'S074', 'S079', 'S084', 'S091', 'S097', 'S103', 'S109']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:24:52] INFO: [Fold 1] split interno → train_sids=69, val_sids=13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 1] split interno → train_sids=69, val_sids=13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:27:06] INFO: [Fold 1] VAL   acc=0.4304 | f1m=0.4289 | n_val=1120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 1] VAL   acc=0.4304 | f1m=0.4289 | n_val=1120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:28:15] INFO: [Fold 1] Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Both Feet     0.3319    0.4521    0.3828       334\n",
      "  Both Fists     0.2724    0.2492    0.2603       329\n",
      "        Left     0.3115    0.2485    0.2765       326\n",
      "       Right     0.3490    0.3200    0.3339       325\n",
      "\n",
      "    accuracy                         0.3181      1314\n",
      "   macro avg     0.3162    0.3175    0.3133      1314\n",
      "weighted avg     0.3162    0.3181    0.3136      1314\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 1] Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Both Feet     0.3319    0.4521    0.3828       334\n",
      "  Both Fists     0.2724    0.2492    0.2603       329\n",
      "        Left     0.3115    0.2485    0.2765       326\n",
      "       Right     0.3490    0.3200    0.3339       325\n",
      "\n",
      "    accuracy                         0.3181      1314\n",
      "   macro avg     0.3162    0.3175    0.3133      1314\n",
      "weighted avg     0.3162    0.3181    0.3136      1314\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1] Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Both Feet     0.3319    0.4521    0.3828       334\n",
      "  Both Fists     0.2724    0.2492    0.2603       329\n",
      "        Left     0.3115    0.2485    0.2765       326\n",
      "       Right     0.3490    0.3200    0.3339       325\n",
      "\n",
      "    accuracy                         0.3181      1314\n",
      "   macro avg     0.3162    0.3175    0.3133      1314\n",
      "weighted avg     0.3162    0.3181    0.3136      1314\n",
      "\n",
      "[06:28:15] INFO: [Fold 1] TEST  acc=0.3181 | f1m=0.3133 | n_test=1314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 1] TEST  acc=0.3181 | f1m=0.3133 | n_test=1314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:28:15] INFO: [Fold 2] train(82): ['S001', 'S003', 'S004', 'S005', 'S006', 'S008', 'S009', 'S010', 'S011', 'S013', 'S014', 'S015', 'S016', 'S018', 'S019', 'S020', 'S021', 'S023', 'S024', 'S025', 'S026', 'S028', 'S029', 'S030', 'S031', 'S033', 'S034', 'S035', 'S036', 'S039', 'S040', 'S041', 'S042', 'S044', 'S045', 'S046', 'S047', 'S049', 'S050', 'S051', 'S052', 'S054', 'S055', 'S056', 'S057', 'S059', 'S060', 'S061', 'S062', 'S064', 'S065', 'S066', 'S067', 'S069', 'S070', 'S071', 'S072', 'S074', 'S075', 'S076', 'S077', 'S079', 'S080', 'S081', 'S082', 'S084', 'S085', 'S086', 'S087', 'S091', 'S093', 'S094', 'S095', 'S097', 'S098', 'S099', 'S101', 'S103', 'S105', 'S106', 'S107', 'S109']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 2] train(82): ['S001', 'S003', 'S004', 'S005', 'S006', 'S008', 'S009', 'S010', 'S011', 'S013', 'S014', 'S015', 'S016', 'S018', 'S019', 'S020', 'S021', 'S023', 'S024', 'S025', 'S026', 'S028', 'S029', 'S030', 'S031', 'S033', 'S034', 'S035', 'S036', 'S039', 'S040', 'S041', 'S042', 'S044', 'S045', 'S046', 'S047', 'S049', 'S050', 'S051', 'S052', 'S054', 'S055', 'S056', 'S057', 'S059', 'S060', 'S061', 'S062', 'S064', 'S065', 'S066', 'S067', 'S069', 'S070', 'S071', 'S072', 'S074', 'S075', 'S076', 'S077', 'S079', 'S080', 'S081', 'S082', 'S084', 'S085', 'S086', 'S087', 'S091', 'S093', 'S094', 'S095', 'S097', 'S098', 'S099', 'S101', 'S103', 'S105', 'S106', 'S107', 'S109']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:28:15] INFO: [Fold 2] test (21): ['S002', 'S007', 'S012', 'S017', 'S022', 'S027', 'S032', 'S037', 'S043', 'S048', 'S053', 'S058', 'S063', 'S068', 'S073', 'S078', 'S083', 'S090', 'S096', 'S102', 'S108']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 2] test (21): ['S002', 'S007', 'S012', 'S017', 'S022', 'S027', 'S032', 'S037', 'S043', 'S048', 'S053', 'S058', 'S063', 'S068', 'S073', 'S078', 'S083', 'S090', 'S096', 'S102', 'S108']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:28:15] INFO: [Fold 2] split interno → train_sids=69, val_sids=13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 2] split interno → train_sids=69, val_sids=13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:30:29] INFO: [Fold 2] VAL   acc=0.3721 | f1m=0.3702 | n_val=1067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 2] VAL   acc=0.3721 | f1m=0.3702 | n_val=1067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:31:40] INFO: [Fold 2] Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Both Feet     0.3905    0.3576    0.3733       344\n",
      "  Both Fists     0.3365    0.3145    0.3252       337\n",
      "        Left     0.3412    0.4248    0.3784       339\n",
      "       Right     0.3754    0.3402    0.3569       341\n",
      "\n",
      "    accuracy                         0.3593      1361\n",
      "   macro avg     0.3609    0.3593    0.3585      1361\n",
      "weighted avg     0.3611    0.3593    0.3586      1361\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 2] Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Both Feet     0.3905    0.3576    0.3733       344\n",
      "  Both Fists     0.3365    0.3145    0.3252       337\n",
      "        Left     0.3412    0.4248    0.3784       339\n",
      "       Right     0.3754    0.3402    0.3569       341\n",
      "\n",
      "    accuracy                         0.3593      1361\n",
      "   macro avg     0.3609    0.3593    0.3585      1361\n",
      "weighted avg     0.3611    0.3593    0.3586      1361\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 2] Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Both Feet     0.3905    0.3576    0.3733       344\n",
      "  Both Fists     0.3365    0.3145    0.3252       337\n",
      "        Left     0.3412    0.4248    0.3784       339\n",
      "       Right     0.3754    0.3402    0.3569       341\n",
      "\n",
      "    accuracy                         0.3593      1361\n",
      "   macro avg     0.3609    0.3593    0.3585      1361\n",
      "weighted avg     0.3611    0.3593    0.3586      1361\n",
      "\n",
      "[06:31:40] INFO: [Fold 2] TEST  acc=0.3593 | f1m=0.3585 | n_test=1361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 2] TEST  acc=0.3593 | f1m=0.3585 | n_test=1361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:31:40] INFO: [Fold 3] train(82): ['S002', 'S003', 'S004', 'S005', 'S007', 'S008', 'S009', 'S010', 'S012', 'S013', 'S014', 'S015', 'S017', 'S018', 'S019', 'S020', 'S022', 'S023', 'S024', 'S025', 'S027', 'S028', 'S029', 'S030', 'S032', 'S033', 'S034', 'S035', 'S037', 'S039', 'S040', 'S041', 'S043', 'S044', 'S045', 'S046', 'S048', 'S049', 'S050', 'S051', 'S053', 'S054', 'S055', 'S056', 'S058', 'S059', 'S060', 'S061', 'S063', 'S064', 'S065', 'S066', 'S068', 'S069', 'S070', 'S071', 'S073', 'S074', 'S075', 'S076', 'S078', 'S079', 'S080', 'S081', 'S083', 'S084', 'S085', 'S086', 'S090', 'S091', 'S093', 'S094', 'S096', 'S097', 'S098', 'S099', 'S102', 'S103', 'S105', 'S106', 'S108', 'S109']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 3] train(82): ['S002', 'S003', 'S004', 'S005', 'S007', 'S008', 'S009', 'S010', 'S012', 'S013', 'S014', 'S015', 'S017', 'S018', 'S019', 'S020', 'S022', 'S023', 'S024', 'S025', 'S027', 'S028', 'S029', 'S030', 'S032', 'S033', 'S034', 'S035', 'S037', 'S039', 'S040', 'S041', 'S043', 'S044', 'S045', 'S046', 'S048', 'S049', 'S050', 'S051', 'S053', 'S054', 'S055', 'S056', 'S058', 'S059', 'S060', 'S061', 'S063', 'S064', 'S065', 'S066', 'S068', 'S069', 'S070', 'S071', 'S073', 'S074', 'S075', 'S076', 'S078', 'S079', 'S080', 'S081', 'S083', 'S084', 'S085', 'S086', 'S090', 'S091', 'S093', 'S094', 'S096', 'S097', 'S098', 'S099', 'S102', 'S103', 'S105', 'S106', 'S108', 'S109']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:31:40] INFO: [Fold 3] test (21): ['S001', 'S006', 'S011', 'S016', 'S021', 'S026', 'S031', 'S036', 'S042', 'S047', 'S052', 'S057', 'S062', 'S067', 'S072', 'S077', 'S082', 'S087', 'S095', 'S101', 'S107']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 3] test (21): ['S001', 'S006', 'S011', 'S016', 'S021', 'S026', 'S031', 'S036', 'S042', 'S047', 'S052', 'S057', 'S062', 'S067', 'S072', 'S077', 'S082', 'S087', 'S095', 'S101', 'S107']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:31:40] INFO: [Fold 3] split interno → train_sids=69, val_sids=13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 3] split interno → train_sids=69, val_sids=13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:33:53] INFO: [Fold 3] VAL   acc=0.3375 | f1m=0.3358 | n_val=1111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 3] VAL   acc=0.3375 | f1m=0.3358 | n_val=1111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:35:04] INFO: [Fold 3] Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Both Feet     0.4648    0.4444    0.4544       342\n",
      "  Both Fists     0.3235    0.4181    0.3648       342\n",
      "        Left     0.3686    0.3324    0.3495       346\n",
      "       Right     0.3785    0.3215    0.3477       339\n",
      "\n",
      "    accuracy                         0.3791      1369\n",
      "   macro avg     0.3839    0.3791    0.3791      1369\n",
      "weighted avg     0.3838    0.3791    0.3791      1369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 3] Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Both Feet     0.4648    0.4444    0.4544       342\n",
      "  Both Fists     0.3235    0.4181    0.3648       342\n",
      "        Left     0.3686    0.3324    0.3495       346\n",
      "       Right     0.3785    0.3215    0.3477       339\n",
      "\n",
      "    accuracy                         0.3791      1369\n",
      "   macro avg     0.3839    0.3791    0.3791      1369\n",
      "weighted avg     0.3838    0.3791    0.3791      1369\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 3] Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Both Feet     0.4648    0.4444    0.4544       342\n",
      "  Both Fists     0.3235    0.4181    0.3648       342\n",
      "        Left     0.3686    0.3324    0.3495       346\n",
      "       Right     0.3785    0.3215    0.3477       339\n",
      "\n",
      "    accuracy                         0.3791      1369\n",
      "   macro avg     0.3839    0.3791    0.3791      1369\n",
      "weighted avg     0.3838    0.3791    0.3791      1369\n",
      "\n",
      "[06:35:04] INFO: [Fold 3] TEST  acc=0.3791 | f1m=0.3791 | n_test=1369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 3] TEST  acc=0.3791 | f1m=0.3791 | n_test=1369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:35:04] INFO: [Fold 4] train(83): ['S001', 'S002', 'S003', 'S004', 'S006', 'S007', 'S008', 'S009', 'S011', 'S012', 'S013', 'S014', 'S016', 'S017', 'S018', 'S019', 'S021', 'S022', 'S023', 'S024', 'S026', 'S027', 'S028', 'S029', 'S031', 'S032', 'S033', 'S034', 'S036', 'S037', 'S039', 'S040', 'S042', 'S043', 'S044', 'S045', 'S047', 'S048', 'S049', 'S050', 'S052', 'S053', 'S054', 'S055', 'S057', 'S058', 'S059', 'S060', 'S062', 'S063', 'S064', 'S065', 'S067', 'S068', 'S069', 'S070', 'S072', 'S073', 'S074', 'S075', 'S077', 'S078', 'S079', 'S080', 'S082', 'S083', 'S084', 'S085', 'S087', 'S090', 'S091', 'S093', 'S095', 'S096', 'S097', 'S098', 'S101', 'S102', 'S103', 'S105', 'S107', 'S108', 'S109']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 4] train(83): ['S001', 'S002', 'S003', 'S004', 'S006', 'S007', 'S008', 'S009', 'S011', 'S012', 'S013', 'S014', 'S016', 'S017', 'S018', 'S019', 'S021', 'S022', 'S023', 'S024', 'S026', 'S027', 'S028', 'S029', 'S031', 'S032', 'S033', 'S034', 'S036', 'S037', 'S039', 'S040', 'S042', 'S043', 'S044', 'S045', 'S047', 'S048', 'S049', 'S050', 'S052', 'S053', 'S054', 'S055', 'S057', 'S058', 'S059', 'S060', 'S062', 'S063', 'S064', 'S065', 'S067', 'S068', 'S069', 'S070', 'S072', 'S073', 'S074', 'S075', 'S077', 'S078', 'S079', 'S080', 'S082', 'S083', 'S084', 'S085', 'S087', 'S090', 'S091', 'S093', 'S095', 'S096', 'S097', 'S098', 'S101', 'S102', 'S103', 'S105', 'S107', 'S108', 'S109']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:35:04] INFO: [Fold 4] test (20): ['S005', 'S010', 'S015', 'S020', 'S025', 'S030', 'S035', 'S041', 'S046', 'S051', 'S056', 'S061', 'S066', 'S071', 'S076', 'S081', 'S086', 'S094', 'S099', 'S106']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 4] test (20): ['S005', 'S010', 'S015', 'S020', 'S025', 'S030', 'S035', 'S041', 'S046', 'S051', 'S056', 'S061', 'S066', 'S071', 'S076', 'S081', 'S086', 'S094', 'S099', 'S106']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:35:04] INFO: [Fold 4] split interno → train_sids=70, val_sids=13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 4] split interno → train_sids=70, val_sids=13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:37:20] INFO: [Fold 4] VAL   acc=0.3243 | f1m=0.3236 | n_val=1101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 4] VAL   acc=0.3243 | f1m=0.3236 | n_val=1101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:38:30] INFO: [Fold 4] Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Both Feet     0.5392    0.3354    0.4135       328\n",
      "  Both Fists     0.3447    0.3353    0.3400       331\n",
      "        Left     0.3376    0.4819    0.3970       332\n",
      "       Right     0.3533    0.3436    0.3484       326\n",
      "\n",
      "    accuracy                         0.3743      1317\n",
      "   macro avg     0.3937    0.3740    0.3747      1317\n",
      "weighted avg     0.3935    0.3743    0.3748      1317\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 4] Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Both Feet     0.5392    0.3354    0.4135       328\n",
      "  Both Fists     0.3447    0.3353    0.3400       331\n",
      "        Left     0.3376    0.4819    0.3970       332\n",
      "       Right     0.3533    0.3436    0.3484       326\n",
      "\n",
      "    accuracy                         0.3743      1317\n",
      "   macro avg     0.3937    0.3740    0.3747      1317\n",
      "weighted avg     0.3935    0.3743    0.3748      1317\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 4] Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Both Feet     0.5392    0.3354    0.4135       328\n",
      "  Both Fists     0.3447    0.3353    0.3400       331\n",
      "        Left     0.3376    0.4819    0.3970       332\n",
      "       Right     0.3533    0.3436    0.3484       326\n",
      "\n",
      "    accuracy                         0.3743      1317\n",
      "   macro avg     0.3937    0.3740    0.3747      1317\n",
      "weighted avg     0.3935    0.3743    0.3748      1317\n",
      "\n",
      "[06:38:30] INFO: [Fold 4] TEST  acc=0.3743 | f1m=0.3747 | n_test=1317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 4] TEST  acc=0.3743 | f1m=0.3747 | n_test=1317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:38:30] INFO: [Fold 5] train(83): ['S001', 'S002', 'S003', 'S005', 'S006', 'S007', 'S008', 'S010', 'S011', 'S012', 'S013', 'S015', 'S016', 'S017', 'S018', 'S020', 'S021', 'S022', 'S023', 'S025', 'S026', 'S027', 'S028', 'S030', 'S031', 'S032', 'S033', 'S035', 'S036', 'S037', 'S039', 'S041', 'S042', 'S043', 'S044', 'S046', 'S047', 'S048', 'S049', 'S051', 'S052', 'S053', 'S054', 'S056', 'S057', 'S058', 'S059', 'S061', 'S062', 'S063', 'S064', 'S066', 'S067', 'S068', 'S069', 'S071', 'S072', 'S073', 'S074', 'S076', 'S077', 'S078', 'S079', 'S081', 'S082', 'S083', 'S084', 'S086', 'S087', 'S090', 'S091', 'S094', 'S095', 'S096', 'S097', 'S099', 'S101', 'S102', 'S103', 'S106', 'S107', 'S108', 'S109']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 5] train(83): ['S001', 'S002', 'S003', 'S005', 'S006', 'S007', 'S008', 'S010', 'S011', 'S012', 'S013', 'S015', 'S016', 'S017', 'S018', 'S020', 'S021', 'S022', 'S023', 'S025', 'S026', 'S027', 'S028', 'S030', 'S031', 'S032', 'S033', 'S035', 'S036', 'S037', 'S039', 'S041', 'S042', 'S043', 'S044', 'S046', 'S047', 'S048', 'S049', 'S051', 'S052', 'S053', 'S054', 'S056', 'S057', 'S058', 'S059', 'S061', 'S062', 'S063', 'S064', 'S066', 'S067', 'S068', 'S069', 'S071', 'S072', 'S073', 'S074', 'S076', 'S077', 'S078', 'S079', 'S081', 'S082', 'S083', 'S084', 'S086', 'S087', 'S090', 'S091', 'S094', 'S095', 'S096', 'S097', 'S099', 'S101', 'S102', 'S103', 'S106', 'S107', 'S108', 'S109']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:38:30] INFO: [Fold 5] test (20): ['S004', 'S009', 'S014', 'S019', 'S024', 'S029', 'S034', 'S040', 'S045', 'S050', 'S055', 'S060', 'S065', 'S070', 'S075', 'S080', 'S085', 'S093', 'S098', 'S105']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 5] test (20): ['S004', 'S009', 'S014', 'S019', 'S024', 'S029', 'S034', 'S040', 'S045', 'S050', 'S055', 'S060', 'S065', 'S070', 'S075', 'S080', 'S085', 'S093', 'S098', 'S105']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:38:30] INFO: [Fold 5] split interno → train_sids=70, val_sids=13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 5] split interno → train_sids=70, val_sids=13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:40:45] INFO: [Fold 5] VAL   acc=0.3119 | f1m=0.3103 | n_val=1106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 5] VAL   acc=0.3119 | f1m=0.3103 | n_val=1106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:41:55] INFO: [Fold 5] Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Both Feet     0.4796    0.5893    0.5288       319\n",
      "  Both Fists     0.3904    0.2808    0.3266       317\n",
      "        Left     0.4211    0.3750    0.3967       320\n",
      "       Right     0.4592    0.5331    0.4934       317\n",
      "\n",
      "    accuracy                         0.4446      1273\n",
      "   macro avg     0.4376    0.4446    0.4364      1273\n",
      "weighted avg     0.4376    0.4446    0.4364      1273\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 5] Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Both Feet     0.4796    0.5893    0.5288       319\n",
      "  Both Fists     0.3904    0.2808    0.3266       317\n",
      "        Left     0.4211    0.3750    0.3967       320\n",
      "       Right     0.4592    0.5331    0.4934       317\n",
      "\n",
      "    accuracy                         0.4446      1273\n",
      "   macro avg     0.4376    0.4446    0.4364      1273\n",
      "weighted avg     0.4376    0.4446    0.4364      1273\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 5] Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Both Feet     0.4796    0.5893    0.5288       319\n",
      "  Both Fists     0.3904    0.2808    0.3266       317\n",
      "        Left     0.4211    0.3750    0.3967       320\n",
      "       Right     0.4592    0.5331    0.4934       317\n",
      "\n",
      "    accuracy                         0.4446      1273\n",
      "   macro avg     0.4376    0.4446    0.4364      1273\n",
      "weighted avg     0.4376    0.4446    0.4364      1273\n",
      "\n",
      "[06:41:55] INFO: [Fold 5] TEST  acc=0.4446 | f1m=0.4364 | n_test=1273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[Fold 5] TEST  acc=0.4446 | f1m=0.4364 | n_test=1273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:41:55] INFO: CSV consolidado → /root/Proyecto/EEG_Clasificador/models/fbcsp_lda/tables/20251013-062452_inter_subject_cv_from_json.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:CSV consolidado → /root/Proyecto/EEG_Clasificador/models/fbcsp_lda/tables/20251013-062452_inter_subject_cv_from_json.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV consolidado → /root/Proyecto/EEG_Clasificador/models/fbcsp_lda/tables/20251013-062452_inter_subject_cv_from_json.csv\n",
      "[06:41:55] INFO: TXT consolidado → /root/Proyecto/EEG_Clasificador/models/fbcsp_lda/logs/metrics_inter_subject_cv_20251013-062452.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:TXT consolidado → /root/Proyecto/EEG_Clasificador/models/fbcsp_lda/logs/metrics_inter_subject_cv_20251013-062452.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TXT consolidado → /root/Proyecto/EEG_Clasificador/models/fbcsp_lda/logs/metrics_inter_subject_cv_20251013-062452.txt\n",
      "[06:41:55] INFO: TXT de classification reports → /root/Proyecto/EEG_Clasificador/models/fbcsp_lda/logs/classification_reports_by_fold_20251013-062452.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:TXT de classification reports → /root/Proyecto/EEG_Clasificador/models/fbcsp_lda/logs/classification_reports_by_fold_20251013-062452.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TXT de classification reports → /root/Proyecto/EEG_Clasificador/models/fbcsp_lda/logs/classification_reports_by_fold_20251013-062452.txt\n",
      "[06:41:56] INFO: Figura consolidada → /root/Proyecto/EEG_Clasificador/models/fbcsp_lda/figures/inter_subject_confusions_20251013-062452_p1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:Figura consolidada → /root/Proyecto/EEG_Clasificador/models/fbcsp_lda/figures/inter_subject_confusions_20251013-062452_p1.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figura consolidada → /root/Proyecto/EEG_Clasificador/models/fbcsp_lda/figures/inter_subject_confusions_20251013-062452_p1.png\n",
      "[06:41:56] INFO: Matriz GLOBAL → /root/Proyecto/EEG_Clasificador/models/fbcsp_lda/figures/inter_subject_global_confusion_20251013-062452.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:Matriz GLOBAL → /root/Proyecto/EEG_Clasificador/models/fbcsp_lda/figures/inter_subject_global_confusion_20251013-062452.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz GLOBAL → /root/Proyecto/EEG_Clasificador/models/fbcsp_lda/figures/inter_subject_global_confusion_20251013-062452.png\n",
      "[06:41:56] INFO: [GLOBAL] VAL_acc=0.355 | VAL_f1m=0.354 | TEST_acc=0.375 | TEST_f1m=0.372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:[GLOBAL] VAL_acc=0.355 | VAL_f1m=0.354 | TEST_acc=0.375 | TEST_f1m=0.372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GLOBAL] VAL_acc=0.355 | VAL_f1m=0.354 | TEST_acc=0.375 | TEST_f1m=0.372\n",
      "[06:41:56] INFO: Log global de esta corrida → /root/Proyecto/EEG_Clasificador/models/fbcsp_lda/logs/20251013-062452_inter_subject_cv_json_20251013-062452.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inter_subject_cv_json_20251013-062452:Log global de esta corrida → /root/Proyecto/EEG_Clasificador/models/fbcsp_lda/logs/20251013-062452_inter_subject_cv_json_20251013-062452.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log global → /root/Proyecto/EEG_Clasificador/models/fbcsp_lda/logs/20251013-062452_inter_subject_cv_json_20251013-062452.txt\n"
     ]
    }
   ],
   "source": [
    "# INTRA en todos los sujetos\n",
    "# df_intra_optimal = run_intra_all(\n",
    "#     k=5,\n",
    "#     random_state=42,\n",
    "#     crop_window=(0.5, 4.5),      # VENTANA ORIGINAL PROBADA\n",
    "#     motor_only=True,\n",
    "#     zscore_epoch=False,\n",
    "#     fb_bands=FB_BANDS_DENSE,     # 11 BANDS ORIGINALES\n",
    "#     n_csp=4,\n",
    "#     save_txt_name=\"intra_optimal_proven.txt\"\n",
    "# )\n",
    "\n",
    "# Inter-subject CV usando JSON de folds (reemplaza LOSO clásico)\n",
    "# Ajusta folds_json_path si usaste otra ruta; por defecto busca PROJ/models/folds/Kfold5.json\n",
    "df_inter = run_inter_subject_cv_from_json(\n",
    "    fif_dir=DATA_PROC,\n",
    "    folds_json_path=PROJ / 'models' / 'folds' / 'Kfold5.json',\n",
    "    crop_window=(0.5, 3.5),\n",
    "    motor_only=True,\n",
    "    zscore_epoch=False,\n",
    "    fb_bands=FB_BANDS_DENSE,\n",
    "    n_csp=4,\n",
    "    calibrate_n=5,\n",
    "    save_csv_name=\"inter_subject_cv_from_json.csv\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
