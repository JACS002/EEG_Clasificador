{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b99cce37",
   "metadata": {},
   "source": [
    "# FBCSP + LDA (intra-sujeto y cross-sujeto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767badd5",
   "metadata": {},
   "source": [
    "### Bloque 1 — Rutas de salida + utilidades de logging y guardado\n",
    "\n",
    "Qué hace: define las carpetas donde se guardarán figuras, tablas y logs bajo models/fbcsp_lda/. Incluye utilidades para inicializar un logger limpio, guardar matrices de confusión (PNG + CSV) y anexar métricas a CSVs acumulativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27eaa450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio de datos procesados: C:\\Users\\joelc\\Desktop\\EEG_Clasificador\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "# %% [PATHS & LOGGING] — rutas de salida + helpers para logs/figuras/tablas\n",
    "import sys, logging, warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import mne\n",
    "\n",
    "# Raíz del repo (este notebook está en models/fbcsp_lda/)\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_PROC = PROJ / 'data' / 'processed'\n",
    "OUT_ROOT = PROJ / 'models' / 'fbcsp_lda'\n",
    "FIG_DIR  = OUT_ROOT / 'figures'\n",
    "TAB_DIR  = OUT_ROOT / 'tables'\n",
    "LOG_DIR  = OUT_ROOT / 'logs'\n",
    "for d in (FIG_DIR, TAB_DIR, LOG_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Directorio de datos procesados: {DATA_PROC}\")\n",
    "\n",
    "def _init_logger(run_name: str):\n",
    "    \"\"\"\n",
    "    Crea un logger que escribe a consola y a TXT en models/fbcsp_lda/logs/.\n",
    "    Reduce la verbosidad de MNE para que los logs sean legibles.\n",
    "    \"\"\"\n",
    "    ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_path = LOG_DIR / f\"{ts}_{run_name}.txt\"\n",
    "\n",
    "    logger = logging.getLogger(run_name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.handlers.clear()\n",
    "\n",
    "    fmt = logging.Formatter(\"[%(asctime)s] %(levelname)s: %(message)s\", datefmt=\"%H:%M:%S\")\n",
    "    ch = logging.StreamHandler(stream=sys.stdout); ch.setLevel(logging.INFO); ch.setFormatter(fmt)\n",
    "    fh = logging.FileHandler(log_path, encoding=\"utf-8\"); fh.setLevel(logging.INFO); fh.setFormatter(fmt)\n",
    "    logger.addHandler(ch); logger.addHandler(fh)\n",
    "\n",
    "    # Silenciar ruido externo (sin afectar tus prints/logs)\n",
    "    mne.set_log_level(\"ERROR\")\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"mne\")\n",
    "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"mne\")\n",
    "    return logger, log_path\n",
    "\n",
    "def _save_confusion(cm: np.ndarray, class_names, title: str, stem: str):\n",
    "    \"\"\"\n",
    "    Guarda matriz de confusión como PNG y CSV en FIG_DIR/TAB_DIR.\n",
    "    - stem: nombre base de archivo (sin extensión).\n",
    "    \"\"\"\n",
    "    # CSV\n",
    "    df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "    csv_path = TAB_DIR / f\"{stem}_confusion.csv\"\n",
    "    df_cm.to_csv(csv_path, index=True)\n",
    "\n",
    "    # PNG\n",
    "    fig, ax = plt.subplots(figsize=(5.4, 4.6), dpi=140)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=class_names)\n",
    "    disp.plot(ax=ax, cmap=\"Blues\", colorbar=True, values_format='d')\n",
    "    ax.set_title(title)\n",
    "    fig.tight_layout()\n",
    "    png_path = FIG_DIR / f\"{stem}_confusion.png\"\n",
    "    fig.savefig(png_path)\n",
    "    plt.close(fig)\n",
    "    return csv_path, png_path\n",
    "\n",
    "def _append_metrics(row: dict, table_name: str):\n",
    "    \"\"\"\n",
    "    Anexa una fila 'row' (dict) a un CSV en TAB_DIR (lo crea si no existe).\n",
    "    Devuelve la ruta del archivo actualizado.\n",
    "    \"\"\"\n",
    "    path = TAB_DIR / table_name\n",
    "    df = pd.DataFrame([row])\n",
    "    if path.exists():\n",
    "        df.to_csv(path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(path, index=False)\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57bbfa1",
   "metadata": {},
   "source": [
    "### Bloque 2 — FBCSP Helpers\n",
    "\n",
    "Qué hace: helpers de modelo. Extraen X/y desde Epochs, aplican Filter-Bank + CSP por bandas y entrenan/escala LDA. Mantiene tus comentarios y añade compatibilidad con versiones de CSP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d10abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [FBCSP Helpers — Mejorado]  — banco de filtros, picks motores, z-score por época, crop\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "from pathlib import Path\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from mne.decoding import CSP\n",
    "\n",
    "# --- Banco de filtros ---\n",
    "# Denso (2 Hz) en 8–30: mejor cobertura de mu/beta para MI\n",
    "FB_BANDS_DENSE = [(f, f+2) for f in range(8, 30, 2)]\n",
    "# Clásico (más corto)\n",
    "FB_BANDS_CLASSIC = [(8,12), (12,16), (16,20), (20,24), (24,28), (28,30)]\n",
    "# Alias por defecto para evitar NameError en otros bloques\n",
    "DEFAULT_FB_BANDS = FB_BANDS_DENSE\n",
    "\n",
    "# Nº componentes CSP por sub-banda (6–8 suele ir bien con bancos densos)\n",
    "DEFAULT_N_CSP = 6\n",
    "\n",
    "# LDA con shrinkage automático robusto\n",
    "LDA_PARAMS = dict(solver='lsqr', shrinkage='auto')\n",
    "\n",
    "# Tokens para picks motores\n",
    "MOTOR_TOKENS = ['C3', 'CZ', 'C4', 'FC3', 'FC4', 'CP3', 'CPZ', 'CP4']\n",
    "\n",
    "def _epochs_to_Xy(epochs: mne.Epochs):\n",
    "    \"\"\"Extrae X (numpy) y y (clases string) desde Epochs (respeta event_id).\"\"\"\n",
    "    X = epochs.get_data()  # (n_epochs, n_channels, n_times)\n",
    "    inv = {v:k for k,v in epochs.event_id.items()}  # int->clase\n",
    "    y = np.array([inv[e[-1]] for e in epochs.events], dtype=object)\n",
    "    return X, y\n",
    "\n",
    "def _find_motor_chs(ch_names, tokens=MOTOR_TOKENS):\n",
    "    \"\"\"Devuelve índices de canales que contienen C3/Cz/C4 (case-insensitive).\"\"\"\n",
    "    up = [c.upper() for c in ch_names]\n",
    "    picks = []\n",
    "    for tok in tokens:\n",
    "        TU = tok.upper()\n",
    "        for i, name in enumerate(up):\n",
    "            if TU in name:\n",
    "                picks.append(i); break\n",
    "    return sorted(set(picks))\n",
    "\n",
    "def _epochwise_zscore(X, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Z-score por época y canal: para cada (epoch, canal) normaliza en el eje tiempo.\n",
    "    X: (n_epochs, n_channels, n_times)  -> retorna mismo shape.\n",
    "    \"\"\"\n",
    "    mean = X.mean(axis=-1, keepdims=True)\n",
    "    std  = X.std(axis=-1, keepdims=True)\n",
    "    return (X - mean) / (std + eps)\n",
    "\n",
    "def _fit_fb_csp_transform(train_ep: mne.Epochs,\n",
    "                          test_ep: mne.Epochs,\n",
    "                          fb_bands=DEFAULT_FB_BANDS,\n",
    "                          n_csp=DEFAULT_N_CSP,\n",
    "                          motor_only=False,\n",
    "                          zscore_epoch=False,\n",
    "                          crop_window=None):\n",
    "    \"\"\"\n",
    "    Aplica FBCSP con opciones:\n",
    "      - crop_window=(tmin,tmax) recorta épocas sin reexportar FIF\n",
    "      - motor_only=True: usa solo canales elegidos\n",
    "      - zscore_epoch=True: z-score dentro de cada época y canal antes de CSP\n",
    "      - fb_bands: lista de sub-bandas [(fmin,fmax), ...]\n",
    "    Devuelve (Xtr_fb, Xte_fb) con features concatenadas.\n",
    "    \"\"\"\n",
    "    tr = train_ep.copy()\n",
    "    te = test_ep.copy()\n",
    "\n",
    "    if crop_window is not None:\n",
    "        tmin, tmax = crop_window\n",
    "        tr.crop(tmin, tmax)\n",
    "        te.crop(tmin, tmax)\n",
    "\n",
    "    if motor_only:\n",
    "        picks = _find_motor_chs(tr.ch_names)\n",
    "        if picks:\n",
    "            tr.pick(picks)\n",
    "            te = te.copy().reorder_channels(tr.ch_names)\n",
    "\n",
    "    Xtr_list, Xte_list = [], []\n",
    "    y_tr = tr.events[:, -1]\n",
    "\n",
    "    for (fmin, fmax) in fb_bands:\n",
    "        tr_b = tr.copy().filter(fmin, fmax, picks='eeg', verbose=False)\n",
    "        te_b = te.copy().filter(fmin, fmax, picks='eeg', verbose=False)\n",
    "\n",
    "        Xtr = tr_b.get_data()\n",
    "        Xte = te_b.get_data()\n",
    "\n",
    "        if zscore_epoch:\n",
    "            Xtr = _epochwise_zscore(Xtr)\n",
    "            Xte = _epochwise_zscore(Xte)\n",
    "\n",
    "        try:\n",
    "            csp = CSP(n_components=n_csp, reg='ledoit_wolf', log=True, norm_trace=False)\n",
    "        except TypeError:\n",
    "            csp = CSP(n_components=n_csp, reg='ledoit_wolf', log=True)\n",
    "\n",
    "        Xtr_c = csp.fit_transform(Xtr, y_tr)\n",
    "        Xte_c = csp.transform(Xte)\n",
    "\n",
    "        Xtr_list.append(Xtr_c)\n",
    "        Xte_list.append(Xte_c)\n",
    "\n",
    "    Xtr_fb = np.concatenate(Xtr_list, axis=1)\n",
    "    Xte_fb = np.concatenate(Xte_list,  axis=1)\n",
    "    return Xtr_fb, Xte_fb\n",
    "\n",
    "def _fit_scale_lda(Xtr, ytr, Xte, lda_params=LDA_PARAMS):\n",
    "    \"\"\"Estandariza features (fit en train) + LDA. Devuelve (yhat, clf, scaler).\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    Xtr_s = scaler.fit_transform(Xtr)\n",
    "    Xte_s = scaler.transform(Xte)\n",
    "\n",
    "    clf = LDA(**lda_params)\n",
    "    clf.fit(Xtr_s, ytr)\n",
    "    yhat = clf.predict(Xte_s)\n",
    "    return yhat, clf, scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aa085e",
   "metadata": {},
   "source": [
    "## Bloque - Diagnostico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7005c45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %% [Running classifier — Diagnóstico de ventana temporal]\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# def run_running_classifier(epochs: mne.Epochs,\n",
    "#                            crop_train=(1.0, 2.0),  # ventana fija para entrenar\n",
    "#                            w_len=0.5,             # ancho de ventana deslizante (s)\n",
    "#                            w_step=0.1,            # paso entre ventanas (s)\n",
    "#                            n_splits=10,\n",
    "#                            n_csp=6):\n",
    "#     \"\"\"\n",
    "#     Entrena CSP+LDA en crop_train y evalúa en ventanas deslizantes a lo largo del epoch completo.\n",
    "#     Devuelve (times, curve) donde curve es la accuracy promedio vs tiempo (centro de ventana).\n",
    "#     Útil para elegir la mejor crop_window para tus experimentos.\n",
    "#     \"\"\"\n",
    "#     # Datos completos (para test deslizante)\n",
    "#     X_full = epochs.get_data()\n",
    "#     inv = {v: k for k, v in epochs.event_id.items()}\n",
    "#     y_str = np.array([inv[e[-1]] for e in epochs.events], dtype=object)\n",
    "#     y = LabelEncoder().fit_transform(y_str)\n",
    "#     sf = epochs.info['sfreq']\n",
    "\n",
    "#     # Conjunto de entrenamiento limitado a la ventana crop_train\n",
    "#     ep_train = epochs.copy().crop(*crop_train)\n",
    "#     Xtr = ep_train.get_data()\n",
    "\n",
    "#     # Ventanas deslizantes (sobre el epoch completo)\n",
    "#     w_len_s = int(sf * w_len)\n",
    "#     w_step_s = int(sf * w_step)\n",
    "#     starts = np.arange(0, X_full.shape[2] - w_len_s, w_step_s)\n",
    "\n",
    "#     skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "#     win_scores = []\n",
    "#     for tr, te in skf.split(Xtr, y):\n",
    "#         pipe = Pipeline([\n",
    "#             ('csp', CSP(n_components=n_csp, reg='ledoit_wolf', log=True)),\n",
    "#             ('lda', LDA(**LDA_PARAMS))\n",
    "#         ])\n",
    "#         pipe.fit(Xtr[tr], y[tr])\n",
    "\n",
    "#         fold_scores = []\n",
    "#         for n in starts:\n",
    "#             Xwin = X_full[te][:, :, n:n+w_len_s]\n",
    "#             fold_scores.append(pipe.score(Xwin, y[te]))\n",
    "#         win_scores.append(fold_scores)\n",
    "\n",
    "#     curve = np.mean(win_scores, axis=0)\n",
    "#     times = (starts + w_len_s/2) / sf + epochs.tmin\n",
    "#     return times, curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1f707b",
   "metadata": {},
   "source": [
    "### Bloque 3 — Inspección de datos\n",
    "\n",
    "Qué hace: muestra rutas y un listado rápido del contenido de data/ y data/processed/ para verificar que los FIF están donde esperamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0adb0212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJ: C:\\Users\\joelc\\Desktop\\EEG_Clasificador\n",
      "DATA: C:\\Users\\joelc\\Desktop\\EEG_Clasificador\\data\n",
      "DATA_PROC: C:\\Users\\joelc\\Desktop\\EEG_Clasificador\\data\\processed\n",
      "\n",
      "Contenido de data (top-level):\n",
      " - processed/\n",
      " - raw/\n",
      "\n",
      "Contenido de data/processed (muestras):\n",
      " - all_subjects-epo.fif\n",
      " - S001_MI-epo.fif\n",
      " - S002_MI-epo.fif\n",
      " - S003_MI-epo.fif\n",
      " - S004_MI-epo.fif\n",
      " - S005_MI-epo.fif\n",
      " - S006_MI-epo.fif\n",
      " - S007_MI-epo.fif\n",
      " - S008_MI-epo.fif\n",
      " - S009_MI-epo.fif\n",
      " - S010_MI-epo.fif\n",
      " - S011_MI-epo.fif\n",
      " - S012_MI-epo.fif\n",
      " - S013_MI-epo.fif\n",
      " - S014_MI-epo.fif\n",
      " - S015_MI-epo.fif\n",
      " - S016_MI-epo.fif\n",
      " - S017_MI-epo.fif\n",
      " - S018_MI-epo.fif\n",
      " - S019_MI-epo.fif\n",
      " - S020_MI-epo.fif\n",
      " - S021_MI-epo.fif\n",
      " - S022_MI-epo.fif\n",
      " - S023_MI-epo.fif\n",
      " - S024_MI-epo.fif\n",
      " - S025_MI-epo.fif\n",
      " - S026_MI-epo.fif\n",
      " - S027_MI-epo.fif\n",
      " - S028_MI-epo.fif\n",
      " - S029_MI-epo.fif\n",
      " - S030_MI-epo.fif\n",
      " - S031_MI-epo.fif\n",
      " - S032_MI-epo.fif\n",
      " - S033_MI-epo.fif\n",
      " - S034_MI-epo.fif\n",
      " - S035_MI-epo.fif\n",
      " - S036_MI-epo.fif\n",
      " - S037_MI-epo.fif\n",
      " - S039_MI-epo.fif\n",
      " - S040_MI-epo.fif\n",
      " - S041_MI-epo.fif\n",
      " - S042_MI-epo.fif\n",
      " - S043_MI-epo.fif\n",
      " - S044_MI-epo.fif\n",
      " - S045_MI-epo.fif\n",
      " - S046_MI-epo.fif\n",
      " - S047_MI-epo.fif\n",
      " - S048_MI-epo.fif\n",
      " - S049_MI-epo.fif\n",
      " - S050_MI-epo.fif\n"
     ]
    }
   ],
   "source": [
    "# %% [Inspect data folders]\n",
    "# Celda añadida automáticamente: muestra rutas y lista contenidos de data y data/processed\n",
    "try:\n",
    "    print(f\"PROJ: {PROJ}\")\n",
    "    print(f\"DATA: {PROJ / 'data'}\")\n",
    "    print(f\"DATA_PROC: {DATA_PROC}\")\n",
    "    print('\\nContenido de data (top-level):')\n",
    "    data_dir = PROJ / 'data'\n",
    "    if data_dir.exists():\n",
    "        for p in sorted(data_dir.iterdir()):\n",
    "            print(f\" - {p.name}{'/' if p.is_dir() else ''}\")\n",
    "    else:\n",
    "        print('  (no existe)')\n",
    "\n",
    "    print('\\nContenido de data/processed (muestras):')\n",
    "    if DATA_PROC.exists():\n",
    "        for p in sorted(DATA_PROC.glob('*'))[:50]:\n",
    "            print(f\" - {p.name}\")\n",
    "    else:\n",
    "        print('  (no existe)')\n",
    "except Exception as e:\n",
    "    print('Error inspeccionando data:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd66c27b",
   "metadata": {},
   "source": [
    "### Bloque 4 — Intra-sujeto (k-Fold CV) con logs + guardado\n",
    "\n",
    "Qué hace: ejecuta CV por sujeto con FBCSP+LDA, imprime métricas limpias, guarda matriz de confusión (PNG/CSV), y escribe métricas en tables/metrics_intra.csv. Además genera un TXT en logs/ con el detalle de la corrida.\n",
    "\n",
    "Añade argumentos crop_window, motor_only, zscore_epoch, fb_bands, n_csp. Guarda lo de siempre (matriz, métricas, log) pero ahora puedes probar rápidamente ventanas/picks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f44cfa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Intra-sujeto] CV por sujeto con crop/picks/zscore/banco denso\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def run_intra_subject(subject_id='S001',\n",
    "                      fif_dir=DATA_PROC,\n",
    "                      k=5,\n",
    "                      random_state=42,\n",
    "                      crop_window=None,          # p.ej. (0.5, 3.5)\n",
    "                      motor_only=False,          # True -> usa solo canales elegidos\n",
    "                      zscore_epoch=False,        # True -> z-score por época\n",
    "                      fb_bands=FB_BANDS_DENSE,   # o FB_BANDS_CLASSIC\n",
    "                      n_csp=DEFAULT_N_CSP):\n",
    "    \"\"\"\n",
    "    K-fold CV (FBCSP+LDA) con opciones de ventana/picks/zscore/fb.\n",
    "    Guarda matriz agregada y métricas en models/fbcsp_lda/{figures,tables,logs}.\n",
    "    \"\"\"\n",
    "    logger, log_path = _init_logger(run_name=f\"intra_{subject_id}_motor_only\")\n",
    "    fif_path = fif_dir / f'{subject_id}_MI-epo.fif'\n",
    "    epochs = mne.read_epochs(fif_path, preload=True, verbose=False)\n",
    "\n",
    "    X, y_str = _epochs_to_Xy(epochs)\n",
    "    le = LabelEncoder(); y = le.fit_transform(y_str)\n",
    "    classes = list(le.classes_)\n",
    "\n",
    "    knobs = dict(crop_window=crop_window, motor_only=motor_only,\n",
    "                 zscore_epoch=zscore_epoch, fb_bands=fb_bands, n_csp=n_csp)\n",
    "    logger.info(f\"INTRA {subject_id} | k={k} | n_epochs={len(y)} | clases={classes} | sfreq={epochs.info['sfreq']}\")\n",
    "    logger.info(f\"Perillas: {knobs}\")\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=random_state)\n",
    "    accs, f1s = [], []\n",
    "    cm_sum = np.zeros((len(classes), len(classes)), dtype=int)\n",
    "\n",
    "    for fold, (tr_idx, te_idx) in enumerate(skf.split(np.zeros(len(y)), y), start=1):\n",
    "        ep_tr = epochs[tr_idx]; ep_te = epochs[te_idx]\n",
    "        with mne.utils.use_log_level(\"ERROR\"):\n",
    "            Xtr_fb, Xte_fb = _fit_fb_csp_transform(ep_tr, ep_te,\n",
    "                                                   fb_bands=fb_bands,\n",
    "                                                   n_csp=n_csp,\n",
    "                                                   motor_only=motor_only,\n",
    "                                                   zscore_epoch=zscore_epoch,\n",
    "                                                   crop_window=crop_window)\n",
    "        yhat, clf, scaler = _fit_scale_lda(Xtr_fb, y[tr_idx], Xte_fb)\n",
    "\n",
    "        acc = accuracy_score(y[te_idx], yhat)\n",
    "        f1m = f1_score(y[te_idx], yhat, average='macro')\n",
    "        cm_sum += confusion_matrix(y[te_idx], yhat, labels=np.arange(len(classes)))\n",
    "        accs.append(acc); f1s.append(f1m)\n",
    "        logger.info(f\"[fold {fold}] acc={acc:.3f} | f1m={f1m:.3f} | n_test={len(te_idx)}\")\n",
    "\n",
    "    acc_mu, acc_sd = float(np.mean(accs)), float(np.std(accs))\n",
    "    f1_mu,  f1_sd  = float(np.mean(f1s)),  float(np.std(f1s))\n",
    "    logger.info(f\"[INTRA {subject_id}] ACC={acc_mu:.3f}±{acc_sd:.3f} | F1m={f1_mu:.3f}±{f1_sd:.3f}\")\n",
    "    logger.info(\"Matriz agregada:\\n\" + pd.DataFrame(cm_sum, index=classes, columns=classes).to_string())\n",
    "\n",
    "    stem = f\"intra_{subject_id}\"\n",
    "    _save_confusion(cm_sum, classes, title=f\"Intra {subject_id} — FBCSP+LDA\", stem=stem)\n",
    "\n",
    "    row = dict(mode=\"intra\", subject=subject_id,\n",
    "               acc_mean=round(acc_mu,4), acc_std=round(acc_sd,4),\n",
    "               f1_macro_mean=round(f1_mu,4), f1_macro_std=round(f1_sd,4),\n",
    "               n_epochs=int(len(y)), n_classes=int(len(classes)),\n",
    "               crop=str(crop_window), motor_only=bool(motor_only),\n",
    "               zscore_epoch=bool(zscore_epoch), n_csp=int(n_csp),\n",
    "               fb_bands=len(fb_bands))\n",
    "    _append_metrics(row, table_name=\"metrics_intra.csv\")\n",
    "    logger.info(f\"Log → {log_path}\")\n",
    "\n",
    "    print(f\"[Intra {subject_id}] k={k} | ACC={acc_mu:.3f}±{acc_sd:.3f} | F1m={f1_mu:.3f}±{f1_sd:.3f}\")\n",
    "    print(\"Clases:\", classes)\n",
    "    print(\"Matriz (suma folds):\\n\", cm_sum)\n",
    "    return dict(accs=accs, f1s=f1s, cm=cm_sum, classes=classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad74f20",
   "metadata": {},
   "source": [
    "### Bloque 5 — Cross-sujeto (LOSO) con logs + guardado\n",
    "\n",
    "Qué hace: para cada sujeto como test, entrena en el resto, calcula métricas y guarda una matriz de confusión por sujeto y una global. También guarda métricas por sujeto en tables/metrics_loso_per_subject.csv y un resumen global en tables/metrics_loso.csv, además de un TXT en logs/\n",
    "\n",
    "- run_loso(..., use_strict=True) hace LOSO clásico sobre el conjunto resuelto (con Strict si está ON).\n",
    "\n",
    "- run_loso_single(test_subject, ...) entrena con todos los demás y prueba sólo en ese sujeto (Strict opcional).\n",
    "\n",
    "- Incluye utilidades de selección Strict y reemplazo para subject_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9122d415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo DROP-only: C:\\Users\\joelc\\Desktop\\EEG_Clasificador\\reports\\tables\\02_prepro\\subjects_strict_DROP.txt (si no existe, no hay exclusiones)\n"
     ]
    }
   ],
   "source": [
    "# [Cross-sujeto LOSO — Mejorado]  — guarda “perillas” en logs y CSVs (clásico, subset y single)\n",
    "from glob import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "STRICT_DROP_TXT = PROJ / 'reports' / 'tables' / '02_prepro' / 'subjects_strict_DROP.txt'\n",
    "print(f\"Archivo DROP-only: {STRICT_DROP_TXT} (si no existe, no hay exclusiones)\")\n",
    "_re_sid = re.compile(r'^S\\d{3}$')\n",
    "\n",
    "def _list_subject_fifs(fif_dir=DATA_PROC, pattern='S???_MI-epo.fif'):\n",
    "    return sorted(glob(str(fif_dir / pattern)))\n",
    "\n",
    "def _list_available_subjects(fif_dir=DATA_PROC):\n",
    "    files = _list_subject_fifs(fif_dir)\n",
    "    return sorted({Path(f).stem.split('_')[0] for f in files})\n",
    "\n",
    "def _read_drop_file(path: Path):\n",
    "    if not path.exists():\n",
    "        return set()\n",
    "    s = set()\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for ln in f:\n",
    "            sid = ln.strip().upper()\n",
    "            if _re_sid.match(sid):\n",
    "                s.add(sid)\n",
    "    return s\n",
    "\n",
    "def _strict_valid_from_drop(avail_ids):\n",
    "    drop = _read_drop_file(STRICT_DROP_TXT)\n",
    "    avail = set(avail_ids)\n",
    "    valid = sorted(avail - drop)\n",
    "    info = f\"DROP-only: {len(drop)} en DROP; válidos={len(valid)}/{len(avail)}\"\n",
    "    if not STRICT_DROP_TXT.exists():\n",
    "        info += \" (archivo DROP no encontrado → sin exclusiones)\"\n",
    "    return valid, info\n",
    "\n",
    "def _knobs_dict(crop_window, motor_only, zscore_epoch, fb_bands, n_csp):\n",
    "    return dict(\n",
    "        crop_window=crop_window if crop_window is not None else None,\n",
    "        motor_only=bool(motor_only),\n",
    "        zscore_epoch=bool(zscore_epoch),\n",
    "        fb_bands=str(fb_bands),\n",
    "        n_csp=int(n_csp)\n",
    "    )\n",
    "\n",
    "def run_loso(fif_dir=DATA_PROC,\n",
    "             subject_list=None,\n",
    "             use_strict=True,\n",
    "             crop_window=None,\n",
    "             motor_only=False,\n",
    "             zscore_epoch=False,\n",
    "             fb_bands=DEFAULT_FB_BANDS,\n",
    "             n_csp=DEFAULT_N_CSP):\n",
    "    \"\"\"LOSO clásico (DROP-only si use_strict=True) con registro de “perillas”.\"\"\"\n",
    "    logger, log_path = _init_logger(run_name=\"loso\")\n",
    "\n",
    "    knobs = _knobs_dict(crop_window, motor_only, zscore_epoch, fb_bands, n_csp)\n",
    "    logger.info(f\"Perillas: {knobs}\")\n",
    "\n",
    "    avail = _list_available_subjects(fif_dir)\n",
    "    if not avail:\n",
    "        logger.error(f\"No hay FIF S???_MI-epo.fif en {fif_dir}\")\n",
    "        return None\n",
    "\n",
    "    if use_strict:\n",
    "        valid, info = _strict_valid_from_drop(avail)\n",
    "        if subject_list:\n",
    "            sids = [s for s in (subject_list or []) if s in valid]\n",
    "            logger.info(f\"(DROP-only) tests solicitados → {sids} | {info}\")\n",
    "        else:\n",
    "            sids = valid\n",
    "            logger.info(f\"Usando todos los válidos ({len(sids)}). {info}\")\n",
    "    else:\n",
    "        sids = subject_list or avail\n",
    "        logger.info(f\"(strict=OFF) tests → {sids[:10]}{' ...' if len(sids)>10 else ''}\")\n",
    "\n",
    "    if not sids:\n",
    "        logger.warning(\"No hay sujetos para LOSO.\")\n",
    "        return None\n",
    "\n",
    "    ep_map = {sid: mne.read_epochs(str(fif_dir / f\"{sid}_MI-epo.fif\"),\n",
    "                                   preload=True, verbose=False) for sid in sids}\n",
    "\n",
    "    classes_global, cm_global, rows = None, None, []\n",
    "    for s_test, ep_te in ep_map.items():\n",
    "        train_ids = [sid for sid in sids if sid != s_test]\n",
    "        if not train_ids:\n",
    "            logger.warning(f\"Sin train para {s_test}.\")\n",
    "            continue\n",
    "\n",
    "        ep_tr = mne.concatenate_epochs([ep_map[s] for s in train_ids], on_mismatch='ignore')\n",
    "        ep_te = ep_te.copy().reorder_channels(ep_tr.ch_names)\n",
    "\n",
    "        _, y_tr_str = _epochs_to_Xy(ep_tr)\n",
    "        _, y_te_str = _epochs_to_Xy(ep_te)\n",
    "        le = LabelEncoder().fit(np.concatenate([y_tr_str, y_te_str]))\n",
    "        y_tr = le.transform(y_tr_str)\n",
    "        y_te = le.transform(y_te_str)\n",
    "        classes = list(le.classes_)\n",
    "\n",
    "        if classes_global is None:\n",
    "            classes_global = classes\n",
    "            cm_global = np.zeros((len(classes), len(classes)), dtype=int)\n",
    "\n",
    "        with mne.utils.use_log_level(\"ERROR\"):\n",
    "            try:\n",
    "                Xtr_fb, Xte_fb = _fit_fb_csp_transform(ep_tr, ep_te,\n",
    "                                                       fb_bands=fb_bands,\n",
    "                                                       n_csp=n_csp,\n",
    "                                                       motor_only=motor_only,\n",
    "                                                       zscore_epoch=zscore_epoch,\n",
    "                                                       crop_window=crop_window)\n",
    "            except TypeError:\n",
    "                Xtr_fb, Xte_fb = _fit_fb_csp_transform(ep_tr, ep_te,\n",
    "                                                       fb_bands=fb_bands,\n",
    "                                                       n_csp=n_csp)\n",
    "\n",
    "        yhat, clf, scaler = _fit_scale_lda(Xtr_fb, y_tr, Xte_fb)\n",
    "\n",
    "        acc = accuracy_score(y_te, yhat)\n",
    "        f1m = f1_score(y_te, yhat, average='macro')\n",
    "        cm = confusion_matrix(y_te, yhat, labels=np.arange(len(classes)))\n",
    "        cm_global += cm\n",
    "\n",
    "        logger.info(f\"[LOSO] test={s_test} | acc={acc:.3f} | f1m={f1m:.3f} | n_test={len(y_te)}\")\n",
    "        _save_confusion(cm, classes, title=f\"LOSO test {s_test} — FBCSP+LDA\",\n",
    "                        stem=f\"loso_test-{s_test}\")\n",
    "\n",
    "        rows.append(dict(mode=\"loso\",\n",
    "                         test_subject=s_test,\n",
    "                         acc=round(acc,4),\n",
    "                         f1_macro=round(f1m,4),\n",
    "                         n_test=int(len(y_te)),\n",
    "                         **knobs))\n",
    "\n",
    "    per_subj_path = TAB_DIR / \"metrics_loso_per_subject.csv\"\n",
    "    pd.DataFrame(rows).sort_values('test_subject').to_csv(per_subj_path, index=False)\n",
    "    logger.info(f\"Métricas por sujeto → {per_subj_path}\")\n",
    "\n",
    "    _save_confusion(cm_global, classes_global, title=\"LOSO Global — FBCSP+LDA\", stem=\"loso_global\")\n",
    "\n",
    "    acc_mu = float(np.mean([r['acc'] for r in rows])) if rows else 0.0\n",
    "    f1_mu  = float(np.mean([r['f1_macro'] for r in rows])) if rows else 0.0\n",
    "    row_g = dict(mode=\"loso_global\",\n",
    "                 acc_mean=round(acc_mu,4),\n",
    "                 f1_macro_mean=round(f1_mu,4),\n",
    "                 n_subjects=len(rows),\n",
    "                 **knobs)\n",
    "    metrics_path = _append_metrics(row_g, table_name=\"metrics_loso.csv\")\n",
    "    logger.info(f\"Resumen global LOSO → {metrics_path}\")\n",
    "    logger.info(f\"Log guardado en → {log_path}\")\n",
    "\n",
    "    print(\"\\nResumen LOSO (promedios): ACC={:.3f} | F1m={:.3f}\".format(acc_mu, f1_mu))\n",
    "    return pd.DataFrame(rows).sort_values('test_subject').reset_index(drop=True)\n",
    "\n",
    "def _split_calibration(ep_te, k_per_class=5):\n",
    "    \"\"\"Toma k épocas por clase como calibración y el resto como test.\"\"\"\n",
    "    if k_per_class <= 0:\n",
    "        return None, ep_te\n",
    "    labels = ep_te.events[:, -1]\n",
    "    ep_calib_list, ep_eval_list = [], []\n",
    "    for code in np.unique(labels):\n",
    "        idx = np.where(labels == code)[0]\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "        take = min(k_per_class, len(idx))\n",
    "        sel = idx[:take]\n",
    "        rem = idx[take:]\n",
    "        if take > 0:\n",
    "            ep_calib_list.append(ep_te.copy()[sel])\n",
    "        if len(rem) > 0:\n",
    "            ep_eval_list.append(ep_te.copy()[rem])\n",
    "    ep_calib = mne.concatenate_epochs(ep_calib_list, on_mismatch='ignore') if ep_calib_list else None\n",
    "    ep_eval  = mne.concatenate_epochs(ep_eval_list,  on_mismatch='ignore') if ep_eval_list  else ep_te\n",
    "    return ep_calib, ep_eval\n",
    "\n",
    "def run_loso_single(test_subject: str,\n",
    "                    fif_dir=DATA_PROC,\n",
    "                    use_strict=True,\n",
    "                    crop_window=None,\n",
    "                    motor_only=False,\n",
    "                    zscore_epoch=False,\n",
    "                    fb_bands=DEFAULT_FB_BANDS,\n",
    "                    n_csp=DEFAULT_N_CSP,\n",
    "                    calibrate_k_per_class=0):\n",
    "    \"\"\"\n",
    "    LOSO single con calibración opcional (reajusta SOLO scaler+LDA con k/cls del test).\n",
    "    \"\"\"\n",
    "    logger, log_path = _init_logger(run_name=f\"loso_single_{test_subject}\")\n",
    "    test_subject = test_subject.upper()\n",
    "    test_path = fif_dir / f\"{test_subject}_MI-epo.fif\"\n",
    "    if not test_path.exists():\n",
    "        logger.error(f\"No existe: {test_path}\")\n",
    "        return None\n",
    "    ep_te_full = mne.read_epochs(str(test_path), preload=True, verbose=False)\n",
    "\n",
    "    knobs = _knobs_dict(crop_window, motor_only, zscore_epoch, fb_bands, n_csp)\n",
    "    knobs['calib_k'] = int(calibrate_k_per_class)\n",
    "    logger.info(f\"Perillas: {knobs}\")\n",
    "\n",
    "    avail = _list_available_subjects(fif_dir)\n",
    "    train_ids = [s for s in avail if s != test_subject]\n",
    "    if use_strict:\n",
    "        valid, info = _strict_valid_from_drop(avail)\n",
    "        train_ids = [s for s in train_ids if s in valid]\n",
    "        logger.info(f\"(DROP-only) {info} | train_ids={train_ids[:10]}{' ...' if len(train_ids)>10 else ''}\")\n",
    "    else:\n",
    "        logger.info(f\"(strict=OFF) train_ids={train_ids[:10]}{' ...' if len(train_ids)>10 else ''}\")\n",
    "\n",
    "    if not train_ids:\n",
    "        logger.error(\"Sin sujetos de entrenamiento disponibles.\")\n",
    "        return None\n",
    "\n",
    "    ep_tr = mne.concatenate_epochs(\n",
    "        [mne.read_epochs(str(fif_dir / f\"{s}_MI-epo.fif\"), preload=True, verbose=False) for s in train_ids],\n",
    "        on_mismatch='ignore'\n",
    "    )\n",
    "    ep_te_full = ep_te_full.copy().reorder_channels(ep_tr.ch_names)\n",
    "\n",
    "    ep_calib, ep_eval = _split_calibration(ep_te_full, k_per_class=calibrate_k_per_class)\n",
    "\n",
    "    _, y_tr_str = _epochs_to_Xy(ep_tr)\n",
    "    _, y_ev_str = _epochs_to_Xy(ep_eval)\n",
    "    le = LabelEncoder().fit(np.concatenate([y_tr_str, y_ev_str]))\n",
    "    y_tr = le.transform(y_tr_str)\n",
    "    y_ev = le.transform(y_ev_str)\n",
    "    classes = list(le.classes_)\n",
    "\n",
    "    with mne.utils.use_log_level(\"ERROR\"):\n",
    "        try:\n",
    "            Xtr_fb, Xev_fb = _fit_fb_csp_transform(ep_tr, ep_eval,\n",
    "                                                   fb_bands=fb_bands,\n",
    "                                                   n_csp=n_csp,\n",
    "                                                   motor_only=motor_only,\n",
    "                                                   zscore_epoch=zscore_epoch,\n",
    "                                                   crop_window=crop_window)\n",
    "        except TypeError:\n",
    "            Xtr_fb, Xev_fb = _fit_fb_csp_transform(ep_tr, ep_eval,\n",
    "                                                   fb_bands=fb_bands,\n",
    "                                                   n_csp=n_csp)\n",
    "\n",
    "    yhat, clf, scaler = _fit_scale_lda(Xtr_fb, y_tr, Xev_fb)\n",
    "\n",
    "    if ep_calib is not None and len(ep_calib) > 0:\n",
    "        _, y_ca_str = _epochs_to_Xy(ep_calib)\n",
    "        y_ca = le.transform(y_ca_str)\n",
    "        with mne.utils.use_log_level(\"ERROR\"):\n",
    "            try:\n",
    "                Xtr_fb2, Xca_fb = _fit_fb_csp_transform(ep_tr, ep_calib,\n",
    "                                                        fb_bands=fb_bands,\n",
    "                                                        n_csp=n_csp,\n",
    "                                                        motor_only=motor_only,\n",
    "                                                        zscore_epoch=zscore_epoch,\n",
    "                                                        crop_window=crop_window)\n",
    "            except TypeError:\n",
    "                Xtr_fb2, Xca_fb = _fit_fb_csp_transform(ep_tr, ep_calib,\n",
    "                                                        fb_bands=fb_bands,\n",
    "                                                        n_csp=n_csp)\n",
    "        scaler2 = StandardScaler()\n",
    "        X_join = np.vstack([Xtr_fb2, Xca_fb])\n",
    "        y_join = np.concatenate([y_tr, y_ca])\n",
    "        X_join_s = scaler2.fit_transform(X_join)\n",
    "        Xev_s = scaler2.transform(Xev_fb)\n",
    "\n",
    "        clf2 = LDA(**LDA_PARAMS)\n",
    "        clf2.fit(X_join_s, y_join)\n",
    "        yhat = clf2.predict(Xev_s)\n",
    "\n",
    "    acc = accuracy_score(y_ev, yhat)\n",
    "    f1m = f1_score(y_ev, yhat, average='macro')\n",
    "    cm = confusion_matrix(y_ev, yhat, labels=np.arange(len(classes)))\n",
    "    logger.info(f\"[LOSO SINGLE] test={test_subject} | acc={acc:.3f} | f1m={f1m:.3f} | n_test={len(y_ev)}\")\n",
    "\n",
    "    _save_confusion(cm, classes, title=f\"LOSO single {test_subject} — FBCSP+LDA\",\n",
    "                    stem=f\"loso_single_test-{test_subject}\")\n",
    "\n",
    "    row = dict(mode=\"loso_single\",\n",
    "               test_subject=test_subject,\n",
    "               acc=round(acc,4),\n",
    "               f1_macro=round(f1m,4),\n",
    "               n_test=int(len(y_ev)),\n",
    "               **knobs)\n",
    "    _append_metrics(row, table_name=\"metrics_loso_single.csv\")\n",
    "\n",
    "    print(f\"[LOSO single] {test_subject} | ACC={acc:.3f} | F1m={f1m:.3f}\")\n",
    "    return row\n",
    "\n",
    "def run_loso_subset(subject_ids=None, fif_dir=DATA_PROC, use_strict=True,\n",
    "                    crop_window=None, motor_only=False, zscore_epoch=False,\n",
    "                    fb_bands=DEFAULT_FB_BANDS, n_csp=DEFAULT_N_CSP, n_default_test=2):\n",
    "    \"\"\"Ejecuta LOSO para un subset de tests y reusa run_loso().\"\"\"\n",
    "    avail = _list_available_subjects(fif_dir)\n",
    "    if not avail:\n",
    "        print(\"No hay sujetos en data/processed.\")\n",
    "        return None\n",
    "    if use_strict:\n",
    "        valid, info = _strict_valid_from_drop(avail)\n",
    "        if subject_ids is None:\n",
    "            subs = valid[:n_default_test]\n",
    "            print(f\"[LOSO SUBSET] tests: {subs} | {info}\")\n",
    "        else:\n",
    "            subs = [s for s in subject_ids if s in valid][:n_default_test]\n",
    "            print(f\"[LOSO SUBSET] tests: {subs} | {info}\")\n",
    "    else:\n",
    "        if subject_ids is None:\n",
    "            subs = avail[:n_default_test]\n",
    "            print(f\"(strict=OFF) tests: {subs}\")\n",
    "        else:\n",
    "            subs = [s for s in subject_ids if s in avail][:n_default_test]\n",
    "            print(f\"(strict=OFF) tests: {subs}\")\n",
    "\n",
    "    if not subs:\n",
    "        print(\"No se encontraron sujetos válidos para loso-subset.\")\n",
    "        return None\n",
    "\n",
    "    return run_loso(fif_dir=fif_dir,\n",
    "                    subject_list=subs,\n",
    "                    use_strict=use_strict,\n",
    "                    crop_window=crop_window,\n",
    "                    motor_only=motor_only,\n",
    "                    zscore_epoch=zscore_epoch,\n",
    "                    fb_bands=fb_bands,\n",
    "                    n_csp=n_csp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8538cc",
   "metadata": {},
   "source": [
    "### Bloque 6 — Lanzadores “batch”: intra para N sujetos y LOSO para M sujetos\n",
    "\n",
    "Qué hace: agrega funciones y un ejemplo de uso para:\n",
    "\n",
    "- run_intra_batch(..., use_strict=True) ejecuta intra para un conjunto de sujetos resueltos (con reemplazo si Strict).\n",
    "\n",
    "- run_loso_subset(..., use_strict=True) corre LOSO sólo en un subset de test subjects resuelto (con reemplazo si Strict).\n",
    "\n",
    "- Ambos ofrecen defaults (4 intra, 2 loso) si no pasas lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67e1f65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [Batch runners] intra/loso con perillas y defaults fuertes\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "def _discover_subject_ids(fif_dir=DATA_PROC, pattern='S???_MI-epo.fif'):\n",
    "    files = sorted(glob(str(fif_dir / pattern)))\n",
    "    return [Path(f).stem.split('_')[0] for f in files]\n",
    "\n",
    "def run_intra_batch(subject_ids=None, k=10, fif_dir=DATA_PROC,\n",
    "                    crop_window=(0.5, 3.5),\n",
    "                    motor_only=True,\n",
    "                    zscore_epoch=True,\n",
    "                    fb_bands=DEFAULT_FB_BANDS,\n",
    "                    n_csp=DEFAULT_N_CSP):\n",
    "    \"\"\"\n",
    "    Ejecuta intra para una lista (o detecta los 4 primeros si None).\n",
    "    Usa por defecto: ventana 0.5–3.5, picks motores, z-score, banco denso, n_csp=6.\n",
    "    \"\"\"\n",
    "    subs_all = _discover_subject_ids(fif_dir)\n",
    "    if not subs_all:\n",
    "        print(\"No se encontraron sujetos en\", fif_dir)\n",
    "        return None\n",
    "\n",
    "    if subject_ids is None:\n",
    "        subject_ids = subs_all[:4]\n",
    "    else:\n",
    "        subject_ids = [s for s in subject_ids if s in subs_all]\n",
    "        if not subject_ids:\n",
    "            print(\"Ningún ID válido en subject_ids.\")\n",
    "            return None\n",
    "\n",
    "    rows = []\n",
    "    print(f\"[INTRA BATCH] sujetos: {subject_ids}\")\n",
    "    for sid in subject_ids:\n",
    "        print(f\"\\n== INTRA {sid} ==\")\n",
    "        res = run_intra_subject(sid, fif_dir=fif_dir, k=k,\n",
    "                                crop_window=crop_window,\n",
    "                                motor_only=motor_only,\n",
    "                                zscore_epoch=zscore_epoch,\n",
    "                                fb_bands=fb_bands,\n",
    "                                n_csp=n_csp)\n",
    "        acc_mu = float(np.mean(res['accs'])) if res['accs'] else 0.0\n",
    "        f1_mu  = float(np.mean(res['f1s']))  if res['f1s']  else 0.0\n",
    "        rows.append(dict(subject=sid, acc_mean=acc_mu, f1_macro_mean=f1_mu,\n",
    "                         k=k, n_classes=len(res['classes']),\n",
    "                         crop=str(crop_window), motor_only=bool(motor_only),\n",
    "                         zscore_epoch=bool(zscore_epoch), n_csp=int(n_csp),\n",
    "                         fb_bands=len(fb_bands)))\n",
    "    if rows:\n",
    "        df = pd.DataFrame(rows).sort_values('subject')\n",
    "        out_path = TAB_DIR / \"metrics_intra_batch.csv\"\n",
    "        df.to_csv(out_path, index=False)\n",
    "        print(\"Resumen intra batch →\", out_path)\n",
    "        display(df)\n",
    "        return df\n",
    "    return None\n",
    "\n",
    "def run_loso_subset_quick(subject_ids=None, fif_dir=DATA_PROC,\n",
    "                          crop_window=(0.5, 3.5),\n",
    "                          motor_only=True,\n",
    "                          zscore_epoch=True,\n",
    "                          fb_bands=DEFAULT_FB_BANDS,\n",
    "                          n_csp=DEFAULT_N_CSP,\n",
    "                          n_default_test=2):\n",
    "    \"\"\"\n",
    "    Azúcar: delega en run_loso_subset (definido en el bloque LOSO).\n",
    "    \"\"\"\n",
    "    return run_loso_subset(subject_ids=subject_ids, fif_dir=fif_dir, use_strict=True,\n",
    "                           crop_window=crop_window, motor_only=motor_only,\n",
    "                           zscore_epoch=zscore_epoch, fb_bands=fb_bands, n_csp=n_csp,\n",
    "                           n_default_test=n_default_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce94a18",
   "metadata": {},
   "source": [
    "### Bloque 7 — Ejemplos de ejecución\n",
    "\n",
    "Qué hace: muestra cómo lanzar los “batch” por defecto (4 intra, 2 LOSO) y cómo pasar propia lista de sujetos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd06edc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INTRA BATCH] sujetos: ['S001', 'S002', 'S003', 'S004']\n",
      "\n",
      "== INTRA S001 ==\n",
      "[21:42:46] INFO: INTRA S001 | k=10 | n_epochs=88 | clases=['Both Feet', 'Both Fists', 'Left', 'Right'] | sfreq=160.0\n",
      "[21:42:46] INFO: Perillas: {'crop_window': (0.5, 4.5), 'motor_only': True, 'zscore_epoch': True, 'fb_bands': [(8, 10), (10, 12), (12, 14), (14, 16), (16, 18), (18, 20), (20, 22), (22, 24), (24, 26), (26, 28), (28, 30)], 'n_csp': 4}\n",
      "[21:42:47] INFO: [fold 1] acc=0.556 | f1m=0.600 | n_test=9\n",
      "[21:42:48] INFO: [fold 2] acc=0.333 | f1m=0.333 | n_test=9\n",
      "[21:42:48] INFO: [fold 3] acc=0.556 | f1m=0.575 | n_test=9\n",
      "[21:42:49] INFO: [fold 4] acc=0.556 | f1m=0.517 | n_test=9\n",
      "[21:42:50] INFO: [fold 5] acc=0.667 | f1m=0.667 | n_test=9\n",
      "[21:42:51] INFO: [fold 6] acc=0.222 | f1m=0.167 | n_test=9\n",
      "[21:42:52] INFO: [fold 7] acc=0.778 | f1m=0.742 | n_test=9\n",
      "[21:42:53] INFO: [fold 8] acc=0.444 | f1m=0.393 | n_test=9\n",
      "[21:42:54] INFO: [fold 9] acc=0.750 | f1m=0.650 | n_test=8\n",
      "[21:42:55] INFO: [fold 10] acc=0.375 | f1m=0.325 | n_test=8\n",
      "[21:42:55] INFO: [INTRA S001] ACC=0.524±0.172 | F1m=0.497±0.175\n",
      "[21:42:55] INFO: Matriz agregada:\n",
      "            Both Feet  Both Fists  Left  Right\n",
      "Both Feet          20           2     0      1\n",
      "Both Fists          2           9     3      7\n",
      "Left                0           6    11      6\n",
      "Right               1           9     5      6\n",
      "[21:42:55] INFO: Log → C:\\Users\\joelc\\Desktop\\EEG_Clasificador\\models\\fbcsp_lda\\logs\\20250926-214246_intra_S001_motor_only.txt\n",
      "[Intra S001] k=10 | ACC=0.524±0.172 | F1m=0.497±0.175\n",
      "Clases: ['Both Feet', 'Both Fists', 'Left', 'Right']\n",
      "Matriz (suma folds):\n",
      " [[20  2  0  1]\n",
      " [ 2  9  3  7]\n",
      " [ 0  6 11  6]\n",
      " [ 1  9  5  6]]\n",
      "\n",
      "== INTRA S002 ==\n",
      "[21:42:55] INFO: INTRA S002 | k=10 | n_epochs=84 | clases=['Both Feet', 'Both Fists', 'Left', 'Right'] | sfreq=160.0\n",
      "[21:42:55] INFO: Perillas: {'crop_window': (0.5, 4.5), 'motor_only': True, 'zscore_epoch': True, 'fb_bands': [(8, 10), (10, 12), (12, 14), (14, 16), (16, 18), (18, 20), (20, 22), (22, 24), (24, 26), (26, 28), (28, 30)], 'n_csp': 4}\n",
      "[21:42:56] INFO: [fold 1] acc=0.778 | f1m=0.767 | n_test=9\n",
      "[21:42:57] INFO: [fold 2] acc=0.778 | f1m=0.650 | n_test=9\n",
      "[21:42:58] INFO: [fold 3] acc=0.444 | f1m=0.408 | n_test=9\n",
      "[21:42:59] INFO: [fold 4] acc=0.556 | f1m=0.468 | n_test=9\n",
      "[21:43:00] INFO: [fold 5] acc=0.875 | f1m=0.867 | n_test=8\n",
      "[21:43:01] INFO: [fold 6] acc=0.625 | f1m=0.642 | n_test=8\n",
      "[21:43:02] INFO: [fold 7] acc=0.500 | f1m=0.433 | n_test=8\n",
      "[21:43:02] INFO: [fold 8] acc=0.500 | f1m=0.500 | n_test=8\n",
      "[21:43:03] INFO: [fold 9] acc=0.625 | f1m=0.575 | n_test=8\n",
      "[21:43:04] INFO: [fold 10] acc=0.625 | f1m=0.575 | n_test=8\n",
      "[21:43:04] INFO: [INTRA S002] ACC=0.631±0.133 | F1m=0.588±0.140\n",
      "[21:43:04] INFO: Matriz agregada:\n",
      "            Both Feet  Both Fists  Left  Right\n",
      "Both Feet          17           0     1      3\n",
      "Both Fists          1           8     6      6\n",
      "Left                3           4    14      0\n",
      "Right               1           3     3     14\n",
      "[21:43:04] INFO: Log → C:\\Users\\joelc\\Desktop\\EEG_Clasificador\\models\\fbcsp_lda\\logs\\20250926-214255_intra_S002_motor_only.txt\n",
      "[Intra S002] k=10 | ACC=0.631±0.133 | F1m=0.588±0.140\n",
      "Clases: ['Both Feet', 'Both Fists', 'Left', 'Right']\n",
      "Matriz (suma folds):\n",
      " [[17  0  1  3]\n",
      " [ 1  8  6  6]\n",
      " [ 3  4 14  0]\n",
      " [ 1  3  3 14]]\n",
      "\n",
      "== INTRA S003 ==\n",
      "[21:43:04] INFO: INTRA S003 | k=10 | n_epochs=83 | clases=['Both Feet', 'Both Fists', 'Left', 'Right'] | sfreq=160.0\n",
      "[21:43:04] INFO: Perillas: {'crop_window': (0.5, 4.5), 'motor_only': True, 'zscore_epoch': True, 'fb_bands': [(8, 10), (10, 12), (12, 14), (14, 16), (16, 18), (18, 20), (20, 22), (22, 24), (24, 26), (26, 28), (28, 30)], 'n_csp': 4}\n",
      "[21:43:05] INFO: [fold 1] acc=0.556 | f1m=0.468 | n_test=9\n",
      "[21:43:06] INFO: [fold 2] acc=0.444 | f1m=0.333 | n_test=9\n",
      "[21:43:07] INFO: [fold 3] acc=0.222 | f1m=0.208 | n_test=9\n",
      "[21:43:08] INFO: [fold 4] acc=0.375 | f1m=0.333 | n_test=8\n",
      "[21:43:09] INFO: [fold 5] acc=0.750 | f1m=0.733 | n_test=8\n",
      "[21:43:09] INFO: [fold 6] acc=0.500 | f1m=0.500 | n_test=8\n",
      "[21:43:10] INFO: [fold 7] acc=0.625 | f1m=0.625 | n_test=8\n",
      "[21:43:11] INFO: [fold 8] acc=0.625 | f1m=0.542 | n_test=8\n",
      "[21:43:12] INFO: [fold 9] acc=0.500 | f1m=0.450 | n_test=8\n",
      "[21:43:13] INFO: [fold 10] acc=0.875 | f1m=0.867 | n_test=8\n",
      "[21:43:13] INFO: [INTRA S003] ACC=0.547±0.176 | F1m=0.506±0.187\n",
      "[21:43:13] INFO: Matriz agregada:\n",
      "            Both Feet  Both Fists  Left  Right\n",
      "Both Feet          15           3     0      2\n",
      "Both Fists          6           9     2      3\n",
      "Left                0           1    13      9\n",
      "Right               0           2    10      8\n",
      "[21:43:13] INFO: Log → C:\\Users\\joelc\\Desktop\\EEG_Clasificador\\models\\fbcsp_lda\\logs\\20250926-214304_intra_S003_motor_only.txt\n",
      "[Intra S003] k=10 | ACC=0.547±0.176 | F1m=0.506±0.187\n",
      "Clases: ['Both Feet', 'Both Fists', 'Left', 'Right']\n",
      "Matriz (suma folds):\n",
      " [[15  3  0  2]\n",
      " [ 6  9  2  3]\n",
      " [ 0  1 13  9]\n",
      " [ 0  2 10  8]]\n",
      "\n",
      "== INTRA S004 ==\n",
      "[21:43:13] INFO: INTRA S004 | k=10 | n_epochs=72 | clases=['Both Feet', 'Both Fists', 'Left', 'Right'] | sfreq=160.0\n",
      "[21:43:13] INFO: Perillas: {'crop_window': (0.5, 4.5), 'motor_only': True, 'zscore_epoch': True, 'fb_bands': [(8, 10), (10, 12), (12, 14), (14, 16), (16, 18), (18, 20), (20, 22), (22, 24), (24, 26), (26, 28), (28, 30)], 'n_csp': 4}\n",
      "[21:43:14] INFO: [fold 1] acc=0.250 | f1m=0.208 | n_test=8\n",
      "[21:43:15] INFO: [fold 2] acc=0.500 | f1m=0.476 | n_test=8\n",
      "[21:43:15] INFO: [fold 3] acc=0.286 | f1m=0.292 | n_test=7\n",
      "[21:43:16] INFO: [fold 4] acc=0.571 | f1m=0.500 | n_test=7\n",
      "[21:43:17] INFO: [fold 5] acc=0.286 | f1m=0.250 | n_test=7\n",
      "[21:43:18] INFO: [fold 6] acc=0.571 | f1m=0.492 | n_test=7\n",
      "[21:43:19] INFO: [fold 7] acc=0.429 | f1m=0.350 | n_test=7\n",
      "[21:43:19] INFO: [fold 8] acc=0.571 | f1m=0.517 | n_test=7\n",
      "[21:43:20] INFO: [fold 9] acc=0.429 | f1m=0.458 | n_test=7\n",
      "[21:43:21] INFO: [fold 10] acc=0.571 | f1m=0.450 | n_test=7\n",
      "[21:43:21] INFO: [INTRA S004] ACC=0.446±0.125 | F1m=0.399±0.108\n",
      "[21:43:21] INFO: Matriz agregada:\n",
      "            Both Feet  Both Fists  Left  Right\n",
      "Both Feet          12           5     2      0\n",
      "Both Fists          5          10     2      3\n",
      "Left                2           6     6      2\n",
      "Right               2           7     4      4\n",
      "[21:43:21] INFO: Log → C:\\Users\\joelc\\Desktop\\EEG_Clasificador\\models\\fbcsp_lda\\logs\\20250926-214313_intra_S004_motor_only.txt\n",
      "[Intra S004] k=10 | ACC=0.446±0.125 | F1m=0.399±0.108\n",
      "Clases: ['Both Feet', 'Both Fists', 'Left', 'Right']\n",
      "Matriz (suma folds):\n",
      " [[12  5  2  0]\n",
      " [ 5 10  2  3]\n",
      " [ 2  6  6  2]\n",
      " [ 2  7  4  4]]\n",
      "Resumen intra batch → C:\\Users\\joelc\\Desktop\\EEG_Clasificador\\models\\fbcsp_lda\\tables\\metrics_intra_batch.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>acc_mean</th>\n",
       "      <th>f1_macro_mean</th>\n",
       "      <th>k</th>\n",
       "      <th>n_classes</th>\n",
       "      <th>crop</th>\n",
       "      <th>motor_only</th>\n",
       "      <th>zscore_epoch</th>\n",
       "      <th>n_csp</th>\n",
       "      <th>fb_bands</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S001</td>\n",
       "      <td>0.523611</td>\n",
       "      <td>0.496786</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>(0.5, 4.5)</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S002</td>\n",
       "      <td>0.630556</td>\n",
       "      <td>0.588452</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>(0.5, 4.5)</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S003</td>\n",
       "      <td>0.547222</td>\n",
       "      <td>0.505952</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>(0.5, 4.5)</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S004</td>\n",
       "      <td>0.446429</td>\n",
       "      <td>0.399286</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>(0.5, 4.5)</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject  acc_mean  f1_macro_mean   k  n_classes        crop  motor_only  \\\n",
       "0    S001  0.523611       0.496786  10          4  (0.5, 4.5)        True   \n",
       "1    S002  0.630556       0.588452  10          4  (0.5, 4.5)        True   \n",
       "2    S003  0.547222       0.505952  10          4  (0.5, 4.5)        True   \n",
       "3    S004  0.446429       0.399286  10          4  (0.5, 4.5)        True   \n",
       "\n",
       "   zscore_epoch  n_csp  fb_bands  \n",
       "0          True      4        11  \n",
       "1          True      4        11  \n",
       "2          True      4        11  \n",
       "3          True      4        11  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% [Examples — Cómo lanzar con buenas prácticas]\n",
    "# 1) Intra de los 4 primeros sujetos con perillas \"fuertes\" para MI\n",
    "_ = run_intra_batch(\n",
    "    k=10,\n",
    "    crop_window=(0.5, 4.5),\n",
    "    motor_only=True,\n",
    "    zscore_epoch=True,\n",
    "    fb_bands=FB_BANDS_DENSE,\n",
    "    n_csp=4\n",
    ")\n",
    "\n",
    "# 2) LOSO de los 2 primeros sujetos como TEST (rápido para validar)\n",
    "# df_loso_subset = run_loso_subset(\n",
    "#     crop_window=(0.5, 3.5),\n",
    "#     motor_only=True,\n",
    "#     zscore_epoch=True,\n",
    "#     fb_bands=FB_BANDS_DENSE,\n",
    "#     n_csp=6\n",
    "# )\n",
    "\n",
    "# 3) LOSO clásico sobre TODOS (cuidado: puede tardar)\n",
    "# df_loso_all = run_loso(\n",
    "#     crop_window=(0.5, 3.5),\n",
    "#     motor_only=True,\n",
    "#     zscore_epoch=True,\n",
    "#     fb_bands=FB_BANDS_DENSE,\n",
    "#     n_csp=6\n",
    "# )\n",
    "\n",
    "# 4) LOSO single con calibrado mínimo (k=5 épocas/clase del sujeto test)\n",
    "# row_single = run_loso_single(\n",
    "#     'S001',\n",
    "#     crop_window=(0.5, 4.5),\n",
    "#     motor_only=True,\n",
    "#     zscore_epoch=False,\n",
    "#     fb_bands=FB_BANDS_DENSE,\n",
    "#     n_csp=6,\n",
    "#     calibrate_k_per_class=5\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
