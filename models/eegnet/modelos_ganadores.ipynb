{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e14fe640",
   "metadata": {},
   "source": [
    "# SHALLOWCNN + FINETUNING INTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c8e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Replicación fiel del paper \"A Deep Learning MI-EEG Classification Model for BCIs\"\n",
    "# Dose et al., EUSIPCO 2018 — Shallow CNN sobre RAW EEG (PhysioNet BCI2000)\n",
    "# Versión con FINE-TUNING progresivo por sujeto (CV 4-fold, LRs discriminativos, L2-SP, early stopping con validación)\n",
    "\n",
    "import os, re, math, random, json, itertools, copy\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# CONFIGURACIÓN GENERAL\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "FOLDS_DIR = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dispositivo y semilla\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "# Escenario de clases\n",
    "CLASS_SCENARIO = '4c'\n",
    "WINDOW_MODE = '3s'\n",
    "FS = 160.0\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Entrenamiento global\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS_GLOBAL = 100\n",
    "LR = 1e-3\n",
    "\n",
    "# Fine-tuning por sujeto (protocolo robusto)\n",
    "CALIB_CV_FOLDS = 4            # 4-fold CV por sujeto ~ 75/25\n",
    "FT_EPOCHS = 30                 # más alto, pero con ES por validación\n",
    "FT_BASE_LR = 5e-5              # convs (temporal+spatial) — más bajo\n",
    "FT_HEAD_LR = 1e-3              # fc+out — más alto\n",
    "FT_L2SP = 1e-4                 # regularización más suave\n",
    "FT_PATIENCE = 5                # early stopping con validación\n",
    "FT_VAL_RATIO = 0.2             # validación dentro del set de calibración\n",
    "\n",
    "# Sujetos excluidos\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "\n",
    "# Runs\n",
    "MI_RUNS_LR = [4, 8, 12]\n",
    "MI_RUNS_OF = [6, 10, 14]\n",
    "BASELINE_RUNS_EO = [1]\n",
    "\n",
    "# Canales (8 en lugar de 64)\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','CPz']\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES DE CANALES\n",
    "# =========================\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = re.sub(r'([A-Za-z])0([0-9])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])Z$', r'\\1z', s)\n",
    "    s = s.replace('fp', 'Fp').replace('FP', 'Fp')\n",
    "    s = ''.join(ch.upper() if ch != 'z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {}\n",
    "    for ch in raw.ch_names:\n",
    "        lab = normalize_label(ch)\n",
    "        lab = lab[:-1] + 'z' if lab.endswith('Z') else lab\n",
    "        lab = re.sub(r'([A-Z])Z$', r'\\1z', lab)\n",
    "        mapping[ch] = lab\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired_channels=EXPECTED_8):\n",
    "    have = [ch for ch in desired_channels if ch in raw.ch_names]\n",
    "    missing = [ch for ch in desired_channels if ch not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"Warning: faltan canales {missing} en archivo {getattr(raw,'filenames', [''])[0]}\")\n",
    "        return None\n",
    "    raw.reorder_channels([ch for ch in raw.ch_names if ch in desired_channels] +\n",
    "                         [ch for ch in raw.ch_names if ch not in desired_channels])\n",
    "    raw.pick_channels(desired_channels, ordered=True)\n",
    "    return raw\n",
    "\n",
    "# =========================\n",
    "# LECTURA DE EDF y EVENTOS\n",
    "# =========================\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(run_id:int):\n",
    "    if run_id in MI_RUNS_LR: return 'LR'\n",
    "    if run_id in MI_RUNS_OF: return 'OF'\n",
    "    if run_id in BASELINE_RUNS_EO: return 'EO'\n",
    "    return None\n",
    "\n",
    "def read_raw_edf(path: Path):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        mont = mne.channels.make_standard_montage('standard_1020')\n",
    "        raw.set_montage(mont, on_missing='ignore')\n",
    "    except Exception:\n",
    "        pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad=\"auto\")\n",
    "    raw = ensure_channels_order(raw, EXPECTED_8)\n",
    "    if raw is None:\n",
    "        return None\n",
    "    # Band-pass opcional (desactivado)\n",
    "    # raw.filter(l_freq=8., h_freq=30., picks='eeg', method='iir', verbose=False)\n",
    "    return raw\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    if raw.annotations is None or len(raw.annotations) == 0:\n",
    "        return []\n",
    "    def _norm(s): return str(s).strip().upper().replace(' ', '')\n",
    "    res = []\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = _norm(desc)\n",
    "        if tag in ('T1','T2'):\n",
    "            res.append((float(onset), tag))\n",
    "    res.sort()\n",
    "    dedup = []\n",
    "    last_t1 = last_t2 = -1e9\n",
    "    for t, tag in res:\n",
    "        if tag == 'T1':\n",
    "            if (t - last_t1) >= 0.5: dedup.append((t, tag)); last_t1 = t\n",
    "        else:\n",
    "            if (t - last_t2) >= 0.5: dedup.append((t, tag)); last_t2 = t\n",
    "    return dedup\n",
    "\n",
    "# =========================\n",
    "# CONSTRUCCIÓN DE DATASETS\n",
    "# =========================\n",
    "def subjects_available():\n",
    "    subs = []\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try:\n",
    "            sid = int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir / f\"S{sid:03d}R{r:02d}.edf\").exists() for r in (MI_RUNS_LR + MI_RUNS_OF))\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "def extract_trials_from_run(edf_path: Path, scenario: str, window_mode: str):\n",
    "    subj, run = parse_subject_run(edf_path)\n",
    "    kind = run_kind(run)\n",
    "    if kind not in ('LR','OF','EO'):\n",
    "        return ([], [])\n",
    "\n",
    "    raw = read_raw_edf(edf_path)\n",
    "    if raw is None:\n",
    "        return ([], [])\n",
    "\n",
    "    data = raw.get_data()\n",
    "    fs = raw.info['sfreq']\n",
    "    assert abs(fs - FS) < 1e-6\n",
    "\n",
    "    out = []\n",
    "\n",
    "    if kind in ('LR','OF'):\n",
    "        events = collect_events_T1T2(raw)\n",
    "        if window_mode == '3s':\n",
    "            rel_start, rel_end = 0.0, 3.0\n",
    "        else:\n",
    "            rel_start, rel_end = -1.0, 5.0\n",
    "\n",
    "        for onset_sec, tag in events:\n",
    "            if kind == 'LR':\n",
    "                if tag == 'T1': label = 'L'\n",
    "                elif tag == 'T2': label = 'R'\n",
    "                else: continue\n",
    "            else:\n",
    "                if tag == 'T1': label = 'BFISTS'\n",
    "                elif tag == 'T2': label = 'BFEET'\n",
    "                else: continue\n",
    "\n",
    "            if scenario == '2c' and label not in ('L','R'):\n",
    "                continue\n",
    "            if scenario == '3c' and label not in ('L','R','BFISTS'):\n",
    "                continue\n",
    "            if scenario == '4c' and label not in ('L','R','BFISTS','BFEET'):\n",
    "                continue\n",
    "\n",
    "            s = int(round((raw.first_time + onset_sec + rel_start) * fs))\n",
    "            e = int(round((raw.first_time + onset_sec + rel_end) * fs))\n",
    "            if s < 0 or e > data.shape[1]:\n",
    "                continue\n",
    "\n",
    "            seg = data[:, s:e].T.astype(np.float32)\n",
    "            # Normalización por época canal-a-canal (z-score)\n",
    "            seg = (seg - seg.mean(axis=0, keepdims=True)) / (seg.std(axis=0, keepdims=True) + 1e-6)\n",
    "\n",
    "            if label == 'L':       y = 0\n",
    "            elif label == 'R':     y = 1\n",
    "            elif label == 'BFISTS':y = 2\n",
    "            elif label == 'BFEET': y = 3\n",
    "            else: continue\n",
    "\n",
    "            out.append((seg, y, subj))\n",
    "\n",
    "    elif kind == 'EO':\n",
    "        return ([], raw.ch_names)\n",
    "\n",
    "    return out, raw.ch_names\n",
    "\n",
    "def build_dataset_all(subjects, scenario='4c', window_mode='3s'):\n",
    "    X, y, groups = [], [], []\n",
    "    ch_template = None\n",
    "\n",
    "    for s in tqdm(subjects, desc=\"Construyendo dataset (RAW)\"):\n",
    "        sdir = DATA_RAW / f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "\n",
    "        trials_L, trials_R, trials_FISTS, trials_FEET = [], [], [], []\n",
    "\n",
    "        for r in MI_RUNS_LR:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 0: trials_L.append(seg)\n",
    "                elif lab == 1: trials_R.append(seg)\n",
    "\n",
    "        for r in MI_RUNS_OF:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 2: trials_FISTS.append(seg)\n",
    "                elif lab == 3: trials_FEET.append(seg)\n",
    "\n",
    "        need_per_class = 21\n",
    "        def pick(trials, n, rng):\n",
    "            if len(trials) < n:\n",
    "                idx = rng.choice(len(trials), size=n, replace=True)\n",
    "                return [trials[i] for i in idx]\n",
    "            rng.shuffle(trials)\n",
    "            return trials[:n]\n",
    "\n",
    "        rng = check_random_state(RANDOM_STATE + s)\n",
    "        if len(trials_L)==0 or len(trials_R)==0 or len(trials_FISTS)==0 or len(trials_FEET)==0:\n",
    "            continue\n",
    "\n",
    "        Lp  = pick(trials_L,     need_per_class, rng)\n",
    "        Rp  = pick(trials_R,     need_per_class, rng)\n",
    "        FIp = pick(trials_FISTS, need_per_class, rng)\n",
    "        FEp = pick(trials_FEET,  need_per_class, rng)\n",
    "\n",
    "        pack = [(Lp, 0), (Rp, 1), (FIp, 2), (FEp, 3)]\n",
    "        for segs, lab in pack:\n",
    "            for seg in segs:\n",
    "                X.append(seg)\n",
    "                y.append(lab)\n",
    "                groups.append(s)\n",
    "\n",
    "    X = np.stack(X, axis=0)\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "    groups = np.asarray(groups, dtype=np.int64)\n",
    "\n",
    "    n, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Dataset construido: N={n} | T={T} | C={C} | clases={n_classes} | sujetos únicos={len(np.unique(groups))}\")\n",
    "    return X, y, groups, ch_template\n",
    "\n",
    "# =========================\n",
    "# SHALLOW CNN\n",
    "# =========================\n",
    "class ShallowDose2018(nn.Module):\n",
    "    def __init__(self, n_ch: int, n_classes: int, kernel_t: int = 30, n_feat: int = 40, pool_t: int = 15):\n",
    "        super().__init__()\n",
    "        self.n_ch = n_ch\n",
    "        self.n_classes = n_classes\n",
    "        self.kernel_t = kernel_t\n",
    "        self.n_feat = n_feat\n",
    "        self.pool_t = pool_t\n",
    "\n",
    "        self.temporal = nn.Conv2d(1, n_feat, kernel_size=(kernel_t, 1),\n",
    "                                  padding=(kernel_t // 2, 0), bias=True)\n",
    "        self.spatial  = nn.Conv2d(n_feat, n_feat, kernel_size=(1, n_ch),\n",
    "                                  padding=(0, 0), bias=True)\n",
    "        self.avgpool  = nn.AvgPool2d(kernel_size=(pool_t, 1), stride=(pool_t, 1))\n",
    "        self.act      = nn.ELU()\n",
    "        self.flatten  = nn.Flatten()\n",
    "\n",
    "        self.fc  = None\n",
    "        self.out = None\n",
    "        self._T_in = None\n",
    "\n",
    "    def _build_head(self, T_in: int, device: torch.device):\n",
    "        T_pool = T_in // self.pool_t\n",
    "        feat_dim = self.n_feat * T_pool\n",
    "        self.fc  = nn.Linear(feat_dim, 80, bias=True).to(device)\n",
    "        self.out = nn.Linear(80, self.n_classes, bias=True).to(device)\n",
    "        self._T_in = T_in\n",
    "\n",
    "    def ensure_head(self, T_in: int, device: torch.device):\n",
    "        if (self.fc is None) or (self.out is None) or (self._T_in != T_in):\n",
    "            self._build_head(T_in, device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, _, T, C = x.shape\n",
    "        self.ensure_head(T, x.device)\n",
    "\n",
    "        z = self.temporal(x); z = self.act(z)\n",
    "        z = self.spatial(z);  z = self.act(z)\n",
    "        z = self.avgpool(z)\n",
    "        z = self.flatten(z)\n",
    "        z = self.fc(z); z = self.act(z)\n",
    "        z = self.out(z)\n",
    "        return z\n",
    "\n",
    "# =========================\n",
    "# TORCH DATASET\n",
    "# =========================\n",
    "class EEGTrials(Dataset):\n",
    "    def __init__(self, X, y, groups):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.g = groups.astype(np.int64)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        x = np.expand_dims(x, 0)                 # (1, T, C)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx])\n",
    "\n",
    "CLASS_NAMES_4C = ['Left', 'Right', 'Both Fists', 'Both Feet']\n",
    "\n",
    "# =========================\n",
    "# ENTRENAMIENTO / EVALUACIÓN\n",
    "# =========================\n",
    "def train_epoch(model, loader, opt, criterion):\n",
    "    model.train()\n",
    "    for xb, yb, _ in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_with_preds(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for xb, yb, _ in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        pred = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "        y_pred.extend(pred)\n",
    "        y_true.extend(yb.numpy().tolist())\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    return y_true, y_pred, float(acc)\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes, title, fname):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.nan_to_num(cm_norm)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm_norm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "        plt.text(j, i, format(cm_norm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm_norm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def print_report(y_true, y_pred, classes):\n",
    "    print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
    "\n",
    "# =========================\n",
    "# FINE-TUNING PROGRESIVO por sujeto (con validación interna)\n",
    "# =========================\n",
    "def _param_groups(model, mode):\n",
    "    \"\"\"\n",
    "    Devuelve los parámetros a entrenar según 'mode':\n",
    "      - 'out'            : solo capa final\n",
    "      - 'head'           : fc + out\n",
    "      - 'spatial+head'   : spatial + fc + out  (temporal queda congelada)\n",
    "    \"\"\"\n",
    "    if mode == 'out':\n",
    "        train = list(model.out.parameters())\n",
    "    elif mode == 'head':\n",
    "        train = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "    elif mode == 'spatial+head':\n",
    "        train = (list(model.spatial.parameters())\n",
    "                 + list(model.fc.parameters())\n",
    "                 + list(model.out.parameters()))\n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "    return train\n",
    "\n",
    "def _freeze_for_mode(model, mode):\n",
    "    # Primero congelamos todo\n",
    "    for p in model.parameters(): p.requires_grad = False\n",
    "    # Siempre dejamos la base temporal congelada en este protocolo\n",
    "    # Descongelamos según 'mode'\n",
    "    if mode == 'out':\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'head':\n",
    "        for p in model.fc.parameters():  p.requires_grad = True\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'spatial+head':\n",
    "        for p in model.spatial.parameters(): p.requires_grad = True\n",
    "        for p in model.fc.parameters():      p.requires_grad = True\n",
    "        for p in model.out.parameters():     p.requires_grad = True\n",
    "\n",
    "def _class_weights(y_np, n_classes):\n",
    "    # Pesos inversos a la frecuencia por clase en el set de calibración\n",
    "    counts = np.bincount(y_np, minlength=n_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    weights = counts.sum() / counts\n",
    "    weights = weights / weights.mean()\n",
    "    return torch.tensor(weights, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "def _make_optimizer(train_params, mode):\n",
    "    # LR más alto para la cabeza, bajo para spatial cuando aplique\n",
    "    if mode == 'spatial+head':\n",
    "        # separar spatial vs head para LRs distintos\n",
    "        spatial, head = [], []\n",
    "        for p in train_params:\n",
    "            # heurística: parámetros que pertenecen a spatial tendrán .shape acorde\n",
    "            # mejor: detectarlos por referencia al módulo\n",
    "            pass\n",
    "        # Como no tenemos tags aquí, armamos dos grupos manualmente en la llamada principal.\n",
    "        # Devolvemos None y lo construimos fuera.\n",
    "        return None\n",
    "    else:\n",
    "        return optim.Adam(train_params, lr=FT_HEAD_LR)\n",
    "\n",
    "def _train_one_mode(model, X_cal, y_cal, n_classes, mode,\n",
    "                    epochs=FT_EPOCHS, batch_size=16,\n",
    "                    head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                    l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO):\n",
    "    \"\"\"\n",
    "    Entrena en 'mode' con early stopping sobre un conjunto de validación interno.\n",
    "    Devuelve el modelo con los mejores pesos (por val loss).\n",
    "    \"\"\"\n",
    "    # Split Cal -> (train_cal, val_cal)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=RANDOM_STATE)\n",
    "    (tr_idx, va_idx), = sss.split(X_cal, y_cal)\n",
    "    Xtr, ytr = X_cal[tr_idx], y_cal[tr_idx]\n",
    "    Xva, yva = X_cal[va_idx], y_cal[va_idx]\n",
    "\n",
    "    # Datasets\n",
    "    ds_tr = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xtr).float().unsqueeze(1),\n",
    "        torch.from_numpy(ytr).long()\n",
    "    )\n",
    "    ds_va = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xva).float().unsqueeze(1),\n",
    "        torch.from_numpy(yva).long()\n",
    "    )\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    # Congelar / descongelar según modo\n",
    "    _freeze_for_mode(model, mode)\n",
    "    # Parámetros a entrenar y referencia L2-SP\n",
    "    if mode == 'spatial+head':\n",
    "        base_params = list(model.spatial.parameters())\n",
    "        head_params = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "        train_params = base_params + head_params\n",
    "        opt = optim.Adam([\n",
    "            {\"params\": base_params, \"lr\": base_lr},\n",
    "            {\"params\": head_params, \"lr\": head_lr},\n",
    "        ])\n",
    "    else:\n",
    "        train_params = _param_groups(model, mode)\n",
    "        opt = optim.Adam(train_params, lr=head_lr)\n",
    "\n",
    "    ref = [p.detach().clone().to(p.device) for p in train_params]\n",
    "    class_w = _class_weights(ytr, n_classes)\n",
    "    crit = nn.CrossEntropyLoss(weight=class_w)\n",
    "\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_val = float('inf')\n",
    "    bad = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # --- train ---\n",
    "        model.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            # L2-SP hacia referencia de los parámetros que estamos entrenando\n",
    "            reg = 0.0\n",
    "            for p_cur, p_ref in zip(train_params, ref):\n",
    "                reg = reg + torch.sum((p_cur - p_ref)**2)\n",
    "            loss = loss + l2sp_lambda * reg\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # --- val ---\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            nval = 0\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                loss = crit(logits, yb)  # misma pérdida con pesos\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "                nval += xb.size(0)\n",
    "            val_loss /= max(1, nval)\n",
    "\n",
    "        if val_loss + 1e-7 < best_val:\n",
    "            best_val = val_loss\n",
    "            bad = 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_numpy(model, X_np, device):\n",
    "    model.eval()\n",
    "    xb = torch.from_numpy(X_np).float().unsqueeze(1).to(device)  # (N,1,T,C)\n",
    "    logits = model(xb)\n",
    "    return logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "def subject_cv_finetune_predict_progressive(model_global, Xs, ys, device,\n",
    "                                            n_splits=CALIB_CV_FOLDS, n_classes=4):\n",
    "    \"\"\"\n",
    "    Para un sujeto: 4-fold StratifiedKFold.\n",
    "      - En cada fold: se entrena con 3 modos progresivos:\n",
    "          1) 'out'            (solo capa final)\n",
    "          2) 'head'           (fc + out)\n",
    "          3) 'spatial+head'   (spatial + fc + out, temporal congelada)\n",
    "        Se evalúa en el test_fold y se elige el mejor.\n",
    "    Devuelve y_true_subj, y_pred_subj (OOF por fold).\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    y_true_full = np.empty_like(ys)\n",
    "    y_pred_full = np.empty_like(ys)\n",
    "\n",
    "    for tr_idx, te_idx in skf.split(Xs, ys):\n",
    "        Xcal, ycal = Xs[tr_idx], ys[tr_idx]\n",
    "        Xho,  yho  = Xs[te_idx], ys[te_idx]\n",
    "\n",
    "        # Stage A: 'out'\n",
    "        m_out = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_out, Xcal, ycal, n_classes, mode='out',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_out = predict_numpy(m_out, Xho, device)\n",
    "        acc_out = (yhat_out == yho).mean()\n",
    "\n",
    "        # Stage B: 'head'\n",
    "        m_head = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_head, Xcal, ycal, n_classes, mode='head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_head = predict_numpy(m_head, Xho, device)\n",
    "        acc_head = (yhat_head == yho).mean()\n",
    "\n",
    "        # Stage C: 'spatial+head'\n",
    "        m_sp = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_sp, Xcal, ycal, n_classes, mode='spatial+head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                        l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_sp = predict_numpy(m_sp, Xho, device)\n",
    "        acc_sp = (yhat_sp == yho).mean()\n",
    "\n",
    "        # Elegir mejor\n",
    "        best_idx = np.argmax([acc_out, acc_head, acc_sp])\n",
    "        yhat_best = [yhat_out, yhat_head, yhat_sp][best_idx]\n",
    "\n",
    "        y_true_full[te_idx] = yho\n",
    "        y_pred_full[te_idx] = yhat_best\n",
    "\n",
    "    return y_true_full, y_pred_full\n",
    "\n",
    "# =========================\n",
    "# FOLDS JSON helpers\n",
    "# =========================\n",
    "def save_group_folds_json_with_indices(subject_ids_str, groups_array, n_splits, out_json_path,\n",
    "                                       created_by=\"dose_experiment\", description=None):\n",
    "    out_json_path = Path(out_json_path)\n",
    "    unique_subjects_int = sorted(np.unique(groups_array).tolist())\n",
    "    subject_ids = [f\"S{sid:03d}\" for sid in unique_subjects_int]\n",
    "\n",
    "    if len(subject_ids) < n_splits:\n",
    "        raise ValueError(f\"n_splits={n_splits} mayor que número de sujetos={len(subject_ids)}\")\n",
    "\n",
    "    groups = np.arange(len(subject_ids))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    folds = []\n",
    "    fold_i = 0\n",
    "    for train_idx_grp, test_idx_grp in gkf.split(groups, groups, groups):\n",
    "        fold_i += 1\n",
    "        train_sids = [subject_ids[int(i)] for i in train_idx_grp]\n",
    "        test_sids  = [subject_ids[int(i)] for i in test_idx_grp]\n",
    "\n",
    "        train_sids_int = [int(s[1:]) for s in train_sids]\n",
    "        test_sids_int  = [int(s[1:]) for s in test_sids]\n",
    "\n",
    "        tr_idx = np.where(np.isin(groups_array, train_sids_int))[0].tolist()\n",
    "        te_idx = np.where(np.isin(groups_array, test_sids_int))[0].tolist()\n",
    "\n",
    "        folds.append({\n",
    "            \"fold\": int(fold_i),\n",
    "            \"train\": train_sids,\n",
    "            \"test\": test_sids,\n",
    "            \"tr_idx\": tr_idx,\n",
    "            \"te_idx\": te_idx\n",
    "        })\n",
    "\n",
    "    payload = {\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"created_by\": created_by,\n",
    "        \"description\": description if description is not None else \"\",\n",
    "        \"n_splits\": int(n_splits),\n",
    "        \"n_subjects\": len(subject_ids),\n",
    "        \"subject_ids\": subject_ids,\n",
    "        \"folds\": folds\n",
    "    }\n",
    "\n",
    "    out_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Folds JSON con índices guardado → {out_json_path}\")\n",
    "    return out_json_path\n",
    "\n",
    "def load_group_folds_json(path_json, expected_subject_ids=None, strict_check=True):\n",
    "    path_json = Path(path_json)\n",
    "    if not path_json.exists():\n",
    "        raise FileNotFoundError(f\"No existe {path_json}\")\n",
    "    with open(path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        payload = json.load(f)\n",
    "\n",
    "    subj_json = payload.get(\"subject_ids\", [])\n",
    "    if expected_subject_ids is not None:\n",
    "        expected = sorted(list(expected_subject_ids))\n",
    "        if subj_json != expected:\n",
    "            msg = (\"Los subject_ids del JSON no coinciden con expected_subject_ids.\\n\"\n",
    "                   f\"JSON has {len(subj_json)} subjects, expected {len(expected)}.\\n\"\n",
    "                   f\"First 10 JSON: {subj_json[:10]}\\nFirst 10 expected: {expected[:10]}\")\n",
    "            if strict_check:\n",
    "                raise ValueError(msg)\n",
    "            else:\n",
    "                print(\"WARNING: \" + msg)\n",
    "    return payload\n",
    "\n",
    "# =========================\n",
    "# EXPERIMENTO\n",
    "# =========================\n",
    "def run_experiment(save_folds_json=True, folds_json_path=FOLDS_DIR, folds_json_description=\"GroupKFold folds for comparison\"):\n",
    "    \"\"\"\n",
    "    - Crea/lee JSON con folds por sujeto (incluye tr_idx/te_idx)\n",
    "    - Entrena modelo global por fold (inter-sujeto puro)\n",
    "    - Evalúa Global acc en test\n",
    "    - Realiza Fine-Tuning PROGRESIVO por sujeto (4-fold CV) y reporta acc\n",
    "    \"\"\"\n",
    "    mne.set_log_level('WARNING')\n",
    "\n",
    "    # sujetos y dataset\n",
    "    subs = subjects_available()\n",
    "    print(f\"Sujetos elegibles: {len(subs)} → {subs[:10]}{'...' if len(subs)>10 else ''}\")\n",
    "\n",
    "    X, y, groups, chs = build_dataset_all(subs, scenario=CLASS_SCENARIO, window_mode=WINDOW_MODE)\n",
    "    N, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Listo para entrenar: N={N} | T={T} | C={C} | clases={n_classes} | sujetos={len(np.unique(groups))}\")\n",
    "\n",
    "    ds = EEGTrials(X, y, groups)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # preparar JSON folds\n",
    "    if folds_json_path is None:\n",
    "        folds_json_path = Path(\"folds\") / f\"group_folds_{N_FOLDS}splits.json\"\n",
    "    else:\n",
    "        folds_json_path = Path(folds_json_path)\n",
    "    folds_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    unique_subs = sorted(np.unique(groups).tolist())\n",
    "    subject_ids_str = [f\"S{s:03d}\" for s in unique_subs]\n",
    "\n",
    "    if not folds_json_path.exists():\n",
    "        if not save_folds_json:\n",
    "            raise FileNotFoundError(f\"Folds JSON no encontrado en {folds_json_path} y save_folds_json=False.\")\n",
    "        save_group_folds_json_with_indices(subject_ids_str, groups, n_splits=N_FOLDS,\n",
    "                                           out_json_path=folds_json_path,\n",
    "                                           created_by=\"Joel_Clasificador\",\n",
    "                                           description=folds_json_description)\n",
    "\n",
    "    payload = load_group_folds_json(folds_json_path, expected_subject_ids=subject_ids_str, strict_check=False)\n",
    "    folds = payload[\"folds\"]\n",
    "\n",
    "    # bucle por folds\n",
    "    global_folds = []\n",
    "    ft_prog_folds = []\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "\n",
    "    for f in folds:\n",
    "        fold = f[\"fold\"]\n",
    "        tr_idx = np.asarray(f.get(\"tr_idx\", []), dtype=int)\n",
    "        te_idx = np.asarray(f.get(\"te_idx\", []), dtype=int)\n",
    "\n",
    "        if tr_idx.size == 0 or te_idx.size == 0:\n",
    "            print(f\"Advertencia: fold {fold} sin índices tr/te válidos. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        tr_loader = DataLoader(Subset(ds, tr_idx), batch_size=BATCH_SIZE, shuffle=True,  drop_last=False)\n",
    "        te_loader = DataLoader(Subset(ds, te_idx), batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "        model = ShallowDose2018(n_ch=C, n_classes=n_classes).to(DEVICE)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "        print(f\"\\n[Fold {fold}/{N_FOLDS}] Entrenando modelo global... (n_train={len(tr_idx)} | n_test={len(te_idx)})\")\n",
    "        for epoch in range(1, EPOCHS_GLOBAL + 1):\n",
    "            train_epoch(model, tr_loader, opt, criterion)\n",
    "            if epoch % 20 == 0:\n",
    "                print(f\"  Época {epoch}/{EPOCHS_GLOBAL}\")\n",
    "\n",
    "        # Evaluación global (inter-sujeto puro)\n",
    "        y_true, y_pred, acc_global = evaluate_with_preds(model, te_loader)\n",
    "        global_folds.append(acc_global)\n",
    "        all_true.append(y_true); all_pred.append(y_pred)\n",
    "\n",
    "        print(f\"[Fold {fold}/{N_FOLDS}] Global acc={acc_global:.4f}\")\n",
    "        print_report(y_true, y_pred, CLASS_NAMES_4C)\n",
    "\n",
    "        # ---------- Fine-tuning PROGRESIVO por sujeto con 4-fold CV ----------\n",
    "        X_te, y_te, g_te = X[te_idx], y[te_idx], groups[te_idx]\n",
    "\n",
    "        y_true_ft_all, y_pred_ft_all = [], []\n",
    "        used_subjects = 0\n",
    "        for sid in np.unique(g_te):\n",
    "            idx = np.where(g_te == sid)[0]\n",
    "            Xs, ys = X_te[idx], y_te[idx]\n",
    "\n",
    "            # Seguridad: requiere al menos n_splits muestras (estratificado).\n",
    "            if len(ys) < CALIB_CV_FOLDS or len(np.unique(ys)) < 2:\n",
    "                continue\n",
    "\n",
    "            y_true_subj, y_pred_subj = subject_cv_finetune_predict_progressive(\n",
    "                model, Xs, ys, DEVICE, n_splits=CALIB_CV_FOLDS, n_classes=n_classes\n",
    "            )\n",
    "            y_true_ft_all.append(y_true_subj)\n",
    "            y_pred_ft_all.append(y_pred_subj)\n",
    "            used_subjects += 1\n",
    "\n",
    "        if len(y_true_ft_all) > 0:\n",
    "            y_true_ft_all = np.concatenate(y_true_ft_all)\n",
    "            y_pred_ft_all = np.concatenate(y_pred_ft_all)\n",
    "            acc_ft = (y_true_ft_all == y_pred_ft_all).mean()\n",
    "            print(f\"  Fine-tuning PROGRESIVO (por sujeto, {CALIB_CV_FOLDS}-fold CV) acc={acc_ft:.4f} | sujetos={used_subjects}\")\n",
    "            print(f\"  Δ(FT-Global) = {acc_ft - acc_global:+.4f}\")\n",
    "        else:\n",
    "            acc_ft = np.nan\n",
    "            print(\"  Fine-tuning PROGRESIVO no ejecutado (sujeto(s) con muestras insuficientes).\")\n",
    "\n",
    "        ft_prog_folds.append(acc_ft)\n",
    "\n",
    "    # ---------- resultados finales ----------\n",
    "    if len(all_true) > 0:\n",
    "        all_true = np.concatenate(all_true)\n",
    "        all_pred = np.concatenate(all_pred)\n",
    "    else:\n",
    "        all_true = np.array([], dtype=int)\n",
    "        all_pred = np.array([], dtype=int)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTADOS FINALES\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Global folds:\", [f\"{a:.4f}\" for a in global_folds])\n",
    "    if len(global_folds) > 0:\n",
    "        print(f\"Global mean: {np.mean(global_folds):.4f}\")\n",
    "\n",
    "    print(\"Fine-tune PROGRESIVO folds:\", [(\"nan\" if (a is None or np.isnan(a)) else f\"{a:.4f}\") for a in ft_prog_folds])\n",
    "    if len(ft_prog_folds) > 0:\n",
    "        print(f\"Fine-tune PROGRESIVO mean: {np.nanmean(ft_prog_folds):.4f}\")\n",
    "        print(f\"Δ(FT-Global) mean: {np.nanmean(ft_prog_folds) - np.mean(global_folds):+.4f}\")\n",
    "\n",
    "    # Matriz de confusión global (sobre todos los folds)\n",
    "    if all_true.size > 0:\n",
    "        plot_confusion(all_true, all_pred, CLASS_NAMES_4C,\n",
    "                       title=\"Confusion Matrix - Global Model (All Folds)\",\n",
    "                       fname=\"confusion_global_allfolds.png\")\n",
    "        print(\"\\n↳ Matriz de confusión guardada: confusion_global_allfolds.png\")\n",
    "\n",
    "    return {\n",
    "        \"global_folds\": global_folds,\n",
    "        \"ft_prog_folds\": ft_prog_folds,\n",
    "        \"all_true\": all_true,\n",
    "        \"all_pred\": all_pred,\n",
    "        \"folds_json_path\": str(folds_json_path)\n",
    "    }\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🧠 INICIANDO EXPERIMENTO CON FINE-TUNING PROGRESIVO (por sujeto, 4-fold CV)\")\n",
    "    print(f\"🔧 Configuración: {CLASS_SCENARIO}, {len(EXPECTED_8)} canales, {WINDOW_MODE}\")\n",
    "    print(f\"⚙️  FT: epochs={FT_EPOCHS}, base_lr={FT_BASE_LR}, head_lr={FT_HEAD_LR}, L2SP={FT_L2SP}, patience={FT_PATIENCE}, CV={CALIB_CV_FOLDS}\")\n",
    "    run_experiment()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b7cb93",
   "metadata": {},
   "source": [
    "### Su intra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bf0061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Experimento INTRA-subject CV completo (5 folds) con Shallow CNN + Feature Extraction\n",
    "# Adaptado del modelo “inter” que compartiste\n",
    "\n",
    "import os, re, math, random, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# CONFIGURACIÓN GENERAL\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "CLASS_SCENARIO = '4c'\n",
    "WINDOW_MODE = '3s'\n",
    "FS = 160.0\n",
    "N_FOLDS = 5\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS_GLOBAL = 50\n",
    "LR = 1e-3\n",
    "\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "MI_RUNS_LR = [4, 8, 12]\n",
    "MI_RUNS_OF = [6, 10, 14]\n",
    "EXPECTED_64 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','CPz']\n",
    "\n",
    "CLASSIFIER_TYPE = 'svm'  # 'svm' o 'logistic'\n",
    "CLASS_NAMES_4C = ['Left','Right','Both Fists','Both Feet']\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES DE CANALES\n",
    "# =========================\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = re.sub(r'([A-Za-z])0([0-9])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])Z$', r'\\1z', s)\n",
    "    s = s.replace('fp', 'Fp').replace('FP', 'Fp')\n",
    "    s = ''.join(ch.upper() if ch != 'z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {ch: normalize_label(ch) for ch in raw.ch_names}\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired_channels=EXPECTED_64) -> mne.io.BaseRaw:\n",
    "    have = [ch for ch in desired_channels if ch in raw.ch_names]\n",
    "    missing = [ch for ch in desired_channels if ch not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"Warning: faltan canales {missing} en archivo {getattr(raw,'filenames',[''])[0]}\")\n",
    "        return None\n",
    "    return raw.reorder_channels(desired_channels)\n",
    "\n",
    "# =========================\n",
    "# LECTURA DE EDF y EVENTOS\n",
    "# =========================\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(run_id:int):\n",
    "    if run_id in MI_RUNS_LR: return 'LR'\n",
    "    if run_id in MI_RUNS_OF: return 'OF'\n",
    "    return None\n",
    "\n",
    "def read_raw_edf(path: Path) -> mne.io.BaseRaw:\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        raw.set_montage(mne.channels.make_standard_montage('standard_1020'), on_missing='ignore')\n",
    "    except: pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad='auto')\n",
    "    return ensure_channels_order(raw)\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    if raw.annotations is None or len(raw.annotations) == 0: return []\n",
    "    res = []\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = str(desc).strip().upper().replace(' ','')\n",
    "        if tag in ('T1','T2'): res.append((float(onset), tag))\n",
    "    res.sort()\n",
    "    dedup = []\n",
    "    last_t1 = last_t2 = -1e9\n",
    "    for t, tag in res:\n",
    "        if tag=='T1' and (t-last_t1)>=0.5: dedup.append((t,tag)); last_t1=t\n",
    "        if tag=='T2' and (t-last_t2)>=0.5: dedup.append((t,tag)); last_t2=t\n",
    "    return dedup\n",
    "\n",
    "# =========================\n",
    "# CONSTRUCCIÓN DE DATASET\n",
    "# =========================\n",
    "def subjects_available():\n",
    "    subs=[]\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try: sid=int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir/f\"S{sid:03d}R{r:02d}.edf\").exists() for r in MI_RUNS_LR+MI_RUNS_OF)\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "def extract_trials_from_run(edf_path: Path):\n",
    "    subj, run = parse_subject_run(edf_path)\n",
    "    kind = run_kind(run)\n",
    "    if kind not in ('LR','OF'): return []\n",
    "\n",
    "    raw = read_raw_edf(edf_path)\n",
    "    data = raw.get_data()\n",
    "    fs = raw.info['sfreq']\n",
    "    events = collect_events_T1T2(raw)\n",
    "\n",
    "    out=[]\n",
    "    for onset, tag in events:\n",
    "        if kind=='LR':\n",
    "            label=0 if tag=='T1' else 1\n",
    "        else:\n",
    "            label=2 if tag=='T1' else 3\n",
    "        s = int(round(onset*fs))\n",
    "        e = int(round((onset+3.0)*fs))\n",
    "        if s<0 or e>data.shape[1]: continue\n",
    "        seg=data[:,s:e].T.astype(np.float32)\n",
    "        out.append((seg,label,subj))\n",
    "    return out\n",
    "\n",
    "def build_dataset_all(subjects):\n",
    "    X,y,groups=[],[],[]\n",
    "    for s in tqdm(subjects,\"Construyendo dataset\"):\n",
    "        sdir = DATA_RAW/f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "        for r in MI_RUNS_LR+MI_RUNS_OF:\n",
    "            path = sdir/f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not path.exists(): continue\n",
    "            trials=extract_trials_from_run(path)\n",
    "            for seg,label,subj in trials:\n",
    "                X.append(seg); y.append(label); groups.append(subj)\n",
    "    X=np.stack(X,0); y=np.array(y,dtype=int); groups=np.array(groups,dtype=int)\n",
    "    print(f\"Dataset construido: N={X.shape[0]} | T={X.shape[1]} | C={X.shape[2]} | sujetos={len(np.unique(groups))}\")\n",
    "    return X,y,groups\n",
    "\n",
    "# =========================\n",
    "# TORCH DATASET\n",
    "# =========================\n",
    "class EEGTrials(Dataset):\n",
    "    def __init__(self,X,y,groups):\n",
    "        self.X=X.astype(np.float32)\n",
    "        self.y=y.astype(np.int64)\n",
    "        self.g=groups.astype(np.int64)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self,idx):\n",
    "        x=self.X[idx]; x=np.expand_dims(x,0)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx])\n",
    "\n",
    "# =========================\n",
    "# SHALLOW CNN\n",
    "# =========================\n",
    "class ShallowDose2018(nn.Module):\n",
    "    def __init__(self,n_ch,n_classes,kernel_t=30,n_feat=40,pool_t=15):\n",
    "        super().__init__()\n",
    "        self.temporal=nn.Conv2d(1,n_feat,(kernel_t,1),padding=(kernel_t//2,0))\n",
    "        self.spatial=nn.Conv2d(n_feat,n_feat,(1,n_ch))\n",
    "        self.avgpool=nn.AvgPool2d((pool_t,1),stride=(pool_t,1))\n",
    "        self.act=nn.ELU()\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.fc=None; self.out=None; self._T_in=None\n",
    "        self.n_classes=n_classes\n",
    "    def _build_head(self,T_in,device):\n",
    "        T_pool=T_in//15\n",
    "        feat_dim=40*T_pool\n",
    "        self.fc=nn.Linear(feat_dim,80).to(device)\n",
    "        self.out=nn.Linear(80,self.n_classes).to(device)\n",
    "        self._T_in=T_in\n",
    "    def ensure_head(self,T_in,device):\n",
    "        if self.fc is None or self.out is None or self._T_in!=T_in:\n",
    "            self._build_head(T_in,device)\n",
    "    def forward(self,x):\n",
    "        B,_,T,C=x.shape\n",
    "        self.ensure_head(T,x.device)\n",
    "        z=self.temporal(x); z=self.act(z)\n",
    "        z=self.spatial(z); z=self.act(z)\n",
    "        z=self.avgpool(z); z=self.flatten(z)\n",
    "        z=self.fc(z); z=self.act(z)\n",
    "        return self.out(z)\n",
    "    def extract_features(self,x):\n",
    "        B,_,T,C=x.shape\n",
    "        self.ensure_head(T,x.device)\n",
    "        z=self.temporal(x); z=self.act(z)\n",
    "        z=self.spatial(z); z=self.act(z)\n",
    "        z=self.avgpool(z); z=self.flatten(z)\n",
    "        z=self.fc(z); z=self.act(z)\n",
    "        return z\n",
    "\n",
    "# =========================\n",
    "# TRAIN + FEATURE EXTRACTION\n",
    "# =========================\n",
    "def train_epoch(model,loader,opt,criterion):\n",
    "    model.train()\n",
    "    for xb,yb,_ in loader:\n",
    "        xb,yb=xb.to(DEVICE),yb.to(DEVICE)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss=criterion(model(xb),yb)\n",
    "        loss.backward(); opt.step()\n",
    "\n",
    "def extract_features_from_model(model,X,batch_size=32):\n",
    "    model.eval()\n",
    "    feats=[]\n",
    "    with torch.no_grad():\n",
    "        for i in range(0,len(X),batch_size):\n",
    "            x_batch=torch.from_numpy(X[i:i+batch_size]).float().unsqueeze(1).to(DEVICE)\n",
    "            feats.append(model.extract_features(x_batch).cpu().numpy())\n",
    "    return np.concatenate(feats,0)\n",
    "\n",
    "# =========================\n",
    "# GLOBAL MODEL + INTRA-SUBJECT SVM\n",
    "# =========================\n",
    "def train_global_model(X, y, model_class=ShallowDose2018, epochs=EPOCHS_GLOBAL):\n",
    "    model = model_class(n_ch=X.shape[2], n_classes=len(np.unique(y))).to(DEVICE)\n",
    "    ds = EEGTrials(X, y, np.zeros(len(y)))\n",
    "    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    opt = optim.Adam(model.parameters(), lr=LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        train_epoch(model, loader, opt, criterion)\n",
    "    return model\n",
    "\n",
    "def intra_subject_svm(features, y, groups, n_splits=5, classifier_type='svm', class_names=None):\n",
    "    subjects = np.unique(groups)\n",
    "    all_accs = []\n",
    "    for s in subjects:\n",
    "        idx = np.where(groups == s)[0]\n",
    "        X_s, y_s = features[idx], y[idx]\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "        subj_accs = []\n",
    "        y_true_all, y_pred_all = [], []\n",
    "        for tr_idx, te_idx in skf.split(X_s, y_s):\n",
    "            X_tr, X_te = X_s[tr_idx], X_s[te_idx]\n",
    "            y_tr, y_te = y_s[tr_idx], y_s[te_idx]\n",
    "\n",
    "            if classifier_type == 'svm':\n",
    "                clf = SVC(kernel='linear', random_state=RANDOM_STATE)\n",
    "            else:\n",
    "                clf = LogisticRegression(max_iter=500, random_state=RANDOM_STATE)\n",
    "            clf.fit(X_tr, y_tr)\n",
    "            y_pred = clf.predict(X_te)\n",
    "            subj_accs.append((y_pred == y_te).mean())\n",
    "            y_true_all.extend(y_te)\n",
    "            y_pred_all.extend(y_pred)\n",
    "\n",
    "        mean_acc = np.mean(subj_accs)\n",
    "        all_accs.append(mean_acc)\n",
    "        print(f\"Sujeto {s} intra-subject acc={mean_acc:.4f}\")\n",
    "\n",
    "        # Matriz de confusión y classification report\n",
    "        if class_names is not None:\n",
    "            cm = confusion_matrix(y_true_all, y_pred_all)\n",
    "            print(f\"Confusion matrix sujeto {s}:\\n{cm}\")\n",
    "            print(f\"Classification report sujeto {s}:\\n{classification_report(y_true_all, y_pred_all, target_names=class_names)}\")\n",
    "\n",
    "    print(f\"\\n✅ Promedio intra-subject accuracy: {np.mean(all_accs):.4f}\")\n",
    "    return all_accs\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "if __name__==\"__main__\":\n",
    "    subs = subjects_available()\n",
    "    X, y, groups = build_dataset_all(subs)\n",
    "\n",
    "    print(\"🔹 Entrenando modelo CNN global...\")\n",
    "    global_model = train_global_model(X, y)\n",
    "\n",
    "    print(\"🔹 Extrayendo features con modelo global...\")\n",
    "    features = extract_features_from_model(global_model, X)\n",
    "\n",
    "    print(\"🔹 Evaluando intra-subject CV usando SVM sobre features...\")\n",
    "    intra_subject_svm(features, y, groups, n_splits=5, classifier_type=CLASSIFIER_TYPE, class_names=CLASS_NAMES_4C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5365ee8",
   "metadata": {},
   "source": [
    "# EGGNET + FINETUNING INTER MEJOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e1ced05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Usando dispositivo: cuda\n",
      "🧠 INICIANDO EXPERIMENTO CON EEGNet + FINE-TUNING PROGRESIVO (por sujeto, 4-fold CV)\n",
      "🔧 Configuración: 4c, 8 canales, 3s\n",
      "⚙️  FT: epochs=30, base_lr=5e-05, head_lr=0.001, L2SP=0.0001, patience=5, CV=4\n",
      "Sujetos elegibles: 103 → [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Construyendo dataset (RAW): 100%|██████████| 103/103 [00:18<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset construido: N=8652 | T=480 | C=8 | clases=4 | sujetos únicos=103\n",
      "Listo para entrenar: N=8652 | T=480 | C=8 | clases=4 | sujetos=103\n",
      "\n",
      "[Fold 1/5] Entrenando modelo global con validación interna por sujetos... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  Época   5 | train_acc=0.4470 | val_acc=0.4038\n",
      "  Época  10 | train_acc=0.4591 | val_acc=0.4038\n",
      "  Época  15 | train_acc=0.4705 | val_acc=0.4258\n",
      "  Época  20 | train_acc=0.4752 | val_acc=0.4377\n",
      "  Época  25 | train_acc=0.4841 | val_acc=0.4313\n",
      "  Época  30 | train_acc=0.4829 | val_acc=0.4267\n",
      "  Época  35 | train_acc=0.4910 | val_acc=0.4267\n",
      "  Época  40 | train_acc=0.4919 | val_acc=0.4368\n",
      "  Época  45 | train_acc=0.4986 | val_acc=0.4267\n",
      "  Época  50 | train_acc=0.4931 | val_acc=0.4295\n",
      "  Época  55 | train_acc=0.4969 | val_acc=0.4313\n",
      "  Época  60 | train_acc=0.4840 | val_acc=0.4167\n",
      "  Época  65 | train_acc=0.4902 | val_acc=0.4231\n",
      "  Época  70 | train_acc=0.5007 | val_acc=0.4332\n",
      "  Early stopping en época 70 (mejor val_acc=0.4377)\n",
      "[Fold 1/5] Global acc=0.4303\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.4878    0.4989    0.4933       441\n",
      "       Right     0.4319    0.5397    0.4798       441\n",
      "  Both Fists     0.3746    0.2404    0.2928       441\n",
      "   Both Feet     0.4071    0.4422    0.4239       441\n",
      "\n",
      "    accuracy                         0.4303      1764\n",
      "   macro avg     0.4254    0.4303    0.4225      1764\n",
      "weighted avg     0.4254    0.4303    0.4225      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.4881 | sujetos=21\n",
      "  Δ(FT-Global) = +0.0578\n",
      "\n",
      "[Fold 2/5] Entrenando modelo global con validación interna por sujetos... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  Época   5 | train_acc=0.4305 | val_acc=0.4011\n",
      "  Época  10 | train_acc=0.4350 | val_acc=0.4194\n",
      "  Época  15 | train_acc=0.4472 | val_acc=0.4139\n",
      "  Época  20 | train_acc=0.4627 | val_acc=0.4350\n",
      "  Época  25 | train_acc=0.4639 | val_acc=0.4295\n",
      "  Época  30 | train_acc=0.4667 | val_acc=0.4222\n",
      "  Época  35 | train_acc=0.4674 | val_acc=0.4277\n",
      "  Época  40 | train_acc=0.4738 | val_acc=0.4277\n",
      "  Época  45 | train_acc=0.4748 | val_acc=0.4249\n",
      "  Época  50 | train_acc=0.4712 | val_acc=0.4524\n",
      "  Época  55 | train_acc=0.4753 | val_acc=0.4386\n",
      "  Época  60 | train_acc=0.4781 | val_acc=0.4286\n",
      "  Época  65 | train_acc=0.4827 | val_acc=0.4304\n",
      "  Época  70 | train_acc=0.4790 | val_acc=0.4341\n",
      "  Época  75 | train_acc=0.4827 | val_acc=0.4441\n",
      "  Época  80 | train_acc=0.4810 | val_acc=0.4451\n",
      "  Época  85 | train_acc=0.4776 | val_acc=0.4396\n",
      "  Época  90 | train_acc=0.4836 | val_acc=0.4304\n",
      "  Época  95 | train_acc=0.4860 | val_acc=0.4432\n",
      "  Época 100 | train_acc=0.4867 | val_acc=0.4368\n",
      "  Early stopping en época 100 (mejor val_acc=0.4524)\n",
      "[Fold 2/5] Global acc=0.4881\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5244    0.5850    0.5531       441\n",
      "       Right     0.5455    0.5714    0.5581       441\n",
      "  Both Fists     0.4567    0.3469    0.3943       441\n",
      "   Both Feet     0.4168    0.4490    0.4323       441\n",
      "\n",
      "    accuracy                         0.4881      1764\n",
      "   macro avg     0.4859    0.4881    0.4845      1764\n",
      "weighted avg     0.4859    0.4881    0.4845      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5488 | sujetos=21\n",
      "  Δ(FT-Global) = +0.0607\n",
      "\n",
      "[Fold 3/5] Entrenando modelo global con validación interna por sujetos... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  Época   5 | train_acc=0.4510 | val_acc=0.4405\n",
      "  Época  10 | train_acc=0.4739 | val_acc=0.4725\n",
      "  Época  15 | train_acc=0.4859 | val_acc=0.4725\n",
      "  Época  20 | train_acc=0.5019 | val_acc=0.4716\n",
      "  Época  25 | train_acc=0.5035 | val_acc=0.4725\n",
      "  Época  30 | train_acc=0.4959 | val_acc=0.4505\n",
      "  Época  35 | train_acc=0.5009 | val_acc=0.4698\n",
      "  Época  40 | train_acc=0.5064 | val_acc=0.4762\n",
      "  Época  45 | train_acc=0.5033 | val_acc=0.4863\n",
      "  Época  50 | train_acc=0.5109 | val_acc=0.4835\n",
      "  Época  55 | train_acc=0.5078 | val_acc=0.4890\n",
      "  Época  60 | train_acc=0.5074 | val_acc=0.4853\n",
      "  Época  65 | train_acc=0.5097 | val_acc=0.4799\n",
      "  Época  70 | train_acc=0.5150 | val_acc=0.4844\n",
      "  Época  75 | train_acc=0.5121 | val_acc=0.4689\n",
      "  Época  80 | train_acc=0.5045 | val_acc=0.4698\n",
      "  Época  85 | train_acc=0.5083 | val_acc=0.4817\n",
      "  Época  90 | train_acc=0.5059 | val_acc=0.4945\n",
      "  Época  95 | train_acc=0.5162 | val_acc=0.4689\n",
      "  Época 100 | train_acc=0.5079 | val_acc=0.4753\n",
      "[Fold 3/5] Global acc=0.4518\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.4866    0.5351    0.5097       441\n",
      "       Right     0.4771    0.4717    0.4743       441\n",
      "  Both Fists     0.3886    0.2925    0.3338       441\n",
      "   Both Feet     0.4384    0.5079    0.4706       441\n",
      "\n",
      "    accuracy                         0.4518      1764\n",
      "   macro avg     0.4476    0.4518    0.4471      1764\n",
      "weighted avg     0.4476    0.4518    0.4471      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5300 | sujetos=21\n",
      "  Δ(FT-Global) = +0.0782\n",
      "\n",
      "[Fold 4/5] Entrenando modelo global con validación interna por sujetos... (n_train=5880 | n_val=1092 | n_test=1680)\n",
      "  Época   5 | train_acc=0.4454 | val_acc=0.4176\n",
      "  Época  10 | train_acc=0.4594 | val_acc=0.4304\n",
      "  Época  15 | train_acc=0.4682 | val_acc=0.4441\n",
      "  Época  20 | train_acc=0.4738 | val_acc=0.4341\n",
      "  Época  25 | train_acc=0.4769 | val_acc=0.4432\n",
      "  Época  30 | train_acc=0.4842 | val_acc=0.4487\n",
      "  Época  35 | train_acc=0.4808 | val_acc=0.4405\n",
      "  Época  40 | train_acc=0.4893 | val_acc=0.4322\n",
      "  Época  45 | train_acc=0.4847 | val_acc=0.4386\n",
      "  Época  50 | train_acc=0.4844 | val_acc=0.4304\n",
      "  Época  55 | train_acc=0.4878 | val_acc=0.4368\n",
      "  Época  60 | train_acc=0.4862 | val_acc=0.4441\n",
      "  Época  65 | train_acc=0.4883 | val_acc=0.4451\n",
      "  Época  70 | train_acc=0.4891 | val_acc=0.4451\n",
      "  Época  75 | train_acc=0.4944 | val_acc=0.4551\n",
      "  Época  80 | train_acc=0.4939 | val_acc=0.4597\n",
      "  Época  85 | train_acc=0.4991 | val_acc=0.4487\n",
      "  Época  90 | train_acc=0.5036 | val_acc=0.4524\n",
      "  Época  95 | train_acc=0.4993 | val_acc=0.4496\n",
      "  Época 100 | train_acc=0.4980 | val_acc=0.4524\n",
      "[Fold 4/5] Global acc=0.4637\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5065    0.5595    0.5317       420\n",
      "       Right     0.5394    0.4238    0.4747       420\n",
      "  Both Fists     0.3951    0.4571    0.4238       420\n",
      "   Both Feet     0.4350    0.4143    0.4244       420\n",
      "\n",
      "    accuracy                         0.4637      1680\n",
      "   macro avg     0.4690    0.4637    0.4636      1680\n",
      "weighted avg     0.4690    0.4637    0.4636      1680\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5274 | sujetos=20\n",
      "  Δ(FT-Global) = +0.0637\n",
      "\n",
      "[Fold 5/5] Entrenando modelo global con validación interna por sujetos... (n_train=5880 | n_val=1092 | n_test=1680)\n",
      "  Época   5 | train_acc=0.4379 | val_acc=0.4011\n",
      "  Época  10 | train_acc=0.4563 | val_acc=0.4332\n",
      "  Época  15 | train_acc=0.4682 | val_acc=0.4341\n",
      "  Época  20 | train_acc=0.4733 | val_acc=0.4432\n",
      "  Época  25 | train_acc=0.4770 | val_acc=0.4405\n",
      "  Época  30 | train_acc=0.4745 | val_acc=0.4570\n",
      "  Época  35 | train_acc=0.4782 | val_acc=0.4487\n",
      "  Época  40 | train_acc=0.4849 | val_acc=0.4615\n",
      "  Época  45 | train_acc=0.4823 | val_acc=0.4679\n",
      "  Época  50 | train_acc=0.4840 | val_acc=0.4734\n",
      "  Época  55 | train_acc=0.4852 | val_acc=0.4597\n",
      "  Época  60 | train_acc=0.4840 | val_acc=0.4670\n",
      "  Época  65 | train_acc=0.4847 | val_acc=0.4579\n",
      "  Época  70 | train_acc=0.4820 | val_acc=0.4606\n",
      "  Época  75 | train_acc=0.4896 | val_acc=0.4670\n",
      "  Época  80 | train_acc=0.4845 | val_acc=0.4734\n",
      "  Época  85 | train_acc=0.4913 | val_acc=0.4679\n",
      "  Época  90 | train_acc=0.4891 | val_acc=0.4570\n",
      "  Época  95 | train_acc=0.4896 | val_acc=0.4515\n",
      "  Época 100 | train_acc=0.4891 | val_acc=0.4606\n",
      "  Early stopping en época 100 (mejor val_acc=0.4734)\n",
      "[Fold 5/5] Global acc=0.4720\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5125    0.6357    0.5675       420\n",
      "       Right     0.4865    0.5595    0.5205       420\n",
      "  Both Fists     0.3852    0.2476    0.3014       420\n",
      "   Both Feet     0.4606    0.4452    0.4528       420\n",
      "\n",
      "    accuracy                         0.4720      1680\n",
      "   macro avg     0.4612    0.4720    0.4606      1680\n",
      "weighted avg     0.4612    0.4720    0.4606      1680\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5315 | sujetos=20\n",
      "  Δ(FT-Global) = +0.0595\n",
      "\n",
      "============================================================\n",
      "RESULTADOS FINALES\n",
      "============================================================\n",
      "Global folds: ['0.4303', '0.4881', '0.4518', '0.4637', '0.4720']\n",
      "Global mean: 0.4612\n",
      "Fine-tune PROGRESIVO folds: ['0.4881', '0.5488', '0.5300', '0.5274', '0.5315']\n",
      "Fine-tune PROGRESIVO mean: 0.5252\n",
      "Δ(FT-Global) mean: +0.0640\n",
      "\n",
      "↳ Matriz de confusión guardada: confusion_global_allfolds.png\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Replicación fiel del paper \"A Deep Learning MI-EEG Classification Model for BCIs\"\n",
    "# Dose et al., EUSIPCO 2018 — ahora con EEGNet (Lawhern et al., 2018) en vez de ShallowConvNet\n",
    "# Protocolo: entrenamiento global inter-sujeto + FINE-TUNING PROGRESIVO por sujeto\n",
    "# (CV 4-fold, LRs discriminativos, L2-SP, early stopping con validación)\n",
    "\n",
    "import os, re, math, random, json, itertools, copy\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold, StratifiedShuffleSplit, GroupShuffleSplit\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# CONFIGURACIÓN GENERAL\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "FOLDS_DIR = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dispositivo y semilla\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "# Escenario de clases\n",
    "CLASS_SCENARIO = '4c'\n",
    "WINDOW_MODE = '3s'\n",
    "FS = 160.0\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Entrenamiento global\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS_GLOBAL = 100\n",
    "LR = 1e-3\n",
    "# >>> Añadidos para diagnóstico/ES global <<<\n",
    "GLOBAL_VAL_SPLIT = 0.15   # fracción de sujetos (dentro del train) para validación\n",
    "GLOBAL_PATIENCE  = 10     # épocas sin mejora en val_acc\n",
    "LOG_EVERY        = 5      # log cada N épocas\n",
    "\n",
    "# Fine-tuning por sujeto (protocolo robusto)\n",
    "CALIB_CV_FOLDS = 4            # 4-fold CV por sujeto ~ 75/25\n",
    "FT_EPOCHS = 30                 # ES con validación\n",
    "FT_BASE_LR = 5e-5              # convs \"base\" (temporal/depthwise) — más bajo\n",
    "FT_HEAD_LR = 1e-3              # fc+out — más alto\n",
    "FT_L2SP = 1e-4                 # regularización suave\n",
    "FT_PATIENCE = 5                # early stopping con validación\n",
    "FT_VAL_RATIO = 0.2             # validación dentro del set de calibración\n",
    "\n",
    "# Sujetos excluidos\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "\n",
    "# Runs\n",
    "MI_RUNS_LR = [4, 8, 12]\n",
    "MI_RUNS_OF = [6, 10, 14]\n",
    "BASELINE_RUNS_EO = [1]\n",
    "\n",
    "# Canales (8 en lugar de 64)\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','CPz']\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES DE CANALES\n",
    "# =========================\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = re.sub(r'([A-Za-z])0([0-9])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])Z$', r'\\1z', s)\n",
    "    s = s.replace('fp', 'Fp').replace('FP', 'Fp')\n",
    "    s = ''.join(ch.upper() if ch != 'z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {}\n",
    "    for ch in raw.ch_names:\n",
    "        lab = normalize_label(ch)\n",
    "        lab = lab[:-1] + 'z' if lab.endswith('Z') else lab\n",
    "        lab = re.sub(r'([A-Z])Z$', r'\\1z', lab)\n",
    "        mapping[ch] = lab\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired_channels=EXPECTED_8):\n",
    "    have = [ch for ch in desired_channels if ch in raw.ch_names]\n",
    "    missing = [ch for ch in desired_channels if ch not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"Warning: faltan canales {missing} en archivo {getattr(raw,'filenames', [''])[0]}\")\n",
    "        return None\n",
    "    # Reordenar y quedarse SOLO con los deseados (8)\n",
    "    raw.reorder_channels([ch for ch in raw.ch_names if ch in desired_channels] +\n",
    "                         [ch for ch in raw.ch_names if ch not in desired_channels])\n",
    "    raw.pick_channels(desired_channels, ordered=True)\n",
    "    return raw\n",
    "\n",
    "# =========================\n",
    "# LECTURA DE EDF y EVENTOS\n",
    "# =========================\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(run_id:int):\n",
    "    if run_id in MI_RUNS_LR: return 'LR'\n",
    "    if run_id in MI_RUNS_OF: return 'OF'\n",
    "    if run_id in BASELINE_RUNS_EO: return 'EO'\n",
    "    return None\n",
    "\n",
    "def read_raw_edf(path: Path):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        mont = mne.channels.make_standard_montage('standard_1020')\n",
    "        raw.set_montage(mont, on_missing='ignore')\n",
    "    except Exception:\n",
    "        pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad=\"auto\")\n",
    "    raw = ensure_channels_order(raw, EXPECTED_8)\n",
    "    if raw is None:\n",
    "        return None\n",
    "    # Band-pass opcional (desactivado)\n",
    "    #raw.filter(l_freq=8., h_freq=30., picks='eeg', method='iir', verbose=False)\n",
    "    return raw\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    if raw.annotations is None or len(raw.annotations) == 0:\n",
    "        return []\n",
    "    def _norm(s): return str(s).strip().upper().replace(' ', '')\n",
    "    res = []\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = _norm(desc)\n",
    "        if tag in ('T1','T2'):\n",
    "            res.append((float(onset), tag))\n",
    "    res.sort()\n",
    "    dedup = []\n",
    "    last_t1 = last_t2 = -1e9\n",
    "    for t, tag in res:\n",
    "        if tag == 'T1':\n",
    "            if (t - last_t1) >= 0.5: dedup.append((t, tag)); last_t1 = t\n",
    "        else:\n",
    "            if (t - last_t2) >= 0.5: dedup.append((t, tag)); last_t2 = t\n",
    "    return dedup\n",
    "\n",
    "# =========================\n",
    "# CONSTRUCCIÓN DE DATASETS\n",
    "# =========================\n",
    "def subjects_available():\n",
    "    subs = []\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try:\n",
    "            sid = int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir / f\"S{sid:03d}R{r:02d}.edf\").exists() for r in (MI_RUNS_LR + MI_RUNS_OF))\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "def extract_trials_from_run(edf_path: Path, scenario: str, window_mode: str):\n",
    "    subj, run = parse_subject_run(edf_path)\n",
    "    kind = run_kind(run)\n",
    "    if kind not in ('LR','OF','EO'):\n",
    "        return ([], [])\n",
    "\n",
    "    raw = read_raw_edf(edf_path)\n",
    "    if raw is None:\n",
    "        return ([], [])\n",
    "\n",
    "    data = raw.get_data()\n",
    "    fs = raw.info['sfreq']\n",
    "    assert abs(fs - FS) < 1e-6\n",
    "\n",
    "    out = []\n",
    "\n",
    "    if kind in ('LR','OF'):\n",
    "        events = collect_events_T1T2(raw)\n",
    "        if window_mode == '3s':\n",
    "            rel_start, rel_end = 0.0, 3.0\n",
    "        else:\n",
    "            rel_start, rel_end = -1.0, 5.0\n",
    "\n",
    "        for onset_sec, tag in events:\n",
    "            if kind == 'LR':\n",
    "                if tag == 'T1': label = 'L'\n",
    "                elif tag == 'T2': label = 'R'\n",
    "                else: continue\n",
    "            else:\n",
    "                if tag == 'T1': label = 'BFISTS'\n",
    "                elif tag == 'T2': label = 'BFEET'\n",
    "                else: continue\n",
    "\n",
    "            if scenario == '2c' and label not in ('L','R'):\n",
    "                continue\n",
    "            if scenario == '3c' and label not in ('L','R','BFISTS'):\n",
    "                continue\n",
    "            if scenario == '4c' and label not in ('L','R','BFISTS','BFEET'):\n",
    "                continue\n",
    "\n",
    "            s = int(round((raw.first_time + onset_sec + rel_start) * fs))\n",
    "            e = int(round((raw.first_time + onset_sec + rel_end) * fs))\n",
    "            if s < 0 or e > data.shape[1]:\n",
    "                continue\n",
    "\n",
    "            seg = data[:, s:e].T.astype(np.float32)\n",
    "            # Normalización por época canal-a-canal (z-score)\n",
    "            seg = (seg - seg.mean(axis=0, keepdims=True)) / (seg.std(axis=0, keepdims=True) + 1e-6)\n",
    "\n",
    "            if label == 'L':       y = 0\n",
    "            elif label == 'R':     y = 1\n",
    "            elif label == 'BFISTS':y = 2\n",
    "            elif label == 'BFEET': y = 3\n",
    "            else: continue\n",
    "\n",
    "            out.append((seg, y, subj))\n",
    "\n",
    "    elif kind == 'EO':\n",
    "        return ([], raw.ch_names)\n",
    "\n",
    "    return out, raw.ch_names\n",
    "\n",
    "def build_dataset_all(subjects, scenario='4c', window_mode='3s'):\n",
    "    X, y, groups = [], [], []\n",
    "    ch_template = None\n",
    "\n",
    "    for s in tqdm(subjects, desc=\"Construyendo dataset (RAW)\"):\n",
    "        sdir = DATA_RAW / f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "\n",
    "        trials_L, trials_R, trials_FISTS, trials_FEET = [], [], [], []\n",
    "\n",
    "        for r in MI_RUNS_LR:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 0: trials_L.append(seg)\n",
    "                elif lab == 1: trials_R.append(seg)\n",
    "\n",
    "        for r in MI_RUNS_OF:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 2: trials_FISTS.append(seg)\n",
    "                elif lab == 3: trials_FEET.append(seg)\n",
    "\n",
    "        need_per_class = 21\n",
    "        def pick(trials, n, rng):\n",
    "            if len(trials) < n:\n",
    "                idx = rng.choice(len(trials), size=n, replace=True)\n",
    "                return [trials[i] for i in idx]\n",
    "            rng.shuffle(trials)\n",
    "            return trials[:n]\n",
    "\n",
    "        rng = check_random_state(RANDOM_STATE + s)\n",
    "        if len(trials_L)==0 or len(trials_R)==0 or len(trials_FISTS)==0 or len(trials_FEET)==0:\n",
    "            continue\n",
    "\n",
    "        Lp  = pick(trials_L,     need_per_class, rng)\n",
    "        Rp  = pick(trials_R,     need_per_class, rng)\n",
    "        FIp = pick(trials_FISTS, need_per_class, rng)\n",
    "        FEp = pick(trials_FEET,  need_per_class, rng)\n",
    "\n",
    "        pack = [(Lp, 0), (Rp, 1), (FIp, 2), (FEp, 3)]\n",
    "        for segs, lab in pack:\n",
    "            for seg in segs:\n",
    "                X.append(seg)\n",
    "                y.append(lab)\n",
    "                groups.append(s)\n",
    "\n",
    "    X = np.stack(X, axis=0)\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "    groups = np.asarray(groups, dtype=np.int64)\n",
    "\n",
    "    n, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Dataset construido: N={n} | T={T} | C={C} | clases={n_classes} | sujetos únicos={len(np.unique(groups))}\")\n",
    "    return X, y, groups, ch_template\n",
    "\n",
    "# =========================\n",
    "# EEGNet (Lawhern et al., 2018) adaptado a (B,1,T,C)\n",
    "# =========================\n",
    "class EEGNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Entrada: x de forma (B, 1, T, C)  [T=tiempo, C=canales]\n",
    "    Bloques:\n",
    "      1) Temporal conv     : Conv2d(1 -> F1, (kernel_t,1), padding 'same'), BN, ELU\n",
    "      2) Depthwise (espacial): Conv2d(F1 -> F1*D, (1,C), groups=F1, BN, ELU, AvgPool(4,1), Dropout\n",
    "      3) Separable temporal: Depthwise temporal (k_sep,1) groups=F1*D + Pointwise 1x1 a F2, BN, ELU, AvgPool(8,1), Dropout\n",
    "      4) FC -> OUT\n",
    "    \"\"\"\n",
    "    def __init__(self, n_ch: int, n_classes: int,\n",
    "                 F1: int = 8, D: int = 2, kernel_t: int = 64, k_sep: int = 16,\n",
    "                 pool1_t: int = 4, pool2_t: int = 8, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.n_ch = n_ch\n",
    "        self.n_classes = n_classes\n",
    "        self.F1 = F1\n",
    "        self.D = D\n",
    "        self.F2 = F1 * D\n",
    "        self.kernel_t = kernel_t\n",
    "        self.k_sep = k_sep\n",
    "        self.pool1_t = pool1_t\n",
    "        self.pool2_t = pool2_t\n",
    "\n",
    "        # Bloque 1: temporal\n",
    "        self.conv_temporal = nn.Conv2d(1, F1, kernel_size=(kernel_t, 1),\n",
    "                                       padding=(kernel_t // 2, 0), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(F1)\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "        # Bloque 2: depthwise (espacial)\n",
    "        self.conv_depthwise = nn.Conv2d(F1, self.F2, kernel_size=(1, n_ch),\n",
    "                                        groups=F1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.F2)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=(pool1_t, 1), stride=(pool1_t, 1))\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "\n",
    "        # Bloque 3: separable temporal (depthwise temporal + pointwise)\n",
    "        self.conv_sep_depth = nn.Conv2d(self.F2, self.F2, kernel_size=(k_sep, 1),\n",
    "                                        groups=self.F2, padding=(k_sep // 2, 0), bias=False)\n",
    "        self.conv_sep_point = nn.Conv2d(self.F2, self.F2, kernel_size=(1, 1), bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.F2)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(pool2_t, 1), stride=(pool2_t, 1))\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Cabeza dinámica\n",
    "        self.fc = None\n",
    "        self.out = None\n",
    "        self._T_in = None\n",
    "\n",
    "    def _build_head(self, T_in: int, device: torch.device):\n",
    "        # Con padding 'same' en temporal y separable temporal,\n",
    "        # el tamaño temporal se reduce por los pools:\n",
    "        T1 = T_in // self.pool1_t\n",
    "        T2 = T1 // self.pool2_t\n",
    "        feat_dim = self.F2 * T2 * 1  # ancho=1 tras conv_depthwise (kernel (1,C))\n",
    "        self.fc = nn.Linear(feat_dim, 80, bias=True).to(device)\n",
    "        self.out = nn.Linear(80, self.n_classes, bias=True).to(device)\n",
    "        self._T_in = T_in\n",
    "\n",
    "    def ensure_head(self, T_in: int, device: torch.device):\n",
    "        if (self.fc is None) or (self.out is None) or (self._T_in != T_in):\n",
    "            self._build_head(T_in, device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B,1,T,C)\n",
    "        B, _, T, C = x.shape\n",
    "        self.ensure_head(T, x.device)\n",
    "\n",
    "        z = self.conv_temporal(x)\n",
    "        z = self.bn1(z); z = self.act(z)\n",
    "\n",
    "        z = self.conv_depthwise(z)   # (B, F2, T, 1)\n",
    "        z = self.bn2(z); z = self.act(z)\n",
    "        z = self.pool1(z)\n",
    "        z = self.drop1(z)\n",
    "\n",
    "        z = self.conv_sep_depth(z)\n",
    "        z = self.conv_sep_point(z)\n",
    "        z = self.bn3(z); z = self.act(z)\n",
    "        z = self.pool2(z)\n",
    "        z = self.drop2(z)\n",
    "\n",
    "        z = self.flatten(z)\n",
    "        z = self.fc(z); z = self.act(z)\n",
    "        z = self.out(z)\n",
    "        return z\n",
    "\n",
    "# =========================\n",
    "# TORCH DATASET\n",
    "# =========================\n",
    "class EEGTrials(Dataset):\n",
    "    def __init__(self, X, y, groups):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.g = groups.astype(np.int64)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        x = np.expand_dims(x, 0)                 # (1, T, C)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx])\n",
    "\n",
    "CLASS_NAMES_4C = ['Left', 'Right', 'Both Fists', 'Both Feet']\n",
    "\n",
    "# =========================\n",
    "# ENTRENAMIENTO / EVALUACIÓN\n",
    "# =========================\n",
    "def train_epoch(model, loader, opt, criterion):\n",
    "    model.train()\n",
    "    for xb, yb, _ in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_with_preds(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for xb, yb, _ in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        pred = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "        y_pred.extend(pred)\n",
    "        y_true.extend(yb.numpy().tolist())\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    return y_true, y_pred, float(acc)\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes, title, fname):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.nan_to_num(cm_norm)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm_norm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "        plt.text(j, i, format(cm_norm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm_norm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def print_report(y_true, y_pred, classes):\n",
    "    print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
    "\n",
    "# =========================\n",
    "# FINE-TUNING PROGRESIVO por sujeto (con validación interna)\n",
    "# =========================\n",
    "def _param_groups(model, mode):\n",
    "    \"\"\"\n",
    "    En EEGNet:\n",
    "      - 'out'            : solo capa final (model.out)\n",
    "      - 'head'           : fc + out\n",
    "      - 'spatial+head'   : depthwise + separable + fc + out  (temporal queda congelada)\n",
    "    \"\"\"\n",
    "    if mode == 'out':\n",
    "        train = list(model.out.parameters())\n",
    "    elif mode == 'head':\n",
    "        train = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "    elif mode == 'spatial+head':\n",
    "        train = (list(model.conv_depthwise.parameters()) +\n",
    "                 list(model.bn2.parameters()) +\n",
    "                 list(model.conv_sep_depth.parameters()) +\n",
    "                 list(model.conv_sep_point.parameters()) +\n",
    "                 list(model.bn3.parameters()) +\n",
    "                 list(model.fc.parameters()) +\n",
    "                 list(model.out.parameters()))\n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "    return train\n",
    "\n",
    "def _freeze_for_mode(model, mode):\n",
    "    # Congelamos todo\n",
    "    for p in model.parameters(): p.requires_grad = False\n",
    "    # Siempre mantenemos CONGELADO el bloque temporal en este protocolo\n",
    "    # (conv_temporal + bn1)\n",
    "    if mode == 'out':\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'head':\n",
    "        for p in model.fc.parameters():  p.requires_grad = True\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'spatial+head':\n",
    "        for p in model.conv_depthwise.parameters(): p.requires_grad = True\n",
    "        for p in model.bn2.parameters():           p.requires_grad = True\n",
    "        for p in model.conv_sep_depth.parameters():p.requires_grad = True\n",
    "        for p in model.conv_sep_point.parameters():p.requires_grad = True\n",
    "        for p in model.bn3.parameters():           p.requires_grad = True\n",
    "        for p in model.fc.parameters():            p.requires_grad = True\n",
    "        for p in model.out.parameters():           p.requires_grad = True\n",
    "\n",
    "def _class_weights(y_np, n_classes):\n",
    "    counts = np.bincount(y_np, minlength=n_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    weights = counts.sum() / counts\n",
    "    weights = weights / weights.mean()\n",
    "    return torch.tensor(weights, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "def _train_one_mode(model, X_cal, y_cal, n_classes, mode,\n",
    "                    epochs=FT_EPOCHS, batch_size=16,\n",
    "                    head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                    l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO):\n",
    "    \"\"\"\n",
    "    Entrena en 'mode' con early stopping sobre un conjunto de validación interno.\n",
    "    Devuelve el modelo con los mejores pesos (por val loss).\n",
    "    \"\"\"\n",
    "    # Split Cal -> (train_cal, val_cal)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=RANDOM_STATE)\n",
    "    (tr_idx, va_idx), = sss.split(X_cal, y_cal)\n",
    "    Xtr, ytr = X_cal[tr_idx], y_cal[tr_idx]\n",
    "    Xva, yva = X_cal[va_idx], y_cal[va_idx]\n",
    "\n",
    "    ds_tr = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xtr).float().unsqueeze(1),\n",
    "        torch.from_numpy(ytr).long()\n",
    "    )\n",
    "    ds_va = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xva).float().unsqueeze(1),\n",
    "        torch.from_numpy(yva).long()\n",
    "    )\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    _freeze_for_mode(model, mode)\n",
    "\n",
    "    if mode == 'spatial+head':\n",
    "        # grupos con LR discriminativos\n",
    "        base_params = (list(model.conv_depthwise.parameters()) +\n",
    "                       list(model.bn2.parameters()) +\n",
    "                       list(model.conv_sep_depth.parameters()) +\n",
    "                       list(model.conv_sep_point.parameters()) +\n",
    "                       list(model.bn3.parameters()))\n",
    "        head_params = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "        train_params = base_params + head_params\n",
    "        opt = optim.Adam([\n",
    "            {\"params\": base_params, \"lr\": base_lr},\n",
    "            {\"params\": head_params, \"lr\": head_lr},\n",
    "        ])\n",
    "    else:\n",
    "        train_params = _param_groups(model, mode)\n",
    "        opt = optim.Adam(train_params, lr=head_lr)\n",
    "\n",
    "    ref = [p.detach().clone().to(p.device) for p in train_params]\n",
    "    class_w = _class_weights(ytr, n_classes)\n",
    "    crit = nn.CrossEntropyLoss(weight=class_w)\n",
    "\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_val = float('inf')\n",
    "    bad = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # --- train ---\n",
    "        model.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            # L2-SP hacia referencia de los parámetros que estamos entrenando\n",
    "            reg = 0.0\n",
    "            for p_cur, p_ref in zip(train_params, ref):\n",
    "                reg = reg + torch.sum((p_cur - p_ref)**2)\n",
    "            loss = loss + l2sp_lambda * reg\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # --- val ---\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            nval = 0\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                loss = crit(logits, yb)\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "                nval += xb.size(0)\n",
    "            val_loss /= max(1, nval)\n",
    "\n",
    "        if val_loss + 1e-7 < best_val:\n",
    "            best_val = val_loss\n",
    "            bad = 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_numpy(model, X_np, device):\n",
    "    model.eval()\n",
    "    xb = torch.from_numpy(X_np).float().unsqueeze(1).to(device)  # (N,1,T,C)\n",
    "    logits = model(xb)\n",
    "    return logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "def subject_cv_finetune_predict_progressive(model_global, Xs, ys, device,\n",
    "                                            n_splits=CALIB_CV_FOLDS, n_classes=4):\n",
    "    \"\"\"\n",
    "    Para un sujeto: 4-fold StratifiedKFold.\n",
    "      - En cada fold: 'out' → 'head' → 'spatial+head' (temporal congelado)\n",
    "      - Se elige el mejor en el split de holdout del sujeto.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    y_true_full = np.empty_like(ys)\n",
    "    y_pred_full = np.empty_like(ys)\n",
    "\n",
    "    for tr_idx, te_idx in skf.split(Xs, ys):\n",
    "        Xcal, ycal = Xs[tr_idx], ys[tr_idx]\n",
    "        Xho,  yho  = Xs[te_idx], ys[te_idx]\n",
    "\n",
    "        # Stage A: 'out'\n",
    "        m_out = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_out, Xcal, ycal, n_classes, mode='out',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_out = predict_numpy(m_out, Xho, device)\n",
    "        acc_out = (yhat_out == yho).mean()\n",
    "\n",
    "        # Stage B: 'head'\n",
    "        m_head = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_head, Xcal, ycal, n_classes, mode='head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_head = predict_numpy(m_head, Xho, device)\n",
    "        acc_head = (yhat_head == yho).mean()\n",
    "\n",
    "        # Stage C: 'spatial+head'\n",
    "        m_sp = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_sp, Xcal, ycal, n_classes, mode='spatial+head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                        l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_sp = predict_numpy(m_sp, Xho, device)\n",
    "        acc_sp = (yhat_sp == yho).mean()\n",
    "\n",
    "        best_idx = np.argmax([acc_out, acc_head, acc_sp])\n",
    "        yhat_best = [yhat_out, yhat_head, yhat_sp][best_idx]\n",
    "\n",
    "        y_true_full[te_idx] = yho\n",
    "        y_pred_full[te_idx] = yhat_best\n",
    "\n",
    "    return y_true_full, y_pred_full\n",
    "\n",
    "# =========================\n",
    "# FOLDS JSON helpers\n",
    "# =========================\n",
    "def save_group_folds_json_with_indices(subject_ids_str, groups_array, n_splits, out_json_path,\n",
    "                                       created_by=\"dose_experiment\", description=None):\n",
    "    out_json_path = Path(out_json_path)\n",
    "    unique_subjects_int = sorted(np.unique(groups_array).tolist())\n",
    "    subject_ids = [f\"S{sid:03d}\" for sid in unique_subjects_int]\n",
    "\n",
    "    if len(subject_ids) < n_splits:\n",
    "        raise ValueError(f\"n_splits={n_splits} mayor que número de sujetos={len(subject_ids)}\")\n",
    "\n",
    "    groups = np.arange(len(subject_ids))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    folds = []\n",
    "    fold_i = 0\n",
    "    for train_idx_grp, test_idx_grp in gkf.split(groups, groups, groups):\n",
    "        fold_i += 1\n",
    "        train_sids = [subject_ids[int(i)] for i in train_idx_grp]\n",
    "        test_sids  = [subject_ids[int(i)] for i in test_idx_grp]\n",
    "\n",
    "        train_sids_int = [int(s[1:]) for s in train_sids]\n",
    "        test_sids_int  = [int(s[1:]) for s in test_sids]\n",
    "\n",
    "        tr_idx = np.where(np.isin(groups_array, train_sids_int))[0].tolist()\n",
    "        te_idx = np.where(np.isin(groups_array, test_sids_int))[0].tolist()\n",
    "\n",
    "        folds.append({\n",
    "            \"fold\": int(fold_i),\n",
    "            \"train\": train_sids,\n",
    "            \"test\": test_sids,\n",
    "            \"tr_idx\": tr_idx,\n",
    "            \"te_idx\": te_idx\n",
    "        })\n",
    "\n",
    "    payload = {\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"created_by\": created_by,\n",
    "        \"description\": description if description is not None else \"\",\n",
    "        \"n_splits\": int(n_splits),\n",
    "        \"n_subjects\": len(subject_ids),\n",
    "        \"subject_ids\": subject_ids,\n",
    "        \"folds\": folds\n",
    "    }\n",
    "\n",
    "    out_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Folds JSON con índices guardado → {out_json_path}\")\n",
    "    return out_json_path\n",
    "\n",
    "def load_group_folds_json(path_json, expected_subject_ids=None, strict_check=True):\n",
    "    path_json = Path(path_json)\n",
    "    if not path_json.exists():\n",
    "        raise FileNotFoundError(f\"No existe {path_json}\")\n",
    "    with open(path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        payload = json.load(f)\n",
    "\n",
    "    subj_json = payload.get(\"subject_ids\", [])\n",
    "    if expected_subject_ids is not None:\n",
    "        expected = sorted(list(expected_subject_ids))\n",
    "        if subj_json != expected:\n",
    "            msg = (\"Los subject_ids del JSON no coinciden con expected_subject_ids.\\n\"\n",
    "                   f\"JSON has {len(subj_json)} subjects, expected {len(expected)}.\\n\"\n",
    "                   f\"First 10 JSON: {subj_json[:10]}\\nFirst 10 expected: {expected[:10]}\")\n",
    "            if strict_check:\n",
    "                raise ValueError(msg)\n",
    "            else:\n",
    "                print(\"WARNING: \" + msg)\n",
    "    return payload\n",
    "\n",
    "# =========================\n",
    "# EXPERIMENTO\n",
    "# =========================\n",
    "def run_experiment(save_folds_json=True, folds_json_path=FOLDS_DIR, folds_json_description=\"GroupKFold folds for comparison\"):\n",
    "    \"\"\"\n",
    "    - Crea/lee JSON con folds por sujeto (incluye tr_idx/te_idx)\n",
    "    - Entrena modelo global por fold (inter-sujeto puro) con validación interna por sujetos + ES\n",
    "    - Evalúa Global acc en test\n",
    "    - Realiza Fine-Tuning PROGRESIVO por sujeto (4-fold CV) y reporta acc\n",
    "    \"\"\"\n",
    "    mne.set_log_level('WARNING')\n",
    "\n",
    "    # sujetos y dataset\n",
    "    subs = subjects_available()\n",
    "    print(f\"Sujetos elegibles: {len(subs)} → {subs[:10]}{'...' if len(subs)>10 else ''}\")\n",
    "\n",
    "    X, y, groups, chs = build_dataset_all(subs, scenario=CLASS_SCENARIO, window_mode=WINDOW_MODE)\n",
    "    N, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Listo para entrenar: N={N} | T={T} | C={C} | clases={n_classes} | sujetos={len(np.unique(groups))}\")\n",
    "\n",
    "    ds = EEGTrials(X, y, groups)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # preparar JSON folds\n",
    "    if folds_json_path is None:\n",
    "        folds_json_path = Path(\"folds\") / f\"group_folds_{N_FOLDS}splits.json\"\n",
    "    else:\n",
    "        folds_json_path = Path(folds_json_path)\n",
    "    folds_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    unique_subs = sorted(np.unique(groups).tolist())\n",
    "    subject_ids_str = [f\"S{s:03d}\" for s in unique_subs]\n",
    "\n",
    "    if not folds_json_path.exists():\n",
    "        if not save_folds_json:\n",
    "            raise FileNotFoundError(f\"Folds JSON no encontrado en {folds_json_path} y save_folds_json=False.\")\n",
    "        save_group_folds_json_with_indices(subject_ids_str, groups, n_splits=N_FOLDS,\n",
    "                                           out_json_path=folds_json_path,\n",
    "                                           created_by=\"Joel_Clasificador\",\n",
    "                                           description=folds_json_description)\n",
    "\n",
    "    payload = load_group_folds_json(folds_json_path, expected_subject_ids=subject_ids_str, strict_check=False)\n",
    "    folds = payload[\"folds\"]\n",
    "\n",
    "    # bucle por folds\n",
    "    global_folds = []\n",
    "    ft_prog_folds = []\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "\n",
    "    for f in folds:\n",
    "        fold = f[\"fold\"]\n",
    "        tr_idx = np.asarray(f.get(\"tr_idx\", []), dtype=int)\n",
    "        te_idx = np.asarray(f.get(\"te_idx\", []), dtype=int)\n",
    "\n",
    "        if tr_idx.size == 0 or te_idx.size == 0:\n",
    "            print(f\"Advertencia: fold {fold} sin índices tr/te válidos. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        # ===== Split de validación por SUJETOS dentro del set de entrenamiento =====\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=GLOBAL_VAL_SPLIT, random_state=RANDOM_STATE)\n",
    "        tr_subj_idx, va_subj_idx = next(gss.split(tr_idx, groups[tr_idx], groups[tr_idx]))\n",
    "        tr_sub_idx = tr_idx[tr_subj_idx]\n",
    "        va_idx     = tr_idx[va_subj_idx]\n",
    "\n",
    "        tr_loader = DataLoader(Subset(ds, tr_sub_idx), batch_size=BATCH_SIZE, shuffle=True,  drop_last=False)\n",
    "        va_loader = DataLoader(Subset(ds, va_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "        te_loader = DataLoader(Subset(ds, te_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "        # ===== EEGNet =====\n",
    "        model = EEGNet(n_ch=C, n_classes=n_classes, F1=8, D=2, kernel_t=64, k_sep=16,\n",
    "                       pool1_t=4, pool2_t=8, dropout=0.5).to(DEVICE)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "        def _acc(loader):\n",
    "            return evaluate_with_preds(model, loader)[2]\n",
    "\n",
    "        # ===== Entrenamiento con logging + EARLY STOPPING por val_acc =====\n",
    "        print(f\"\\n[Fold {fold}/{N_FOLDS}] Entrenando modelo global con validación interna por sujetos...\"\n",
    "              f\" (n_train={len(tr_sub_idx)} | n_val={len(va_idx)} | n_test={len(te_idx)})\")\n",
    "\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        best_val = -1.0\n",
    "        bad = 0\n",
    "\n",
    "        for epoch in range(1, EPOCHS_GLOBAL + 1):\n",
    "            train_epoch(model, tr_loader, opt, criterion)\n",
    "\n",
    "            if epoch % LOG_EVERY == 0:\n",
    "                tr_acc = _acc(tr_loader)\n",
    "                va_acc = _acc(va_loader)\n",
    "                print(f\"  Época {epoch:3d} | train_acc={tr_acc:.4f} | val_acc={va_acc:.4f}\")\n",
    "\n",
    "                if va_acc > best_val + 1e-4:\n",
    "                    best_val = va_acc\n",
    "                    best_state = copy.deepcopy(model.state_dict())\n",
    "                    bad = 0\n",
    "                else:\n",
    "                    bad += 1\n",
    "                    if bad >= GLOBAL_PATIENCE:\n",
    "                        print(f\"  Early stopping en época {epoch} (mejor val_acc={best_val:.4f})\")\n",
    "                        break\n",
    "\n",
    "        # cargar mejor estado antes de evaluar en test\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "        # ===== Evaluación global (inter-sujeto puro) =====\n",
    "        y_true, y_pred, acc_global = evaluate_with_preds(model, te_loader)\n",
    "        global_folds.append(acc_global)\n",
    "        all_true.append(y_true); all_pred.append(y_pred)\n",
    "\n",
    "        print(f\"[Fold {fold}/{N_FOLDS}] Global acc={acc_global:.4f}\")\n",
    "        print_report(y_true, y_pred, CLASS_NAMES_4C)\n",
    "\n",
    "        # ---------- Fine-tuning PROGRESIVO por sujeto con 4-fold CV ----------\n",
    "        X_te, y_te, g_te = X[te_idx], y[te_idx], groups[te_idx]\n",
    "\n",
    "        y_true_ft_all, y_pred_ft_all = [], []\n",
    "        used_subjects = 0\n",
    "        for sid in np.unique(g_te):\n",
    "            idx = np.where(g_te == sid)[0]\n",
    "            Xs, ys = X_te[idx], y_te[idx]\n",
    "\n",
    "            # Seguridad\n",
    "            if len(ys) < CALIB_CV_FOLDS or len(np.unique(ys)) < 2:\n",
    "                continue\n",
    "\n",
    "            y_true_subj, y_pred_subj = subject_cv_finetune_predict_progressive(\n",
    "                model, Xs, ys, DEVICE, n_splits=CALIB_CV_FOLDS, n_classes=n_classes\n",
    "            )\n",
    "            y_true_ft_all.append(y_true_subj)\n",
    "            y_pred_ft_all.append(y_pred_subj)\n",
    "            used_subjects += 1\n",
    "\n",
    "        if len(y_true_ft_all) > 0:\n",
    "            y_true_ft_all = np.concatenate(y_true_ft_all)\n",
    "            y_pred_ft_all = np.concatenate(y_pred_ft_all)\n",
    "            acc_ft = (y_true_ft_all == y_pred_ft_all).mean()\n",
    "            print(f\"  Fine-tuning PROGRESIVO (por sujeto, {CALIB_CV_FOLDS}-fold CV) acc={acc_ft:.4f} | sujetos={used_subjects}\")\n",
    "            print(f\"  Δ(FT-Global) = {acc_ft - acc_global:+.4f}\")\n",
    "        else:\n",
    "            acc_ft = np.nan\n",
    "            print(\"  Fine-tuning PROGRESIVO no ejecutado (sujeto(s) con muestras insuficientes).\")\n",
    "\n",
    "        ft_prog_folds.append(acc_ft)\n",
    "\n",
    "    # ---------- resultados finales ----------\n",
    "    if len(all_true) > 0:\n",
    "        all_true = np.concatenate(all_true)\n",
    "        all_pred = np.concatenate(all_pred)\n",
    "    else:\n",
    "        all_true = np.array([], dtype=int)\n",
    "        all_pred = np.array([], dtype=int)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTADOS FINALES\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Global folds:\", [f\"{a:.4f}\" for a in global_folds])\n",
    "    if len(global_folds) > 0:\n",
    "        print(f\"Global mean: {np.mean(global_folds):.4f}\")\n",
    "\n",
    "    print(\"Fine-tune PROGRESIVO folds:\", [(\"nan\" if (a is None or np.isnan(a)) else f\"{a:.4f}\") for a in ft_prog_folds])\n",
    "    if len(ft_prog_folds) > 0:\n",
    "        print(f\"Fine-tune PROGRESIVO mean: {np.nanmean(ft_prog_folds):.4f}\")\n",
    "        print(f\"Δ(FT-Global) mean: {np.nanmean(ft_prog_folds) - np.mean(global_folds):+.4f}\")\n",
    "\n",
    "    # Matriz de confusión global (sobre todos los folds)\n",
    "    if all_true.size > 0:\n",
    "        plot_confusion(all_true, all_pred, CLASS_NAMES_4C,\n",
    "                       title=\"Confusion Matrix - Global Model (All Folds)\",\n",
    "                       fname=\"confusion_global_allfolds.png\")\n",
    "        print(\"\\n↳ Matriz de confusión guardada: confusion_global_allfolds.png\")\n",
    "\n",
    "    return {\n",
    "        \"global_folds\": global_folds,\n",
    "        \"ft_prog_folds\": ft_prog_folds,\n",
    "        \"all_true\": all_true,\n",
    "        \"all_pred\": all_pred,\n",
    "        \"folds_json_path\": str(folds_json_path)\n",
    "    }\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🧠 INICIANDO EXPERIMENTO CON EEGNet + FINE-TUNING PROGRESIVO (por sujeto, 4-fold CV)\")\n",
    "    print(f\"🔧 Configuración: {CLASS_SCENARIO}, {len(EXPECTED_8)} canales, {WINDOW_MODE}\")\n",
    "    print(f\"⚙️  FT: epochs={FT_EPOCHS}, base_lr={FT_BASE_LR}, head_lr={FT_HEAD_LR}, L2SP={FT_L2SP}, patience={FT_PATIENCE}, CV={CALIB_CV_FOLDS}\")\n",
    "    run_experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f43e24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Usando dispositivo: cuda\n",
      "Sujetos elegibles: 103 → [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]...\n",
      "🔧 Configuración común: escenario=4c, ventana=3s, fs=160.0\n",
      "Sujetos elegibles: 103 → [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Construyendo dataset INTER (balanceado): 100%|██████████| 103/103 [00:20<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INTER] Dataset: N=8652 | T=480 | C=8 | clases=4 | sujetos=103\n",
      "\n",
      "[Fold 1/5] Entrenamiento global (val por sujetos)\n",
      "  Época   5 | train_acc=0.4470 | val_acc=0.4038\n",
      "  Época  10 | train_acc=0.4591 | val_acc=0.4038\n",
      "  Época  15 | train_acc=0.4705 | val_acc=0.4258\n",
      "  Época  20 | train_acc=0.4752 | val_acc=0.4377\n",
      "  Época  25 | train_acc=0.4841 | val_acc=0.4313\n",
      "  Época  30 | train_acc=0.4829 | val_acc=0.4267\n",
      "  Época  35 | train_acc=0.4910 | val_acc=0.4267\n",
      "  Época  40 | train_acc=0.4919 | val_acc=0.4368\n",
      "  Época  45 | train_acc=0.4986 | val_acc=0.4267\n",
      "  Época  50 | train_acc=0.4931 | val_acc=0.4295\n",
      "  Época  55 | train_acc=0.4969 | val_acc=0.4313\n",
      "  Época  60 | train_acc=0.4840 | val_acc=0.4167\n",
      "  Época  65 | train_acc=0.4902 | val_acc=0.4231\n",
      "  Época  70 | train_acc=0.5007 | val_acc=0.4332\n",
      "  Early stopping en época 70 (mejor val_acc=0.4377)\n",
      "[Fold 1] Global acc=0.4303\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.4878    0.4989    0.4933       441\n",
      "       Right     0.4319    0.5397    0.4798       441\n",
      "  Both Fists     0.3746    0.2404    0.2928       441\n",
      "   Both Feet     0.4071    0.4422    0.4239       441\n",
      "\n",
      "    accuracy                         0.4303      1764\n",
      "   macro avg     0.4254    0.4303    0.4225      1764\n",
      "weighted avg     0.4254    0.4303    0.4225      1764\n",
      "\n",
      "  FT PROGRESIVO (por sujeto, 4-fold) acc=0.4881 | sujetos=21\n",
      "  Δ(FT-Global) = +0.0578\n",
      "\n",
      "[Fold 2/5] Entrenamiento global (val por sujetos)\n",
      "  Época   5 | train_acc=0.4305 | val_acc=0.4011\n",
      "  Época  10 | train_acc=0.4350 | val_acc=0.4194\n",
      "  Época  15 | train_acc=0.4472 | val_acc=0.4139\n",
      "  Época  20 | train_acc=0.4627 | val_acc=0.4350\n",
      "  Época  25 | train_acc=0.4639 | val_acc=0.4295\n",
      "  Época  30 | train_acc=0.4667 | val_acc=0.4222\n",
      "  Época  35 | train_acc=0.4674 | val_acc=0.4277\n",
      "  Época  40 | train_acc=0.4738 | val_acc=0.4277\n",
      "  Época  45 | train_acc=0.4748 | val_acc=0.4249\n",
      "  Época  50 | train_acc=0.4712 | val_acc=0.4524\n",
      "  Época  55 | train_acc=0.4753 | val_acc=0.4386\n",
      "  Época  60 | train_acc=0.4781 | val_acc=0.4286\n",
      "  Época  65 | train_acc=0.4827 | val_acc=0.4304\n",
      "  Época  70 | train_acc=0.4790 | val_acc=0.4341\n",
      "  Época  75 | train_acc=0.4827 | val_acc=0.4441\n",
      "  Época  80 | train_acc=0.4810 | val_acc=0.4451\n",
      "  Época  85 | train_acc=0.4776 | val_acc=0.4396\n",
      "  Época  90 | train_acc=0.4836 | val_acc=0.4304\n",
      "  Época  95 | train_acc=0.4860 | val_acc=0.4432\n",
      "  Época 100 | train_acc=0.4867 | val_acc=0.4368\n",
      "  Early stopping en época 100 (mejor val_acc=0.4524)\n",
      "[Fold 2] Global acc=0.4881\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5244    0.5850    0.5531       441\n",
      "       Right     0.5455    0.5714    0.5581       441\n",
      "  Both Fists     0.4567    0.3469    0.3943       441\n",
      "   Both Feet     0.4168    0.4490    0.4323       441\n",
      "\n",
      "    accuracy                         0.4881      1764\n",
      "   macro avg     0.4859    0.4881    0.4845      1764\n",
      "weighted avg     0.4859    0.4881    0.4845      1764\n",
      "\n",
      "  FT PROGRESIVO (por sujeto, 4-fold) acc=0.5488 | sujetos=21\n",
      "  Δ(FT-Global) = +0.0607\n",
      "\n",
      "[Fold 3/5] Entrenamiento global (val por sujetos)\n",
      "  Época   5 | train_acc=0.4510 | val_acc=0.4405\n",
      "  Época  10 | train_acc=0.4739 | val_acc=0.4725\n",
      "  Época  15 | train_acc=0.4859 | val_acc=0.4725\n",
      "  Época  20 | train_acc=0.5019 | val_acc=0.4716\n",
      "  Época  25 | train_acc=0.5035 | val_acc=0.4725\n",
      "  Época  30 | train_acc=0.4959 | val_acc=0.4505\n",
      "  Época  35 | train_acc=0.5009 | val_acc=0.4698\n",
      "  Época  40 | train_acc=0.5064 | val_acc=0.4762\n",
      "  Época  45 | train_acc=0.5033 | val_acc=0.4863\n",
      "  Época  50 | train_acc=0.5109 | val_acc=0.4835\n",
      "  Época  55 | train_acc=0.5078 | val_acc=0.4890\n",
      "  Época  60 | train_acc=0.5074 | val_acc=0.4853\n",
      "  Época  65 | train_acc=0.5097 | val_acc=0.4799\n",
      "  Época  70 | train_acc=0.5150 | val_acc=0.4844\n",
      "  Época  75 | train_acc=0.5121 | val_acc=0.4689\n",
      "  Época  80 | train_acc=0.5045 | val_acc=0.4698\n",
      "  Época  85 | train_acc=0.5083 | val_acc=0.4817\n",
      "  Época  90 | train_acc=0.5059 | val_acc=0.4945\n",
      "  Época  95 | train_acc=0.5162 | val_acc=0.4689\n",
      "  Época 100 | train_acc=0.5079 | val_acc=0.4753\n",
      "[Fold 3] Global acc=0.4518\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.4866    0.5351    0.5097       441\n",
      "       Right     0.4771    0.4717    0.4743       441\n",
      "  Both Fists     0.3886    0.2925    0.3338       441\n",
      "   Both Feet     0.4384    0.5079    0.4706       441\n",
      "\n",
      "    accuracy                         0.4518      1764\n",
      "   macro avg     0.4476    0.4518    0.4471      1764\n",
      "weighted avg     0.4476    0.4518    0.4471      1764\n",
      "\n",
      "  FT PROGRESIVO (por sujeto, 4-fold) acc=0.5300 | sujetos=21\n",
      "  Δ(FT-Global) = +0.0782\n",
      "\n",
      "[Fold 4/5] Entrenamiento global (val por sujetos)\n",
      "  Época   5 | train_acc=0.4454 | val_acc=0.4176\n",
      "  Época  10 | train_acc=0.4594 | val_acc=0.4304\n",
      "  Época  15 | train_acc=0.4682 | val_acc=0.4441\n",
      "  Época  20 | train_acc=0.4738 | val_acc=0.4341\n",
      "  Época  25 | train_acc=0.4769 | val_acc=0.4432\n",
      "  Época  30 | train_acc=0.4842 | val_acc=0.4487\n",
      "  Época  35 | train_acc=0.4808 | val_acc=0.4405\n",
      "  Época  40 | train_acc=0.4893 | val_acc=0.4322\n",
      "  Época  45 | train_acc=0.4847 | val_acc=0.4386\n",
      "  Época  50 | train_acc=0.4844 | val_acc=0.4304\n",
      "  Época  55 | train_acc=0.4878 | val_acc=0.4368\n",
      "  Época  60 | train_acc=0.4862 | val_acc=0.4441\n",
      "  Época  65 | train_acc=0.4883 | val_acc=0.4451\n",
      "  Época  70 | train_acc=0.4891 | val_acc=0.4451\n",
      "  Época  75 | train_acc=0.4944 | val_acc=0.4551\n",
      "  Época  80 | train_acc=0.4939 | val_acc=0.4597\n",
      "  Época  85 | train_acc=0.4991 | val_acc=0.4487\n",
      "  Época  90 | train_acc=0.5036 | val_acc=0.4524\n",
      "  Época  95 | train_acc=0.4993 | val_acc=0.4496\n",
      "  Época 100 | train_acc=0.4980 | val_acc=0.4524\n",
      "[Fold 4] Global acc=0.4637\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5065    0.5595    0.5317       420\n",
      "       Right     0.5394    0.4238    0.4747       420\n",
      "  Both Fists     0.3951    0.4571    0.4238       420\n",
      "   Both Feet     0.4350    0.4143    0.4244       420\n",
      "\n",
      "    accuracy                         0.4637      1680\n",
      "   macro avg     0.4690    0.4637    0.4636      1680\n",
      "weighted avg     0.4690    0.4637    0.4636      1680\n",
      "\n",
      "  FT PROGRESIVO (por sujeto, 4-fold) acc=0.5274 | sujetos=20\n",
      "  Δ(FT-Global) = +0.0637\n",
      "\n",
      "[Fold 5/5] Entrenamiento global (val por sujetos)\n",
      "  Época   5 | train_acc=0.4379 | val_acc=0.4011\n",
      "  Época  10 | train_acc=0.4563 | val_acc=0.4332\n",
      "  Época  15 | train_acc=0.4682 | val_acc=0.4341\n",
      "  Época  20 | train_acc=0.4733 | val_acc=0.4432\n",
      "  Época  25 | train_acc=0.4770 | val_acc=0.4405\n",
      "  Época  30 | train_acc=0.4745 | val_acc=0.4570\n",
      "  Época  35 | train_acc=0.4782 | val_acc=0.4487\n",
      "  Época  40 | train_acc=0.4849 | val_acc=0.4615\n",
      "  Época  45 | train_acc=0.4823 | val_acc=0.4679\n",
      "  Época  50 | train_acc=0.4840 | val_acc=0.4734\n",
      "  Época  55 | train_acc=0.4852 | val_acc=0.4597\n",
      "  Época  60 | train_acc=0.4840 | val_acc=0.4670\n",
      "  Época  65 | train_acc=0.4847 | val_acc=0.4579\n",
      "  Época  70 | train_acc=0.4820 | val_acc=0.4606\n",
      "  Época  75 | train_acc=0.4896 | val_acc=0.4670\n",
      "  Época  80 | train_acc=0.4845 | val_acc=0.4734\n",
      "  Época  85 | train_acc=0.4913 | val_acc=0.4679\n",
      "  Época  90 | train_acc=0.4891 | val_acc=0.4570\n",
      "  Época  95 | train_acc=0.4896 | val_acc=0.4515\n",
      "  Época 100 | train_acc=0.4891 | val_acc=0.4606\n",
      "  Early stopping en época 100 (mejor val_acc=0.4734)\n",
      "[Fold 5] Global acc=0.4720\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5125    0.6357    0.5675       420\n",
      "       Right     0.4865    0.5595    0.5205       420\n",
      "  Both Fists     0.3852    0.2476    0.3014       420\n",
      "   Both Feet     0.4606    0.4452    0.4528       420\n",
      "\n",
      "    accuracy                         0.4720      1680\n",
      "   macro avg     0.4612    0.4720    0.4606      1680\n",
      "weighted avg     0.4612    0.4720    0.4606      1680\n",
      "\n",
      "  FT PROGRESIVO (por sujeto, 4-fold) acc=0.5315 | sujetos=20\n",
      "  Δ(FT-Global) = +0.0595\n",
      "\n",
      "============================================================\n",
      "RESULTADOS INTER\n",
      "============================================================\n",
      "Global folds: ['0.4303', '0.4881', '0.4518', '0.4637', '0.4720']\n",
      "Global mean: 0.4612\n",
      "Fine-tune PROGRESIVO folds: ['0.4881', '0.5488', '0.5300', '0.5274', '0.5315']\n",
      "Fine-tune PROGRESIVO mean: 0.5252\n",
      "Δ(FT-Global) mean: +0.0640\n",
      "↳ Matriz de confusión guardada: confusion_inter_allfolds.png\n",
      "[INTRA] Dataset: N=9270 | T=480 | C=8 | clases=4 | sujetos=103\n",
      "🔹 Entrenando modelo GLOBAL (pretrain sobre todos los sujetos mezclados)...\n",
      "[GLOBAL PRETRAIN] Época   5/100 | train_acc=0.4498\n",
      "[GLOBAL PRETRAIN] Época  10/100 | train_acc=0.4659\n",
      "[GLOBAL PRETRAIN] Época  15/100 | train_acc=0.4736\n",
      "[GLOBAL PRETRAIN] Época  20/100 | train_acc=0.4752\n",
      "[GLOBAL PRETRAIN] Época  25/100 | train_acc=0.4827\n",
      "[GLOBAL PRETRAIN] Época  30/100 | train_acc=0.4844\n",
      "[GLOBAL PRETRAIN] Época  35/100 | train_acc=0.4907\n",
      "[GLOBAL PRETRAIN] Época  40/100 | train_acc=0.4893\n",
      "[GLOBAL PRETRAIN] Época  45/100 | train_acc=0.4962\n",
      "[GLOBAL PRETRAIN] Época  50/100 | train_acc=0.5003\n",
      "[GLOBAL PRETRAIN] Época  55/100 | train_acc=0.4970\n",
      "[GLOBAL PRETRAIN] Época  60/100 | train_acc=0.4948\n",
      "[GLOBAL PRETRAIN] Época  65/100 | train_acc=0.4987\n",
      "[GLOBAL PRETRAIN] Época  70/100 | train_acc=0.5020\n",
      "[GLOBAL PRETRAIN] Época  75/100 | train_acc=0.5046\n",
      "[GLOBAL PRETRAIN] Época  80/100 | train_acc=0.5031\n",
      "[GLOBAL PRETRAIN] Época  85/100 | train_acc=0.5055\n",
      "[GLOBAL PRETRAIN] Época  90/100 | train_acc=0.5018\n",
      "[GLOBAL PRETRAIN] Época  95/100 | train_acc=0.5018\n",
      "[GLOBAL PRETRAIN] Época 100/100 | train_acc=0.5082\n",
      "\n",
      "== Sujeto S001 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.5000 | f1m=0.4833\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.5556 | f1m=0.4864\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.6667 | f1m=0.6318\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.7222 | f1m=0.7151\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.5333) | TEST acc=0.5556 | f1m=0.5770\n",
      "  ⇒ Sujeto S001 | ACC=0.600 | F1m=0.579\n",
      "\n",
      "== Sujeto S002 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.5556 | f1m=0.5333\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.9333) | TEST acc=0.5556 | f1m=0.4861\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.7333) | TEST acc=0.6667 | f1m=0.6167\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.5000 | f1m=0.4587\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.5556 | f1m=0.5458\n",
      "  ⇒ Sujeto S002 | ACC=0.567 | F1m=0.528\n",
      "\n",
      "== Sujeto S003 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8000) | TEST acc=0.5000 | f1m=0.4372\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.4444 | f1m=0.4250\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8000) | TEST acc=0.5556 | f1m=0.5636\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.5556 | f1m=0.5375\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.5333) | TEST acc=0.6667 | f1m=0.6234\n",
      "  ⇒ Sujeto S003 | ACC=0.544 | F1m=0.517\n",
      "\n",
      "== Sujeto S004 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.8000) | TEST acc=0.7778 | f1m=0.7819\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.7222 | f1m=0.7208\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.6667 | f1m=0.6477\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.8667) | TEST acc=0.5556 | f1m=0.5506\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.8000) | TEST acc=0.6667 | f1m=0.6421\n",
      "  ⇒ Sujeto S004 | ACC=0.678 | F1m=0.669\n",
      "\n",
      "== Sujeto S005 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.5333) | TEST acc=0.3333 | f1m=0.3131\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.3333) | TEST acc=0.3889 | f1m=0.3583\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.3333) | TEST acc=0.5000 | f1m=0.5242\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.8000) | TEST acc=0.5556 | f1m=0.5631\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.3889 | f1m=0.3500\n",
      "  ⇒ Sujeto S005 | ACC=0.433 | F1m=0.422\n",
      "\n",
      "== Sujeto S006 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6667) | TEST acc=0.7222 | f1m=0.7125\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.5000 | f1m=0.4262\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.5556 | f1m=0.4705\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.7333) | TEST acc=0.6111 | f1m=0.5923\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.6667 | f1m=0.6374\n",
      "  ⇒ Sujeto S006 | ACC=0.611 | F1m=0.568\n",
      "\n",
      "== Sujeto S007 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8667) | TEST acc=0.8333 | f1m=0.8319\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.8000) | TEST acc=0.8333 | f1m=0.8415\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8000) | TEST acc=0.7778 | f1m=0.7651\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.9333) | TEST acc=0.8333 | f1m=0.8245\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.9333) | TEST acc=0.8889 | f1m=0.8870\n",
      "  ⇒ Sujeto S007 | ACC=0.833 | F1m=0.830\n",
      "\n",
      "== Sujeto S008 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.5000 | f1m=0.5095\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4667) | TEST acc=0.3889 | f1m=0.3290\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.5556 | f1m=0.5524\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.6111 | f1m=0.6012\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.5000 | f1m=0.4821\n",
      "  ⇒ Sujeto S008 | ACC=0.511 | F1m=0.495\n",
      "\n",
      "== Sujeto S009 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.5333) | TEST acc=0.5000 | f1m=0.5129\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.5000 | f1m=0.4493\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6000) | TEST acc=0.3889 | f1m=0.3447\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.6667 | f1m=0.6753\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.6111 | f1m=0.6062\n",
      "  ⇒ Sujeto S009 | ACC=0.533 | F1m=0.518\n",
      "\n",
      "== Sujeto S010 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.3333) | TEST acc=0.5556 | f1m=0.5625\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4667) | TEST acc=0.2778 | f1m=0.2708\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6667) | TEST acc=0.6111 | f1m=0.5864\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4000) | TEST acc=0.6111 | f1m=0.5694\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.5000 | f1m=0.4836\n",
      "  ⇒ Sujeto S010 | ACC=0.511 | F1m=0.495\n",
      "\n",
      "== Sujeto S011 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4000) | TEST acc=0.5000 | f1m=0.4701\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4667) | TEST acc=0.4444 | f1m=0.4071\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4667) | TEST acc=0.3333 | f1m=0.3167\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.3889 | f1m=0.3917\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6000) | TEST acc=0.6111 | f1m=0.5907\n",
      "  ⇒ Sujeto S011 | ACC=0.456 | F1m=0.435\n",
      "\n",
      "== Sujeto S012 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6000) | TEST acc=0.7222 | f1m=0.7125\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.4444 | f1m=0.4162\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.6667 | f1m=0.5590\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.5000 | f1m=0.5088\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.5333) | TEST acc=0.5000 | f1m=0.5042\n",
      "  ⇒ Sujeto S012 | ACC=0.567 | F1m=0.540\n",
      "\n",
      "== Sujeto S013 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.2778 | f1m=0.2330\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4667) | TEST acc=0.4444 | f1m=0.4328\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4667) | TEST acc=0.3889 | f1m=0.3770\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4000) | TEST acc=0.3333 | f1m=0.2517\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4667) | TEST acc=0.3333 | f1m=0.3169\n",
      "  ⇒ Sujeto S013 | ACC=0.356 | F1m=0.322\n",
      "\n",
      "== Sujeto S014 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6000) | TEST acc=0.2222 | f1m=0.2083\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4667) | TEST acc=0.3889 | f1m=0.3958\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.3333 | f1m=0.2768\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6667) | TEST acc=0.2778 | f1m=0.2465\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4667) | TEST acc=0.5000 | f1m=0.4873\n",
      "  ⇒ Sujeto S014 | ACC=0.344 | F1m=0.323\n",
      "\n",
      "== Sujeto S015 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.8667) | TEST acc=0.8333 | f1m=0.8485\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8667) | TEST acc=0.7222 | f1m=0.7235\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.6667 | f1m=0.6419\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.7333) | TEST acc=0.7778 | f1m=0.7313\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.8667) | TEST acc=0.7222 | f1m=0.6883\n",
      "  ⇒ Sujeto S015 | ACC=0.744 | F1m=0.727\n",
      "\n",
      "== Sujeto S016 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.3889 | f1m=0.2917\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4667) | TEST acc=0.5556 | f1m=0.5331\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.3889 | f1m=0.3075\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.4667) | TEST acc=0.6111 | f1m=0.6381\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.5333) | TEST acc=0.3333 | f1m=0.3353\n",
      "  ⇒ Sujeto S016 | ACC=0.456 | F1m=0.421\n",
      "\n",
      "== Sujeto S017 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.4444 | f1m=0.4520\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.4444 | f1m=0.4086\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8000) | TEST acc=0.4444 | f1m=0.4520\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.6667 | f1m=0.6577\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.4444 | f1m=0.3452\n",
      "  ⇒ Sujeto S017 | ACC=0.489 | F1m=0.463\n",
      "\n",
      "== Sujeto S018 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.8889 | f1m=0.8831\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.6111 | f1m=0.6097\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.8667) | TEST acc=0.6667 | f1m=0.6374\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.7222 | f1m=0.6916\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8000) | TEST acc=0.7778 | f1m=0.7651\n",
      "  ⇒ Sujeto S018 | ACC=0.733 | F1m=0.717\n",
      "\n",
      "== Sujeto S019 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4000) | TEST acc=0.5556 | f1m=0.5348\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.6667 | f1m=0.6708\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4667) | TEST acc=0.6667 | f1m=0.6497\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6667) | TEST acc=0.3333 | f1m=0.3373\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8000) | TEST acc=0.4444 | f1m=0.4328\n",
      "  ⇒ Sujeto S019 | ACC=0.533 | F1m=0.525\n",
      "\n",
      "== Sujeto S020 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.3889 | f1m=0.3793\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.6667 | f1m=0.6524\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.7222 | f1m=0.7208\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.6667 | f1m=0.6652\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.7222 | f1m=0.7131\n",
      "  ⇒ Sujeto S020 | ACC=0.633 | F1m=0.626\n",
      "\n",
      "== Sujeto S021 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.3889 | f1m=0.3748\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.5000 | f1m=0.4444\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.5556 | f1m=0.5530\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.6667 | f1m=0.6750\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.4444 | f1m=0.3732\n",
      "  ⇒ Sujeto S021 | ACC=0.511 | F1m=0.484\n",
      "\n",
      "== Sujeto S022 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8667) | TEST acc=0.7222 | f1m=0.7167\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8667) | TEST acc=0.7778 | f1m=0.7976\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.8889 | f1m=0.8712\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.8667) | TEST acc=0.7222 | f1m=0.6777\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.9333) | TEST acc=0.7778 | f1m=0.7717\n",
      "  ⇒ Sujeto S022 | ACC=0.778 | F1m=0.767\n",
      "\n",
      "== Sujeto S023 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.5333) | TEST acc=0.5000 | f1m=0.4280\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6000) | TEST acc=0.4444 | f1m=0.3868\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.7333) | TEST acc=0.3889 | f1m=0.3909\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6667) | TEST acc=0.3889 | f1m=0.3700\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.5000 | f1m=0.5014\n",
      "  ⇒ Sujeto S023 | ACC=0.444 | F1m=0.415\n",
      "\n",
      "== Sujeto S024 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.8000) | TEST acc=0.5000 | f1m=0.4921\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.6111 | f1m=0.6153\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.4444 | f1m=0.3904\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.3889 | f1m=0.3847\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6667) | TEST acc=0.5000 | f1m=0.4917\n",
      "  ⇒ Sujeto S024 | ACC=0.489 | F1m=0.475\n",
      "\n",
      "== Sujeto S025 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4000) | TEST acc=0.3333 | f1m=0.3098\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4667) | TEST acc=0.3889 | f1m=0.3381\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.3889 | f1m=0.3872\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4667) | TEST acc=0.3889 | f1m=0.3750\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4667) | TEST acc=0.3889 | f1m=0.3731\n",
      "  ⇒ Sujeto S025 | ACC=0.378 | F1m=0.357\n",
      "\n",
      "== Sujeto S026 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.5000 | f1m=0.4657\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4667) | TEST acc=0.5000 | f1m=0.4802\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.5333) | TEST acc=0.3333 | f1m=0.3194\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.5333) | TEST acc=0.3333 | f1m=0.3125\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.4444 | f1m=0.4078\n",
      "  ⇒ Sujeto S026 | ACC=0.422 | F1m=0.397\n",
      "\n",
      "== Sujeto S027 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.6111 | f1m=0.5958\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.5000 | f1m=0.4610\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6000) | TEST acc=0.3333 | f1m=0.3333\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.8667) | TEST acc=0.4444 | f1m=0.3792\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.3889 | f1m=0.3489\n",
      "  ⇒ Sujeto S027 | ACC=0.456 | F1m=0.424\n",
      "\n",
      "== Sujeto S028 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.5333) | TEST acc=0.5000 | f1m=0.4318\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.7333) | TEST acc=0.5556 | f1m=0.5500\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.5556 | f1m=0.5528\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.4444 | f1m=0.4249\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.3889 | f1m=0.3489\n",
      "  ⇒ Sujeto S028 | ACC=0.489 | F1m=0.462\n",
      "\n",
      "== Sujeto S029 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6000) | TEST acc=0.3889 | f1m=0.3535\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8667) | TEST acc=0.7778 | f1m=0.7576\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8667) | TEST acc=0.6667 | f1m=0.6407\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6667) | TEST acc=0.5000 | f1m=0.5042\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.5000 | f1m=0.5172\n",
      "  ⇒ Sujeto S029 | ACC=0.567 | F1m=0.555\n",
      "\n",
      "== Sujeto S030 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.6111 | f1m=0.6126\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6667) | TEST acc=0.6667 | f1m=0.5840\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.5556 | f1m=0.5595\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.8000) | TEST acc=0.3889 | f1m=0.3703\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.7333) | TEST acc=0.6111 | f1m=0.6083\n",
      "  ⇒ Sujeto S030 | ACC=0.567 | F1m=0.547\n",
      "\n",
      "== Sujeto S031 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4000) | TEST acc=0.3333 | f1m=0.2898\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.5556 | f1m=0.5458\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.5000 | f1m=0.4831\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.4667) | TEST acc=0.5556 | f1m=0.5467\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.3889 | f1m=0.3847\n",
      "  ⇒ Sujeto S031 | ACC=0.467 | F1m=0.450\n",
      "\n",
      "== Sujeto S032 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.7222 | f1m=0.6937\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.8667) | TEST acc=0.7778 | f1m=0.7708\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.8667) | TEST acc=0.6667 | f1m=0.6548\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.9333) | TEST acc=0.5000 | f1m=0.4394\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8000) | TEST acc=0.7778 | f1m=0.7747\n",
      "  ⇒ Sujeto S032 | ACC=0.689 | F1m=0.667\n",
      "\n",
      "== Sujeto S033 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8000) | TEST acc=0.7778 | f1m=0.7662\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.5556 | f1m=0.5241\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.7333) | TEST acc=0.7222 | f1m=0.6859\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.6111 | f1m=0.6040\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8000) | TEST acc=0.6111 | f1m=0.6141\n",
      "  ⇒ Sujeto S033 | ACC=0.656 | F1m=0.639\n",
      "\n",
      "== Sujeto S034 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4667) | TEST acc=0.7222 | f1m=0.6828\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.5333) | TEST acc=0.5556 | f1m=0.5270\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.5556 | f1m=0.4967\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.4444 | f1m=0.4068\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.5000 | f1m=0.4864\n",
      "  ⇒ Sujeto S034 | ACC=0.556 | F1m=0.520\n",
      "\n",
      "== Sujeto S035 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8667) | TEST acc=0.8333 | f1m=0.8319\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.7333) | TEST acc=0.7222 | f1m=0.7282\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.9333) | TEST acc=0.8333 | f1m=0.8431\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=1.0000) | TEST acc=0.7778 | f1m=0.7231\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.8667) | TEST acc=0.6667 | f1m=0.5675\n",
      "  ⇒ Sujeto S035 | ACC=0.767 | F1m=0.739\n",
      "\n",
      "== Sujeto S036 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6667) | TEST acc=0.8889 | f1m=0.8944\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.3889 | f1m=0.3830\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4667) | TEST acc=0.7222 | f1m=0.6792\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.5000 | f1m=0.5000\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.5333) | TEST acc=0.5556 | f1m=0.4848\n",
      "  ⇒ Sujeto S036 | ACC=0.611 | F1m=0.588\n",
      "\n",
      "== Sujeto S037 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.5000 | f1m=0.4893\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4667) | TEST acc=0.4444 | f1m=0.4607\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6000) | TEST acc=0.2778 | f1m=0.2409\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6667) | TEST acc=0.5000 | f1m=0.5014\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6667) | TEST acc=0.4444 | f1m=0.4401\n",
      "  ⇒ Sujeto S037 | ACC=0.433 | F1m=0.426\n",
      "\n",
      "== Sujeto S039 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4667) | TEST acc=0.3333 | f1m=0.3205\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4000) | TEST acc=0.2778 | f1m=0.2520\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4000) | TEST acc=0.5556 | f1m=0.4833\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.2667) | TEST acc=0.4444 | f1m=0.4698\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.3333) | TEST acc=0.1667 | f1m=0.1667\n",
      "  ⇒ Sujeto S039 | ACC=0.356 | F1m=0.338\n",
      "\n",
      "== Sujeto S040 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.3889 | f1m=0.4208\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.3889 | f1m=0.3030\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.5333) | TEST acc=0.7778 | f1m=0.7368\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4667) | TEST acc=0.5000 | f1m=0.4610\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.5556 | f1m=0.5038\n",
      "  ⇒ Sujeto S040 | ACC=0.522 | F1m=0.485\n",
      "\n",
      "== Sujeto S041 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.3889 | f1m=0.4232\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.5333) | TEST acc=0.5556 | f1m=0.5635\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.5556 | f1m=0.5631\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.4444 | f1m=0.4623\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.9333) | TEST acc=0.3333 | f1m=0.2667\n",
      "  ⇒ Sujeto S041 | ACC=0.456 | F1m=0.456\n",
      "\n",
      "== Sujeto S042 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.5556 | f1m=0.5500\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.5556 | f1m=0.5469\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.7333) | TEST acc=0.5000 | f1m=0.5028\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4667) | TEST acc=0.6667 | f1m=0.6796\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.3333 | f1m=0.3270\n",
      "  ⇒ Sujeto S042 | ACC=0.522 | F1m=0.521\n",
      "\n",
      "== Sujeto S043 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4667) | TEST acc=0.7778 | f1m=0.7310\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.7333) | TEST acc=0.3889 | f1m=0.3328\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.8000) | TEST acc=0.5556 | f1m=0.5411\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.5556 | f1m=0.5530\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.9333) | TEST acc=0.5556 | f1m=0.5325\n",
      "  ⇒ Sujeto S043 | ACC=0.567 | F1m=0.538\n",
      "\n",
      "== Sujeto S044 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.6111 | f1m=0.5423\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.3889 | f1m=0.3701\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6667) | TEST acc=0.3889 | f1m=0.3750\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4000) | TEST acc=0.6667 | f1m=0.6375\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.7778 | f1m=0.7313\n",
      "  ⇒ Sujeto S044 | ACC=0.567 | F1m=0.531\n",
      "\n",
      "== Sujeto S045 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.6111 | f1m=0.6111\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.6667 | f1m=0.6708\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.6111 | f1m=0.6012\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.6111 | f1m=0.6024\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.5000 | f1m=0.5112\n",
      "  ⇒ Sujeto S045 | ACC=0.600 | F1m=0.599\n",
      "\n",
      "== Sujeto S046 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.5556 | f1m=0.5500\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.4667) | TEST acc=0.3333 | f1m=0.3125\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6000) | TEST acc=0.2222 | f1m=0.2020\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.6111 | f1m=0.6256\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.6667 | f1m=0.6298\n",
      "  ⇒ Sujeto S046 | ACC=0.478 | F1m=0.464\n",
      "\n",
      "== Sujeto S047 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.5333) | TEST acc=0.3889 | f1m=0.3731\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.4667) | TEST acc=0.5556 | f1m=0.5524\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.5556 | f1m=0.5330\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.3333 | f1m=0.2444\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4667) | TEST acc=0.3333 | f1m=0.3076\n",
      "  ⇒ Sujeto S047 | ACC=0.433 | F1m=0.402\n",
      "\n",
      "== Sujeto S048 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.8000) | TEST acc=0.6667 | f1m=0.6711\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.6111 | f1m=0.6125\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.8000) | TEST acc=0.6667 | f1m=0.6764\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.7333) | TEST acc=0.4444 | f1m=0.3838\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.7222 | f1m=0.7207\n",
      "  ⇒ Sujeto S048 | ACC=0.622 | F1m=0.613\n",
      "\n",
      "== Sujeto S049 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.8000) | TEST acc=0.2778 | f1m=0.2361\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.2000) | TEST acc=0.5000 | f1m=0.5040\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.3333 | f1m=0.3364\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.3889 | f1m=0.3198\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6667) | TEST acc=0.2222 | f1m=0.2236\n",
      "  ⇒ Sujeto S049 | ACC=0.344 | F1m=0.324\n",
      "\n",
      "== Sujeto S050 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.3333 | f1m=0.3006\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.5000 | f1m=0.4872\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.5000 | f1m=0.4280\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.4444 | f1m=0.3821\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.7333) | TEST acc=0.3333 | f1m=0.3292\n",
      "  ⇒ Sujeto S050 | ACC=0.422 | F1m=0.385\n",
      "\n",
      "== Sujeto S051 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.9333) | TEST acc=0.5556 | f1m=0.5529\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.5556 | f1m=0.5282\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.8667) | TEST acc=0.6111 | f1m=0.6225\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.7333) | TEST acc=0.8333 | f1m=0.8162\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.8000) | TEST acc=0.4444 | f1m=0.4108\n",
      "  ⇒ Sujeto S051 | ACC=0.600 | F1m=0.586\n",
      "\n",
      "== Sujeto S052 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.7333) | TEST acc=0.5556 | f1m=0.5360\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.7333) | TEST acc=0.4444 | f1m=0.3988\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.5000 | f1m=0.4861\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.3889 | f1m=0.3839\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4667) | TEST acc=0.4444 | f1m=0.4030\n",
      "  ⇒ Sujeto S052 | ACC=0.467 | F1m=0.442\n",
      "\n",
      "== Sujeto S053 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.5556 | f1m=0.4857\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.7778 | f1m=0.7833\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.7333) | TEST acc=0.6111 | f1m=0.6151\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.5333) | TEST acc=0.5000 | f1m=0.4345\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.8000) | TEST acc=0.3889 | f1m=0.3611\n",
      "  ⇒ Sujeto S053 | ACC=0.567 | F1m=0.536\n",
      "\n",
      "== Sujeto S054 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.7333) | TEST acc=0.6667 | f1m=0.6583\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.6111 | f1m=0.5890\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.4444 | f1m=0.4590\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.7778 | f1m=0.7764\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.3889 | f1m=0.3715\n",
      "  ⇒ Sujeto S054 | ACC=0.578 | F1m=0.571\n",
      "\n",
      "== Sujeto S055 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.7222 | f1m=0.6833\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.8000) | TEST acc=0.7778 | f1m=0.7850\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.5556 | f1m=0.4643\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8667) | TEST acc=0.4444 | f1m=0.4128\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.9333) | TEST acc=0.6111 | f1m=0.6167\n",
      "  ⇒ Sujeto S055 | ACC=0.622 | F1m=0.592\n",
      "\n",
      "== Sujeto S056 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.5333) | TEST acc=0.7778 | f1m=0.7609\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.4444 | f1m=0.4409\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.5556 | f1m=0.5472\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.5556 | f1m=0.5310\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.4444 | f1m=0.4528\n",
      "  ⇒ Sujeto S056 | ACC=0.556 | F1m=0.547\n",
      "\n",
      "== Sujeto S057 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.8333 | f1m=0.8146\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.7222 | f1m=0.7161\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.6667 | f1m=0.5707\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.6111 | f1m=0.5951\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8000) | TEST acc=0.7778 | f1m=0.7273\n",
      "  ⇒ Sujeto S057 | ACC=0.722 | F1m=0.685\n",
      "\n",
      "== Sujeto S058 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.5000 | f1m=0.5130\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.7333) | TEST acc=0.5000 | f1m=0.4821\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.7333) | TEST acc=0.6111 | f1m=0.5800\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6000) | TEST acc=0.4444 | f1m=0.4677\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.5556 | f1m=0.5595\n",
      "  ⇒ Sujeto S058 | ACC=0.522 | F1m=0.520\n",
      "\n",
      "== Sujeto S059 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.3333 | f1m=0.3292\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.5333) | TEST acc=0.2222 | f1m=0.2621\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4000) | TEST acc=0.1667 | f1m=0.1806\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4000) | TEST acc=0.5000 | f1m=0.5028\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.4000) | TEST acc=0.5556 | f1m=0.5595\n",
      "  ⇒ Sujeto S059 | ACC=0.356 | F1m=0.367\n",
      "\n",
      "== Sujeto S060 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.5556 | f1m=0.5527\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.7778 | f1m=0.7708\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.5000 | f1m=0.4464\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.8000) | TEST acc=0.5000 | f1m=0.5187\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.6111 | f1m=0.6083\n",
      "  ⇒ Sujeto S060 | ACC=0.589 | F1m=0.579\n",
      "\n",
      "== Sujeto S061 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.6111 | f1m=0.6083\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6000) | TEST acc=0.6667 | f1m=0.6578\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8000) | TEST acc=0.6667 | f1m=0.6548\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8000) | TEST acc=0.6111 | f1m=0.5997\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8000) | TEST acc=0.6111 | f1m=0.5737\n",
      "  ⇒ Sujeto S061 | ACC=0.633 | F1m=0.619\n",
      "\n",
      "== Sujeto S062 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8000) | TEST acc=0.6667 | f1m=0.6298\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.7333) | TEST acc=0.5556 | f1m=0.5570\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.8333 | f1m=0.8234\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.7333) | TEST acc=0.7778 | f1m=0.7389\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.6667 | f1m=0.6583\n",
      "  ⇒ Sujeto S062 | ACC=0.700 | F1m=0.681\n",
      "\n",
      "== Sujeto S063 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.7333) | TEST acc=0.5556 | f1m=0.4952\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.3333) | TEST acc=0.3889 | f1m=0.3075\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.5333) | TEST acc=0.5556 | f1m=0.4750\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.7333) | TEST acc=0.5000 | f1m=0.4958\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6000) | TEST acc=0.4444 | f1m=0.4208\n",
      "  ⇒ Sujeto S063 | ACC=0.489 | F1m=0.439\n",
      "\n",
      "== Sujeto S064 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4667) | TEST acc=0.6111 | f1m=0.6190\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.4667) | TEST acc=0.6667 | f1m=0.6298\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.5556 | f1m=0.5530\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.8667) | TEST acc=0.4444 | f1m=0.4196\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.3333 | f1m=0.2987\n",
      "  ⇒ Sujeto S064 | ACC=0.522 | F1m=0.504\n",
      "\n",
      "== Sujeto S065 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.5000 | f1m=0.4586\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4667) | TEST acc=0.5556 | f1m=0.4735\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6667) | TEST acc=0.5556 | f1m=0.5265\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4000) | TEST acc=0.2778 | f1m=0.2544\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.5556 | f1m=0.4596\n",
      "  ⇒ Sujeto S065 | ACC=0.489 | F1m=0.435\n",
      "\n",
      "== Sujeto S066 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.7333) | TEST acc=0.4444 | f1m=0.4068\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.3889 | f1m=0.3895\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.4667) | TEST acc=0.5000 | f1m=0.4612\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.7333) | TEST acc=0.6667 | f1m=0.6580\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.6111 | f1m=0.6249\n",
      "  ⇒ Sujeto S066 | ACC=0.522 | F1m=0.508\n",
      "\n",
      "== Sujeto S067 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4000) | TEST acc=0.3889 | f1m=0.3873\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.4667) | TEST acc=0.2222 | f1m=0.2288\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4667) | TEST acc=0.3333 | f1m=0.3167\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4000) | TEST acc=0.5556 | f1m=0.5489\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6667) | TEST acc=0.5000 | f1m=0.4975\n",
      "  ⇒ Sujeto S067 | ACC=0.400 | F1m=0.396\n",
      "\n",
      "== Sujeto S068 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.2778 | f1m=0.2667\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.3333 | f1m=0.3292\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4667) | TEST acc=0.3333 | f1m=0.2905\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6000) | TEST acc=0.7222 | f1m=0.7250\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.5333) | TEST acc=0.4444 | f1m=0.4563\n",
      "  ⇒ Sujeto S068 | ACC=0.422 | F1m=0.414\n",
      "\n",
      "== Sujeto S069 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6667) | TEST acc=0.3889 | f1m=0.3582\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.7222 | f1m=0.7139\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.7333) | TEST acc=0.5000 | f1m=0.4703\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.3333 | f1m=0.3347\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.5333) | TEST acc=0.3333 | f1m=0.3457\n",
      "  ⇒ Sujeto S069 | ACC=0.456 | F1m=0.445\n",
      "\n",
      "== Sujeto S070 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=1.0000) | TEST acc=0.5556 | f1m=0.4575\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.8667) | TEST acc=0.7778 | f1m=0.7860\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.7222 | f1m=0.7103\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.8000) | TEST acc=0.7222 | f1m=0.6806\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.8000) | TEST acc=0.7222 | f1m=0.7211\n",
      "  ⇒ Sujeto S070 | ACC=0.700 | F1m=0.671\n",
      "\n",
      "== Sujeto S071 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.7333) | TEST acc=0.5556 | f1m=0.5595\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.8000) | TEST acc=0.7778 | f1m=0.7810\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.9333) | TEST acc=0.7778 | f1m=0.7666\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.5000 | f1m=0.4722\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.5000 | f1m=0.5097\n",
      "  ⇒ Sujeto S071 | ACC=0.622 | F1m=0.618\n",
      "\n",
      "== Sujeto S072 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.7333) | TEST acc=0.6667 | f1m=0.6538\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.5556 | f1m=0.5375\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8667) | TEST acc=0.8333 | f1m=0.8245\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8667) | TEST acc=0.8333 | f1m=0.8336\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.7333) | TEST acc=0.7222 | f1m=0.7264\n",
      "  ⇒ Sujeto S072 | ACC=0.722 | F1m=0.715\n",
      "\n",
      "== Sujeto S073 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.5000 | f1m=0.4673\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.6667 | f1m=0.6750\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.3889 | f1m=0.3409\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.5000 | f1m=0.4742\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.2222 | f1m=0.2292\n",
      "  ⇒ Sujeto S073 | ACC=0.456 | F1m=0.437\n",
      "\n",
      "== Sujeto S074 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.5333) | TEST acc=0.2778 | f1m=0.2748\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.2778 | f1m=0.2424\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.5333) | TEST acc=0.3889 | f1m=0.3846\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4667) | TEST acc=0.6111 | f1m=0.5985\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.4667) | TEST acc=0.3889 | f1m=0.3262\n",
      "  ⇒ Sujeto S074 | ACC=0.389 | F1m=0.365\n",
      "\n",
      "== Sujeto S075 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6667) | TEST acc=0.4444 | f1m=0.4625\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4667) | TEST acc=0.4444 | f1m=0.4203\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4667) | TEST acc=0.5556 | f1m=0.5595\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.5000 | f1m=0.4586\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.7333) | TEST acc=0.3889 | f1m=0.3500\n",
      "  ⇒ Sujeto S075 | ACC=0.467 | F1m=0.450\n",
      "\n",
      "== Sujeto S076 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.5333) | TEST acc=0.4444 | f1m=0.4583\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.4667) | TEST acc=0.5556 | f1m=0.4452\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.2222 | f1m=0.1964\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.4667) | TEST acc=0.3889 | f1m=0.3808\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.2222 | f1m=0.1913\n",
      "  ⇒ Sujeto S076 | ACC=0.367 | F1m=0.334\n",
      "\n",
      "== Sujeto S077 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.2222 | f1m=0.2484\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.4667) | TEST acc=0.1111 | f1m=0.1111\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6667) | TEST acc=0.3333 | f1m=0.2818\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.3333) | TEST acc=0.5556 | f1m=0.5000\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4667) | TEST acc=0.5000 | f1m=0.4657\n",
      "  ⇒ Sujeto S077 | ACC=0.344 | F1m=0.321\n",
      "\n",
      "== Sujeto S078 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4667) | TEST acc=0.3333 | f1m=0.3463\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.3333) | TEST acc=0.4444 | f1m=0.4217\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4000) | TEST acc=0.2778 | f1m=0.2705\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.5333) | TEST acc=0.2778 | f1m=0.2806\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.5333) | TEST acc=0.3889 | f1m=0.3750\n",
      "  ⇒ Sujeto S078 | ACC=0.344 | F1m=0.339\n",
      "\n",
      "== Sujeto S079 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.4444 | f1m=0.4498\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.2667) | TEST acc=0.5556 | f1m=0.5625\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4667) | TEST acc=0.5556 | f1m=0.5570\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4000) | TEST acc=0.6667 | f1m=0.6580\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.3333 | f1m=0.3560\n",
      "  ⇒ Sujeto S079 | ACC=0.511 | F1m=0.517\n",
      "\n",
      "== Sujeto S080 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4000) | TEST acc=0.5000 | f1m=0.5088\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.5333) | TEST acc=0.5000 | f1m=0.4904\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.6111 | f1m=0.6220\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.2222 | f1m=0.2500\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.3333 | f1m=0.3347\n",
      "  ⇒ Sujeto S080 | ACC=0.433 | F1m=0.441\n",
      "\n",
      "== Sujeto S081 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.5556 | f1m=0.5667\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.5333) | TEST acc=0.6111 | f1m=0.6167\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.8000) | TEST acc=0.3333 | f1m=0.2987\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.6667 | f1m=0.6625\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.8000) | TEST acc=0.4444 | f1m=0.4236\n",
      "  ⇒ Sujeto S081 | ACC=0.522 | F1m=0.514\n",
      "\n",
      "== Sujeto S082 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.6111 | f1m=0.6220\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.8000) | TEST acc=0.5556 | f1m=0.5318\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.6667 | f1m=0.6553\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.8000) | TEST acc=0.5556 | f1m=0.5595\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.6111 | f1m=0.6269\n",
      "  ⇒ Sujeto S082 | ACC=0.600 | F1m=0.599\n",
      "\n",
      "== Sujeto S083 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.4667) | TEST acc=0.4444 | f1m=0.4876\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.4444 | f1m=0.4310\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.3889 | f1m=0.3654\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4667) | TEST acc=0.4444 | f1m=0.4472\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.4000) | TEST acc=0.3889 | f1m=0.3794\n",
      "  ⇒ Sujeto S083 | ACC=0.422 | F1m=0.422\n",
      "\n",
      "== Sujeto S084 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4000) | TEST acc=0.3889 | f1m=0.3155\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.5333) | TEST acc=0.4444 | f1m=0.3919\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.3333 | f1m=0.2650\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6000) | TEST acc=0.4444 | f1m=0.4143\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4000) | TEST acc=0.3889 | f1m=0.3750\n",
      "  ⇒ Sujeto S084 | ACC=0.400 | F1m=0.352\n",
      "\n",
      "== Sujeto S085 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.8000) | TEST acc=0.5000 | f1m=0.4539\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.9333) | TEST acc=0.5000 | f1m=0.4975\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.4444 | f1m=0.3864\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8667) | TEST acc=0.6111 | f1m=0.6025\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.7222 | f1m=0.6286\n",
      "  ⇒ Sujeto S085 | ACC=0.556 | F1m=0.514\n",
      "\n",
      "== Sujeto S086 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8667) | TEST acc=0.8333 | f1m=0.8146\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8000) | TEST acc=0.8333 | f1m=0.8365\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.7333) | TEST acc=0.8333 | f1m=0.8162\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.9333) | TEST acc=0.6111 | f1m=0.6095\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.8667) | TEST acc=0.8333 | f1m=0.8365\n",
      "  ⇒ Sujeto S086 | ACC=0.789 | F1m=0.783\n",
      "\n",
      "== Sujeto S087 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.4000) | TEST acc=0.3333 | f1m=0.2709\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.5333) | TEST acc=0.5000 | f1m=0.4831\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.3333) | TEST acc=0.4444 | f1m=0.4484\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.5333) | TEST acc=0.1667 | f1m=0.1500\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6000) | TEST acc=0.3333 | f1m=0.3290\n",
      "  ⇒ Sujeto S087 | ACC=0.356 | F1m=0.336\n",
      "\n",
      "== Sujeto S090 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.3333 | f1m=0.2250\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.4444 | f1m=0.4028\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.6667 | f1m=0.6170\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8667) | TEST acc=0.5000 | f1m=0.4626\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.6111 | f1m=0.6086\n",
      "  ⇒ Sujeto S090 | ACC=0.511 | F1m=0.463\n",
      "\n",
      "== Sujeto S091 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.9333) | TEST acc=0.4444 | f1m=0.4463\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.6667 | f1m=0.6201\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.5556 | f1m=0.4917\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.6111 | f1m=0.6028\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.5333) | TEST acc=0.5000 | f1m=0.4805\n",
      "  ⇒ Sujeto S091 | ACC=0.556 | F1m=0.528\n",
      "\n",
      "== Sujeto S093 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.5556 | f1m=0.5484\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6000) | TEST acc=0.5000 | f1m=0.5251\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.8000) | TEST acc=0.6111 | f1m=0.6153\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.3333 | f1m=0.2992\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.7333) | TEST acc=0.6667 | f1m=0.6389\n",
      "  ⇒ Sujeto S093 | ACC=0.533 | F1m=0.525\n",
      "\n",
      "== Sujeto S094 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.8000) | TEST acc=0.5556 | f1m=0.5437\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.8000) | TEST acc=0.6111 | f1m=0.6098\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.5333) | TEST acc=0.6667 | f1m=0.6810\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.6111 | f1m=0.5833\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.5000 | f1m=0.4173\n",
      "  ⇒ Sujeto S094 | ACC=0.589 | F1m=0.567\n",
      "\n",
      "== Sujeto S095 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.3333 | f1m=0.3419\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4000) | TEST acc=0.3333 | f1m=0.3333\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.7333) | TEST acc=0.1667 | f1m=0.1455\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.4667) | TEST acc=0.5000 | f1m=0.4595\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.5333) | TEST acc=0.2222 | f1m=0.2111\n",
      "  ⇒ Sujeto S095 | ACC=0.311 | F1m=0.298\n",
      "\n",
      "== Sujeto S096 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.5556 | f1m=0.4872\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8000) | TEST acc=0.3889 | f1m=0.3417\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.7333) | TEST acc=0.5556 | f1m=0.5336\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.5556 | f1m=0.5364\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.8000) | TEST acc=0.5000 | f1m=0.5097\n",
      "  ⇒ Sujeto S096 | ACC=0.511 | F1m=0.482\n",
      "\n",
      "== Sujeto S097 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.7333) | TEST acc=0.5000 | f1m=0.5040\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.4444 | f1m=0.4208\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.4444 | f1m=0.4199\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6000) | TEST acc=0.3333 | f1m=0.3270\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4667) | TEST acc=0.5000 | f1m=0.4455\n",
      "  ⇒ Sujeto S097 | ACC=0.444 | F1m=0.423\n",
      "\n",
      "== Sujeto S098 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.7333) | TEST acc=0.6111 | f1m=0.5833\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6000) | TEST acc=0.4444 | f1m=0.4540\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.3333 | f1m=0.3270\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.5000 | f1m=0.4578\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4667) | TEST acc=0.3889 | f1m=0.3523\n",
      "  ⇒ Sujeto S098 | ACC=0.456 | F1m=0.435\n",
      "\n",
      "== Sujeto S099 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.3333 | f1m=0.2881\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4667) | TEST acc=0.3889 | f1m=0.3847\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.3333) | TEST acc=0.5000 | f1m=0.4913\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4667) | TEST acc=0.3889 | f1m=0.3364\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.7333) | TEST acc=0.2778 | f1m=0.2715\n",
      "  ⇒ Sujeto S099 | ACC=0.378 | F1m=0.354\n",
      "\n",
      "== Sujeto S101 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.5000 | f1m=0.4667\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.5333) | TEST acc=0.5556 | f1m=0.5333\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.5556 | f1m=0.5446\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.5556 | f1m=0.5458\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.5556 | f1m=0.4679\n",
      "  ⇒ Sujeto S101 | ACC=0.544 | F1m=0.512\n",
      "\n",
      "== Sujeto S102 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4667) | TEST acc=0.5556 | f1m=0.5792\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.5556 | f1m=0.5411\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.7333) | TEST acc=0.6111 | f1m=0.5893\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6667) | TEST acc=0.5000 | f1m=0.4733\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.7333) | TEST acc=0.5556 | f1m=0.5467\n",
      "  ⇒ Sujeto S102 | ACC=0.556 | F1m=0.546\n",
      "\n",
      "== Sujeto S103 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4667) | TEST acc=0.6667 | f1m=0.6524\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4667) | TEST acc=0.6111 | f1m=0.6095\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.8000) | TEST acc=0.3889 | f1m=0.3826\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.7333) | TEST acc=0.6111 | f1m=0.5800\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.7333) | TEST acc=0.5556 | f1m=0.5310\n",
      "  ⇒ Sujeto S103 | ACC=0.567 | F1m=0.551\n",
      "\n",
      "== Sujeto S105 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.8667) | TEST acc=0.5556 | f1m=0.4750\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.8667) | TEST acc=0.5000 | f1m=0.4475\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.8000) | TEST acc=0.7778 | f1m=0.7836\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.7333) | TEST acc=0.7222 | f1m=0.7262\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.6667) | TEST acc=0.7222 | f1m=0.6818\n",
      "  ⇒ Sujeto S105 | ACC=0.656 | F1m=0.623\n",
      "\n",
      "== Sujeto S106 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4667) | TEST acc=0.5000 | f1m=0.4842\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4667) | TEST acc=0.3889 | f1m=0.3344\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4000) | TEST acc=0.3333 | f1m=0.3250\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.2222 | f1m=0.2159\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4000) | TEST acc=0.6667 | f1m=0.6622\n",
      "  ⇒ Sujeto S106 | ACC=0.422 | F1m=0.404\n",
      "\n",
      "== Sujeto S107 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4667) | TEST acc=0.2778 | f1m=0.2443\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.5333) | TEST acc=0.2778 | f1m=0.3040\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4000) | TEST acc=0.3889 | f1m=0.3753\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.4000) | TEST acc=0.3889 | f1m=0.3919\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4667) | TEST acc=0.2778 | f1m=0.2715\n",
      "  ⇒ Sujeto S107 | ACC=0.322 | F1m=0.317\n",
      "\n",
      "== Sujeto S108 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.4667) | TEST acc=0.7778 | f1m=0.7768\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6667) | TEST acc=0.6667 | f1m=0.6494\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.8667) | TEST acc=0.5000 | f1m=0.5095\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6667) | TEST acc=0.5000 | f1m=0.4904\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.5556 | f1m=0.5556\n",
      "  ⇒ Sujeto S108 | ACC=0.600 | F1m=0.596\n",
      "\n",
      "== Sujeto S109 ==  N=90 | T=480 | C=8\n",
      "  [Fold 1/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4667) | TEST acc=0.4444 | f1m=0.3750\n",
      "  [Fold 2/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.6000) | TEST acc=0.3889 | f1m=0.3806\n",
      "  [Fold 3/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=out (val_acc=0.6000) | TEST acc=0.4444 | f1m=0.3723\n",
      "  [Fold 4/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=head (val_acc=0.4667) | TEST acc=0.3889 | f1m=0.3792\n",
      "  [Fold 5/5] train_cal=57 | val=15 | test=18\n",
      "    → Best stage=spatial+head (val_acc=0.5333) | TEST acc=0.2778 | f1m=0.2633\n",
      "  ⇒ Sujeto S109 | ACC=0.389 | F1m=0.354\n",
      "\n",
      "📄 CSV → /root/Proyecto/EEG_Clasificador/models/eegnet_intra_ft/tables/20251014-042453_eegnet_intra_ft_0to3000ms_metrics.csv\n",
      "📝 TXT → /root/Proyecto/EEG_Clasificador/models/eegnet_intra_ft/logs/20251014-042453_eegnet_intra_ft_0to3000ms_metrics.txt\n",
      "🖼️  Fig → /root/Proyecto/EEG_Clasificador/models/eegnet_intra_ft/figures/20251014-042453_eegnet_intra_ft_0to3000ms_confusions_p1.png\n",
      "🖼️  Fig → /root/Proyecto/EEG_Clasificador/models/eegnet_intra_ft/figures/20251014-042453_eegnet_intra_ft_0to3000ms_confusions_p2.png\n",
      "🖼️  Fig → /root/Proyecto/EEG_Clasificador/models/eegnet_intra_ft/figures/20251014-042453_eegnet_intra_ft_0to3000ms_confusions_p3.png\n",
      "🖼️  Fig → /root/Proyecto/EEG_Clasificador/models/eegnet_intra_ft/figures/20251014-042453_eegnet_intra_ft_0to3000ms_confusions_p4.png\n",
      "🖼️  Fig → /root/Proyecto/EEG_Clasificador/models/eegnet_intra_ft/figures/20251014-042453_eegnet_intra_ft_0to3000ms_confusions_p5.png\n",
      "🖼️  Fig → /root/Proyecto/EEG_Clasificador/models/eegnet_intra_ft/figures/20251014-042453_eegnet_intra_ft_0to3000ms_confusions_p6.png\n",
      "🖼️  Fig → /root/Proyecto/EEG_Clasificador/models/eegnet_intra_ft/figures/20251014-042453_eegnet_intra_ft_0to3000ms_confusions_p7.png\n",
      "🖼️  Fig → /root/Proyecto/EEG_Clasificador/models/eegnet_intra_ft/figures/20251014-042453_eegnet_intra_ft_0to3000ms_confusions_p8.png\n",
      "🖼️  Fig → /root/Proyecto/EEG_Clasificador/models/eegnet_intra_ft/figures/20251014-042453_eegnet_intra_ft_0to3000ms_confusions_p9.png\n",
      "🖼️  Global Fig → /root/Proyecto/EEG_Clasificador/models/eegnet_intra_ft/figures/20251014-042453_global_confusion.png\n",
      "\n",
      "[GLOBAL INTRA FT] ACC=0.523±0.114 | F1m=0.504±0.116\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# EEGNet unificado: INTER (cross-subject) + INTRA (per-subject CV) con FT progresivo\n",
    "# Mantiene comportamientos y resultados de tus dos scripts originales.\n",
    "\n",
    "import os, re, math, random, json, itertools, copy, argparse\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import (GroupKFold, StratifiedKFold, StratifiedShuffleSplit,\n",
    "                                     GroupShuffleSplit)\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# CONFIGURACIÓN GLOBAL\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "FOLDS_JSON_DEFAULT = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# determinismo\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "# canales / runs / escenario\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','CPz']\n",
    "MI_RUNS_LR = [4, 8, 12]\n",
    "MI_RUNS_OF = [6, 10, 14]\n",
    "BASELINE_RUNS_EO = [1]   # solo para lectura de nombres de canales si hiciera falta\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "\n",
    "CLASS_SCENARIO = '4c'\n",
    "WINDOW_MODE = '3s'  # 0–3s\n",
    "FS = 160.0\n",
    "CLASS_NAMES_4C = ['Left', 'Right', 'Both Fists', 'Both Feet']\n",
    "\n",
    "# Hiperparámetros compartidos\n",
    "N_FOLDS_INTER = 5\n",
    "BATCH_SIZE_INTER = 16\n",
    "EPOCHS_GLOBAL_INTER = 100\n",
    "LR_GLOBAL_INTER = 1e-3\n",
    "GLOBAL_VAL_SPLIT = 0.15\n",
    "GLOBAL_PATIENCE = 10\n",
    "LOG_EVERY = 5\n",
    "\n",
    "# Fine-tuning (compartidos)\n",
    "CALIB_CV_FOLDS = 4\n",
    "FT_EPOCHS = 30\n",
    "FT_BASE_LR = 5e-5\n",
    "FT_HEAD_LR = 1e-3\n",
    "FT_L2SP = 1e-4\n",
    "FT_PATIENCE = 5\n",
    "FT_VAL_RATIO = 0.2\n",
    "\n",
    "# INTRA específicos (mismos valores que tu script)\n",
    "N_FOLDS_INTRA = 5\n",
    "BATCH_SIZE_GLOBAL_INTRA = 16\n",
    "EPOCHS_GLOBAL_INTRA = 100\n",
    "LR_GLOBAL_INTRA = 1e-3\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES EEG / EDF\n",
    "# =========================\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = re.sub(r'([A-Za-z])0([0-9])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])Z$', r'\\1z', s)\n",
    "    s = s.replace('fp', 'Fp').replace('FP', 'Fp')\n",
    "    s = ''.join(ch.upper() if ch != 'z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {}\n",
    "    for ch in raw.ch_names:\n",
    "        lab = normalize_label(ch)\n",
    "        lab = lab[:-1] + 'z' if lab.endswith('Z') else lab\n",
    "        lab = re.sub(r'([A-Z])Z$', r'\\1z', lab)\n",
    "        mapping[ch] = lab\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired_channels=EXPECTED_8):\n",
    "    have = [ch for ch in desired_channels if ch in raw.ch_names]\n",
    "    missing = [ch for ch in desired_channels if ch not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"Warning: faltan canales {missing} en archivo {getattr(raw,'filenames', [''])[0]}\")\n",
    "        return None\n",
    "    raw.reorder_channels([ch for ch in raw.ch_names if ch in desired_channels] +\n",
    "                         [ch for ch in raw.ch_names if ch not in desired_channels])\n",
    "    raw.pick_channels(desired_channels, ordered=True)\n",
    "    return raw\n",
    "\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(run_id:int):\n",
    "    if run_id in MI_RUNS_LR: return 'LR'\n",
    "    if run_id in MI_RUNS_OF: return 'OF'\n",
    "    if run_id in BASELINE_RUNS_EO: return 'EO'\n",
    "    return None\n",
    "\n",
    "def read_raw_edf(path: Path):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        mont = mne.channels.make_standard_montage('standard_1020')\n",
    "        raw.set_montage(mont, on_missing='ignore')\n",
    "    except Exception:\n",
    "        pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad=\"auto\")\n",
    "    raw = ensure_channels_order(raw, EXPECTED_8)\n",
    "    if raw is None: return None\n",
    "    return raw\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    if raw.annotations is None or len(raw.annotations) == 0:\n",
    "        return []\n",
    "    def _norm(s): return str(s).strip().upper().replace(' ', '')\n",
    "    res = []\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = _norm(desc)\n",
    "        if tag in ('T1','T2'):\n",
    "            res.append((float(onset), tag))\n",
    "    res.sort()\n",
    "    # deduplicación mínima\n",
    "    dedup, last_t1, last_t2 = [], -1e9, -1e9\n",
    "    for t, tag in res:\n",
    "        if tag == 'T1':\n",
    "            if (t - last_t1) >= 0.5: dedup.append((t, tag)); last_t1 = t\n",
    "        else:\n",
    "            if (t - last_t2) >= 0.5: dedup.append((t, tag)); last_t2 = t\n",
    "    return dedup\n",
    "\n",
    "# =========================\n",
    "# DATASETS (dos variantes)\n",
    "# =========================\n",
    "def extract_trials_from_run_common(edf_path: Path, scenario: str, window_mode: str):\n",
    "    subj, run = parse_subject_run(edf_path)\n",
    "    kind = run_kind(run)\n",
    "    if kind not in ('LR','OF','EO'):\n",
    "        return ([], [])\n",
    "    raw = read_raw_edf(edf_path)\n",
    "    if raw is None: return ([], [])\n",
    "    data = raw.get_data()\n",
    "    fs = raw.info['sfreq']; assert abs(fs - FS) < 1e-6\n",
    "    out = []\n",
    "\n",
    "    if kind in ('LR','OF'):\n",
    "        events = collect_events_T1T2(raw)\n",
    "        rel_start, rel_end = (0.0, 3.0) if window_mode == '3s' else (-1.0, 5.0)\n",
    "        for onset_sec, tag in events:\n",
    "            if kind == 'LR':\n",
    "                label = 0 if tag == 'T1' else 1\n",
    "            else:\n",
    "                label = 2 if tag == 'T1' else 3\n",
    "            if scenario == '2c' and label not in (0,1): continue\n",
    "            if scenario == '3c' and label not in (0,1,2): continue\n",
    "            s = int(round((raw.first_time + onset_sec + rel_start) * fs))\n",
    "            e = int(round((raw.first_time + onset_sec + rel_end) * fs))\n",
    "            if s < 0 or e > data.shape[1]: continue\n",
    "            seg = data[:, s:e].T.astype(np.float32)\n",
    "            seg = (seg - seg.mean(axis=0, keepdims=True)) / (seg.std(axis=0, keepdims=True) + 1e-6)\n",
    "            out.append((seg, label, subj))\n",
    "    elif kind == 'EO':\n",
    "        return ([], raw.ch_names)\n",
    "\n",
    "    return out, raw.ch_names\n",
    "\n",
    "# INTER: balancea a 21 ensayos por clase/sujeto (como tu script INTER)\n",
    "def build_dataset_all_inter(subjects, scenario='4c', window_mode='3s'):\n",
    "    X, y, groups, ch_template = [], [], [], None\n",
    "    for s in tqdm(subjects, desc=\"Construyendo dataset INTER (balanceado)\"):\n",
    "        sdir = DATA_RAW / f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "        trials = {0:[],1:[],2:[],3:[]}\n",
    "        for r in MI_RUNS_LR + MI_RUNS_OF:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run_common(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                trials[lab].append(seg)\n",
    "        # skip if falta alguna clase\n",
    "        if any(len(trials[k])==0 for k in (0,1,2,3)): continue\n",
    "        need = 21\n",
    "        rng = check_random_state(RANDOM_STATE + s)\n",
    "        def pick(arr):\n",
    "            if len(arr) < need:\n",
    "                idx = rng.choice(len(arr), size=need, replace=True); return [arr[i] for i in idx]\n",
    "            rng.shuffle(arr); return arr[:need]\n",
    "        for lab in (0,1,2,3):\n",
    "            for seg in pick(trials[lab]):\n",
    "                X.append(seg); y.append(lab); groups.append(s)\n",
    "    X = np.stack(X, axis=0); y = np.asarray(y, np.int64); groups = np.asarray(groups, np.int64)\n",
    "    n, T, C = X.shape\n",
    "    print(f\"[INTER] Dataset: N={n} | T={T} | C={C} | clases={len(np.unique(y))} | sujetos={len(np.unique(groups))}\")\n",
    "    return X, y, groups, ch_template\n",
    "\n",
    "# INTRA: sin balanceo artificial (como tu script INTRA)\n",
    "def build_dataset_all_intra(subjects, scenario='4c', window_mode='3s'):\n",
    "    X, y, groups = [], [], []\n",
    "    for s in subjects:\n",
    "        sdir = DATA_RAW / f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "        for r in MI_RUNS_LR + MI_RUNS_OF:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, _ = extract_trials_from_run_common(p, scenario, window_mode)\n",
    "            for seg, lab, subj in outs:\n",
    "                X.append(seg); y.append(lab); groups.append(subj)\n",
    "    X = np.stack(X, axis=0); y = np.asarray(y, np.int64); groups = np.asarray(groups, np.int64)\n",
    "    n, T, C = X.shape\n",
    "    print(f\"[INTRA] Dataset: N={n} | T={T} | C={C} | clases={len(np.unique(y))} | sujetos={len(np.unique(groups))}\")\n",
    "    return X, y, groups\n",
    "\n",
    "def subjects_available():\n",
    "    subs = []\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try: sid = int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir / f\"S{sid:03d}R{r:02d}.edf\").exists() for r in (MI_RUNS_LR + MI_RUNS_OF))\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "# =========================\n",
    "# MODELO\n",
    "# =========================\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, n_ch: int, n_classes: int,\n",
    "                 F1: int = 8, D: int = 2, kernel_t: int = 64, k_sep: int = 16,\n",
    "                 pool1_t: int = 4, pool2_t: int = 8, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.n_ch = n_ch; self.n_classes = n_classes\n",
    "        self.F1 = F1; self.D = D; self.F2 = F1 * D\n",
    "        self.kernel_t = kernel_t; self.k_sep = k_sep\n",
    "        self.pool1_t = pool1_t; self.pool2_t = pool2_t\n",
    "        self.conv_temporal = nn.Conv2d(1, F1, kernel_size=(kernel_t, 1),\n",
    "                                       padding=(kernel_t // 2, 0), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(F1); self.act = nn.ELU()\n",
    "        self.conv_depthwise = nn.Conv2d(F1, self.F2, kernel_size=(1, n_ch),\n",
    "                                        groups=F1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.F2)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=(pool1_t, 1), stride=(pool1_t, 1))\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.conv_sep_depth = nn.Conv2d(self.F2, self.F2, kernel_size=(k_sep, 1),\n",
    "                                        groups=self.F2, padding=(k_sep // 2, 0), bias=False)\n",
    "        self.conv_sep_point = nn.Conv2d(self.F2, self.F2, kernel_size=(1, 1), bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.F2)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(pool2_t, 1), stride=(pool2_t, 1))\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = None; self.out = None; self._T_in = None\n",
    "\n",
    "    def _build_head(self, T_in: int, device: torch.device):\n",
    "        T1 = T_in // self.pool1_t; T2 = T1 // self.pool2_t\n",
    "        feat_dim = self.F2 * T2 * 1\n",
    "        self.fc  = nn.Linear(feat_dim, 80, bias=True).to(device)\n",
    "        self.out = nn.Linear(80, self.n_classes, bias=True).to(device)\n",
    "        self._T_in = T_in\n",
    "\n",
    "    def ensure_head(self, T_in: int, device: torch.device):\n",
    "        if (self.fc is None) or (self.out is None) or (self._T_in != T_in):\n",
    "            self._build_head(T_in, device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, _, T, C = x.shape\n",
    "        self.ensure_head(T, x.device)\n",
    "        z = self.conv_temporal(x); z = self.bn1(z); z = self.act(z)\n",
    "        z = self.conv_depthwise(z); z = self.bn2(z); z = self.act(z)\n",
    "        z = self.pool1(z); z = self.drop1(z)\n",
    "        z = self.conv_sep_depth(z); z = self.conv_sep_point(z)\n",
    "        z = self.bn3(z); z = self.act(z)\n",
    "        z = self.pool2(z); z = self.drop2(z)\n",
    "        z = self.flatten(z)\n",
    "        z = self.fc(z); z = self.act(z)\n",
    "        return self.out(z)  # logits; CrossEntropyLoss aplica softmax internamente\n",
    "\n",
    "# =========================\n",
    "# DATASETS TORCH\n",
    "# =========================\n",
    "class EEGTrials(Dataset):\n",
    "    def __init__(self, X, y, groups):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.g = groups.astype(np.int64)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        x = np.expand_dims(x, 0)  # (1,T,C)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx])\n",
    "\n",
    "# =========================\n",
    "# ENTRENAR / EVALUAR\n",
    "# =========================\n",
    "def train_epoch(model, loader, opt, criterion):\n",
    "    model.train()\n",
    "    for xb, yb, _ in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward(); opt.step()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_with_preds(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for xb, yb, _ in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        pred = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "        y_pred.extend(pred); y_true.extend(yb.numpy().tolist())\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    return y_true, y_pred, float(acc)\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes, title, fname):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.nan_to_num(cm_norm)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title); plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    ticks = np.arange(len(classes))\n",
    "    plt.xticks(ticks, classes, rotation=45, ha='right'); plt.yticks(ticks, classes)\n",
    "    fmt = '.2f'; thresh = cm_norm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "        plt.text(j, i, format(cm_norm[i, j], fmt),\n",
    "                 ha=\"center\", color=\"white\" if cm_norm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True label'); plt.xlabel('Predicted label')\n",
    "    plt.tight_layout(); plt.savefig(fname, dpi=150, bbox_inches='tight'); plt.close()\n",
    "\n",
    "def print_report(y_true, y_pred, classes):\n",
    "    print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
    "\n",
    "# =========================\n",
    "# FINE-TUNING PROGRESIVO (compartido)\n",
    "# =========================\n",
    "def _param_groups(model, mode):\n",
    "    if mode == 'out':\n",
    "        train = list(model.out.parameters())\n",
    "    elif mode == 'head':\n",
    "        train = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "    elif mode == 'spatial+head':\n",
    "        train = (list(model.conv_depthwise.parameters()) +\n",
    "                 list(model.bn2.parameters()) +\n",
    "                 list(model.conv_sep_depth.parameters()) +\n",
    "                 list(model.conv_sep_point.parameters()) +\n",
    "                 list(model.bn3.parameters()) +\n",
    "                 list(model.fc.parameters()) +\n",
    "                 list(model.out.parameters()))\n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "    return train\n",
    "\n",
    "def _freeze_for_mode(model, mode):\n",
    "    for p in model.parameters(): p.requires_grad = False\n",
    "    if mode == 'out':\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'head':\n",
    "        for p in model.fc.parameters():  p.requires_grad = True\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'spatial+head':\n",
    "        for p in model.conv_depthwise.parameters(): p.requires_grad = True\n",
    "        for p in model.bn2.parameters():           p.requires_grad = True\n",
    "        for p in model.conv_sep_depth.parameters():p.requires_grad = True\n",
    "        for p in model.conv_sep_point.parameters():p.requires_grad = True\n",
    "        for p in model.bn3.parameters():           p.requires_grad = True\n",
    "        for p in model.fc.parameters():            p.requires_grad = True\n",
    "        for p in model.out.parameters():           p.requires_grad = True\n",
    "\n",
    "def _class_weights(y_np, n_classes):\n",
    "    counts = np.bincount(y_np, minlength=n_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    w = counts.sum() / counts\n",
    "    w = w / w.mean()\n",
    "    return torch.tensor(w, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "def _train_one_mode_inter(model, X_cal, y_cal, n_classes, mode,\n",
    "                          epochs=FT_EPOCHS, batch_size=16,\n",
    "                          head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                          l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO):\n",
    "    \"\"\"Versión INTER: early stopping por val_loss (como tu INTER).\"\"\"\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=RANDOM_STATE)\n",
    "    (tr_idx, va_idx), = sss.split(X_cal, y_cal)\n",
    "    Xtr, ytr = X_cal[tr_idx], y_cal[tr_idx]\n",
    "    Xva, yva = X_cal[va_idx], y_cal[va_idx]\n",
    "\n",
    "    ds_tr = TensorDataset(torch.from_numpy(Xtr).float().unsqueeze(1),\n",
    "                          torch.from_numpy(ytr).long())\n",
    "    ds_va = TensorDataset(torch.from_numpy(Xva).float().unsqueeze(1),\n",
    "                          torch.from_numpy(yva).long())\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    _freeze_for_mode(model, mode)\n",
    "\n",
    "    if mode == 'spatial+head':\n",
    "        base_params = (list(model.conv_depthwise.parameters()) +\n",
    "                       list(model.bn2.parameters()) +\n",
    "                       list(model.conv_sep_depth.parameters()) +\n",
    "                       list(model.conv_sep_point.parameters()) +\n",
    "                       list(model.bn3.parameters()))\n",
    "        head_params = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "        train_params = base_params + head_params\n",
    "        opt = optim.Adam([{\"params\": base_params, \"lr\": base_lr},\n",
    "                          {\"params\": head_params, \"lr\": head_lr}])\n",
    "    else:\n",
    "        train_params = _param_groups(model, mode)\n",
    "        opt = optim.Adam(train_params, lr=head_lr)\n",
    "\n",
    "    ref = [p.detach().clone().to(p.device) for p in train_params]\n",
    "    class_w = _class_weights(ytr, n_classes)\n",
    "    crit = nn.CrossEntropyLoss(weight=class_w)\n",
    "\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_val = float('inf')\n",
    "    bad = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # train\n",
    "        model.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            reg = 0.0\n",
    "            for p_cur, p_ref in zip(train_params, ref):\n",
    "                reg = reg + torch.sum((p_cur - p_ref)**2)\n",
    "            loss = loss + l2sp_lambda * reg\n",
    "            loss.backward(); opt.step()\n",
    "        # val -> val_loss\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, nval = 0.0, 0\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                loss = crit(logits, yb)\n",
    "                val_loss += loss.item() * xb.size(0); nval += xb.size(0)\n",
    "            val_loss /= max(1, nval)\n",
    "\n",
    "        if val_loss + 1e-7 < best_val:\n",
    "            best_val = val_loss; bad = 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience: break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def _train_one_mode_intra(model, X_tr, y_tr, X_va, y_va, mode, n_classes,\n",
    "                          epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                          l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, batch_size=16):\n",
    "    \"\"\"Versión INTRA: selección por val_acc (como tu INTRA).\"\"\"\n",
    "    _freeze_for_mode(model, mode)\n",
    "    ds_tr = TensorDataset(torch.from_numpy(X_tr).float().unsqueeze(1),\n",
    "                          torch.from_numpy(y_tr).long())\n",
    "    ds_va = TensorDataset(torch.from_numpy(X_va).float().unsqueeze(1),\n",
    "                          torch.from_numpy(y_va).long())\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    if mode == 'spatial+head':\n",
    "        base_params = (list(model.conv_depthwise.parameters()) +\n",
    "                       list(model.bn2.parameters()) +\n",
    "                       list(model.conv_sep_depth.parameters()) +\n",
    "                       list(model.conv_sep_point.parameters()) +\n",
    "                       list(model.bn3.parameters()))\n",
    "        head_params = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "        train_params = base_params + head_params\n",
    "        opt = optim.Adam([{\"params\": base_params, \"lr\": base_lr},\n",
    "                          {\"params\": head_params, \"lr\": head_lr}])\n",
    "    elif mode == 'head':\n",
    "        train_params = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "        opt = optim.Adam(train_params, lr=head_lr)\n",
    "    else:  # out\n",
    "        train_params = list(model.out.parameters())\n",
    "        opt = optim.Adam(train_params, lr=head_lr)\n",
    "\n",
    "    ref = [p.detach().clone().to(p.device) for p in train_params]\n",
    "    class_w = _class_weights(y_tr, n_classes)\n",
    "    crit = nn.CrossEntropyLoss(weight=class_w)\n",
    "\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_val_acc = -1.0\n",
    "    bad = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # train\n",
    "        model.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            reg = 0.0\n",
    "            for p_cur, p_ref in zip(train_params, ref):\n",
    "                reg = reg + torch.sum((p_cur - p_ref)**2)\n",
    "            loss = loss + l2sp_lambda * reg\n",
    "            loss.backward(); opt.step()\n",
    "        # val -> val_acc\n",
    "        y_true_va, y_pred_va, acc_va = _eval_dl(model, dl_va)\n",
    "        if acc_va > best_val_acc + 1e-6:\n",
    "            best_val_acc = acc_va; best_state = copy.deepcopy(model.state_dict()); bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience: break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return copy.deepcopy(model), float(best_val_acc)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _eval_dl(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        y_pred.extend(logits.argmax(1).cpu().numpy().tolist())\n",
    "        y_true.extend(yb.numpy().tolist())\n",
    "    y_true = np.asarray(y_true, int); y_pred = np.asarray(y_pred, int)\n",
    "    acc = (y_true == y_pred).mean() if y_true.size else 0.0\n",
    "    return y_true, y_pred, float(acc)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_numpy(model, X_np):\n",
    "    model.eval()\n",
    "    xb = torch.from_numpy(X_np).float().unsqueeze(1).to(DEVICE)\n",
    "    logits = model(xb)\n",
    "    return logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "def subject_cv_finetune_predict_progressive_inter(model_global, Xs, ys, n_classes):\n",
    "    \"\"\"INTER: 4-fold en el sujeto, entrena out/head/spatial+head (val_loss) y elige por accuracy en holdout.\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=CALIB_CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    y_true_full = np.empty_like(ys); y_pred_full = np.empty_like(ys)\n",
    "    for tr_idx, te_idx in skf.split(Xs, ys):\n",
    "        Xcal, ycal = Xs[tr_idx], ys[tr_idx]\n",
    "        Xho,  yho  = Xs[te_idx], ys[te_idx]\n",
    "        # out\n",
    "        m_out = copy.deepcopy(model_global)\n",
    "        _train_one_mode_inter(m_out, Xcal, ycal, n_classes, mode='out',\n",
    "                              epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                              patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_out = predict_numpy(m_out, Xho); acc_out = (yhat_out == yho).mean()\n",
    "        # head\n",
    "        m_head = copy.deepcopy(model_global)\n",
    "        _train_one_mode_inter(m_head, Xcal, ycal, n_classes, mode='head',\n",
    "                              epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                              patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_head = predict_numpy(m_head, Xho); acc_head = (yhat_head == yho).mean()\n",
    "        # spatial+head\n",
    "        m_sp = copy.deepcopy(model_global)\n",
    "        _train_one_mode_inter(m_sp, Xcal, ycal, n_classes, mode='spatial+head',\n",
    "                              epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                              l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_sp = predict_numpy(m_sp, Xho); acc_sp = (yhat_sp == yho).mean()\n",
    "        best_idx = np.argmax([acc_out, acc_head, acc_sp])\n",
    "        yhat_best = [yhat_out, yhat_head, yhat_sp][best_idx]\n",
    "        y_true_full[te_idx] = yho; y_pred_full[te_idx] = yhat_best\n",
    "    return y_true_full, y_pred_full\n",
    "\n",
    "# =========================\n",
    "# PIPELINE INTER (cross-subject)\n",
    "# =========================\n",
    "def run_inter(save_folds_json=True, folds_json_path=FOLDS_JSON_DEFAULT,\n",
    "              folds_json_description=\"GroupKFold folds for comparison\"):\n",
    "    mne.set_log_level('WARNING')\n",
    "    subs = subjects_available()\n",
    "    print(f\"Sujetos elegibles: {len(subs)} → {subs[:10]}{'...' if len(subs)>10 else ''}\")\n",
    "\n",
    "    X, y, groups, chs = build_dataset_all_inter(subs, scenario=CLASS_SCENARIO, window_mode=WINDOW_MODE)\n",
    "    N, T, C = X.shape; n_classes = len(np.unique(y))\n",
    "    ds = EEGTrials(X, y, groups)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    folds_json_path = Path(folds_json_path) if folds_json_path else Path(\"folds/group_folds_5.json\")\n",
    "    folds_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    subject_ids_str = [f\"S{s:03d}\" for s in sorted(np.unique(groups).tolist())]\n",
    "\n",
    "    if not folds_json_path.exists():\n",
    "        if not save_folds_json:\n",
    "            raise FileNotFoundError(f\"No existe {folds_json_path} y save_folds_json=False.\")\n",
    "        save_group_folds_json_with_indices(subject_ids_str, groups, n_splits=N_FOLDS_INTER,\n",
    "                                           out_json_path=folds_json_path,\n",
    "                                           created_by=\"EEGNet_INTER\",\n",
    "                                           description=folds_json_description)\n",
    "\n",
    "    payload = load_group_folds_json(folds_json_path, expected_subject_ids=subject_ids_str, strict_check=False)\n",
    "    folds = payload[\"folds\"]\n",
    "\n",
    "    global_folds, ft_prog_folds, all_true, all_pred = [], [], [], []\n",
    "\n",
    "    for f in folds:\n",
    "        fold = f[\"fold\"]\n",
    "        tr_idx = np.asarray(f.get(\"tr_idx\", []), dtype=int)\n",
    "        te_idx = np.asarray(f.get(\"te_idx\", []), dtype=int)\n",
    "        if tr_idx.size == 0 or te_idx.size == 0:\n",
    "            print(f\"Advertencia: fold {fold} vacío. Saltando.\"); continue\n",
    "\n",
    "        # split de validación por sujetos dentro de TRAIN\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=GLOBAL_VAL_SPLIT, random_state=RANDOM_STATE)\n",
    "        tr_subj_idx, va_subj_idx = next(gss.split(tr_idx, groups[tr_idx], groups[tr_idx]))\n",
    "        tr_sub_idx = tr_idx[tr_subj_idx]; va_idx = tr_idx[va_subj_idx]\n",
    "\n",
    "        tr_loader = DataLoader(Subset(ds, tr_sub_idx), batch_size=BATCH_SIZE_INTER, shuffle=True,  drop_last=False)\n",
    "        va_loader = DataLoader(Subset(ds, va_idx),     batch_size=BATCH_SIZE_INTER, shuffle=False, drop_last=False)\n",
    "        te_loader = DataLoader(Subset(ds, te_idx),     batch_size=BATCH_SIZE_INTER, shuffle=False, drop_last=False)\n",
    "\n",
    "        model = EEGNet(n_ch=C, n_classes=n_classes, F1=8, D=2, kernel_t=64, k_sep=16,\n",
    "                       pool1_t=4, pool2_t=8, dropout=0.5).to(DEVICE)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_GLOBAL_INTER)\n",
    "\n",
    "        def _acc(loader): return evaluate_with_preds(model, loader)[2]\n",
    "\n",
    "        print(f\"\\n[Fold {fold}/{N_FOLDS_INTER}] Entrenamiento global (val por sujetos)\")\n",
    "        best_state = copy.deepcopy(model.state_dict()); best_val = -1.0; bad = 0\n",
    "        for epoch in range(1, EPOCHS_GLOBAL_INTER + 1):\n",
    "            train_epoch(model, tr_loader, opt, criterion)\n",
    "            if epoch % LOG_EVERY == 0:\n",
    "                tr_acc = _acc(tr_loader); va_acc = _acc(va_loader)\n",
    "                print(f\"  Época {epoch:3d} | train_acc={tr_acc:.4f} | val_acc={va_acc:.4f}\")\n",
    "                if va_acc > best_val + 1e-4:\n",
    "                    best_val = va_acc; best_state = copy.deepcopy(model.state_dict()); bad = 0\n",
    "                else:\n",
    "                    bad += 1\n",
    "                    if bad >= GLOBAL_PATIENCE:\n",
    "                        print(f\"  Early stopping en época {epoch} (mejor val_acc={best_val:.4f})\")\n",
    "                        break\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "        # test inter-sujeto\n",
    "        y_true, y_pred, acc_global = evaluate_with_preds(model, te_loader)\n",
    "        global_folds.append(acc_global); all_true.append(y_true); all_pred.append(y_pred)\n",
    "        print(f\"[Fold {fold}] Global acc={acc_global:.4f}\")\n",
    "        print_report(y_true, y_pred, CLASS_NAMES_4C)\n",
    "\n",
    "        # FT progresivo por sujeto en TEST\n",
    "        X_te, y_te, g_te = X[te_idx], y[te_idx], groups[te_idx]\n",
    "        y_true_ft_all, y_pred_ft_all, used_subjects = [], [], 0\n",
    "        for sid in np.unique(g_te):\n",
    "            idx = np.where(g_te == sid)[0]\n",
    "            Xs, ys = X_te[idx], y_te[idx]\n",
    "            if len(ys) < CALIB_CV_FOLDS or len(np.unique(ys)) < 2:\n",
    "                continue\n",
    "            y_true_subj, y_pred_subj = subject_cv_finetune_predict_progressive_inter(\n",
    "                model, Xs, ys, n_classes)\n",
    "            y_true_ft_all.append(y_true_subj); y_pred_ft_all.append(y_pred_subj); used_subjects += 1\n",
    "        if len(y_true_ft_all) > 0:\n",
    "            y_true_ft_all = np.concatenate(y_true_ft_all)\n",
    "            y_pred_ft_all = np.concatenate(y_pred_ft_all)\n",
    "            acc_ft = (y_true_ft_all == y_pred_ft_all).mean()\n",
    "            print(f\"  FT PROGRESIVO (por sujeto, {CALIB_CV_FOLDS}-fold) acc={acc_ft:.4f} | sujetos={used_subjects}\")\n",
    "            print(f\"  Δ(FT-Global) = {acc_ft - acc_global:+.4f}\")\n",
    "        else:\n",
    "            acc_ft = np.nan\n",
    "            print(\"  FT PROGRESIVO no ejecutado (sujeto(s) insuficientes).\")\n",
    "        ft_prog_folds.append(acc_ft)\n",
    "\n",
    "    all_true = np.concatenate(all_true) if len(all_true)>0 else np.array([], int)\n",
    "    all_pred = np.concatenate(all_pred) if len(all_pred)>0 else np.array([], int)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTADOS INTER\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Global folds:\", [f\"{a:.4f}\" for a in global_folds])\n",
    "    if len(global_folds)>0: print(f\"Global mean: {np.mean(global_folds):.4f}\")\n",
    "    print(\"Fine-tune PROGRESIVO folds:\", [(\"nan\" if (a is None or np.isnan(a)) else f\"{a:.4f}\") for a in ft_prog_folds])\n",
    "    if len(ft_prog_folds)>0:\n",
    "        print(f\"Fine-tune PROGRESIVO mean: {np.nanmean(ft_prog_folds):.4f}\")\n",
    "        print(f\"Δ(FT-Global) mean: {np.nanmean(ft_prog_folds) - np.mean(global_folds):+.4f}\")\n",
    "\n",
    "    if all_true.size > 0:\n",
    "        plot_confusion(all_true, all_pred, CLASS_NAMES_4C,\n",
    "                       title=\"INTER — Confusion Matrix (All Folds)\",\n",
    "                       fname=\"confusion_inter_allfolds.png\")\n",
    "        print(\"↳ Matriz de confusión guardada: confusion_inter_allfolds.png\")\n",
    "\n",
    "# =========================\n",
    "# PIPELINE INTRA (per-subject CV)\n",
    "# =========================\n",
    "def train_global_model_intra(X, y, epochs=EPOCHS_GLOBAL_INTRA):\n",
    "    n_ch = X.shape[2]; n_classes = len(np.unique(y))\n",
    "    model = EEGNet(n_ch=n_ch, n_classes=n_classes, F1=8, D=2, kernel_t=64, k_sep=16,\n",
    "                   pool1_t=4, pool2_t=8, dropout=0.5).to(DEVICE)\n",
    "    ds = EEGTrials(X, y, np.zeros(len(y)))\n",
    "    dl = DataLoader(ds, batch_size=BATCH_SIZE_GLOBAL_INTRA, shuffle=True, drop_last=False)\n",
    "    crit = nn.CrossEntropyLoss(); opt = optim.Adam(model.parameters(), lr=LR_GLOBAL_INTRA)\n",
    "    for ep in range(1, epochs+1):\n",
    "        train_epoch(model, dl, opt, crit)\n",
    "        if ep % LOG_EVERY == 0:\n",
    "            y_t, y_p, acc = evaluate_with_preds(model, dl)\n",
    "            print(f\"[GLOBAL PRETRAIN] Época {ep:3d}/{epochs} | train_acc={acc:.4f}\")\n",
    "    return model\n",
    "\n",
    "def run_intra(n_folds=N_FOLDS_INTRA):\n",
    "    ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    OUT_BASE = PROJ / 'models' / 'eegnet_intra_ft'\n",
    "    TAB_DIR = OUT_BASE / 'tables'; LOG_DIR = OUT_BASE / 'logs'; FIG_DIR = OUT_BASE / 'figures'\n",
    "    for d in (TAB_DIR, LOG_DIR, FIG_DIR): d.mkdir(parents=True, exist_ok=True)\n",
    "    mne.set_log_level('WARNING')\n",
    "\n",
    "    subs = subjects_available()\n",
    "    X, y, groups = build_dataset_all_intra(subs, scenario=CLASS_SCENARIO, window_mode=WINDOW_MODE)\n",
    "    print(\"🔹 Entrenando modelo GLOBAL (pretrain sobre todos los sujetos mezclados)...\")\n",
    "    model_global = train_global_model_intra(X, y, epochs=EPOCHS_GLOBAL_INTRA)\n",
    "\n",
    "    subjects = np.unique(groups)\n",
    "    rows, cm_items = [], []\n",
    "    all_true_global, all_pred_global = [], []\n",
    "\n",
    "    for sid in subjects:\n",
    "        idx = np.where(groups == sid)[0]\n",
    "        Xs, ys = X[idx], y[idx]\n",
    "        print(f\"\\n== Sujeto S{sid:03d} ==  N={len(ys)} | T={X.shape[1]} | C={X.shape[2]}\")\n",
    "        skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "        fold_accs, fold_f1s = [], []\n",
    "        y_true_full, y_pred_full = [], []\n",
    "\n",
    "        for fold_i, (tr_idx, te_idx) in enumerate(skf.split(Xs, ys), start=1):\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=FT_VAL_RATIO, random_state=RANDOM_STATE+fold_i)\n",
    "            (cal_idx, va_idx), = sss.split(Xs[tr_idx], ys[tr_idx])\n",
    "            cal_idx = tr_idx[cal_idx]; va_idx = tr_idx[va_idx]; te_idx_ = te_idx\n",
    "            Xcal, ycal = Xs[cal_idx], ys[cal_idx]\n",
    "            Xval, yval = Xs[va_idx], ys[va_idx]\n",
    "            Xtest, ytest = Xs[te_idx_], ys[te_idx_]\n",
    "            print(f\"  [Fold {fold_i}/{n_folds}] train_cal={len(ycal)} | val={len(yval)} | test={len(ytest)}\")\n",
    "\n",
    "            # clones del global\n",
    "            m_out  = copy.deepcopy(model_global).to(DEVICE)\n",
    "            m_head = copy.deepcopy(model_global).to(DEVICE)\n",
    "            m_sp   = copy.deepcopy(model_global).to(DEVICE)\n",
    "            n_classes = len(np.unique(y))\n",
    "\n",
    "            # out/head/spatial+head con selección por val_acc\n",
    "            m_out,  accA = _train_one_mode_intra(m_out,  Xcal, ycal, Xval, yval, mode='out',\n",
    "                                                 n_classes=n_classes, epochs=FT_EPOCHS,\n",
    "                                                 head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                                                 patience=FT_PATIENCE, batch_size=16)\n",
    "            m_head, accB = _train_one_mode_intra(m_head, Xcal, ycal, Xval, yval, mode='head',\n",
    "                                                 n_classes=n_classes, epochs=FT_EPOCHS,\n",
    "                                                 head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                                                 patience=FT_PATIENCE, batch_size=16)\n",
    "            m_sp,   accC = _train_one_mode_intra(m_sp,   Xcal, ycal, Xval, yval, mode='spatial+head',\n",
    "                                                 n_classes=n_classes, epochs=FT_EPOCHS,\n",
    "                                                 head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                                                 l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, batch_size=16)\n",
    "\n",
    "            stages = [('out',m_out,accA), ('head',m_head,accB), ('spatial+head',m_sp,accC)]\n",
    "            best_name, best_model, best_val = max(stages, key=lambda t: t[2])\n",
    "\n",
    "            yhat = predict_numpy(best_model, Xtest)\n",
    "            acc = (yhat == ytest).mean()\n",
    "            f1m = f1_score(ytest, yhat, average='macro')\n",
    "            print(f\"    → Best stage={best_name} (val_acc={best_val:.4f}) | TEST acc={acc:.4f} | f1m={f1m:.4f}\")\n",
    "\n",
    "            fold_accs.append(float(acc)); fold_f1s.append(float(f1m))\n",
    "            y_true_full.extend(ytest.tolist()); y_pred_full.extend(yhat.tolist())\n",
    "\n",
    "        acc_mu = float(np.mean(fold_accs)) if fold_accs else 0.0\n",
    "        f1_mu  = float(np.mean(fold_f1s)) if fold_f1s else 0.0\n",
    "        rows.append(dict(subject=f\"S{sid:03d}\", acc_mean=acc_mu, f1_macro_mean=f1_mu, k=n_folds))\n",
    "        print(f\"  ⇒ Sujeto S{sid:03d} | ACC={acc_mu:.3f} | F1m={f1_mu:.3f}\")\n",
    "\n",
    "        cm = confusion_matrix(y_true_full, y_pred_full, labels=list(range(len(CLASS_NAMES_4C))))\n",
    "        cm_items.append((f\"S{sid:03d}\", cm, CLASS_NAMES_4C))\n",
    "        all_true_global.extend(y_true_full); all_pred_global.extend(y_pred_full)\n",
    "\n",
    "    # resumen global y guardados (idéntico a tu INTRA)\n",
    "    df = pd.DataFrame(rows).sort_values(\"subject\")\n",
    "    acc_mu_glob = float(df['acc_mean'].mean()) if not df.empty else 0.0\n",
    "    acc_sd_glob = float(df['acc_mean'].std(ddof=0)) if not df.empty else 0.0\n",
    "    f1_mu_glob  = float(df['f1_macro_mean'].mean()) if not df.empty else 0.0\n",
    "    f1_sd_glob  = float(df['f1_macro_mean'].std(ddof=0)) if not df.empty else 0.0\n",
    "\n",
    "    df_glob = pd.DataFrame([{'subject':'GLOBAL','acc_mean':acc_mu_glob,'f1_macro_mean':f1_mu_glob,'k':n_folds}])\n",
    "    df_out = pd.concat([df, df_glob], ignore_index=True)\n",
    "\n",
    "    run_tag = f\"{ts}_eegnet_intra_ft_{'0to3000ms' if WINDOW_MODE=='3s' else 'custom'}\"\n",
    "    out_csv = TAB_DIR / f\"{run_tag}_metrics.csv\"\n",
    "    df_out.to_csv(out_csv, index=False); print(f\"\\n📄 CSV → {out_csv}\")\n",
    "\n",
    "    out_txt = LOG_DIR / f\"{run_tag}_metrics.txt\"\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"EEGNet INTRA FT (progresivo) — {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"k-fold por sujeto: {n_folds}\\n\\n\")\n",
    "        f.write(\"subject | acc_mean | f1_macro_mean | k\\n\")\n",
    "        f.write(\"-\"*60 + \"\\n\")\n",
    "        for _, r in df_out.iterrows():\n",
    "            f.write(f\"{r['subject']} | {r['acc_mean']:.4f} | {r['f1_macro_mean']:.4f} | {int(r['k'])}\\n\")\n",
    "        f.write(\"\\nGLOBAL:\\n\")\n",
    "        f.write(f\"ACC={acc_mu_glob:.3f}±{acc_sd_glob:.3f} | F1m={f1_mu_glob:.3f}±{f1_sd_glob:.3f}\\n\")\n",
    "    print(f\"📝 TXT → {out_txt}\")\n",
    "\n",
    "    # mosaicos por sujeto\n",
    "    def _plot_mosaic(cm_items, per_fig=12, n_cols=4):\n",
    "        n = len(cm_items)\n",
    "        if n == 0: return\n",
    "        n_figs = int(np.ceil(n / per_fig))\n",
    "        for fig_idx in range(n_figs):\n",
    "            start, end = fig_idx*per_fig, min((fig_idx+1)*per_fig, n)\n",
    "            chunk = cm_items[start:end]\n",
    "            count = len(chunk); n_rows = int(np.ceil(count / n_cols))\n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(4.5*n_cols, 3.8*n_rows), dpi=140)\n",
    "            axes = np.atleast_2d(axes).flatten()\n",
    "            for ax_i, (label, cm_sum, classes) in enumerate(chunk):\n",
    "                ax = axes[ax_i]\n",
    "                with np.errstate(invalid='ignore'):\n",
    "                    cm_norm = cm_sum.astype('float') / cm_sum.sum(axis=1, keepdims=True)\n",
    "                cm_norm = np.nan_to_num(cm_norm)\n",
    "                ax.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues, vmin=0.0, vmax=1.0)\n",
    "                ax.set_title(label); ax.set_xticks(range(len(classes))); ax.set_yticks(range(len(classes)))\n",
    "                ax.set_xticklabels(classes, rotation=45, ha='right', fontsize=9)\n",
    "                ax.set_yticklabels(classes, fontsize=9)\n",
    "                for i, j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "                    ax.text(j, i, f\"{cm_norm[i, j]:.2f}\", ha=\"center\",\n",
    "                            color=\"white\" if cm_norm[i, j] > 0.5 else \"black\")\n",
    "                ax.set_xlabel(\"\"); ax.set_ylabel(\"\")\n",
    "            for j in range(ax_i + 1, len(axes)): axes[j].axis(\"off\")\n",
    "            fig.suptitle(f\"INTRA — Matrices de confusión por sujeto (página {fig_idx+1}/{n_figs})\", y=0.995, fontsize=14)\n",
    "            fig.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "            out_png = (PROJ / 'models' / 'eegnet_intra_ft' / 'figures' / f\"{run_tag}_confusions_p{fig_idx+1}.png\")\n",
    "            fig.savefig(out_png); plt.close(fig); print(f\"🖼️  Fig → {out_png}\")\n",
    "\n",
    "    _plot_mosaic(cm_items, per_fig=12, n_cols=4)\n",
    "\n",
    "    # confusión global\n",
    "    if len(all_true_global) > 0:\n",
    "        all_true = np.array(all_true_global, int); all_pred = np.array(all_pred_global, int)\n",
    "        cmG = confusion_matrix(all_true, all_pred, labels=list(range(len(CLASS_NAMES_4C))))\n",
    "        plt.figure(figsize=(6.5,5.2), dpi=140)\n",
    "        with np.errstate(invalid='ignore'):\n",
    "            cm_norm = cmG.astype('float') / cmG.sum(axis=1, keepdims=True)\n",
    "        cm_norm = np.nan_to_num(cm_norm)\n",
    "        plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues, vmin=0, vmax=1)\n",
    "        plt.title(\"INTRA — Matriz de confusión GLOBAL\"); plt.colorbar(fraction=0.046, pad=0.04)\n",
    "        ticks = np.arange(len(CLASS_NAMES_4C))\n",
    "        plt.xticks(ticks, CLASS_NAMES_4C, rotation=45, ha='right'); plt.yticks(ticks, CLASS_NAMES_4C)\n",
    "        for i, j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "            plt.text(j, i, f\"{cm_norm[i, j]:.2f}\",\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if cm_norm[i, j] > 0.5 else \"black\")\n",
    "        plt.tight_layout()\n",
    "        out_png = PROJ / 'models' / 'eegnet_intra_ft' / 'figures' / f\"{ts}_global_confusion.png\"\n",
    "        plt.savefig(out_png); plt.close(); print(f\"🖼️  Global Fig → {out_png}\")\n",
    "\n",
    "    print(f\"\\n[GLOBAL INTRA FT] ACC={acc_mu_glob:.3f}±{acc_sd_glob:.3f} | F1m={f1_mu_glob:.3f}±{f1_sd_glob:.3f}\")\n",
    "\n",
    "# =========================\n",
    "# FOLDS JSON helpers (INTER)\n",
    "# =========================\n",
    "def save_group_folds_json_with_indices(subject_ids_str, groups_array, n_splits, out_json_path,\n",
    "                                       created_by=\"eegnet_unificado\", description=None):\n",
    "    out_json_path = Path(out_json_path)\n",
    "    unique_subjects_int = sorted(np.unique(groups_array).tolist())\n",
    "    subject_ids = [f\"S{sid:03d}\" for sid in unique_subjects_int]\n",
    "    if len(subject_ids) < n_splits:\n",
    "        raise ValueError(f\"n_splits={n_splits} > n_subjects={len(subject_ids)}\")\n",
    "    groups = np.arange(len(subject_ids))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    folds = []; fold_i = 0\n",
    "    for train_idx_grp, test_idx_grp in gkf.split(groups, groups, groups):\n",
    "        fold_i += 1\n",
    "        train_sids = [subject_ids[int(i)] for i in train_idx_grp]\n",
    "        test_sids  = [subject_ids[int(i)] for i in test_idx_grp]\n",
    "        train_sids_int = [int(s[1:]) for s in train_sids]\n",
    "        test_sids_int  = [int(s[1:]) for s in test_sids]\n",
    "        tr_idx = np.where(np.isin(groups_array, train_sids_int))[0].tolist()\n",
    "        te_idx = np.where(np.isin(groups_array, test_sids_int))[0].tolist()\n",
    "        folds.append({\"fold\": int(fold_i), \"train\": train_sids, \"test\": test_sids,\n",
    "                      \"tr_idx\": tr_idx, \"te_idx\": te_idx})\n",
    "    payload = {\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"created_by\": created_by,\n",
    "        \"description\": description if description is not None else \"\",\n",
    "        \"n_splits\": int(n_splits),\n",
    "        \"n_subjects\": len(subject_ids),\n",
    "        \"subject_ids\": subject_ids,\n",
    "        \"folds\": folds\n",
    "    }\n",
    "    out_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Folds JSON con índices guardado → {out_json_path}\")\n",
    "    return out_json_path\n",
    "\n",
    "def load_group_folds_json(path_json, expected_subject_ids=None, strict_check=True):\n",
    "    path_json = Path(path_json)\n",
    "    if not path_json.exists(): raise FileNotFoundError(f\"No existe {path_json}\")\n",
    "    with open(path_json, \"r\", encoding=\"utf-8\") as f: payload = json.load(f)\n",
    "    subj_json = payload.get(\"subject_ids\", [])\n",
    "    if expected_subject_ids is not None:\n",
    "        expected = sorted(list(expected_subject_ids))\n",
    "        if subj_json != expected:\n",
    "            msg = (\"subject_ids del JSON no coinciden.\\n\"\n",
    "                   f\"JSON has {len(subj_json)} vs expected {len(expected)}.\")\n",
    "            if strict_check: raise ValueError(msg)\n",
    "            else: print(\"WARNING:\", msg)\n",
    "    return payload\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Lista de sujetos elegibles (solo para feedback rápido)\n",
    "    subs = subjects_available()\n",
    "    if len(subs) == 0:\n",
    "        print(\"No hay sujetos elegibles en data/raw (o todos excluidos).\")\n",
    "    else:\n",
    "        print(f\"Sujetos elegibles: {len(subs)} → {subs[:10]}{'...' if len(subs)>10 else ''}\")\n",
    "\n",
    "    print(f\"🔧 Configuración común: escenario={CLASS_SCENARIO}, ventana={WINDOW_MODE}, fs={FS}\")\n",
    "\n",
    "    # === Elige UNO: descomenta la línea del modo que quieras correr ===\n",
    "\n",
    "    # --- MODO INTER-SUJETO (cross-subject) ---\n",
    "    run_inter(\n",
    "        save_folds_json=True,\n",
    "        folds_json_path=FOLDS_JSON_DEFAULT,\n",
    "        folds_json_description=\"GroupKFold folds for comparison\"\n",
    "    )\n",
    "\n",
    "    # --- MODO INTRA-SUJETO (k-fold por sujeto) ---\n",
    "    run_intra(n_folds=N_FOLDS_INTRA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ff4bc3",
   "metadata": {},
   "source": [
    "# EEGNET + PREENTRENAMIENTO CONTRASTIVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1074d128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Usando dispositivo: cuda\n",
      "🧠 INICIANDO EXPERIMENTO CON EEGNet + FINE-TUNING PROGRESIVO (por sujeto, 4-fold CV)\n",
      "🔧 Configuración: 4c, 8 canales, 3s\n",
      "⚙️  FT: epochs=30, base_lr=5e-05, head_lr=0.001, L2SP=0.0001, patience=5, CV=4\n",
      "🧲 SupCon: enabled=True, epochs=40, batch=64, temp=0.07\n",
      "Sujetos elegibles: 103 → [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Construyendo dataset (RAW): 100%|██████████| 103/103 [00:13<00:00,  7.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset construido: N=8652 | T=480 | C=8 | clases=4 | sujetos únicos=103\n",
      "Listo para entrenar: N=8652 | T=480 | C=8 | clases=4 | sujetos=103\n",
      "[Fold 1/5] SupCon pretrain on 5796 trials...\n",
      "[SupCon] Epoch 005/40 | loss=5.2721\n",
      "[SupCon] Epoch 010/40 | loss=5.2681\n",
      "[SupCon] Epoch 015/40 | loss=5.2670\n",
      "[SupCon] Epoch 020/40 | loss=5.2654\n",
      "[SupCon] Epoch 025/40 | loss=5.2655\n",
      "[SupCon] Epoch 030/40 | loss=5.2654\n",
      "[SupCon] Epoch 035/40 | loss=5.2648\n",
      "[SupCon] Epoch 040/40 | loss=5.2650\n",
      "[SupCon] Preentrenamiento contrastivo completado (backbone inicializado).\n",
      "\n",
      "[Fold 1/5] Entrenando modelo global con validación interna por sujetos... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  Época   5 | train_acc=0.5012 | val_acc=0.4267\n",
      "  Época  10 | train_acc=0.5173 | val_acc=0.4423\n",
      "  Época  15 | train_acc=0.5317 | val_acc=0.4551\n",
      "  Época  20 | train_acc=0.5404 | val_acc=0.4524\n",
      "  Época  25 | train_acc=0.5540 | val_acc=0.4505\n",
      "  Época  30 | train_acc=0.5659 | val_acc=0.4533\n",
      "  Época  35 | train_acc=0.5773 | val_acc=0.4505\n",
      "  Época  40 | train_acc=0.6059 | val_acc=0.4359\n",
      "  Época  45 | train_acc=0.6168 | val_acc=0.4414\n",
      "  Época  50 | train_acc=0.6292 | val_acc=0.4432\n",
      "  Época  55 | train_acc=0.6366 | val_acc=0.4341\n",
      "  Época  60 | train_acc=0.6511 | val_acc=0.4423\n",
      "  Época  65 | train_acc=0.6767 | val_acc=0.4322\n",
      "  Early stopping en época 65 (mejor val_acc=0.4551)\n",
      "[Fold 1/5] Global acc=0.4535\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5021    0.5306    0.5160       441\n",
      "       Right     0.4641    0.5283    0.4942       441\n",
      "  Both Fists     0.3584    0.2812    0.3151       441\n",
      "   Both Feet     0.4644    0.4739    0.4691       441\n",
      "\n",
      "    accuracy                         0.4535      1764\n",
      "   macro avg     0.4473    0.4535    0.4486      1764\n",
      "weighted avg     0.4473    0.4535    0.4486      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5074 | sujetos=21\n",
      "  Δ(FT-Global) = +0.0539\n",
      "[Fold 2/5] SupCon pretrain on 5796 trials...\n",
      "[SupCon] Epoch 005/40 | loss=5.2726\n",
      "[SupCon] Epoch 010/40 | loss=5.2683\n",
      "[SupCon] Epoch 015/40 | loss=5.2668\n",
      "[SupCon] Epoch 020/40 | loss=5.2655\n",
      "[SupCon] Epoch 025/40 | loss=5.2649\n",
      "[SupCon] Epoch 030/40 | loss=5.2655\n",
      "[SupCon] Epoch 035/40 | loss=5.2651\n",
      "[SupCon] Epoch 040/40 | loss=5.2641\n",
      "[SupCon] Preentrenamiento contrastivo completado (backbone inicializado).\n",
      "\n",
      "[Fold 2/5] Entrenando modelo global con validación interna por sujetos... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  Época   5 | train_acc=0.4741 | val_acc=0.4386\n",
      "  Época  10 | train_acc=0.4938 | val_acc=0.4725\n",
      "  Época  15 | train_acc=0.5140 | val_acc=0.4579\n",
      "  Época  20 | train_acc=0.5141 | val_acc=0.4551\n",
      "  Época  25 | train_acc=0.5316 | val_acc=0.4496\n",
      "  Época  30 | train_acc=0.5547 | val_acc=0.4643\n",
      "  Época  35 | train_acc=0.5685 | val_acc=0.4634\n",
      "  Época  40 | train_acc=0.5982 | val_acc=0.4606\n",
      "  Época  45 | train_acc=0.6061 | val_acc=0.4679\n",
      "  Época  50 | train_acc=0.6285 | val_acc=0.4762\n",
      "  Época  55 | train_acc=0.6518 | val_acc=0.4615\n",
      "  Época  60 | train_acc=0.6610 | val_acc=0.4625\n",
      "  Época  65 | train_acc=0.6843 | val_acc=0.4542\n",
      "  Época  70 | train_acc=0.7013 | val_acc=0.4487\n",
      "  Época  75 | train_acc=0.7067 | val_acc=0.4515\n",
      "  Época  80 | train_acc=0.7276 | val_acc=0.4423\n",
      "  Época  85 | train_acc=0.7348 | val_acc=0.4652\n",
      "  Época  90 | train_acc=0.7459 | val_acc=0.4460\n",
      "  Época  95 | train_acc=0.7384 | val_acc=0.4386\n",
      "  Época 100 | train_acc=0.7465 | val_acc=0.4551\n",
      "  Early stopping en época 100 (mejor val_acc=0.4762)\n",
      "[Fold 2/5] Global acc=0.5091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5692    0.5782    0.5737       441\n",
      "       Right     0.5644    0.5760    0.5701       441\n",
      "  Both Fists     0.4226    0.5261    0.4687       441\n",
      "   Both Feet     0.4953    0.3560    0.4142       441\n",
      "\n",
      "    accuracy                         0.5091      1764\n",
      "   macro avg     0.5129    0.5091    0.5067      1764\n",
      "weighted avg     0.5129    0.5091    0.5067      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5788 | sujetos=21\n",
      "  Δ(FT-Global) = +0.0697\n",
      "[Fold 3/5] SupCon pretrain on 5796 trials...\n",
      "[SupCon] Epoch 005/40 | loss=5.2713\n",
      "[SupCon] Epoch 010/40 | loss=5.2679\n",
      "[SupCon] Epoch 015/40 | loss=5.2666\n",
      "[SupCon] Epoch 020/40 | loss=5.2662\n",
      "[SupCon] Epoch 025/40 | loss=5.2656\n",
      "[SupCon] Epoch 030/40 | loss=5.2653\n",
      "[SupCon] Epoch 035/40 | loss=5.2654\n",
      "[SupCon] Epoch 040/40 | loss=5.2642\n",
      "[SupCon] Preentrenamiento contrastivo completado (backbone inicializado).\n",
      "\n",
      "[Fold 3/5] Entrenando modelo global con validación interna por sujetos... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  Época   5 | train_acc=0.5060 | val_acc=0.4734\n",
      "  Época  10 | train_acc=0.5278 | val_acc=0.4780\n",
      "  Época  15 | train_acc=0.5309 | val_acc=0.4890\n",
      "  Época  20 | train_acc=0.5457 | val_acc=0.5000\n",
      "  Época  25 | train_acc=0.5609 | val_acc=0.5027\n",
      "  Época  30 | train_acc=0.5754 | val_acc=0.4780\n",
      "  Época  35 | train_acc=0.5925 | val_acc=0.4936\n",
      "  Época  40 | train_acc=0.6102 | val_acc=0.4881\n",
      "  Época  45 | train_acc=0.6241 | val_acc=0.4936\n",
      "  Época  50 | train_acc=0.6349 | val_acc=0.4881\n",
      "  Época  55 | train_acc=0.6561 | val_acc=0.4945\n",
      "  Época  60 | train_acc=0.6511 | val_acc=0.4945\n",
      "  Época  65 | train_acc=0.6674 | val_acc=0.4844\n",
      "  Época  70 | train_acc=0.6900 | val_acc=0.4716\n",
      "  Época  75 | train_acc=0.6844 | val_acc=0.4808\n",
      "  Early stopping en época 75 (mejor val_acc=0.5027)\n",
      "[Fold 3/5] Global acc=0.4376\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.4729    0.5147    0.4929       441\n",
      "       Right     0.5154    0.4558    0.4838       441\n",
      "  Both Fists     0.3855    0.3855    0.3855       441\n",
      "   Both Feet     0.3841    0.3946    0.3893       441\n",
      "\n",
      "    accuracy                         0.4376      1764\n",
      "   macro avg     0.4395    0.4376    0.4379      1764\n",
      "weighted avg     0.4395    0.4376    0.4379      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.4779 | sujetos=21\n",
      "  Δ(FT-Global) = +0.0402\n",
      "[Fold 4/5] SupCon pretrain on 5880 trials...\n",
      "[SupCon] Epoch 005/40 | loss=5.2720\n",
      "[SupCon] Epoch 010/40 | loss=5.2680\n",
      "[SupCon] Epoch 015/40 | loss=5.2665\n",
      "[SupCon] Epoch 020/40 | loss=5.2659\n",
      "[SupCon] Epoch 025/40 | loss=5.2665\n",
      "[SupCon] Epoch 030/40 | loss=5.2653\n",
      "[SupCon] Epoch 035/40 | loss=5.2655\n",
      "[SupCon] Epoch 040/40 | loss=5.2649\n",
      "[SupCon] Preentrenamiento contrastivo completado (backbone inicializado).\n",
      "\n",
      "[Fold 4/5] Entrenando modelo global con validación interna por sujetos... (n_train=5880 | n_val=1092 | n_test=1680)\n",
      "  Época   5 | train_acc=0.4990 | val_acc=0.4405\n",
      "  Época  10 | train_acc=0.5158 | val_acc=0.4505\n",
      "  Época  15 | train_acc=0.5376 | val_acc=0.4762\n",
      "  Época  20 | train_acc=0.5541 | val_acc=0.4652\n",
      "  Época  25 | train_acc=0.5616 | val_acc=0.4835\n",
      "  Época  30 | train_acc=0.5740 | val_acc=0.4789\n",
      "  Época  35 | train_acc=0.6061 | val_acc=0.4753\n",
      "  Época  40 | train_acc=0.6075 | val_acc=0.4634\n",
      "  Época  45 | train_acc=0.6342 | val_acc=0.4872\n",
      "  Época  50 | train_acc=0.6417 | val_acc=0.4652\n",
      "  Época  55 | train_acc=0.6553 | val_acc=0.4634\n",
      "  Época  60 | train_acc=0.6733 | val_acc=0.4670\n",
      "  Época  65 | train_acc=0.6881 | val_acc=0.4652\n",
      "  Época  70 | train_acc=0.6908 | val_acc=0.4707\n",
      "  Época  75 | train_acc=0.7048 | val_acc=0.4524\n",
      "  Época  80 | train_acc=0.7026 | val_acc=0.4689\n",
      "  Época  85 | train_acc=0.7230 | val_acc=0.4707\n",
      "  Época  90 | train_acc=0.7400 | val_acc=0.4469\n",
      "  Época  95 | train_acc=0.7378 | val_acc=0.4661\n",
      "  Early stopping en época 95 (mejor val_acc=0.4872)\n",
      "[Fold 4/5] Global acc=0.4899\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5252    0.5452    0.5350       420\n",
      "       Right     0.5220    0.5643    0.5423       420\n",
      "  Both Fists     0.4033    0.4667    0.4327       420\n",
      "   Both Feet     0.5296    0.3833    0.4448       420\n",
      "\n",
      "    accuracy                         0.4899      1680\n",
      "   macro avg     0.4950    0.4899    0.4887      1680\n",
      "weighted avg     0.4950    0.4899    0.4887      1680\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5655 | sujetos=20\n",
      "  Δ(FT-Global) = +0.0756\n",
      "[Fold 5/5] SupCon pretrain on 5880 trials...\n",
      "[SupCon] Epoch 005/40 | loss=5.2713\n",
      "[SupCon] Epoch 010/40 | loss=5.2678\n",
      "[SupCon] Epoch 015/40 | loss=5.2661\n",
      "[SupCon] Epoch 020/40 | loss=5.2655\n",
      "[SupCon] Epoch 025/40 | loss=5.2656\n",
      "[SupCon] Epoch 030/40 | loss=5.2652\n",
      "[SupCon] Epoch 035/40 | loss=5.2648\n",
      "[SupCon] Epoch 040/40 | loss=5.2647\n",
      "[SupCon] Preentrenamiento contrastivo completado (backbone inicializado).\n",
      "\n",
      "[Fold 5/5] Entrenando modelo global con validación interna por sujetos... (n_train=5880 | n_val=1092 | n_test=1680)\n",
      "  Época   5 | train_acc=0.4934 | val_acc=0.4496\n",
      "  Época  10 | train_acc=0.5196 | val_acc=0.4551\n",
      "  Época  15 | train_acc=0.5199 | val_acc=0.4478\n",
      "  Época  20 | train_acc=0.5381 | val_acc=0.4515\n",
      "  Época  25 | train_acc=0.5478 | val_acc=0.4661\n",
      "  Época  30 | train_acc=0.5634 | val_acc=0.4505\n",
      "  Época  35 | train_acc=0.5772 | val_acc=0.4560\n",
      "  Época  40 | train_acc=0.5981 | val_acc=0.4441\n",
      "  Época  45 | train_acc=0.6128 | val_acc=0.4515\n",
      "  Época  50 | train_acc=0.6274 | val_acc=0.4643\n",
      "  Época  55 | train_acc=0.6490 | val_acc=0.4423\n",
      "  Época  60 | train_acc=0.6599 | val_acc=0.4451\n",
      "  Época  65 | train_acc=0.6755 | val_acc=0.4487\n",
      "  Época  70 | train_acc=0.6922 | val_acc=0.4451\n",
      "  Época  75 | train_acc=0.7017 | val_acc=0.4377\n",
      "  Early stopping en época 75 (mejor val_acc=0.4661)\n",
      "[Fold 5/5] Global acc=0.5095\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5927    0.6167    0.6044       420\n",
      "       Right     0.5241    0.5429    0.5333       420\n",
      "  Both Fists     0.4190    0.3881    0.4030       420\n",
      "   Both Feet     0.4916    0.4905    0.4911       420\n",
      "\n",
      "    accuracy                         0.5095      1680\n",
      "   macro avg     0.5069    0.5095    0.5079      1680\n",
      "weighted avg     0.5069    0.5095    0.5079      1680\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5512 | sujetos=20\n",
      "  Δ(FT-Global) = +0.0417\n",
      "\n",
      "============================================================\n",
      "RESULTADOS FINALES\n",
      "============================================================\n",
      "Global folds: ['0.4535', '0.5091', '0.4376', '0.4899', '0.5095']\n",
      "Global mean: 0.4799\n",
      "Fine-tune PROGRESIVO folds: ['0.5074', '0.5788', '0.4779', '0.5655', '0.5512']\n",
      "Fine-tune PROGRESIVO mean: 0.5361\n",
      "Δ(FT-Global) mean: +0.0562\n",
      "\n",
      "↳ Matriz de confusión guardada: confusion_global_allfolds.png\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Replicación fiel del paper \"A Deep Learning MI-EEG Classification Model for BCIs\"\n",
    "# Dose et al., EUSIPCO 2018 — ahora con EEGNet (Lawhern et al., 2018) en vez de ShallowConvNet\n",
    "# Protocolo: entrenamiento global inter-sujeto + FINE-TUNING PROGRESIVO por sujeto\n",
    "# (CV 4-fold, LRs discriminativos, L2-SP, early stopping con validación)\n",
    "# + SupCon preentrenamiento contrastivo supervisado (opcional)\n",
    "\n",
    "import os, re, math, random, json, itertools, copy\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold, StratifiedShuffleSplit, GroupShuffleSplit\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# CONFIGURACIÓN GENERAL\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "FOLDS_DIR = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dispositivo y semilla\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "# Escenario de clases\n",
    "CLASS_SCENARIO = '4c'\n",
    "WINDOW_MODE = '3s'\n",
    "FS = 160.0\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Entrenamiento global\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS_GLOBAL = 100\n",
    "LR = 1e-3\n",
    "# >>> Añadidos para diagnóstico/ES global <<<\n",
    "GLOBAL_VAL_SPLIT = 0.15   # fracción de sujetos (dentro del train) para validación\n",
    "GLOBAL_PATIENCE  = 10     # épocas sin mejora en val_acc\n",
    "LOG_EVERY        = 5      # log cada N épocas\n",
    "\n",
    "# Fine-tuning por sujeto (protocolo robusto)\n",
    "CALIB_CV_FOLDS = 4            # 4-fold CV por sujeto ~ 75/25\n",
    "FT_EPOCHS = 30                 # ES con validación\n",
    "FT_BASE_LR = 5e-5              # convs \"base\" (temporal/depthwise) — más bajo\n",
    "FT_HEAD_LR = 1e-3              # fc+out — más alto\n",
    "FT_L2SP = 1e-4                 # regularización suave\n",
    "FT_PATIENCE = 5                # early stopping con validación\n",
    "FT_VAL_RATIO = 0.2             # validación dentro del set de calibración\n",
    "\n",
    "# Sujetos excluidos\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "\n",
    "# Runs\n",
    "MI_RUNS_LR = [4, 8, 12]\n",
    "MI_RUNS_OF = [6, 10, 14]\n",
    "BASELINE_RUNS_EO = [1]\n",
    "\n",
    "# Canales (8 en lugar de 64)\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','CPz']\n",
    "\n",
    "# ====== SupCon (preentrenamiento contrastivo supervisado) ======\n",
    "USE_SUPCON_PRETRAIN = True      # <- pon False para desactivarlo\n",
    "SUPCON_EPOCHS = 40\n",
    "SUPCON_BATCH = 64               # intenta que sea >=64 si cabe en GPU\n",
    "SUPCON_LR = 1e-3\n",
    "SUPCON_TEMP = 0.07\n",
    "SUPCON_PROJ_DIM = 128\n",
    "SUPCON_LOG_EVERY = 5\n",
    "\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES DE CANALES\n",
    "# =========================\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = re.sub(r'([A-Za-z])0([0-9])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])Z$', r'\\1z', s)\n",
    "    s = s.replace('fp', 'Fp').replace('FP', 'Fp')\n",
    "    s = ''.join(ch.upper() if ch != 'z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {}\n",
    "    for ch in raw.ch_names:\n",
    "        lab = normalize_label(ch)\n",
    "        lab = lab[:-1] + 'z' if lab.endswith('Z') else lab\n",
    "        lab = re.sub(r'([A-Z])Z$', r'\\1z', lab)\n",
    "        mapping[ch] = lab\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired_channels=EXPECTED_8):\n",
    "    have = [ch for ch in desired_channels if ch in raw.ch_names]\n",
    "    missing = [ch for ch in desired_channels if ch not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"Warning: faltan canales {missing} en archivo {getattr(raw,'filenames', [''])[0]}\")\n",
    "        return None\n",
    "    # Reordenar y quedarse SOLO con los deseados (8)\n",
    "    raw.reorder_channels([ch for ch in raw.ch_names if ch in desired_channels] +\n",
    "                         [ch for ch in raw.ch_names if ch not in desired_channels])\n",
    "    raw.pick_channels(desired_channels, ordered=True)\n",
    "    return raw\n",
    "\n",
    "# =========================\n",
    "# LECTURA DE EDF y EVENTOS\n",
    "# =========================\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(run_id:int):\n",
    "    if run_id in MI_RUNS_LR: return 'LR'\n",
    "    if run_id in MI_RUNS_OF: return 'OF'\n",
    "    if run_id in BASELINE_RUNS_EO: return 'EO'\n",
    "    return None\n",
    "\n",
    "def read_raw_edf(path: Path):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        mont = mne.channels.make_standard_montage('standard_1020')\n",
    "        raw.set_montage(mont, on_missing='ignore')\n",
    "    except Exception:\n",
    "        pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad=\"auto\")\n",
    "    raw = ensure_channels_order(raw, EXPECTED_8)\n",
    "    if raw is None:\n",
    "        return None\n",
    "    # Band-pass opcional (desactivado)\n",
    "    #raw.filter(l_freq=8., h_freq=30., picks='eeg', method='iir', verbose=False)\n",
    "    return raw\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    if raw.annotations is None or len(raw.annotations) == 0:\n",
    "        return []\n",
    "    def _norm(s): return str(s).strip().upper().replace(' ', '')\n",
    "    res = []\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = _norm(desc)\n",
    "        if tag in ('T1','T2'):\n",
    "            res.append((float(onset), tag))\n",
    "    res.sort()\n",
    "    dedup = []\n",
    "    last_t1 = last_t2 = -1e9\n",
    "    for t, tag in res:\n",
    "        if tag == 'T1':\n",
    "            if (t - last_t1) >= 0.5: dedup.append((t, tag)); last_t1 = t\n",
    "        else:\n",
    "            if (t - last_t2) >= 0.5: dedup.append((t, tag)); last_t2 = t\n",
    "    return dedup\n",
    "\n",
    "# =========================\n",
    "# CONSTRUCCIÓN DE DATASETS\n",
    "# =========================\n",
    "def subjects_available():\n",
    "    subs = []\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try:\n",
    "            sid = int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir / f\"S{sid:03d}R{r:02d}.edf\").exists() for r in (MI_RUNS_LR + MI_RUNS_OF))\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "def extract_trials_from_run(edf_path: Path, scenario: str, window_mode: str):\n",
    "    subj, run = parse_subject_run(edf_path)\n",
    "    kind = run_kind(run)\n",
    "    if kind not in ('LR','OF','EO'):\n",
    "        return ([], [])\n",
    "\n",
    "    raw = read_raw_edf(edf_path)\n",
    "    if raw is None:\n",
    "        return ([], [])\n",
    "\n",
    "    data = raw.get_data()\n",
    "    fs = raw.info['sfreq']\n",
    "    assert abs(fs - FS) < 1e-6\n",
    "\n",
    "    out = []\n",
    "\n",
    "    if kind in ('LR','OF'):\n",
    "        events = collect_events_T1T2(raw)\n",
    "        if window_mode == '3s':\n",
    "            rel_start, rel_end = 0.0, 3.0\n",
    "        else:\n",
    "            rel_start, rel_end = -1.0, 5.0\n",
    "\n",
    "        for onset_sec, tag in events:\n",
    "            if kind == 'LR':\n",
    "                if tag == 'T1': label = 'L'\n",
    "                elif tag == 'T2': label = 'R'\n",
    "                else: continue\n",
    "            else:\n",
    "                if tag == 'T1': label = 'BFISTS'\n",
    "                elif tag == 'T2': label = 'BFEET'\n",
    "                else: continue\n",
    "\n",
    "            if scenario == '2c' and label not in ('L','R'):\n",
    "                continue\n",
    "            if scenario == '3c' and label not in ('L','R','BFISTS'):\n",
    "                continue\n",
    "            if scenario == '4c' and label not in ('L','R','BFISTS','BFEET'):\n",
    "                continue\n",
    "\n",
    "            s = int(round((raw.first_time + onset_sec + rel_start) * fs))\n",
    "            e = int(round((raw.first_time + onset_sec + rel_end) * fs))\n",
    "            if s < 0 or e > data.shape[1]:\n",
    "                continue\n",
    "\n",
    "            seg = data[:, s:e].T.astype(np.float32)\n",
    "            # Normalización por época canal-a-canal (z-score)\n",
    "            seg = (seg - seg.mean(axis=0, keepdims=True)) / (seg.std(axis=0, keepdims=True) + 1e-6)\n",
    "\n",
    "            if label == 'L':       y = 0\n",
    "            elif label == 'R':     y = 1\n",
    "            elif label == 'BFISTS':y = 2\n",
    "            elif label == 'BFEET': y = 3\n",
    "            else: continue\n",
    "\n",
    "            out.append((seg, y, subj))\n",
    "\n",
    "    elif kind == 'EO':\n",
    "        return ([], raw.ch_names)\n",
    "\n",
    "    return out, raw.ch_names\n",
    "\n",
    "def build_dataset_all(subjects, scenario='4c', window_mode='3s'):\n",
    "    X, y, groups = [], [], []\n",
    "    ch_template = None\n",
    "\n",
    "    for s in tqdm(subjects, desc=\"Construyendo dataset (RAW)\"):\n",
    "        sdir = DATA_RAW / f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "\n",
    "        trials_L, trials_R, trials_FISTS, trials_FEET = [], [], [], []\n",
    "\n",
    "        for r in MI_RUNS_LR:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 0: trials_L.append(seg)\n",
    "                elif lab == 1: trials_R.append(seg)\n",
    "\n",
    "        for r in MI_RUNS_OF:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 2: trials_FISTS.append(seg)\n",
    "                elif lab == 3: trials_FEET.append(seg)\n",
    "\n",
    "        need_per_class = 21\n",
    "        def pick(trials, n, rng):\n",
    "            if len(trials) < n:\n",
    "                idx = rng.choice(len(trials), size=n, replace=True)\n",
    "                return [trials[i] for i in idx]\n",
    "            rng.shuffle(trials)\n",
    "            return trials[:n]\n",
    "\n",
    "        rng = check_random_state(RANDOM_STATE + s)\n",
    "        if len(trials_L)==0 or len(trials_R)==0 or len(trials_FISTS)==0 or len(trials_FEET)==0:\n",
    "            continue\n",
    "\n",
    "        Lp  = pick(trials_L,     need_per_class, rng)\n",
    "        Rp  = pick(trials_R,     need_per_class, rng)\n",
    "        FIp = pick(trials_FISTS, need_per_class, rng)\n",
    "        FEp = pick(trials_FEET,  need_per_class, rng)\n",
    "\n",
    "        pack = [(Lp, 0), (Rp, 1), (FIp, 2), (FEp, 3)]\n",
    "        for segs, lab in pack:\n",
    "            for seg in segs:\n",
    "                X.append(seg)\n",
    "                y.append(lab)\n",
    "                groups.append(s)\n",
    "\n",
    "    X = np.stack(X, axis=0)\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "    groups = np.asarray(groups, dtype=np.int64)\n",
    "\n",
    "    n, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Dataset construido: N={n} | T={T} | C={C} | clases={n_classes} | sujetos únicos={len(np.unique(groups))}\")\n",
    "    return X, y, groups, ch_template\n",
    "\n",
    "# =========================\n",
    "# EEGNet (Lawhern et al., 2018) adaptado a (B,1,T,C)\n",
    "# =========================\n",
    "class EEGNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Entrada: x de forma (B, 1, T, C)  [T=tiempo, C=canales]\n",
    "    Bloques:\n",
    "      1) Temporal conv     : Conv2d(1 -> F1, (kernel_t,1), padding 'same'), BN, ELU\n",
    "      2) Depthwise (espacial): Conv2d(F1 -> F1*D, (1,C), groups=F1, BN, ELU, AvgPool(4,1), Dropout\n",
    "      3) Separable temporal: Depthwise temporal (k_sep,1) groups=F1*D + Pointwise 1x1 a F2, BN, ELU, AvgPool(8,1), Dropout\n",
    "      4) FC -> OUT\n",
    "    \"\"\"\n",
    "    def __init__(self, n_ch: int, n_classes: int,\n",
    "                 F1: int = 8, D: int = 2, kernel_t: int = 64, k_sep: int = 16,\n",
    "                 pool1_t: int = 4, pool2_t: int = 8, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.n_ch = n_ch\n",
    "        self.n_classes = n_classes\n",
    "        self.F1 = F1\n",
    "        self.D = D\n",
    "        self.F2 = F1 * D\n",
    "        self.kernel_t = kernel_t\n",
    "        self.k_sep = k_sep\n",
    "        self.pool1_t = pool1_t\n",
    "        self.pool2_t = pool2_t\n",
    "\n",
    "        # Bloque 1: temporal\n",
    "        self.conv_temporal = nn.Conv2d(1, F1, kernel_size=(kernel_t, 1),\n",
    "                                       padding=(kernel_t // 2, 0), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(F1)\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "        # Bloque 2: depthwise (espacial)\n",
    "        self.conv_depthwise = nn.Conv2d(F1, self.F2, kernel_size=(1, n_ch),\n",
    "                                        groups=F1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.F2)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=(pool1_t, 1), stride=(pool1_t, 1))\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "\n",
    "        # Bloque 3: separable temporal (depthwise temporal + pointwise)\n",
    "        self.conv_sep_depth = nn.Conv2d(self.F2, self.F2, kernel_size=(k_sep, 1),\n",
    "                                        groups=self.F2, padding=(k_sep // 2, 0), bias=False)\n",
    "        self.conv_sep_point = nn.Conv2d(self.F2, self.F2, kernel_size=(1, 1), bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.F2)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(pool2_t, 1), stride=(pool2_t, 1))\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Cabeza dinámica\n",
    "        self.fc = None\n",
    "        self.out = None\n",
    "        self._T_in = None\n",
    "\n",
    "    def _build_head(self, T_in: int, device: torch.device):\n",
    "        # Con padding 'same' en temporal y separable temporal,\n",
    "        # el tamaño temporal se reduce por los pools:\n",
    "        T1 = T_in // self.pool1_t\n",
    "        T2 = T1 // self.pool2_t\n",
    "        feat_dim = self.F2 * T2 * 1  # ancho=1 tras conv_depthwise (kernel (1,C))\n",
    "        self.fc = nn.Linear(feat_dim, 80, bias=True).to(device)\n",
    "        self.out = nn.Linear(80, self.n_classes, bias=True).to(device)\n",
    "        self._T_in = T_in\n",
    "\n",
    "    def ensure_head(self, T_in: int, device: torch.device):\n",
    "        if (self.fc is None) or (self.out is None) or (self._T_in != T_in):\n",
    "            self._build_head(T_in, device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B,1,T,C)\n",
    "        B, _, T, C = x.shape\n",
    "        self.ensure_head(T, x.device)\n",
    "\n",
    "        z = self.conv_temporal(x)\n",
    "        z = self.bn1(z); z = self.act(z)\n",
    "\n",
    "        z = self.conv_depthwise(z)   # (B, F2, T, 1)\n",
    "        z = self.bn2(z); z = self.act(z)\n",
    "        z = self.pool1(z)\n",
    "        z = self.drop1(z)\n",
    "\n",
    "        z = self.conv_sep_depth(z)\n",
    "        z = self.conv_sep_point(z)\n",
    "        z = self.bn3(z); z = self.act(z)\n",
    "        z = self.pool2(z)\n",
    "        z = self.drop2(z)\n",
    "\n",
    "        z = self.flatten(z)\n",
    "        z = self.fc(z); z = self.act(z)\n",
    "        z = self.out(z)\n",
    "        return z\n",
    "\n",
    "    def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Devuelve el embedding pre-logits tras fc+ELU (dim=80).\n",
    "        x: (B,1,T,C)\n",
    "        \"\"\"\n",
    "        B, _, T, C = x.shape\n",
    "        self.ensure_head(T, x.device)\n",
    "\n",
    "        z = self.conv_temporal(x); z = self.bn1(z); z = self.act(z)\n",
    "        z = self.conv_depthwise(z); z = self.bn2(z); z = self.act(z)\n",
    "        z = self.pool1(z); z = self.drop1(z)\n",
    "        z = self.conv_sep_depth(z); z = self.conv_sep_point(z)\n",
    "        z = self.bn3(z); z = self.act(z)\n",
    "        z = self.pool2(z); z = self.drop2(z)\n",
    "        z = self.flatten(z)\n",
    "        z = self.fc(z); z = self.act(z)   # embedding 80-D\n",
    "        return z\n",
    "\n",
    "# =========================\n",
    "# TORCH DATASET\n",
    "# =========================\n",
    "class EEGTrials(Dataset):\n",
    "    def __init__(self, X, y, groups):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.g = groups.astype(np.int64)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        x = np.expand_dims(x, 0)                 # (1, T, C)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx])\n",
    "\n",
    "CLASS_NAMES_4C = ['Left', 'Right', 'Both Fists', 'Both Feet']\n",
    "\n",
    "# ====== Augmentations EEG para SupCon ======\n",
    "class EEGAugment(torch.nn.Module):\n",
    "    def __init__(self, p_jitter=0.5, max_jitter=16,  # ~100 ms a 160 Hz\n",
    "                 p_noise=0.8, noise_std=0.02,\n",
    "                 p_tmask=0.4, tmask_max=32,          # ~200 ms\n",
    "                 p_cdrop=0.2, cdrop_max=1):          # drop 0-1 canales\n",
    "        super().__init__()\n",
    "        self.p_jitter = p_jitter\n",
    "        self.max_jitter = max_jitter\n",
    "        self.p_noise = p_noise\n",
    "        self.noise_std = noise_std\n",
    "        self.p_tmask = p_tmask\n",
    "        self.tmask_max = tmask_max\n",
    "        self.p_cdrop = p_cdrop\n",
    "        self.cdrop_max = cdrop_max\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Acepta:\n",
    "          - (1, T, C)  -> ejemplo único (3D)\n",
    "          - (B, 1, T, C) -> batch (4D)\n",
    "        Devuelve con la misma dimensionalidad de entrada.\n",
    "        \"\"\"\n",
    "        original_3d = False\n",
    "        if x.dim() == 3:           # (1, T, C)\n",
    "            original_3d = True\n",
    "            x = x.unsqueeze(0)     # -> (B=1, 1, T, C)\n",
    "        elif x.dim() != 4:\n",
    "            raise ValueError(f\"EEGAugment espera 3D o 4D, recibido {x.dim()}D\")\n",
    "\n",
    "        B, _, T, C = x.shape\n",
    "        out = x.clone()\n",
    "\n",
    "        # Jitter temporal\n",
    "        if torch.rand(1).item() < self.p_jitter and T > 2:\n",
    "            shift = int(torch.randint(-self.max_jitter, self.max_jitter + 1, (1,)).item())\n",
    "            if shift > 0:\n",
    "                out[:, :, shift:, :] = out[:, :, :-shift, :].clone()\n",
    "            elif shift < 0:\n",
    "                out[:, :, :shift, :] = out[:, :, -shift:, :].clone()\n",
    "\n",
    "        # Ruido gaussiano leve\n",
    "        if torch.rand(1).item() < self.p_noise:\n",
    "            out = out + torch.randn_like(out) * self.noise_std\n",
    "\n",
    "        # Time masking corto\n",
    "        if torch.rand(1).item() < self.p_tmask and T > 4:\n",
    "            w = int(torch.randint(1, self.tmask_max + 1, (1,)).item())\n",
    "            s = int(torch.randint(0, max(1, T - w), (1,)).item())\n",
    "            out[:, :, s:s + w, :] = 0.0\n",
    "\n",
    "        # Channel dropout\n",
    "        if torch.rand(1).item() < self.p_cdrop and C > 1:\n",
    "            k = int(torch.randint(1, self.cdrop_max + 1, (1,)).item())\n",
    "            ch = torch.randperm(C)[:k]\n",
    "            out[:, :, :, ch] = 0.0\n",
    "\n",
    "        if original_3d:\n",
    "            out = out.squeeze(0)   # vuelve a (1, T, C)\n",
    "        return out\n",
    "\n",
    "class ContrastiveTrials(Dataset):\n",
    "    \"\"\" Devuelve dos vistas aumentadas + etiqueta \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.aug = EEGAugment()\n",
    "\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]               # (T,C)\n",
    "        y = self.y[idx]\n",
    "        x = np.expand_dims(x, 0)      # (1,T,C)\n",
    "        x = torch.from_numpy(x)\n",
    "        v1 = self.aug(x.clone())\n",
    "        v2 = self.aug(x.clone())\n",
    "        return v1, v2, torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# ====== Proyección + pérdida SupCon ======\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, in_dim=80, proj_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, in_dim, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_dim, proj_dim, bias=True)\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        z = self.net(z)\n",
    "        z = torch.nn.functional.normalize(z, dim=-1)\n",
    "        return z\n",
    "\n",
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Supervised Contrastive Loss (Khosla et al.)\n",
    "    features: (B, n_views, D) normalizadas\n",
    "    labels  : (B,)\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.tau = temperature\n",
    "\n",
    "    def forward(self, features, labels):\n",
    "        device = features.device\n",
    "        B, V, D = features.shape\n",
    "        feat = features.view(B*V, D)\n",
    "        labels = labels.view(B)\n",
    "        labels = labels.contiguous().view(-1, 1)                    # (B,1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(device)        # (B,B)\n",
    "\n",
    "        sim = torch.div(torch.matmul(feat, feat.T), self.tau)       # (BV,BV)\n",
    "        logits_mask = torch.ones_like(sim) - torch.eye(B*V, device=device)\n",
    "        sim = sim * logits_mask\n",
    "\n",
    "        mask = mask.repeat(V, V)                                     # (BV,BV)\n",
    "\n",
    "        exp_sim = torch.exp(sim) * logits_mask\n",
    "        log_prob = sim - torch.log(exp_sim.sum(dim=1, keepdim=True) + 1e-12)\n",
    "\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(dim=1) / (mask.sum(dim=1) + 1e-12)\n",
    "        loss = - mean_log_prob_pos.mean()\n",
    "        return loss\n",
    "\n",
    "def supcon_pretrain(model: EEGNet, X_tr: np.ndarray, y_tr: np.ndarray,\n",
    "                    epochs: int = SUPCON_EPOCHS, batch_size: int = SUPCON_BATCH,\n",
    "                    lr: float = SUPCON_LR, temperature: float = SUPCON_TEMP,\n",
    "                    proj_dim: int = SUPCON_PROJ_DIM, log_every: int = SUPCON_LOG_EVERY):\n",
    "    \"\"\"\n",
    "    Preentrena el backbone de EEGNet con SupCon usando etiquetas (positivos = misma clase).\n",
    "    Usa dos vistas augmentadas por ensayo.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    # asegurar construcción de cabeza (necesitamos conocer T)\n",
    "    with torch.no_grad():\n",
    "        dummy = torch.from_numpy(X_tr[:2]).float().unsqueeze(1).to(DEVICE)\n",
    "        _ = model(dummy)\n",
    "\n",
    "    proj = ProjectionHead(in_dim=80, proj_dim=proj_dim).to(DEVICE)\n",
    "    crit = SupConLoss(temperature=temperature)\n",
    "    ds = ContrastiveTrials(X_tr, y_tr)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    # Entrenamos backbone + proyector\n",
    "    opt = optim.Adam(list(model.parameters()) + list(proj.parameters()), lr=lr)\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        running = 0.0\n",
    "        n = 0\n",
    "        for v1, v2, yb in dl:\n",
    "            v1 = v1.to(DEVICE)  # (B,1,T,C)\n",
    "            v2 = v2.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            z1 = model.forward_features(v1)         # (B,80)\n",
    "            z2 = model.forward_features(v2)         # (B,80)\n",
    "            p1 = proj(z1)                           # (B,D)\n",
    "            p2 = proj(z2)                           # (B,D)\n",
    "            feats = torch.stack([p1, p2], dim=1)    # (B,2,D)\n",
    "\n",
    "            loss = crit(feats, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            bs = yb.size(0)\n",
    "            running += loss.item() * bs\n",
    "            n += bs\n",
    "\n",
    "        if ep % log_every == 0:\n",
    "            print(f\"[SupCon] Epoch {ep:03d}/{epochs} | loss={running/max(1,n):.4f}\")\n",
    "\n",
    "    print(\"[SupCon] Preentrenamiento contrastivo completado (backbone inicializado).\")\n",
    "\n",
    "# =========================\n",
    "# ENTRENAMIENTO / EVALUACIÓN\n",
    "# =========================\n",
    "def train_epoch(model, loader, opt, criterion):\n",
    "    model.train()\n",
    "    for xb, yb, _ in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_with_preds(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for xb, yb, _ in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        pred = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "        y_pred.extend(pred)\n",
    "        y_true.extend(yb.numpy().tolist())\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    return y_true, y_pred, float(acc)\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes, title, fname):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.nan_to_num(cm_norm)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm_norm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "        plt.text(j, i, format(cm_norm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm_norm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def print_report(y_true, y_pred, classes):\n",
    "    print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
    "\n",
    "# =========================\n",
    "# FINE-TUNING PROGRESIVO por sujeto (con validación interna)\n",
    "# =========================\n",
    "def _param_groups(model, mode):\n",
    "    \"\"\"\n",
    "    En EEGNet:\n",
    "      - 'out'            : solo capa final (model.out)\n",
    "      - 'head'           : fc + out\n",
    "      - 'spatial+head'   : depthwise + separable + fc + out  (temporal queda congelada)\n",
    "    \"\"\"\n",
    "    if mode == 'out':\n",
    "        train = list(model.out.parameters())\n",
    "    elif mode == 'head':\n",
    "        train = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "    elif mode == 'spatial+head':\n",
    "        train = (list(model.conv_depthwise.parameters()) +\n",
    "                 list(model.bn2.parameters()) +\n",
    "                 list(model.conv_sep_depth.parameters()) +\n",
    "                 list(model.conv_sep_point.parameters()) +\n",
    "                 list(model.bn3.parameters()) +\n",
    "                 list(model.fc.parameters()) +\n",
    "                 list(model.out.parameters()))\n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "    return train\n",
    "\n",
    "def _freeze_for_mode(model, mode):\n",
    "    # Congelamos todo\n",
    "    for p in model.parameters(): p.requires_grad = False\n",
    "    # Siempre mantenemos CONGELADO el bloque temporal en este protocolo\n",
    "    # (conv_temporal + bn1)\n",
    "    if mode == 'out':\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'head':\n",
    "        for p in model.fc.parameters():  p.requires_grad = True\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'spatial+head':\n",
    "        for p in model.conv_depthwise.parameters(): p.requires_grad = True\n",
    "        for p in model.bn2.parameters():           p.requires_grad = True\n",
    "        for p in model.conv_sep_depth.parameters():p.requires_grad = True\n",
    "        for p in model.conv_sep_point.parameters():p.requires_grad = True\n",
    "        for p in model.bn3.parameters():           p.requires_grad = True\n",
    "        for p in model.fc.parameters():            p.requires_grad = True\n",
    "        for p in model.out.parameters():           p.requires_grad = True\n",
    "\n",
    "def _class_weights(y_np, n_classes):\n",
    "    counts = np.bincount(y_np, minlength=n_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    weights = counts.sum() / counts\n",
    "    weights = weights / weights.mean()\n",
    "    return torch.tensor(weights, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "def _train_one_mode(model, X_cal, y_cal, n_classes, mode,\n",
    "                    epochs=FT_EPOCHS, batch_size=16,\n",
    "                    head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                    l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO):\n",
    "    \"\"\"\n",
    "    Entrena en 'mode' con early stopping sobre un conjunto de validación interno.\n",
    "    Devuelve el modelo con los mejores pesos (por val loss).\n",
    "    \"\"\"\n",
    "    # Split Cal -> (train_cal, val_cal)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=RANDOM_STATE)\n",
    "    (tr_idx, va_idx), = sss.split(X_cal, y_cal)\n",
    "    Xtr, ytr = X_cal[tr_idx], y_cal[tr_idx]\n",
    "    Xva, yva = X_cal[va_idx], y_cal[va_idx]\n",
    "\n",
    "    ds_tr = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xtr).float().unsqueeze(1),\n",
    "        torch.from_numpy(ytr).long()\n",
    "    )\n",
    "    ds_va = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xva).float().unsqueeze(1),\n",
    "        torch.from_numpy(yva).long()\n",
    "    )\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    _freeze_for_mode(model, mode)\n",
    "\n",
    "    if mode == 'spatial+head':\n",
    "        # grupos con LR discriminativos\n",
    "        base_params = (list(model.conv_depthwise.parameters()) +\n",
    "                       list(model.bn2.parameters()) +\n",
    "                       list(model.conv_sep_depth.parameters()) +\n",
    "                       list(model.conv_sep_point.parameters()) +\n",
    "                       list(model.bn3.parameters()))\n",
    "        head_params = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "        train_params = base_params + head_params\n",
    "        opt = optim.Adam([\n",
    "            {\"params\": base_params, \"lr\": base_lr},\n",
    "            {\"params\": head_params, \"lr\": head_lr},\n",
    "        ])\n",
    "    else:\n",
    "        train_params = _param_groups(model, mode)\n",
    "        opt = optim.Adam(train_params, lr=head_lr)\n",
    "\n",
    "    ref = [p.detach().clone().to(p.device) for p in train_params]\n",
    "    class_w = _class_weights(ytr, n_classes)\n",
    "    crit = nn.CrossEntropyLoss(weight=class_w)\n",
    "\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_val = float('inf')\n",
    "    bad = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # --- train ---\n",
    "        model.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            # L2-SP hacia referencia de los parámetros que estamos entrenando\n",
    "            reg = 0.0\n",
    "            for p_cur, p_ref in zip(train_params, ref):\n",
    "                reg = reg + torch.sum((p_cur - p_ref)**2)\n",
    "            loss = loss + l2sp_lambda * reg\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # --- val ---\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            nval = 0\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                loss = crit(logits, yb)\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "                nval += xb.size(0)\n",
    "            val_loss /= max(1, nval)\n",
    "\n",
    "        if val_loss + 1e-7 < best_val:\n",
    "            best_val = val_loss\n",
    "            bad = 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_numpy(model, X_np, device):\n",
    "    model.eval()\n",
    "    xb = torch.from_numpy(X_np).float().unsqueeze(1).to(device)  # (N,1,T,C)\n",
    "    logits = model(xb)\n",
    "    return logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "def subject_cv_finetune_predict_progressive(model_global, Xs, ys, device,\n",
    "                                            n_splits=CALIB_CV_FOLDS, n_classes=4):\n",
    "    \"\"\"\n",
    "    Para un sujeto: 4-fold StratifiedKFold.\n",
    "      - En cada fold: 'out' → 'head' → 'spatial+head' (temporal congelado)\n",
    "      - Se elige el mejor en el split de holdout del sujeto.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    y_true_full = np.empty_like(ys)\n",
    "    y_pred_full = np.empty_like(ys)\n",
    "\n",
    "    for tr_idx, te_idx in skf.split(Xs, ys):\n",
    "        Xcal, ycal = Xs[tr_idx], ys[tr_idx]\n",
    "        Xho,  yho  = Xs[te_idx], ys[te_idx]\n",
    "\n",
    "        # Stage A: 'out'\n",
    "        m_out = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_out, Xcal, ycal, n_classes, mode='out',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_out = predict_numpy(m_out, Xho, device)\n",
    "\n",
    "        # Stage B: 'head'\n",
    "        m_head = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_head, Xcal, ycal, n_classes, mode='head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_head = predict_numpy(m_head, Xho, device)\n",
    "\n",
    "        # Stage C: 'spatial+head'\n",
    "        m_sp = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_sp, Xcal, ycal, n_classes, mode='spatial+head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                        l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_sp = predict_numpy(m_sp, Xho, device)\n",
    "\n",
    "        # mejor de las tres\n",
    "        accs = [ (yhat_out == yho).mean(), (yhat_head == yho).mean(), (yhat_sp == yho).mean() ]\n",
    "        best_idx = int(np.argmax(accs))\n",
    "        yhat_best = [yhat_out, yhat_head, yhat_sp][best_idx]\n",
    "\n",
    "        y_true_full[te_idx] = yho\n",
    "        y_pred_full[te_idx] = yhat_best\n",
    "\n",
    "    return y_true_full, y_pred_full\n",
    "\n",
    "# =========================\n",
    "# FOLDS JSON helpers\n",
    "# =========================\n",
    "def save_group_folds_json_with_indices(subject_ids_str, groups_array, n_splits, out_json_path,\n",
    "                                       created_by=\"dose_experiment\", description=None):\n",
    "    out_json_path = Path(out_json_path)\n",
    "    unique_subjects_int = sorted(np.unique(groups_array).tolist())\n",
    "    subject_ids = [f\"S{sid:03d}\" for sid in unique_subjects_int]\n",
    "\n",
    "    if len(subject_ids) < n_splits:\n",
    "        raise ValueError(f\"n_splits={n_splits} mayor que número de sujetos={len(subject_ids)}\")\n",
    "\n",
    "    groups = np.arange(len(subject_ids))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    folds = []\n",
    "    fold_i = 0\n",
    "    for train_idx_grp, test_idx_grp in gkf.split(groups, groups, groups):\n",
    "        fold_i += 1\n",
    "        train_sids = [subject_ids[int(i)] for i in train_idx_grp]\n",
    "        test_sids  = [subject_ids[int(i)] for i in test_idx_grp]\n",
    "\n",
    "        train_sids_int = [int(s[1:]) for s in train_sids]\n",
    "        test_sids_int  = [int(s[1:]) for s in test_sids]\n",
    "\n",
    "        tr_idx = np.where(np.isin(groups_array, train_sids_int))[0].tolist()\n",
    "        te_idx = np.where(np.isin(groups_array, test_sids_int))[0].tolist()\n",
    "\n",
    "        folds.append({\n",
    "            \"fold\": int(fold_i),\n",
    "            \"train\": train_sids,\n",
    "            \"test\": test_sids,\n",
    "            \"tr_idx\": tr_idx,\n",
    "            \"te_idx\": te_idx\n",
    "        })\n",
    "\n",
    "    payload = {\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"created_by\": created_by,\n",
    "        \"description\": description if description is not None else \"\",\n",
    "        \"n_splits\": int(n_splits),\n",
    "        \"n_subjects\": len(subject_ids),\n",
    "        \"subject_ids\": subject_ids,\n",
    "        \"folds\": folds\n",
    "    }\n",
    "\n",
    "    out_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Folds JSON con índices guardado → {out_json_path}\")\n",
    "    return out_json_path\n",
    "\n",
    "def load_group_folds_json(path_json, expected_subject_ids=None, strict_check=True):\n",
    "    path_json = Path(path_json)\n",
    "    if not path_json.exists():\n",
    "        raise FileNotFoundError(f\"No existe {path_json}\")\n",
    "    with open(path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        payload = json.load(f)\n",
    "\n",
    "    subj_json = payload.get(\"subject_ids\", [])\n",
    "    if expected_subject_ids is not None:\n",
    "        expected = sorted(list(expected_subject_ids))\n",
    "        if subj_json != expected:\n",
    "            msg = (\"Los subject_ids del JSON no coinciden con expected_subject_ids.\\n\"\n",
    "                   f\"JSON has {len(subj_json)} subjects, expected {len(expected)}.\\n\"\n",
    "                   f\"First 10 JSON: {subj_json[:10]}\\nFirst 10 expected: {expected[:10]}\")\n",
    "            if strict_check:\n",
    "                raise ValueError(msg)\n",
    "            else:\n",
    "                print(\"WARNING: \" + msg)\n",
    "    return payload\n",
    "\n",
    "# =========================\n",
    "# EXPERIMENTO\n",
    "# =========================\n",
    "def run_experiment(save_folds_json=True, folds_json_path=FOLDS_DIR, folds_json_description=\"GroupKFold folds for comparison\"):\n",
    "    \"\"\"\n",
    "    - Crea/lee JSON con folds por sujeto (incluye tr_idx/te_idx)\n",
    "    - Entrena modelo global por fold (inter-sujeto puro) con validación interna por sujetos + ES\n",
    "    - Evalúa Global acc en test\n",
    "    - Realiza Fine-Tuning PROGRESIVO por sujeto (4-fold CV) y reporta acc\n",
    "    \"\"\"\n",
    "    mne.set_log_level('WARNING')\n",
    "\n",
    "    # sujetos y dataset\n",
    "    subs = subjects_available()\n",
    "    print(f\"Sujetos elegibles: {len(subs)} → {subs[:10]}{'...' if len(subs)>10 else ''}\")\n",
    "\n",
    "    X, y, groups, chs = build_dataset_all(subs, scenario=CLASS_SCENARIO, window_mode=WINDOW_MODE)\n",
    "    N, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Listo para entrenar: N={N} | T={T} | C={C} | clases={n_classes} | sujetos={len(np.unique(groups))}\")\n",
    "\n",
    "    ds = EEGTrials(X, y, groups)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # preparar JSON folds\n",
    "    if folds_json_path is None:\n",
    "        folds_json_path = Path(\"folds\") / f\"group_folds_{N_FOLDS}splits.json\"\n",
    "    else:\n",
    "        folds_json_path = Path(folds_json_path)\n",
    "    folds_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    unique_subs = sorted(np.unique(groups).tolist())\n",
    "    subject_ids_str = [f\"S{s:03d}\" for s in unique_subs]\n",
    "\n",
    "    if not folds_json_path.exists():\n",
    "        if not save_folds_json:\n",
    "            raise FileNotFoundError(f\"Folds JSON no encontrado en {folds_json_path} y save_folds_json=False.\")\n",
    "        save_group_folds_json_with_indices(subject_ids_str, groups, n_splits=N_FOLDS,\n",
    "                                           out_json_path=folds_json_path,\n",
    "                                           created_by=\"Joel_Clasificador\",\n",
    "                                           description=folds_json_description)\n",
    "\n",
    "    payload = load_group_folds_json(folds_json_path, expected_subject_ids=subject_ids_str, strict_check=False)\n",
    "    folds = payload[\"folds\"]\n",
    "\n",
    "    # bucle por folds\n",
    "    global_folds = []\n",
    "    ft_prog_folds = []\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "\n",
    "    for f in folds:\n",
    "        fold = f[\"fold\"]\n",
    "        tr_idx = np.asarray(f.get(\"tr_idx\", []), dtype=int)\n",
    "        te_idx = np.asarray(f.get(\"te_idx\", []), dtype=int)\n",
    "\n",
    "        if tr_idx.size == 0 or te_idx.size == 0:\n",
    "            print(f\"Advertencia: fold {fold} sin índices tr/te válidos. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        # ===== Split de validación por SUJETOS dentro del set de entrenamiento =====\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=GLOBAL_VAL_SPLIT, random_state=RANDOM_STATE)\n",
    "        tr_subj_idx, va_subj_idx = next(gss.split(tr_idx, groups[tr_idx], groups[tr_idx]))\n",
    "        tr_sub_idx = tr_idx[tr_subj_idx]\n",
    "        va_idx     = tr_idx[va_subj_idx]\n",
    "\n",
    "        tr_loader = DataLoader(Subset(ds, tr_sub_idx), batch_size=BATCH_SIZE, shuffle=True,  drop_last=False)\n",
    "        va_loader = DataLoader(Subset(ds, va_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "        te_loader = DataLoader(Subset(ds, te_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "        # ===== EEGNet =====\n",
    "        model = EEGNet(n_ch=C, n_classes=n_classes, F1=8, D=2, kernel_t=64, k_sep=16,\n",
    "                       pool1_t=4, pool2_t=8, dropout=0.5).to(DEVICE)\n",
    "\n",
    "        # ===== PREENTRENAMIENTO SUPCON (opcional) =====\n",
    "        if USE_SUPCON_PRETRAIN:\n",
    "            X_tr_sup = X[tr_sub_idx]\n",
    "            y_tr_sup = y[tr_sub_idx]\n",
    "            print(f\"[Fold {fold}/{N_FOLDS}] SupCon pretrain on {len(X_tr_sup)} trials...\")\n",
    "            supcon_pretrain(model, X_tr_sup, y_tr_sup,\n",
    "                            epochs=SUPCON_EPOCHS, batch_size=SUPCON_BATCH,\n",
    "                            lr=SUPCON_LR, temperature=SUPCON_TEMP, proj_dim=SUPCON_PROJ_DIM,\n",
    "                            log_every=SUPCON_LOG_EVERY)\n",
    "\n",
    "        # Optimizador para entrenamiento supervisado\n",
    "        opt = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "        def _acc(loader):\n",
    "            return evaluate_with_preds(model, loader)[2]\n",
    "\n",
    "        # ===== Entrenamiento con logging + EARLY STOPPING por val_acc =====\n",
    "        print(f\"\\n[Fold {fold}/{N_FOLDS}] Entrenando modelo global con validación interna por sujetos...\"\n",
    "              f\" (n_train={len(tr_sub_idx)} | n_val={len(va_idx)} | n_test={len(te_idx)})\")\n",
    "\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        best_val = -1.0\n",
    "        bad = 0\n",
    "\n",
    "        for epoch in range(1, EPOCHS_GLOBAL + 1):\n",
    "            train_epoch(model, tr_loader, opt, criterion)\n",
    "\n",
    "            if epoch % LOG_EVERY == 0:\n",
    "                tr_acc = _acc(tr_loader)\n",
    "                va_acc = _acc(va_loader)\n",
    "                print(f\"  Época {epoch:3d} | train_acc={tr_acc:.4f} | val_acc={va_acc:.4f}\")\n",
    "\n",
    "                if va_acc > best_val + 1e-4:\n",
    "                    best_val = va_acc\n",
    "                    best_state = copy.deepcopy(model.state_dict())\n",
    "                    bad = 0\n",
    "                else:\n",
    "                    bad += 1\n",
    "                    if bad >= GLOBAL_PATIENCE:\n",
    "                        print(f\"  Early stopping en época {epoch} (mejor val_acc={best_val:.4f})\")\n",
    "                        break\n",
    "\n",
    "        # cargar mejor estado antes de evaluar en test\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "        # ===== Evaluación global (inter-sujeto puro) =====\n",
    "        y_true, y_pred, acc_global = evaluate_with_preds(model, te_loader)\n",
    "        global_folds.append(acc_global)\n",
    "        all_true.append(y_true); all_pred.append(y_pred)\n",
    "\n",
    "        print(f\"[Fold {fold}/{N_FOLDS}] Global acc={acc_global:.4f}\")\n",
    "        print_report(y_true, y_pred, CLASS_NAMES_4C)\n",
    "\n",
    "        # ---------- Fine-tuning PROGRESIVO por sujeto con 4-fold CV ----------\n",
    "        X_te, y_te, g_te = X[te_idx], y[te_idx], groups[te_idx]\n",
    "\n",
    "        y_true_ft_all, y_pred_ft_all = [], []\n",
    "        used_subjects = 0\n",
    "        for sid in np.unique(g_te):\n",
    "            idx = np.where(g_te == sid)[0]\n",
    "            Xs, ys = X_te[idx], y_te[idx]\n",
    "\n",
    "            # Seguridad\n",
    "            if len(ys) < CALIB_CV_FOLDS or len(np.unique(ys)) < 2:\n",
    "                continue\n",
    "\n",
    "            y_true_subj, y_pred_subj = subject_cv_finetune_predict_progressive(\n",
    "                model, Xs, ys, DEVICE, n_splits=CALIB_CV_FOLDS, n_classes=n_classes\n",
    "            )\n",
    "            y_true_ft_all.append(y_true_subj)\n",
    "            y_pred_ft_all.append(y_pred_subj)\n",
    "            used_subjects += 1\n",
    "\n",
    "        if len(y_true_ft_all) > 0:\n",
    "            y_true_ft_all = np.concatenate(y_true_ft_all)\n",
    "            y_pred_ft_all = np.concatenate(y_pred_ft_all)\n",
    "            acc_ft = (y_true_ft_all == y_pred_ft_all).mean()\n",
    "            print(f\"  Fine-tuning PROGRESIVO (por sujeto, {CALIB_CV_FOLDS}-fold CV) acc={acc_ft:.4f} | sujetos={used_subjects}\")\n",
    "            print(f\"  Δ(FT-Global) = {acc_ft - acc_global:+.4f}\")\n",
    "        else:\n",
    "            acc_ft = np.nan\n",
    "            print(\"  Fine-tuning PROGRESIVO no ejecutado (sujeto(s) con muestras insuficientes).\")\n",
    "\n",
    "        ft_prog_folds.append(acc_ft)\n",
    "\n",
    "    # ---------- resultados finales ----------\n",
    "    if len(all_true) > 0:\n",
    "        all_true = np.concatenate(all_true)\n",
    "        all_pred = np.concatenate(all_pred)\n",
    "    else:\n",
    "        all_true = np.array([], dtype=int)\n",
    "        all_pred = np.array([], dtype=int)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTADOS FINALES\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Global folds:\", [f\"{a:.4f}\" for a in global_folds])\n",
    "    if len(global_folds) > 0:\n",
    "        print(f\"Global mean: {np.mean(global_folds):.4f}\")\n",
    "\n",
    "    print(\"Fine-tune PROGRESIVO folds:\", [(\"nan\" if (a is None or np.isnan(a)) else f\"{a:.4f}\") for a in ft_prog_folds])\n",
    "    if len(ft_prog_folds) > 0:\n",
    "        print(f\"Fine-tune PROGRESIVO mean: {np.nanmean(ft_prog_folds):.4f}\")\n",
    "        print(f\"Δ(FT-Global) mean: {np.nanmean(ft_prog_folds) - np.mean(global_folds):+.4f}\")\n",
    "\n",
    "    # Matriz de confusión global (sobre todos los folds)\n",
    "    if all_true.size > 0:\n",
    "        plot_confusion(all_true, all_pred, CLASS_NAMES_4C,\n",
    "                       title=\"Confusion Matrix - Global Model (All Folds)\",\n",
    "                       fname=\"confusion_global_allfolds.png\")\n",
    "        print(\"\\n↳ Matriz de confusión guardada: confusion_global_allfolds.png\")\n",
    "\n",
    "    return {\n",
    "        \"global_folds\": global_folds,\n",
    "        \"ft_prog_folds\": ft_prog_folds,\n",
    "        \"all_true\": all_true,\n",
    "        \"all_pred\": all_pred,\n",
    "        \"folds_json_path\": str(folds_json_path)\n",
    "    }\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🧠 INICIANDO EXPERIMENTO CON EEGNet + FINE-TUNING PROGRESIVO (por sujeto, 4-fold CV)\")\n",
    "    print(f\"🔧 Configuración: {CLASS_SCENARIO}, {len(EXPECTED_8)} canales, {WINDOW_MODE}\")\n",
    "    print(f\"⚙️  FT: epochs={FT_EPOCHS}, base_lr={FT_BASE_LR}, head_lr={FT_HEAD_LR}, L2SP={FT_L2SP}, patience={FT_PATIENCE}, CV={CALIB_CV_FOLDS}\")\n",
    "    print(f\"🧲 SupCon: enabled={USE_SUPCON_PRETRAIN}, epochs={SUPCON_EPOCHS}, batch={SUPCON_BATCH}, temp={SUPCON_TEMP}\")\n",
    "    run_experiment()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
