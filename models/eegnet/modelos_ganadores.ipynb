{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e14fe640",
   "metadata": {},
   "source": [
    "# SHALLOWCNN + FINETUNING INTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c8e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Replicaci√≥n fiel del paper \"A Deep Learning MI-EEG Classification Model for BCIs\"\n",
    "# Dose et al., EUSIPCO 2018 ‚Äî Shallow CNN sobre RAW EEG (PhysioNet BCI2000)\n",
    "# Versi√≥n con FINE-TUNING progresivo por sujeto (CV 4-fold, LRs discriminativos, L2-SP, early stopping con validaci√≥n)\n",
    "\n",
    "import os, re, math, random, json, itertools, copy\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# CONFIGURACI√ìN GENERAL\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "FOLDS_DIR = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dispositivo y semilla\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "# Escenario de clases\n",
    "CLASS_SCENARIO = '4c'\n",
    "WINDOW_MODE = '3s'\n",
    "FS = 160.0\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Entrenamiento global\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS_GLOBAL = 100\n",
    "LR = 1e-3\n",
    "\n",
    "# Fine-tuning por sujeto (protocolo robusto)\n",
    "CALIB_CV_FOLDS = 4            # 4-fold CV por sujeto ~ 75/25\n",
    "FT_EPOCHS = 30                 # m√°s alto, pero con ES por validaci√≥n\n",
    "FT_BASE_LR = 5e-5              # convs (temporal+spatial) ‚Äî m√°s bajo\n",
    "FT_HEAD_LR = 1e-3              # fc+out ‚Äî m√°s alto\n",
    "FT_L2SP = 1e-4                 # regularizaci√≥n m√°s suave\n",
    "FT_PATIENCE = 5                # early stopping con validaci√≥n\n",
    "FT_VAL_RATIO = 0.2             # validaci√≥n dentro del set de calibraci√≥n\n",
    "\n",
    "# Sujetos excluidos\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "\n",
    "# Runs\n",
    "MI_RUNS_LR = [4, 8, 12]\n",
    "MI_RUNS_OF = [6, 10, 14]\n",
    "BASELINE_RUNS_EO = [1]\n",
    "\n",
    "# Canales (8 en lugar de 64)\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','CPz']\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES DE CANALES\n",
    "# =========================\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = re.sub(r'([A-Za-z])0([0-9])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])Z$', r'\\1z', s)\n",
    "    s = s.replace('fp', 'Fp').replace('FP', 'Fp')\n",
    "    s = ''.join(ch.upper() if ch != 'z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {}\n",
    "    for ch in raw.ch_names:\n",
    "        lab = normalize_label(ch)\n",
    "        lab = lab[:-1] + 'z' if lab.endswith('Z') else lab\n",
    "        lab = re.sub(r'([A-Z])Z$', r'\\1z', lab)\n",
    "        mapping[ch] = lab\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired_channels=EXPECTED_8):\n",
    "    have = [ch for ch in desired_channels if ch in raw.ch_names]\n",
    "    missing = [ch for ch in desired_channels if ch not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"Warning: faltan canales {missing} en archivo {getattr(raw,'filenames', [''])[0]}\")\n",
    "        return None\n",
    "    raw.reorder_channels([ch for ch in raw.ch_names if ch in desired_channels] +\n",
    "                         [ch for ch in raw.ch_names if ch not in desired_channels])\n",
    "    raw.pick_channels(desired_channels, ordered=True)\n",
    "    return raw\n",
    "\n",
    "# =========================\n",
    "# LECTURA DE EDF y EVENTOS\n",
    "# =========================\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(run_id:int):\n",
    "    if run_id in MI_RUNS_LR: return 'LR'\n",
    "    if run_id in MI_RUNS_OF: return 'OF'\n",
    "    if run_id in BASELINE_RUNS_EO: return 'EO'\n",
    "    return None\n",
    "\n",
    "def read_raw_edf(path: Path):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        mont = mne.channels.make_standard_montage('standard_1020')\n",
    "        raw.set_montage(mont, on_missing='ignore')\n",
    "    except Exception:\n",
    "        pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad=\"auto\")\n",
    "    raw = ensure_channels_order(raw, EXPECTED_8)\n",
    "    if raw is None:\n",
    "        return None\n",
    "    # Band-pass opcional (desactivado)\n",
    "    # raw.filter(l_freq=8., h_freq=30., picks='eeg', method='iir', verbose=False)\n",
    "    return raw\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    if raw.annotations is None or len(raw.annotations) == 0:\n",
    "        return []\n",
    "    def _norm(s): return str(s).strip().upper().replace(' ', '')\n",
    "    res = []\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = _norm(desc)\n",
    "        if tag in ('T1','T2'):\n",
    "            res.append((float(onset), tag))\n",
    "    res.sort()\n",
    "    dedup = []\n",
    "    last_t1 = last_t2 = -1e9\n",
    "    for t, tag in res:\n",
    "        if tag == 'T1':\n",
    "            if (t - last_t1) >= 0.5: dedup.append((t, tag)); last_t1 = t\n",
    "        else:\n",
    "            if (t - last_t2) >= 0.5: dedup.append((t, tag)); last_t2 = t\n",
    "    return dedup\n",
    "\n",
    "# =========================\n",
    "# CONSTRUCCI√ìN DE DATASETS\n",
    "# =========================\n",
    "def subjects_available():\n",
    "    subs = []\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try:\n",
    "            sid = int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir / f\"S{sid:03d}R{r:02d}.edf\").exists() for r in (MI_RUNS_LR + MI_RUNS_OF))\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "def extract_trials_from_run(edf_path: Path, scenario: str, window_mode: str):\n",
    "    subj, run = parse_subject_run(edf_path)\n",
    "    kind = run_kind(run)\n",
    "    if kind not in ('LR','OF','EO'):\n",
    "        return ([], [])\n",
    "\n",
    "    raw = read_raw_edf(edf_path)\n",
    "    if raw is None:\n",
    "        return ([], [])\n",
    "\n",
    "    data = raw.get_data()\n",
    "    fs = raw.info['sfreq']\n",
    "    assert abs(fs - FS) < 1e-6\n",
    "\n",
    "    out = []\n",
    "\n",
    "    if kind in ('LR','OF'):\n",
    "        events = collect_events_T1T2(raw)\n",
    "        if window_mode == '3s':\n",
    "            rel_start, rel_end = 0.0, 3.0\n",
    "        else:\n",
    "            rel_start, rel_end = -1.0, 5.0\n",
    "\n",
    "        for onset_sec, tag in events:\n",
    "            if kind == 'LR':\n",
    "                if tag == 'T1': label = 'L'\n",
    "                elif tag == 'T2': label = 'R'\n",
    "                else: continue\n",
    "            else:\n",
    "                if tag == 'T1': label = 'BFISTS'\n",
    "                elif tag == 'T2': label = 'BFEET'\n",
    "                else: continue\n",
    "\n",
    "            if scenario == '2c' and label not in ('L','R'):\n",
    "                continue\n",
    "            if scenario == '3c' and label not in ('L','R','BFISTS'):\n",
    "                continue\n",
    "            if scenario == '4c' and label not in ('L','R','BFISTS','BFEET'):\n",
    "                continue\n",
    "\n",
    "            s = int(round((raw.first_time + onset_sec + rel_start) * fs))\n",
    "            e = int(round((raw.first_time + onset_sec + rel_end) * fs))\n",
    "            if s < 0 or e > data.shape[1]:\n",
    "                continue\n",
    "\n",
    "            seg = data[:, s:e].T.astype(np.float32)\n",
    "            # Normalizaci√≥n por √©poca canal-a-canal (z-score)\n",
    "            seg = (seg - seg.mean(axis=0, keepdims=True)) / (seg.std(axis=0, keepdims=True) + 1e-6)\n",
    "\n",
    "            if label == 'L':       y = 0\n",
    "            elif label == 'R':     y = 1\n",
    "            elif label == 'BFISTS':y = 2\n",
    "            elif label == 'BFEET': y = 3\n",
    "            else: continue\n",
    "\n",
    "            out.append((seg, y, subj))\n",
    "\n",
    "    elif kind == 'EO':\n",
    "        return ([], raw.ch_names)\n",
    "\n",
    "    return out, raw.ch_names\n",
    "\n",
    "def build_dataset_all(subjects, scenario='4c', window_mode='3s'):\n",
    "    X, y, groups = [], [], []\n",
    "    ch_template = None\n",
    "\n",
    "    for s in tqdm(subjects, desc=\"Construyendo dataset (RAW)\"):\n",
    "        sdir = DATA_RAW / f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "\n",
    "        trials_L, trials_R, trials_FISTS, trials_FEET = [], [], [], []\n",
    "\n",
    "        for r in MI_RUNS_LR:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 0: trials_L.append(seg)\n",
    "                elif lab == 1: trials_R.append(seg)\n",
    "\n",
    "        for r in MI_RUNS_OF:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 2: trials_FISTS.append(seg)\n",
    "                elif lab == 3: trials_FEET.append(seg)\n",
    "\n",
    "        need_per_class = 21\n",
    "        def pick(trials, n, rng):\n",
    "            if len(trials) < n:\n",
    "                idx = rng.choice(len(trials), size=n, replace=True)\n",
    "                return [trials[i] for i in idx]\n",
    "            rng.shuffle(trials)\n",
    "            return trials[:n]\n",
    "\n",
    "        rng = check_random_state(RANDOM_STATE + s)\n",
    "        if len(trials_L)==0 or len(trials_R)==0 or len(trials_FISTS)==0 or len(trials_FEET)==0:\n",
    "            continue\n",
    "\n",
    "        Lp  = pick(trials_L,     need_per_class, rng)\n",
    "        Rp  = pick(trials_R,     need_per_class, rng)\n",
    "        FIp = pick(trials_FISTS, need_per_class, rng)\n",
    "        FEp = pick(trials_FEET,  need_per_class, rng)\n",
    "\n",
    "        pack = [(Lp, 0), (Rp, 1), (FIp, 2), (FEp, 3)]\n",
    "        for segs, lab in pack:\n",
    "            for seg in segs:\n",
    "                X.append(seg)\n",
    "                y.append(lab)\n",
    "                groups.append(s)\n",
    "\n",
    "    X = np.stack(X, axis=0)\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "    groups = np.asarray(groups, dtype=np.int64)\n",
    "\n",
    "    n, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Dataset construido: N={n} | T={T} | C={C} | clases={n_classes} | sujetos √∫nicos={len(np.unique(groups))}\")\n",
    "    return X, y, groups, ch_template\n",
    "\n",
    "# =========================\n",
    "# SHALLOW CNN\n",
    "# =========================\n",
    "class ShallowDose2018(nn.Module):\n",
    "    def __init__(self, n_ch: int, n_classes: int, kernel_t: int = 30, n_feat: int = 40, pool_t: int = 15):\n",
    "        super().__init__()\n",
    "        self.n_ch = n_ch\n",
    "        self.n_classes = n_classes\n",
    "        self.kernel_t = kernel_t\n",
    "        self.n_feat = n_feat\n",
    "        self.pool_t = pool_t\n",
    "\n",
    "        self.temporal = nn.Conv2d(1, n_feat, kernel_size=(kernel_t, 1),\n",
    "                                  padding=(kernel_t // 2, 0), bias=True)\n",
    "        self.spatial  = nn.Conv2d(n_feat, n_feat, kernel_size=(1, n_ch),\n",
    "                                  padding=(0, 0), bias=True)\n",
    "        self.avgpool  = nn.AvgPool2d(kernel_size=(pool_t, 1), stride=(pool_t, 1))\n",
    "        self.act      = nn.ELU()\n",
    "        self.flatten  = nn.Flatten()\n",
    "\n",
    "        self.fc  = None\n",
    "        self.out = None\n",
    "        self._T_in = None\n",
    "\n",
    "    def _build_head(self, T_in: int, device: torch.device):\n",
    "        T_pool = T_in // self.pool_t\n",
    "        feat_dim = self.n_feat * T_pool\n",
    "        self.fc  = nn.Linear(feat_dim, 80, bias=True).to(device)\n",
    "        self.out = nn.Linear(80, self.n_classes, bias=True).to(device)\n",
    "        self._T_in = T_in\n",
    "\n",
    "    def ensure_head(self, T_in: int, device: torch.device):\n",
    "        if (self.fc is None) or (self.out is None) or (self._T_in != T_in):\n",
    "            self._build_head(T_in, device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, _, T, C = x.shape\n",
    "        self.ensure_head(T, x.device)\n",
    "\n",
    "        z = self.temporal(x); z = self.act(z)\n",
    "        z = self.spatial(z);  z = self.act(z)\n",
    "        z = self.avgpool(z)\n",
    "        z = self.flatten(z)\n",
    "        z = self.fc(z); z = self.act(z)\n",
    "        z = self.out(z)\n",
    "        return z\n",
    "\n",
    "# =========================\n",
    "# TORCH DATASET\n",
    "# =========================\n",
    "class EEGTrials(Dataset):\n",
    "    def __init__(self, X, y, groups):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.g = groups.astype(np.int64)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        x = np.expand_dims(x, 0)                 # (1, T, C)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx])\n",
    "\n",
    "CLASS_NAMES_4C = ['Left', 'Right', 'Both Fists', 'Both Feet']\n",
    "\n",
    "# =========================\n",
    "# ENTRENAMIENTO / EVALUACI√ìN\n",
    "# =========================\n",
    "def train_epoch(model, loader, opt, criterion):\n",
    "    model.train()\n",
    "    for xb, yb, _ in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_with_preds(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for xb, yb, _ in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        pred = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "        y_pred.extend(pred)\n",
    "        y_true.extend(yb.numpy().tolist())\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    return y_true, y_pred, float(acc)\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes, title, fname):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.nan_to_num(cm_norm)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm_norm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "        plt.text(j, i, format(cm_norm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm_norm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def print_report(y_true, y_pred, classes):\n",
    "    print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
    "\n",
    "# =========================\n",
    "# FINE-TUNING PROGRESIVO por sujeto (con validaci√≥n interna)\n",
    "# =========================\n",
    "def _param_groups(model, mode):\n",
    "    \"\"\"\n",
    "    Devuelve los par√°metros a entrenar seg√∫n 'mode':\n",
    "      - 'out'            : solo capa final\n",
    "      - 'head'           : fc + out\n",
    "      - 'spatial+head'   : spatial + fc + out  (temporal queda congelada)\n",
    "    \"\"\"\n",
    "    if mode == 'out':\n",
    "        train = list(model.out.parameters())\n",
    "    elif mode == 'head':\n",
    "        train = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "    elif mode == 'spatial+head':\n",
    "        train = (list(model.spatial.parameters())\n",
    "                 + list(model.fc.parameters())\n",
    "                 + list(model.out.parameters()))\n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "    return train\n",
    "\n",
    "def _freeze_for_mode(model, mode):\n",
    "    # Primero congelamos todo\n",
    "    for p in model.parameters(): p.requires_grad = False\n",
    "    # Siempre dejamos la base temporal congelada en este protocolo\n",
    "    # Descongelamos seg√∫n 'mode'\n",
    "    if mode == 'out':\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'head':\n",
    "        for p in model.fc.parameters():  p.requires_grad = True\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'spatial+head':\n",
    "        for p in model.spatial.parameters(): p.requires_grad = True\n",
    "        for p in model.fc.parameters():      p.requires_grad = True\n",
    "        for p in model.out.parameters():     p.requires_grad = True\n",
    "\n",
    "def _class_weights(y_np, n_classes):\n",
    "    # Pesos inversos a la frecuencia por clase en el set de calibraci√≥n\n",
    "    counts = np.bincount(y_np, minlength=n_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    weights = counts.sum() / counts\n",
    "    weights = weights / weights.mean()\n",
    "    return torch.tensor(weights, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "def _make_optimizer(train_params, mode):\n",
    "    # LR m√°s alto para la cabeza, bajo para spatial cuando aplique\n",
    "    if mode == 'spatial+head':\n",
    "        # separar spatial vs head para LRs distintos\n",
    "        spatial, head = [], []\n",
    "        for p in train_params:\n",
    "            # heur√≠stica: par√°metros que pertenecen a spatial tendr√°n .shape acorde\n",
    "            # mejor: detectarlos por referencia al m√≥dulo\n",
    "            pass\n",
    "        # Como no tenemos tags aqu√≠, armamos dos grupos manualmente en la llamada principal.\n",
    "        # Devolvemos None y lo construimos fuera.\n",
    "        return None\n",
    "    else:\n",
    "        return optim.Adam(train_params, lr=FT_HEAD_LR)\n",
    "\n",
    "def _train_one_mode(model, X_cal, y_cal, n_classes, mode,\n",
    "                    epochs=FT_EPOCHS, batch_size=16,\n",
    "                    head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                    l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO):\n",
    "    \"\"\"\n",
    "    Entrena en 'mode' con early stopping sobre un conjunto de validaci√≥n interno.\n",
    "    Devuelve el modelo con los mejores pesos (por val loss).\n",
    "    \"\"\"\n",
    "    # Split Cal -> (train_cal, val_cal)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=RANDOM_STATE)\n",
    "    (tr_idx, va_idx), = sss.split(X_cal, y_cal)\n",
    "    Xtr, ytr = X_cal[tr_idx], y_cal[tr_idx]\n",
    "    Xva, yva = X_cal[va_idx], y_cal[va_idx]\n",
    "\n",
    "    # Datasets\n",
    "    ds_tr = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xtr).float().unsqueeze(1),\n",
    "        torch.from_numpy(ytr).long()\n",
    "    )\n",
    "    ds_va = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xva).float().unsqueeze(1),\n",
    "        torch.from_numpy(yva).long()\n",
    "    )\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    # Congelar / descongelar seg√∫n modo\n",
    "    _freeze_for_mode(model, mode)\n",
    "    # Par√°metros a entrenar y referencia L2-SP\n",
    "    if mode == 'spatial+head':\n",
    "        base_params = list(model.spatial.parameters())\n",
    "        head_params = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "        train_params = base_params + head_params\n",
    "        opt = optim.Adam([\n",
    "            {\"params\": base_params, \"lr\": base_lr},\n",
    "            {\"params\": head_params, \"lr\": head_lr},\n",
    "        ])\n",
    "    else:\n",
    "        train_params = _param_groups(model, mode)\n",
    "        opt = optim.Adam(train_params, lr=head_lr)\n",
    "\n",
    "    ref = [p.detach().clone().to(p.device) for p in train_params]\n",
    "    class_w = _class_weights(ytr, n_classes)\n",
    "    crit = nn.CrossEntropyLoss(weight=class_w)\n",
    "\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_val = float('inf')\n",
    "    bad = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # --- train ---\n",
    "        model.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            # L2-SP hacia referencia de los par√°metros que estamos entrenando\n",
    "            reg = 0.0\n",
    "            for p_cur, p_ref in zip(train_params, ref):\n",
    "                reg = reg + torch.sum((p_cur - p_ref)**2)\n",
    "            loss = loss + l2sp_lambda * reg\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # --- val ---\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            nval = 0\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                loss = crit(logits, yb)  # misma p√©rdida con pesos\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "                nval += xb.size(0)\n",
    "            val_loss /= max(1, nval)\n",
    "\n",
    "        if val_loss + 1e-7 < best_val:\n",
    "            best_val = val_loss\n",
    "            bad = 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_numpy(model, X_np, device):\n",
    "    model.eval()\n",
    "    xb = torch.from_numpy(X_np).float().unsqueeze(1).to(device)  # (N,1,T,C)\n",
    "    logits = model(xb)\n",
    "    return logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "def subject_cv_finetune_predict_progressive(model_global, Xs, ys, device,\n",
    "                                            n_splits=CALIB_CV_FOLDS, n_classes=4):\n",
    "    \"\"\"\n",
    "    Para un sujeto: 4-fold StratifiedKFold.\n",
    "      - En cada fold: se entrena con 3 modos progresivos:\n",
    "          1) 'out'            (solo capa final)\n",
    "          2) 'head'           (fc + out)\n",
    "          3) 'spatial+head'   (spatial + fc + out, temporal congelada)\n",
    "        Se eval√∫a en el test_fold y se elige el mejor.\n",
    "    Devuelve y_true_subj, y_pred_subj (OOF por fold).\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    y_true_full = np.empty_like(ys)\n",
    "    y_pred_full = np.empty_like(ys)\n",
    "\n",
    "    for tr_idx, te_idx in skf.split(Xs, ys):\n",
    "        Xcal, ycal = Xs[tr_idx], ys[tr_idx]\n",
    "        Xho,  yho  = Xs[te_idx], ys[te_idx]\n",
    "\n",
    "        # Stage A: 'out'\n",
    "        m_out = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_out, Xcal, ycal, n_classes, mode='out',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_out = predict_numpy(m_out, Xho, device)\n",
    "        acc_out = (yhat_out == yho).mean()\n",
    "\n",
    "        # Stage B: 'head'\n",
    "        m_head = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_head, Xcal, ycal, n_classes, mode='head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_head = predict_numpy(m_head, Xho, device)\n",
    "        acc_head = (yhat_head == yho).mean()\n",
    "\n",
    "        # Stage C: 'spatial+head'\n",
    "        m_sp = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_sp, Xcal, ycal, n_classes, mode='spatial+head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                        l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_sp = predict_numpy(m_sp, Xho, device)\n",
    "        acc_sp = (yhat_sp == yho).mean()\n",
    "\n",
    "        # Elegir mejor\n",
    "        best_idx = np.argmax([acc_out, acc_head, acc_sp])\n",
    "        yhat_best = [yhat_out, yhat_head, yhat_sp][best_idx]\n",
    "\n",
    "        y_true_full[te_idx] = yho\n",
    "        y_pred_full[te_idx] = yhat_best\n",
    "\n",
    "    return y_true_full, y_pred_full\n",
    "\n",
    "# =========================\n",
    "# FOLDS JSON helpers\n",
    "# =========================\n",
    "def save_group_folds_json_with_indices(subject_ids_str, groups_array, n_splits, out_json_path,\n",
    "                                       created_by=\"dose_experiment\", description=None):\n",
    "    out_json_path = Path(out_json_path)\n",
    "    unique_subjects_int = sorted(np.unique(groups_array).tolist())\n",
    "    subject_ids = [f\"S{sid:03d}\" for sid in unique_subjects_int]\n",
    "\n",
    "    if len(subject_ids) < n_splits:\n",
    "        raise ValueError(f\"n_splits={n_splits} mayor que n√∫mero de sujetos={len(subject_ids)}\")\n",
    "\n",
    "    groups = np.arange(len(subject_ids))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    folds = []\n",
    "    fold_i = 0\n",
    "    for train_idx_grp, test_idx_grp in gkf.split(groups, groups, groups):\n",
    "        fold_i += 1\n",
    "        train_sids = [subject_ids[int(i)] for i in train_idx_grp]\n",
    "        test_sids  = [subject_ids[int(i)] for i in test_idx_grp]\n",
    "\n",
    "        train_sids_int = [int(s[1:]) for s in train_sids]\n",
    "        test_sids_int  = [int(s[1:]) for s in test_sids]\n",
    "\n",
    "        tr_idx = np.where(np.isin(groups_array, train_sids_int))[0].tolist()\n",
    "        te_idx = np.where(np.isin(groups_array, test_sids_int))[0].tolist()\n",
    "\n",
    "        folds.append({\n",
    "            \"fold\": int(fold_i),\n",
    "            \"train\": train_sids,\n",
    "            \"test\": test_sids,\n",
    "            \"tr_idx\": tr_idx,\n",
    "            \"te_idx\": te_idx\n",
    "        })\n",
    "\n",
    "    payload = {\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"created_by\": created_by,\n",
    "        \"description\": description if description is not None else \"\",\n",
    "        \"n_splits\": int(n_splits),\n",
    "        \"n_subjects\": len(subject_ids),\n",
    "        \"subject_ids\": subject_ids,\n",
    "        \"folds\": folds\n",
    "    }\n",
    "\n",
    "    out_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Folds JSON con √≠ndices guardado ‚Üí {out_json_path}\")\n",
    "    return out_json_path\n",
    "\n",
    "def load_group_folds_json(path_json, expected_subject_ids=None, strict_check=True):\n",
    "    path_json = Path(path_json)\n",
    "    if not path_json.exists():\n",
    "        raise FileNotFoundError(f\"No existe {path_json}\")\n",
    "    with open(path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        payload = json.load(f)\n",
    "\n",
    "    subj_json = payload.get(\"subject_ids\", [])\n",
    "    if expected_subject_ids is not None:\n",
    "        expected = sorted(list(expected_subject_ids))\n",
    "        if subj_json != expected:\n",
    "            msg = (\"Los subject_ids del JSON no coinciden con expected_subject_ids.\\n\"\n",
    "                   f\"JSON has {len(subj_json)} subjects, expected {len(expected)}.\\n\"\n",
    "                   f\"First 10 JSON: {subj_json[:10]}\\nFirst 10 expected: {expected[:10]}\")\n",
    "            if strict_check:\n",
    "                raise ValueError(msg)\n",
    "            else:\n",
    "                print(\"WARNING: \" + msg)\n",
    "    return payload\n",
    "\n",
    "# =========================\n",
    "# EXPERIMENTO\n",
    "# =========================\n",
    "def run_experiment(save_folds_json=True, folds_json_path=FOLDS_DIR, folds_json_description=\"GroupKFold folds for comparison\"):\n",
    "    \"\"\"\n",
    "    - Crea/lee JSON con folds por sujeto (incluye tr_idx/te_idx)\n",
    "    - Entrena modelo global por fold (inter-sujeto puro)\n",
    "    - Eval√∫a Global acc en test\n",
    "    - Realiza Fine-Tuning PROGRESIVO por sujeto (4-fold CV) y reporta acc\n",
    "    \"\"\"\n",
    "    mne.set_log_level('WARNING')\n",
    "\n",
    "    # sujetos y dataset\n",
    "    subs = subjects_available()\n",
    "    print(f\"Sujetos elegibles: {len(subs)} ‚Üí {subs[:10]}{'...' if len(subs)>10 else ''}\")\n",
    "\n",
    "    X, y, groups, chs = build_dataset_all(subs, scenario=CLASS_SCENARIO, window_mode=WINDOW_MODE)\n",
    "    N, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Listo para entrenar: N={N} | T={T} | C={C} | clases={n_classes} | sujetos={len(np.unique(groups))}\")\n",
    "\n",
    "    ds = EEGTrials(X, y, groups)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # preparar JSON folds\n",
    "    if folds_json_path is None:\n",
    "        folds_json_path = Path(\"folds\") / f\"group_folds_{N_FOLDS}splits.json\"\n",
    "    else:\n",
    "        folds_json_path = Path(folds_json_path)\n",
    "    folds_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    unique_subs = sorted(np.unique(groups).tolist())\n",
    "    subject_ids_str = [f\"S{s:03d}\" for s in unique_subs]\n",
    "\n",
    "    if not folds_json_path.exists():\n",
    "        if not save_folds_json:\n",
    "            raise FileNotFoundError(f\"Folds JSON no encontrado en {folds_json_path} y save_folds_json=False.\")\n",
    "        save_group_folds_json_with_indices(subject_ids_str, groups, n_splits=N_FOLDS,\n",
    "                                           out_json_path=folds_json_path,\n",
    "                                           created_by=\"Joel_Clasificador\",\n",
    "                                           description=folds_json_description)\n",
    "\n",
    "    payload = load_group_folds_json(folds_json_path, expected_subject_ids=subject_ids_str, strict_check=False)\n",
    "    folds = payload[\"folds\"]\n",
    "\n",
    "    # bucle por folds\n",
    "    global_folds = []\n",
    "    ft_prog_folds = []\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "\n",
    "    for f in folds:\n",
    "        fold = f[\"fold\"]\n",
    "        tr_idx = np.asarray(f.get(\"tr_idx\", []), dtype=int)\n",
    "        te_idx = np.asarray(f.get(\"te_idx\", []), dtype=int)\n",
    "\n",
    "        if tr_idx.size == 0 or te_idx.size == 0:\n",
    "            print(f\"Advertencia: fold {fold} sin √≠ndices tr/te v√°lidos. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        tr_loader = DataLoader(Subset(ds, tr_idx), batch_size=BATCH_SIZE, shuffle=True,  drop_last=False)\n",
    "        te_loader = DataLoader(Subset(ds, te_idx), batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "        model = ShallowDose2018(n_ch=C, n_classes=n_classes).to(DEVICE)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "        print(f\"\\n[Fold {fold}/{N_FOLDS}] Entrenando modelo global... (n_train={len(tr_idx)} | n_test={len(te_idx)})\")\n",
    "        for epoch in range(1, EPOCHS_GLOBAL + 1):\n",
    "            train_epoch(model, tr_loader, opt, criterion)\n",
    "            if epoch % 20 == 0:\n",
    "                print(f\"  √âpoca {epoch}/{EPOCHS_GLOBAL}\")\n",
    "\n",
    "        # Evaluaci√≥n global (inter-sujeto puro)\n",
    "        y_true, y_pred, acc_global = evaluate_with_preds(model, te_loader)\n",
    "        global_folds.append(acc_global)\n",
    "        all_true.append(y_true); all_pred.append(y_pred)\n",
    "\n",
    "        print(f\"[Fold {fold}/{N_FOLDS}] Global acc={acc_global:.4f}\")\n",
    "        print_report(y_true, y_pred, CLASS_NAMES_4C)\n",
    "\n",
    "        # ---------- Fine-tuning PROGRESIVO por sujeto con 4-fold CV ----------\n",
    "        X_te, y_te, g_te = X[te_idx], y[te_idx], groups[te_idx]\n",
    "\n",
    "        y_true_ft_all, y_pred_ft_all = [], []\n",
    "        used_subjects = 0\n",
    "        for sid in np.unique(g_te):\n",
    "            idx = np.where(g_te == sid)[0]\n",
    "            Xs, ys = X_te[idx], y_te[idx]\n",
    "\n",
    "            # Seguridad: requiere al menos n_splits muestras (estratificado).\n",
    "            if len(ys) < CALIB_CV_FOLDS or len(np.unique(ys)) < 2:\n",
    "                continue\n",
    "\n",
    "            y_true_subj, y_pred_subj = subject_cv_finetune_predict_progressive(\n",
    "                model, Xs, ys, DEVICE, n_splits=CALIB_CV_FOLDS, n_classes=n_classes\n",
    "            )\n",
    "            y_true_ft_all.append(y_true_subj)\n",
    "            y_pred_ft_all.append(y_pred_subj)\n",
    "            used_subjects += 1\n",
    "\n",
    "        if len(y_true_ft_all) > 0:\n",
    "            y_true_ft_all = np.concatenate(y_true_ft_all)\n",
    "            y_pred_ft_all = np.concatenate(y_pred_ft_all)\n",
    "            acc_ft = (y_true_ft_all == y_pred_ft_all).mean()\n",
    "            print(f\"  Fine-tuning PROGRESIVO (por sujeto, {CALIB_CV_FOLDS}-fold CV) acc={acc_ft:.4f} | sujetos={used_subjects}\")\n",
    "            print(f\"  Œî(FT-Global) = {acc_ft - acc_global:+.4f}\")\n",
    "        else:\n",
    "            acc_ft = np.nan\n",
    "            print(\"  Fine-tuning PROGRESIVO no ejecutado (sujeto(s) con muestras insuficientes).\")\n",
    "\n",
    "        ft_prog_folds.append(acc_ft)\n",
    "\n",
    "    # ---------- resultados finales ----------\n",
    "    if len(all_true) > 0:\n",
    "        all_true = np.concatenate(all_true)\n",
    "        all_pred = np.concatenate(all_pred)\n",
    "    else:\n",
    "        all_true = np.array([], dtype=int)\n",
    "        all_pred = np.array([], dtype=int)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTADOS FINALES\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Global folds:\", [f\"{a:.4f}\" for a in global_folds])\n",
    "    if len(global_folds) > 0:\n",
    "        print(f\"Global mean: {np.mean(global_folds):.4f}\")\n",
    "\n",
    "    print(\"Fine-tune PROGRESIVO folds:\", [(\"nan\" if (a is None or np.isnan(a)) else f\"{a:.4f}\") for a in ft_prog_folds])\n",
    "    if len(ft_prog_folds) > 0:\n",
    "        print(f\"Fine-tune PROGRESIVO mean: {np.nanmean(ft_prog_folds):.4f}\")\n",
    "        print(f\"Œî(FT-Global) mean: {np.nanmean(ft_prog_folds) - np.mean(global_folds):+.4f}\")\n",
    "\n",
    "    # Matriz de confusi√≥n global (sobre todos los folds)\n",
    "    if all_true.size > 0:\n",
    "        plot_confusion(all_true, all_pred, CLASS_NAMES_4C,\n",
    "                       title=\"Confusion Matrix - Global Model (All Folds)\",\n",
    "                       fname=\"confusion_global_allfolds.png\")\n",
    "        print(\"\\n‚Ü≥ Matriz de confusi√≥n guardada: confusion_global_allfolds.png\")\n",
    "\n",
    "    return {\n",
    "        \"global_folds\": global_folds,\n",
    "        \"ft_prog_folds\": ft_prog_folds,\n",
    "        \"all_true\": all_true,\n",
    "        \"all_pred\": all_pred,\n",
    "        \"folds_json_path\": str(folds_json_path)\n",
    "    }\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üß† INICIANDO EXPERIMENTO CON FINE-TUNING PROGRESIVO (por sujeto, 4-fold CV)\")\n",
    "    print(f\"üîß Configuraci√≥n: {CLASS_SCENARIO}, {len(EXPECTED_8)} canales, {WINDOW_MODE}\")\n",
    "    print(f\"‚öôÔ∏è  FT: epochs={FT_EPOCHS}, base_lr={FT_BASE_LR}, head_lr={FT_HEAD_LR}, L2SP={FT_L2SP}, patience={FT_PATIENCE}, CV={CALIB_CV_FOLDS}\")\n",
    "    run_experiment()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b7cb93",
   "metadata": {},
   "source": [
    "### Su intra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bf0061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Experimento INTRA-subject CV completo (5 folds) con Shallow CNN + Feature Extraction\n",
    "# Adaptado del modelo ‚Äúinter‚Äù que compartiste\n",
    "\n",
    "import os, re, math, random, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# CONFIGURACI√ìN GENERAL\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "CLASS_SCENARIO = '4c'\n",
    "WINDOW_MODE = '3s'\n",
    "FS = 160.0\n",
    "N_FOLDS = 5\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS_GLOBAL = 50\n",
    "LR = 1e-3\n",
    "\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "MI_RUNS_LR = [4, 8, 12]\n",
    "MI_RUNS_OF = [6, 10, 14]\n",
    "EXPECTED_64 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','CPz']\n",
    "\n",
    "CLASSIFIER_TYPE = 'svm'  # 'svm' o 'logistic'\n",
    "CLASS_NAMES_4C = ['Left','Right','Both Fists','Both Feet']\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES DE CANALES\n",
    "# =========================\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = re.sub(r'([A-Za-z])0([0-9])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])Z$', r'\\1z', s)\n",
    "    s = s.replace('fp', 'Fp').replace('FP', 'Fp')\n",
    "    s = ''.join(ch.upper() if ch != 'z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {ch: normalize_label(ch) for ch in raw.ch_names}\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired_channels=EXPECTED_64) -> mne.io.BaseRaw:\n",
    "    have = [ch for ch in desired_channels if ch in raw.ch_names]\n",
    "    missing = [ch for ch in desired_channels if ch not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"Warning: faltan canales {missing} en archivo {getattr(raw,'filenames',[''])[0]}\")\n",
    "        return None\n",
    "    return raw.reorder_channels(desired_channels)\n",
    "\n",
    "# =========================\n",
    "# LECTURA DE EDF y EVENTOS\n",
    "# =========================\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(run_id:int):\n",
    "    if run_id in MI_RUNS_LR: return 'LR'\n",
    "    if run_id in MI_RUNS_OF: return 'OF'\n",
    "    return None\n",
    "\n",
    "def read_raw_edf(path: Path) -> mne.io.BaseRaw:\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        raw.set_montage(mne.channels.make_standard_montage('standard_1020'), on_missing='ignore')\n",
    "    except: pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad='auto')\n",
    "    return ensure_channels_order(raw)\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    if raw.annotations is None or len(raw.annotations) == 0: return []\n",
    "    res = []\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = str(desc).strip().upper().replace(' ','')\n",
    "        if tag in ('T1','T2'): res.append((float(onset), tag))\n",
    "    res.sort()\n",
    "    dedup = []\n",
    "    last_t1 = last_t2 = -1e9\n",
    "    for t, tag in res:\n",
    "        if tag=='T1' and (t-last_t1)>=0.5: dedup.append((t,tag)); last_t1=t\n",
    "        if tag=='T2' and (t-last_t2)>=0.5: dedup.append((t,tag)); last_t2=t\n",
    "    return dedup\n",
    "\n",
    "# =========================\n",
    "# CONSTRUCCI√ìN DE DATASET\n",
    "# =========================\n",
    "def subjects_available():\n",
    "    subs=[]\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try: sid=int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir/f\"S{sid:03d}R{r:02d}.edf\").exists() for r in MI_RUNS_LR+MI_RUNS_OF)\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "def extract_trials_from_run(edf_path: Path):\n",
    "    subj, run = parse_subject_run(edf_path)\n",
    "    kind = run_kind(run)\n",
    "    if kind not in ('LR','OF'): return []\n",
    "\n",
    "    raw = read_raw_edf(edf_path)\n",
    "    data = raw.get_data()\n",
    "    fs = raw.info['sfreq']\n",
    "    events = collect_events_T1T2(raw)\n",
    "\n",
    "    out=[]\n",
    "    for onset, tag in events:\n",
    "        if kind=='LR':\n",
    "            label=0 if tag=='T1' else 1\n",
    "        else:\n",
    "            label=2 if tag=='T1' else 3\n",
    "        s = int(round(onset*fs))\n",
    "        e = int(round((onset+3.0)*fs))\n",
    "        if s<0 or e>data.shape[1]: continue\n",
    "        seg=data[:,s:e].T.astype(np.float32)\n",
    "        out.append((seg,label,subj))\n",
    "    return out\n",
    "\n",
    "def build_dataset_all(subjects):\n",
    "    X,y,groups=[],[],[]\n",
    "    for s in tqdm(subjects,\"Construyendo dataset\"):\n",
    "        sdir = DATA_RAW/f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "        for r in MI_RUNS_LR+MI_RUNS_OF:\n",
    "            path = sdir/f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not path.exists(): continue\n",
    "            trials=extract_trials_from_run(path)\n",
    "            for seg,label,subj in trials:\n",
    "                X.append(seg); y.append(label); groups.append(subj)\n",
    "    X=np.stack(X,0); y=np.array(y,dtype=int); groups=np.array(groups,dtype=int)\n",
    "    print(f\"Dataset construido: N={X.shape[0]} | T={X.shape[1]} | C={X.shape[2]} | sujetos={len(np.unique(groups))}\")\n",
    "    return X,y,groups\n",
    "\n",
    "# =========================\n",
    "# TORCH DATASET\n",
    "# =========================\n",
    "class EEGTrials(Dataset):\n",
    "    def __init__(self,X,y,groups):\n",
    "        self.X=X.astype(np.float32)\n",
    "        self.y=y.astype(np.int64)\n",
    "        self.g=groups.astype(np.int64)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self,idx):\n",
    "        x=self.X[idx]; x=np.expand_dims(x,0)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx])\n",
    "\n",
    "# =========================\n",
    "# SHALLOW CNN\n",
    "# =========================\n",
    "class ShallowDose2018(nn.Module):\n",
    "    def __init__(self,n_ch,n_classes,kernel_t=30,n_feat=40,pool_t=15):\n",
    "        super().__init__()\n",
    "        self.temporal=nn.Conv2d(1,n_feat,(kernel_t,1),padding=(kernel_t//2,0))\n",
    "        self.spatial=nn.Conv2d(n_feat,n_feat,(1,n_ch))\n",
    "        self.avgpool=nn.AvgPool2d((pool_t,1),stride=(pool_t,1))\n",
    "        self.act=nn.ELU()\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.fc=None; self.out=None; self._T_in=None\n",
    "        self.n_classes=n_classes\n",
    "    def _build_head(self,T_in,device):\n",
    "        T_pool=T_in//15\n",
    "        feat_dim=40*T_pool\n",
    "        self.fc=nn.Linear(feat_dim,80).to(device)\n",
    "        self.out=nn.Linear(80,self.n_classes).to(device)\n",
    "        self._T_in=T_in\n",
    "    def ensure_head(self,T_in,device):\n",
    "        if self.fc is None or self.out is None or self._T_in!=T_in:\n",
    "            self._build_head(T_in,device)\n",
    "    def forward(self,x):\n",
    "        B,_,T,C=x.shape\n",
    "        self.ensure_head(T,x.device)\n",
    "        z=self.temporal(x); z=self.act(z)\n",
    "        z=self.spatial(z); z=self.act(z)\n",
    "        z=self.avgpool(z); z=self.flatten(z)\n",
    "        z=self.fc(z); z=self.act(z)\n",
    "        return self.out(z)\n",
    "    def extract_features(self,x):\n",
    "        B,_,T,C=x.shape\n",
    "        self.ensure_head(T,x.device)\n",
    "        z=self.temporal(x); z=self.act(z)\n",
    "        z=self.spatial(z); z=self.act(z)\n",
    "        z=self.avgpool(z); z=self.flatten(z)\n",
    "        z=self.fc(z); z=self.act(z)\n",
    "        return z\n",
    "\n",
    "# =========================\n",
    "# TRAIN + FEATURE EXTRACTION\n",
    "# =========================\n",
    "def train_epoch(model,loader,opt,criterion):\n",
    "    model.train()\n",
    "    for xb,yb,_ in loader:\n",
    "        xb,yb=xb.to(DEVICE),yb.to(DEVICE)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss=criterion(model(xb),yb)\n",
    "        loss.backward(); opt.step()\n",
    "\n",
    "def extract_features_from_model(model,X,batch_size=32):\n",
    "    model.eval()\n",
    "    feats=[]\n",
    "    with torch.no_grad():\n",
    "        for i in range(0,len(X),batch_size):\n",
    "            x_batch=torch.from_numpy(X[i:i+batch_size]).float().unsqueeze(1).to(DEVICE)\n",
    "            feats.append(model.extract_features(x_batch).cpu().numpy())\n",
    "    return np.concatenate(feats,0)\n",
    "\n",
    "# =========================\n",
    "# GLOBAL MODEL + INTRA-SUBJECT SVM\n",
    "# =========================\n",
    "def train_global_model(X, y, model_class=ShallowDose2018, epochs=EPOCHS_GLOBAL):\n",
    "    model = model_class(n_ch=X.shape[2], n_classes=len(np.unique(y))).to(DEVICE)\n",
    "    ds = EEGTrials(X, y, np.zeros(len(y)))\n",
    "    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    opt = optim.Adam(model.parameters(), lr=LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        train_epoch(model, loader, opt, criterion)\n",
    "    return model\n",
    "\n",
    "def intra_subject_svm(features, y, groups, n_splits=5, classifier_type='svm', class_names=None):\n",
    "    subjects = np.unique(groups)\n",
    "    all_accs = []\n",
    "    for s in subjects:\n",
    "        idx = np.where(groups == s)[0]\n",
    "        X_s, y_s = features[idx], y[idx]\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "        subj_accs = []\n",
    "        y_true_all, y_pred_all = [], []\n",
    "        for tr_idx, te_idx in skf.split(X_s, y_s):\n",
    "            X_tr, X_te = X_s[tr_idx], X_s[te_idx]\n",
    "            y_tr, y_te = y_s[tr_idx], y_s[te_idx]\n",
    "\n",
    "            if classifier_type == 'svm':\n",
    "                clf = SVC(kernel='linear', random_state=RANDOM_STATE)\n",
    "            else:\n",
    "                clf = LogisticRegression(max_iter=500, random_state=RANDOM_STATE)\n",
    "            clf.fit(X_tr, y_tr)\n",
    "            y_pred = clf.predict(X_te)\n",
    "            subj_accs.append((y_pred == y_te).mean())\n",
    "            y_true_all.extend(y_te)\n",
    "            y_pred_all.extend(y_pred)\n",
    "\n",
    "        mean_acc = np.mean(subj_accs)\n",
    "        all_accs.append(mean_acc)\n",
    "        print(f\"Sujeto {s} intra-subject acc={mean_acc:.4f}\")\n",
    "\n",
    "        # Matriz de confusi√≥n y classification report\n",
    "        if class_names is not None:\n",
    "            cm = confusion_matrix(y_true_all, y_pred_all)\n",
    "            print(f\"Confusion matrix sujeto {s}:\\n{cm}\")\n",
    "            print(f\"Classification report sujeto {s}:\\n{classification_report(y_true_all, y_pred_all, target_names=class_names)}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Promedio intra-subject accuracy: {np.mean(all_accs):.4f}\")\n",
    "    return all_accs\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "if __name__==\"__main__\":\n",
    "    subs = subjects_available()\n",
    "    X, y, groups = build_dataset_all(subs)\n",
    "\n",
    "    print(\"üîπ Entrenando modelo CNN global...\")\n",
    "    global_model = train_global_model(X, y)\n",
    "\n",
    "    print(\"üîπ Extrayendo features con modelo global...\")\n",
    "    features = extract_features_from_model(global_model, X)\n",
    "\n",
    "    print(\"üîπ Evaluando intra-subject CV usando SVM sobre features...\")\n",
    "    intra_subject_svm(features, y, groups, n_splits=5, classifier_type=CLASSIFIER_TYPE, class_names=CLASS_NAMES_4C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5365ee8",
   "metadata": {},
   "source": [
    "# EGGNET + FINETUNING INTER MEJOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e1ced05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Usando dispositivo: cuda\n",
      "üß† INICIANDO EXPERIMENTO CON EEGNet + FINE-TUNING PROGRESIVO (por sujeto, 4-fold CV)\n",
      "üîß Configuraci√≥n: 4c, 8 canales, 6s\n",
      "‚öôÔ∏è  FT: epochs=30, base_lr=5e-05, head_lr=0.001, L2SP=0.0001, patience=5, CV=4\n",
      "Sujetos elegibles: 103 ‚Üí [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Construyendo dataset (RAW):   0%|          | 0/103 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Construyendo dataset (RAW): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:12<00:00,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset construido: N=8652 | T=960 | C=8 | clases=4 | sujetos √∫nicos=103\n",
      "Listo para entrenar: N=8652 | T=960 | C=8 | clases=4 | sujetos=103\n",
      "\n",
      "[Fold 1/5] Entrenando modelo global con validaci√≥n interna por sujetos... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  √âpoca   5 | train_acc=0.4476 | val_acc=0.4267\n",
      "  √âpoca  10 | train_acc=0.4777 | val_acc=0.4304\n",
      "  √âpoca  15 | train_acc=0.4941 | val_acc=0.4359\n",
      "  √âpoca  20 | train_acc=0.4976 | val_acc=0.4551\n",
      "  √âpoca  25 | train_acc=0.5074 | val_acc=0.4469\n",
      "  √âpoca  30 | train_acc=0.5055 | val_acc=0.4652\n",
      "  √âpoca  35 | train_acc=0.5097 | val_acc=0.4689\n",
      "  √âpoca  40 | train_acc=0.5043 | val_acc=0.4698\n",
      "  √âpoca  45 | train_acc=0.5192 | val_acc=0.4734\n",
      "  √âpoca  50 | train_acc=0.5164 | val_acc=0.4771\n",
      "  √âpoca  55 | train_acc=0.5110 | val_acc=0.4625\n",
      "  √âpoca  60 | train_acc=0.5212 | val_acc=0.4734\n",
      "  √âpoca  65 | train_acc=0.5126 | val_acc=0.4634\n",
      "  √âpoca  70 | train_acc=0.5228 | val_acc=0.4780\n",
      "  √âpoca  75 | train_acc=0.5181 | val_acc=0.4670\n",
      "  √âpoca  80 | train_acc=0.5193 | val_acc=0.4808\n",
      "  √âpoca  85 | train_acc=0.5250 | val_acc=0.4661\n",
      "  √âpoca  90 | train_acc=0.5254 | val_acc=0.4789\n",
      "  √âpoca  95 | train_acc=0.5202 | val_acc=0.4844\n",
      "  √âpoca 100 | train_acc=0.5298 | val_acc=0.4799\n",
      "[Fold 1/5] Global acc=0.4405\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.4858    0.5442    0.5134       441\n",
      "       Right     0.4657    0.5850    0.5186       441\n",
      "  Both Fists     0.3597    0.3401    0.3497       441\n",
      "   Both Feet     0.4314    0.2925    0.3486       441\n",
      "\n",
      "    accuracy                         0.4405      1764\n",
      "   macro avg     0.4357    0.4405    0.4326      1764\n",
      "weighted avg     0.4357    0.4405    0.4326      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5295 | sujetos=21\n",
      "  Œî(FT-Global) = +0.0890\n",
      "\n",
      "[Fold 2/5] Entrenando modelo global con validaci√≥n interna por sujetos... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  √âpoca   5 | train_acc=0.4111 | val_acc=0.3947\n",
      "  √âpoca  10 | train_acc=0.4329 | val_acc=0.4011\n",
      "  √âpoca  15 | train_acc=0.4443 | val_acc=0.4148\n",
      "  √âpoca  20 | train_acc=0.4522 | val_acc=0.4258\n",
      "  √âpoca  25 | train_acc=0.4572 | val_acc=0.4194\n",
      "  √âpoca  30 | train_acc=0.4551 | val_acc=0.4396\n",
      "  √âpoca  35 | train_acc=0.4664 | val_acc=0.4277\n",
      "  √âpoca  40 | train_acc=0.4676 | val_acc=0.4240\n",
      "  √âpoca  45 | train_acc=0.4677 | val_acc=0.4313\n",
      "  √âpoca  50 | train_acc=0.4676 | val_acc=0.4322\n",
      "  √âpoca  55 | train_acc=0.4722 | val_acc=0.4304\n",
      "  √âpoca  60 | train_acc=0.4720 | val_acc=0.4267\n",
      "  √âpoca  65 | train_acc=0.4758 | val_acc=0.4341\n",
      "  √âpoca  70 | train_acc=0.4758 | val_acc=0.4533\n",
      "  √âpoca  75 | train_acc=0.4783 | val_acc=0.4451\n",
      "  √âpoca  80 | train_acc=0.4815 | val_acc=0.4414\n",
      "  √âpoca  85 | train_acc=0.4802 | val_acc=0.4405\n",
      "  √âpoca  90 | train_acc=0.4772 | val_acc=0.4405\n",
      "  √âpoca  95 | train_acc=0.4812 | val_acc=0.4332\n",
      "  √âpoca 100 | train_acc=0.4841 | val_acc=0.4451\n",
      "[Fold 2/5] Global acc=0.4887\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5573    0.4739    0.5123       441\n",
      "       Right     0.5435    0.5238    0.5335       441\n",
      "  Both Fists     0.4043    0.4694    0.4344       441\n",
      "   Both Feet     0.4757    0.4875    0.4815       441\n",
      "\n",
      "    accuracy                         0.4887      1764\n",
      "   macro avg     0.4952    0.4887    0.4904      1764\n",
      "weighted avg     0.4952    0.4887    0.4904      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5805 | sujetos=21\n",
      "  Œî(FT-Global) = +0.0918\n",
      "\n",
      "[Fold 3/5] Entrenando modelo global con validaci√≥n interna por sujetos... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  √âpoca   5 | train_acc=0.4153 | val_acc=0.4240\n",
      "  √âpoca  10 | train_acc=0.4450 | val_acc=0.4478\n",
      "  √âpoca  15 | train_acc=0.4584 | val_acc=0.4615\n",
      "  √âpoca  20 | train_acc=0.4624 | val_acc=0.4716\n",
      "  √âpoca  25 | train_acc=0.4743 | val_acc=0.4625\n",
      "  √âpoca  30 | train_acc=0.4815 | val_acc=0.4808\n",
      "  √âpoca  35 | train_acc=0.4834 | val_acc=0.4570\n",
      "  √âpoca  40 | train_acc=0.4891 | val_acc=0.4753\n",
      "  √âpoca  45 | train_acc=0.4884 | val_acc=0.4689\n",
      "  √âpoca  50 | train_acc=0.4926 | val_acc=0.4689\n",
      "  √âpoca  55 | train_acc=0.5014 | val_acc=0.4762\n",
      "  √âpoca  60 | train_acc=0.4962 | val_acc=0.4707\n",
      "  √âpoca  65 | train_acc=0.4971 | val_acc=0.4661\n",
      "  √âpoca  70 | train_acc=0.5003 | val_acc=0.4826\n",
      "  √âpoca  75 | train_acc=0.5083 | val_acc=0.4853\n",
      "  √âpoca  80 | train_acc=0.4979 | val_acc=0.4560\n",
      "  √âpoca  85 | train_acc=0.5017 | val_acc=0.4707\n",
      "  √âpoca  90 | train_acc=0.4988 | val_acc=0.4799\n",
      "  √âpoca  95 | train_acc=0.5041 | val_acc=0.4716\n",
      "  √âpoca 100 | train_acc=0.5072 | val_acc=0.4652\n",
      "[Fold 3/5] Global acc=0.4393\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.4908    0.5420    0.5151       441\n",
      "       Right     0.5186    0.4739    0.4953       441\n",
      "  Both Fists     0.3785    0.3356    0.3558       441\n",
      "   Both Feet     0.3706    0.4059    0.3874       441\n",
      "\n",
      "    accuracy                         0.4393      1764\n",
      "   macro avg     0.4396    0.4393    0.4384      1764\n",
      "weighted avg     0.4396    0.4393    0.4384      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5113 | sujetos=21\n",
      "  Œî(FT-Global) = +0.0720\n",
      "\n",
      "[Fold 4/5] Entrenando modelo global con validaci√≥n interna por sujetos... (n_train=5880 | n_val=1092 | n_test=1680)\n",
      "  √âpoca   5 | train_acc=0.4372 | val_acc=0.4194\n",
      "  √âpoca  10 | train_acc=0.4658 | val_acc=0.4359\n",
      "  √âpoca  15 | train_acc=0.4755 | val_acc=0.4625\n",
      "  √âpoca  20 | train_acc=0.4837 | val_acc=0.4542\n",
      "  √âpoca  25 | train_acc=0.4854 | val_acc=0.4460\n",
      "  √âpoca  30 | train_acc=0.4786 | val_acc=0.4542\n",
      "  √âpoca  35 | train_acc=0.4901 | val_acc=0.4679\n",
      "  √âpoca  40 | train_acc=0.4878 | val_acc=0.4597\n",
      "  √âpoca  45 | train_acc=0.5034 | val_acc=0.4698\n",
      "  √âpoca  50 | train_acc=0.5002 | val_acc=0.4634\n",
      "  √âpoca  55 | train_acc=0.5027 | val_acc=0.4744\n",
      "  √âpoca  60 | train_acc=0.5053 | val_acc=0.4643\n",
      "  √âpoca  65 | train_acc=0.5054 | val_acc=0.4734\n",
      "  √âpoca  70 | train_acc=0.5041 | val_acc=0.4597\n",
      "  √âpoca  75 | train_acc=0.5015 | val_acc=0.4670\n",
      "  √âpoca  80 | train_acc=0.5080 | val_acc=0.4689\n",
      "  √âpoca  85 | train_acc=0.5083 | val_acc=0.4652\n",
      "  √âpoca  90 | train_acc=0.5090 | val_acc=0.4652\n",
      "  √âpoca  95 | train_acc=0.5104 | val_acc=0.4716\n",
      "  √âpoca 100 | train_acc=0.5119 | val_acc=0.4753\n",
      "[Fold 4/5] Global acc=0.4369\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.4976    0.4952    0.4964       420\n",
      "       Right     0.5108    0.5071    0.5090       420\n",
      "  Both Fists     0.3590    0.4667    0.4058       420\n",
      "   Both Feet     0.3913    0.2786    0.3255       420\n",
      "\n",
      "    accuracy                         0.4369      1680\n",
      "   macro avg     0.4397    0.4369    0.4342      1680\n",
      "weighted avg     0.4397    0.4369    0.4342      1680\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5577 | sujetos=20\n",
      "  Œî(FT-Global) = +0.1208\n",
      "\n",
      "[Fold 5/5] Entrenando modelo global con validaci√≥n interna por sujetos... (n_train=5880 | n_val=1092 | n_test=1680)\n",
      "  √âpoca   5 | train_acc=0.3886 | val_acc=0.4011\n",
      "  √âpoca  10 | train_acc=0.4196 | val_acc=0.4368\n",
      "  √âpoca  15 | train_acc=0.4342 | val_acc=0.4322\n",
      "  √âpoca  20 | train_acc=0.4461 | val_acc=0.4414\n",
      "  √âpoca  25 | train_acc=0.4493 | val_acc=0.4441\n",
      "  √âpoca  30 | train_acc=0.4553 | val_acc=0.4405\n",
      "  √âpoca  35 | train_acc=0.4497 | val_acc=0.4368\n",
      "  √âpoca  40 | train_acc=0.4628 | val_acc=0.4560\n",
      "  √âpoca  45 | train_acc=0.4682 | val_acc=0.4579\n",
      "  √âpoca  50 | train_acc=0.4687 | val_acc=0.4588\n",
      "  √âpoca  55 | train_acc=0.4706 | val_acc=0.4625\n",
      "  √âpoca  60 | train_acc=0.4730 | val_acc=0.4570\n",
      "  √âpoca  65 | train_acc=0.4636 | val_acc=0.4542\n",
      "  √âpoca  70 | train_acc=0.4796 | val_acc=0.4725\n",
      "  √âpoca  75 | train_acc=0.4813 | val_acc=0.4606\n",
      "  √âpoca  80 | train_acc=0.4794 | val_acc=0.4579\n",
      "  √âpoca  85 | train_acc=0.4709 | val_acc=0.4625\n",
      "  √âpoca  90 | train_acc=0.4850 | val_acc=0.4734\n",
      "  √âpoca  95 | train_acc=0.4820 | val_acc=0.4670\n",
      "  √âpoca 100 | train_acc=0.4796 | val_acc=0.4469\n",
      "[Fold 5/5] Global acc=0.4685\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5140    0.6119    0.5587       420\n",
      "       Right     0.5230    0.4881    0.5049       420\n",
      "  Both Fists     0.4103    0.2833    0.3352       420\n",
      "   Both Feet     0.4137    0.4905    0.4488       420\n",
      "\n",
      "    accuracy                         0.4685      1680\n",
      "   macro avg     0.4652    0.4685    0.4619      1680\n",
      "weighted avg     0.4652    0.4685    0.4619      1680\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5619 | sujetos=20\n",
      "  Œî(FT-Global) = +0.0935\n",
      "\n",
      "============================================================\n",
      "RESULTADOS FINALES\n",
      "============================================================\n",
      "Global folds: ['0.4405', '0.4887', '0.4393', '0.4369', '0.4685']\n",
      "Global mean: 0.4548\n",
      "Fine-tune PROGRESIVO folds: ['0.5295', '0.5805', '0.5113', '0.5577', '0.5619']\n",
      "Fine-tune PROGRESIVO mean: 0.5482\n",
      "Œî(FT-Global) mean: +0.0934\n",
      "\n",
      "‚Ü≥ Matriz de confusi√≥n guardada: confusion_global_allfolds.png\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Replicaci√≥n fiel del paper \"A Deep Learning MI-EEG Classification Model for BCIs\"\n",
    "# Dose et al., EUSIPCO 2018 ‚Äî ahora con EEGNet (Lawhern et al., 2018) en vez de ShallowConvNet\n",
    "# Protocolo: entrenamiento global inter-sujeto + FINE-TUNING PROGRESIVO por sujeto\n",
    "# (CV 4-fold, LRs discriminativos, L2-SP, early stopping con validaci√≥n)\n",
    "\n",
    "import os, re, math, random, json, itertools, copy\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold, StratifiedShuffleSplit, GroupShuffleSplit\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# CONFIGURACI√ìN GENERAL\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "FOLDS_DIR = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dispositivo y semilla\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "# Escenario de clases\n",
    "CLASS_SCENARIO = '4c'\n",
    "WINDOW_MODE = '6s'\n",
    "FS = 160.0\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Entrenamiento global\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS_GLOBAL = 100\n",
    "LR = 1e-3\n",
    "# >>> A√±adidos para diagn√≥stico/ES global <<<\n",
    "GLOBAL_VAL_SPLIT = 0.15   # fracci√≥n de sujetos (dentro del train) para validaci√≥n\n",
    "GLOBAL_PATIENCE  = 10     # √©pocas sin mejora en val_acc\n",
    "LOG_EVERY        = 5      # log cada N √©pocas\n",
    "\n",
    "# Fine-tuning por sujeto (protocolo robusto)\n",
    "CALIB_CV_FOLDS = 4            # 4-fold CV por sujeto ~ 75/25\n",
    "FT_EPOCHS = 30                 # ES con validaci√≥n\n",
    "FT_BASE_LR = 5e-5              # convs \"base\" (temporal/depthwise) ‚Äî m√°s bajo\n",
    "FT_HEAD_LR = 1e-3              # fc+out ‚Äî m√°s alto\n",
    "FT_L2SP = 1e-4                 # regularizaci√≥n suave\n",
    "FT_PATIENCE = 5                # early stopping con validaci√≥n\n",
    "FT_VAL_RATIO = 0.2             # validaci√≥n dentro del set de calibraci√≥n\n",
    "\n",
    "# Sujetos excluidos\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "\n",
    "# Runs\n",
    "MI_RUNS_LR = [4, 8, 12]\n",
    "MI_RUNS_OF = [6, 10, 14]\n",
    "BASELINE_RUNS_EO = [1]\n",
    "\n",
    "# Canales (8 en lugar de 64)\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','FCz']\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES DE CANALES\n",
    "# =========================\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = re.sub(r'([A-Za-z])0([0-9])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])Z$', r'\\1z', s)\n",
    "    s = s.replace('fp', 'Fp').replace('FP', 'Fp')\n",
    "    s = ''.join(ch.upper() if ch != 'z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {}\n",
    "    for ch in raw.ch_names:\n",
    "        lab = normalize_label(ch)\n",
    "        lab = lab[:-1] + 'z' if lab.endswith('Z') else lab\n",
    "        lab = re.sub(r'([A-Z])Z$', r'\\1z', lab)\n",
    "        mapping[ch] = lab\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired_channels=EXPECTED_8):\n",
    "    have = [ch for ch in desired_channels if ch in raw.ch_names]\n",
    "    missing = [ch for ch in desired_channels if ch not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"Warning: faltan canales {missing} en archivo {getattr(raw,'filenames', [''])[0]}\")\n",
    "        return None\n",
    "    # Reordenar y quedarse SOLO con los deseados (8)\n",
    "    raw.reorder_channels([ch for ch in raw.ch_names if ch in desired_channels] +\n",
    "                         [ch for ch in raw.ch_names if ch not in desired_channels])\n",
    "    raw.pick_channels(desired_channels, ordered=True)\n",
    "    return raw\n",
    "\n",
    "# =========================\n",
    "# LECTURA DE EDF y EVENTOS\n",
    "# =========================\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(run_id:int):\n",
    "    if run_id in MI_RUNS_LR: return 'LR'\n",
    "    if run_id in MI_RUNS_OF: return 'OF'\n",
    "    if run_id in BASELINE_RUNS_EO: return 'EO'\n",
    "    return None\n",
    "\n",
    "def read_raw_edf(path: Path):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        mont = mne.channels.make_standard_montage('standard_1020')\n",
    "        raw.set_montage(mont, on_missing='ignore')\n",
    "    except Exception:\n",
    "        pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad=\"auto\")\n",
    "    raw = ensure_channels_order(raw, EXPECTED_8)\n",
    "    if raw is None:\n",
    "        return None\n",
    "    # Band-pass opcional (desactivado)\n",
    "    #raw.filter(l_freq=8., h_freq=30., picks='eeg', method='iir', verbose=False)\n",
    "    return raw\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    if raw.annotations is None or len(raw.annotations) == 0:\n",
    "        return []\n",
    "    def _norm(s): return str(s).strip().upper().replace(' ', '')\n",
    "    res = []\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = _norm(desc)\n",
    "        if tag in ('T1','T2'):\n",
    "            res.append((float(onset), tag))\n",
    "    res.sort()\n",
    "    dedup = []\n",
    "    last_t1 = last_t2 = -1e9\n",
    "    for t, tag in res:\n",
    "        if tag == 'T1':\n",
    "            if (t - last_t1) >= 0.5: dedup.append((t, tag)); last_t1 = t\n",
    "        else:\n",
    "            if (t - last_t2) >= 0.5: dedup.append((t, tag)); last_t2 = t\n",
    "    return dedup\n",
    "\n",
    "# =========================\n",
    "# CONSTRUCCI√ìN DE DATASETS\n",
    "# =========================\n",
    "def subjects_available():\n",
    "    subs = []\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try:\n",
    "            sid = int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir / f\"S{sid:03d}R{r:02d}.edf\").exists() for r in (MI_RUNS_LR + MI_RUNS_OF))\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "def extract_trials_from_run(edf_path: Path, scenario: str, window_mode: str):\n",
    "    subj, run = parse_subject_run(edf_path)\n",
    "    kind = run_kind(run)\n",
    "    if kind not in ('LR','OF','EO'):\n",
    "        return ([], [])\n",
    "\n",
    "    raw = read_raw_edf(edf_path)\n",
    "    if raw is None:\n",
    "        return ([], [])\n",
    "\n",
    "    data = raw.get_data()\n",
    "    fs = raw.info['sfreq']\n",
    "    assert abs(fs - FS) < 1e-6\n",
    "\n",
    "    out = []\n",
    "\n",
    "    if kind in ('LR','OF'):\n",
    "        events = collect_events_T1T2(raw)\n",
    "        if window_mode == '3s':\n",
    "            rel_start, rel_end = 0.0, 3.0\n",
    "        else:\n",
    "            rel_start, rel_end = -1.0, 5.0\n",
    "\n",
    "        for onset_sec, tag in events:\n",
    "            if kind == 'LR':\n",
    "                if tag == 'T1': label = 'L'\n",
    "                elif tag == 'T2': label = 'R'\n",
    "                else: continue\n",
    "            else:\n",
    "                if tag == 'T1': label = 'BFISTS'\n",
    "                elif tag == 'T2': label = 'BFEET'\n",
    "                else: continue\n",
    "\n",
    "            if scenario == '2c' and label not in ('L','R'):\n",
    "                continue\n",
    "            if scenario == '3c' and label not in ('L','R','BFISTS'):\n",
    "                continue\n",
    "            if scenario == '4c' and label not in ('L','R','BFISTS','BFEET'):\n",
    "                continue\n",
    "\n",
    "            s = int(round((raw.first_time + onset_sec + rel_start) * fs))\n",
    "            e = int(round((raw.first_time + onset_sec + rel_end) * fs))\n",
    "            if s < 0 or e > data.shape[1]:\n",
    "                continue\n",
    "\n",
    "            seg = data[:, s:e].T.astype(np.float32)\n",
    "            # Normalizaci√≥n por √©poca canal-a-canal (z-score)\n",
    "            seg = (seg - seg.mean(axis=0, keepdims=True)) / (seg.std(axis=0, keepdims=True) + 1e-6)\n",
    "\n",
    "            if label == 'L':       y = 0\n",
    "            elif label == 'R':     y = 1\n",
    "            elif label == 'BFISTS':y = 2\n",
    "            elif label == 'BFEET': y = 3\n",
    "            else: continue\n",
    "\n",
    "            out.append((seg, y, subj))\n",
    "\n",
    "    elif kind == 'EO':\n",
    "        return ([], raw.ch_names)\n",
    "\n",
    "    return out, raw.ch_names\n",
    "\n",
    "def build_dataset_all(subjects, scenario='4c', window_mode='3s'):\n",
    "    X, y, groups = [], [], []\n",
    "    ch_template = None\n",
    "\n",
    "    for s in tqdm(subjects, desc=\"Construyendo dataset (RAW)\"):\n",
    "        sdir = DATA_RAW / f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "\n",
    "        trials_L, trials_R, trials_FISTS, trials_FEET = [], [], [], []\n",
    "\n",
    "        for r in MI_RUNS_LR:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 0: trials_L.append(seg)\n",
    "                elif lab == 1: trials_R.append(seg)\n",
    "\n",
    "        for r in MI_RUNS_OF:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 2: trials_FISTS.append(seg)\n",
    "                elif lab == 3: trials_FEET.append(seg)\n",
    "\n",
    "        need_per_class = 21\n",
    "        def pick(trials, n, rng):\n",
    "            if len(trials) < n:\n",
    "                idx = rng.choice(len(trials), size=n, replace=True)\n",
    "                return [trials[i] for i in idx]\n",
    "            rng.shuffle(trials)\n",
    "            return trials[:n]\n",
    "\n",
    "        rng = check_random_state(RANDOM_STATE + s)\n",
    "        if len(trials_L)==0 or len(trials_R)==0 or len(trials_FISTS)==0 or len(trials_FEET)==0:\n",
    "            continue\n",
    "\n",
    "        Lp  = pick(trials_L,     need_per_class, rng)\n",
    "        Rp  = pick(trials_R,     need_per_class, rng)\n",
    "        FIp = pick(trials_FISTS, need_per_class, rng)\n",
    "        FEp = pick(trials_FEET,  need_per_class, rng)\n",
    "\n",
    "        pack = [(Lp, 0), (Rp, 1), (FIp, 2), (FEp, 3)]\n",
    "        for segs, lab in pack:\n",
    "            for seg in segs:\n",
    "                X.append(seg)\n",
    "                y.append(lab)\n",
    "                groups.append(s)\n",
    "\n",
    "    X = np.stack(X, axis=0)\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "    groups = np.asarray(groups, dtype=np.int64)\n",
    "\n",
    "    n, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Dataset construido: N={n} | T={T} | C={C} | clases={n_classes} | sujetos √∫nicos={len(np.unique(groups))}\")\n",
    "    return X, y, groups, ch_template\n",
    "\n",
    "# =========================\n",
    "# EEGNet (Lawhern et al., 2018) adaptado a (B,1,T,C)\n",
    "# =========================\n",
    "class EEGNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Entrada: x de forma (B, 1, T, C)  [T=tiempo, C=canales]\n",
    "    Bloques:\n",
    "      1) Temporal conv     : Conv2d(1 -> F1, (kernel_t,1), padding 'same'), BN, ELU\n",
    "      2) Depthwise (espacial): Conv2d(F1 -> F1*D, (1,C), groups=F1, BN, ELU, AvgPool(4,1), Dropout\n",
    "      3) Separable temporal: Depthwise temporal (k_sep,1) groups=F1*D + Pointwise 1x1 a F2, BN, ELU, AvgPool(8,1), Dropout\n",
    "      4) FC -> OUT\n",
    "    \"\"\"\n",
    "    def __init__(self, n_ch: int, n_classes: int,\n",
    "                 F1: int = 8, D: int = 2, kernel_t: int = 64, k_sep: int = 16,\n",
    "                 pool1_t: int = 4, pool2_t: int = 8, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.n_ch = n_ch\n",
    "        self.n_classes = n_classes\n",
    "        self.F1 = F1\n",
    "        self.D = D\n",
    "        self.F2 = F1 * D\n",
    "        self.kernel_t = kernel_t\n",
    "        self.k_sep = k_sep\n",
    "        self.pool1_t = pool1_t\n",
    "        self.pool2_t = pool2_t\n",
    "\n",
    "        # Bloque 1: temporal\n",
    "        self.conv_temporal = nn.Conv2d(1, F1, kernel_size=(kernel_t, 1),\n",
    "                                       padding=(kernel_t // 2, 0), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(F1)\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "        # Bloque 2: depthwise (espacial)\n",
    "        self.conv_depthwise = nn.Conv2d(F1, self.F2, kernel_size=(1, n_ch),\n",
    "                                        groups=F1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.F2)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=(pool1_t, 1), stride=(pool1_t, 1))\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "\n",
    "        # Bloque 3: separable temporal (depthwise temporal + pointwise)\n",
    "        self.conv_sep_depth = nn.Conv2d(self.F2, self.F2, kernel_size=(k_sep, 1),\n",
    "                                        groups=self.F2, padding=(k_sep // 2, 0), bias=False)\n",
    "        self.conv_sep_point = nn.Conv2d(self.F2, self.F2, kernel_size=(1, 1), bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.F2)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(pool2_t, 1), stride=(pool2_t, 1))\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Cabeza din√°mica\n",
    "        self.fc = None\n",
    "        self.out = None\n",
    "        self._T_in = None\n",
    "\n",
    "    def _build_head(self, T_in: int, device: torch.device):\n",
    "        # Con padding 'same' en temporal y separable temporal,\n",
    "        # el tama√±o temporal se reduce por los pools:\n",
    "        T1 = T_in // self.pool1_t\n",
    "        T2 = T1 // self.pool2_t\n",
    "        feat_dim = self.F2 * T2 * 1  # ancho=1 tras conv_depthwise (kernel (1,C))\n",
    "        self.fc = nn.Linear(feat_dim, 80, bias=True).to(device)\n",
    "        self.out = nn.Linear(80, self.n_classes, bias=True).to(device)\n",
    "        self._T_in = T_in\n",
    "\n",
    "    def ensure_head(self, T_in: int, device: torch.device):\n",
    "        if (self.fc is None) or (self.out is None) or (self._T_in != T_in):\n",
    "            self._build_head(T_in, device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B,1,T,C)\n",
    "        B, _, T, C = x.shape\n",
    "        self.ensure_head(T, x.device)\n",
    "\n",
    "        z = self.conv_temporal(x)\n",
    "        z = self.bn1(z); z = self.act(z)\n",
    "\n",
    "        z = self.conv_depthwise(z)   # (B, F2, T, 1)\n",
    "        z = self.bn2(z); z = self.act(z)\n",
    "        z = self.pool1(z)\n",
    "        z = self.drop1(z)\n",
    "\n",
    "        z = self.conv_sep_depth(z)\n",
    "        z = self.conv_sep_point(z)\n",
    "        z = self.bn3(z); z = self.act(z)\n",
    "        z = self.pool2(z)\n",
    "        z = self.drop2(z)\n",
    "\n",
    "        z = self.flatten(z)\n",
    "        z = self.fc(z); z = self.act(z)\n",
    "        z = self.out(z)\n",
    "        return z\n",
    "\n",
    "# =========================\n",
    "# TORCH DATASET\n",
    "# =========================\n",
    "class EEGTrials(Dataset):\n",
    "    def __init__(self, X, y, groups):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.g = groups.astype(np.int64)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        x = np.expand_dims(x, 0)                 # (1, T, C)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx])\n",
    "\n",
    "CLASS_NAMES_4C = ['Left', 'Right', 'Both Fists', 'Both Feet']\n",
    "\n",
    "# =========================\n",
    "# ENTRENAMIENTO / EVALUACI√ìN\n",
    "# =========================\n",
    "def train_epoch(model, loader, opt, criterion):\n",
    "    model.train()\n",
    "    for xb, yb, _ in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_with_preds(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for xb, yb, _ in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        pred = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "        y_pred.extend(pred)\n",
    "        y_true.extend(yb.numpy().tolist())\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    return y_true, y_pred, float(acc)\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes, title, fname):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.nan_to_num(cm_norm)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm_norm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "        plt.text(j, i, format(cm_norm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm_norm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def print_report(y_true, y_pred, classes):\n",
    "    print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
    "\n",
    "# =========================\n",
    "# FINE-TUNING PROGRESIVO por sujeto (con validaci√≥n interna)\n",
    "# =========================\n",
    "def _param_groups(model, mode):\n",
    "    \"\"\"\n",
    "    En EEGNet:\n",
    "      - 'out'            : solo capa final (model.out)\n",
    "      - 'head'           : fc + out\n",
    "      - 'spatial+head'   : depthwise + separable + fc + out  (temporal queda congelada)\n",
    "    \"\"\"\n",
    "    if mode == 'out':\n",
    "        train = list(model.out.parameters())\n",
    "    elif mode == 'head':\n",
    "        train = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "    elif mode == 'spatial+head':\n",
    "        train = (list(model.conv_depthwise.parameters()) +\n",
    "                 list(model.bn2.parameters()) +\n",
    "                 list(model.conv_sep_depth.parameters()) +\n",
    "                 list(model.conv_sep_point.parameters()) +\n",
    "                 list(model.bn3.parameters()) +\n",
    "                 list(model.fc.parameters()) +\n",
    "                 list(model.out.parameters()))\n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "    return train\n",
    "\n",
    "def _freeze_for_mode(model, mode):\n",
    "    # Congelamos todo\n",
    "    for p in model.parameters(): p.requires_grad = False\n",
    "    # Siempre mantenemos CONGELADO el bloque temporal en este protocolo\n",
    "    # (conv_temporal + bn1)\n",
    "    if mode == 'out':\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'head':\n",
    "        for p in model.fc.parameters():  p.requires_grad = True\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'spatial+head':\n",
    "        for p in model.conv_depthwise.parameters(): p.requires_grad = True\n",
    "        for p in model.bn2.parameters():           p.requires_grad = True\n",
    "        for p in model.conv_sep_depth.parameters():p.requires_grad = True\n",
    "        for p in model.conv_sep_point.parameters():p.requires_grad = True\n",
    "        for p in model.bn3.parameters():           p.requires_grad = True\n",
    "        for p in model.fc.parameters():            p.requires_grad = True\n",
    "        for p in model.out.parameters():           p.requires_grad = True\n",
    "\n",
    "def _class_weights(y_np, n_classes):\n",
    "    counts = np.bincount(y_np, minlength=n_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    weights = counts.sum() / counts\n",
    "    weights = weights / weights.mean()\n",
    "    return torch.tensor(weights, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "def _train_one_mode(model, X_cal, y_cal, n_classes, mode,\n",
    "                    epochs=FT_EPOCHS, batch_size=16,\n",
    "                    head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                    l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO):\n",
    "    \"\"\"\n",
    "    Entrena en 'mode' con early stopping sobre un conjunto de validaci√≥n interno.\n",
    "    Devuelve el modelo con los mejores pesos (por val loss).\n",
    "    \"\"\"\n",
    "    # Split Cal -> (train_cal, val_cal)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=RANDOM_STATE)\n",
    "    (tr_idx, va_idx), = sss.split(X_cal, y_cal)\n",
    "    Xtr, ytr = X_cal[tr_idx], y_cal[tr_idx]\n",
    "    Xva, yva = X_cal[va_idx], y_cal[va_idx]\n",
    "\n",
    "    ds_tr = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xtr).float().unsqueeze(1),\n",
    "        torch.from_numpy(ytr).long()\n",
    "    )\n",
    "    ds_va = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xva).float().unsqueeze(1),\n",
    "        torch.from_numpy(yva).long()\n",
    "    )\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    _freeze_for_mode(model, mode)\n",
    "\n",
    "    if mode == 'spatial+head':\n",
    "        # grupos con LR discriminativos\n",
    "        base_params = (list(model.conv_depthwise.parameters()) +\n",
    "                       list(model.bn2.parameters()) +\n",
    "                       list(model.conv_sep_depth.parameters()) +\n",
    "                       list(model.conv_sep_point.parameters()) +\n",
    "                       list(model.bn3.parameters()))\n",
    "        head_params = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "        train_params = base_params + head_params\n",
    "        opt = optim.Adam([\n",
    "            {\"params\": base_params, \"lr\": base_lr},\n",
    "            {\"params\": head_params, \"lr\": head_lr},\n",
    "        ])\n",
    "    else:\n",
    "        train_params = _param_groups(model, mode)\n",
    "        opt = optim.Adam(train_params, lr=head_lr)\n",
    "\n",
    "    ref = [p.detach().clone().to(p.device) for p in train_params]\n",
    "    class_w = _class_weights(ytr, n_classes)\n",
    "    crit = nn.CrossEntropyLoss(weight=class_w)\n",
    "\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_val = float('inf')\n",
    "    bad = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # --- train ---\n",
    "        model.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            # L2-SP hacia referencia de los par√°metros que estamos entrenando\n",
    "            reg = 0.0\n",
    "            for p_cur, p_ref in zip(train_params, ref):\n",
    "                reg = reg + torch.sum((p_cur - p_ref)**2)\n",
    "            loss = loss + l2sp_lambda * reg\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # --- val ---\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            nval = 0\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                loss = crit(logits, yb)\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "                nval += xb.size(0)\n",
    "            val_loss /= max(1, nval)\n",
    "\n",
    "        if val_loss + 1e-7 < best_val:\n",
    "            best_val = val_loss\n",
    "            bad = 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_numpy(model, X_np, device):\n",
    "    model.eval()\n",
    "    xb = torch.from_numpy(X_np).float().unsqueeze(1).to(device)  # (N,1,T,C)\n",
    "    logits = model(xb)\n",
    "    return logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "def subject_cv_finetune_predict_progressive(model_global, Xs, ys, device,\n",
    "                                            n_splits=CALIB_CV_FOLDS, n_classes=4):\n",
    "    \"\"\"\n",
    "    Para un sujeto: 4-fold StratifiedKFold.\n",
    "      - En cada fold: 'out' ‚Üí 'head' ‚Üí 'spatial+head' (temporal congelado)\n",
    "      - Se elige el mejor en el split de holdout del sujeto.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    y_true_full = np.empty_like(ys)\n",
    "    y_pred_full = np.empty_like(ys)\n",
    "\n",
    "    for tr_idx, te_idx in skf.split(Xs, ys):\n",
    "        Xcal, ycal = Xs[tr_idx], ys[tr_idx]\n",
    "        Xho,  yho  = Xs[te_idx], ys[te_idx]\n",
    "\n",
    "        # Stage A: 'out'\n",
    "        m_out = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_out, Xcal, ycal, n_classes, mode='out',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_out = predict_numpy(m_out, Xho, device)\n",
    "        acc_out = (yhat_out == yho).mean()\n",
    "\n",
    "        # Stage B: 'head'\n",
    "        m_head = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_head, Xcal, ycal, n_classes, mode='head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_head = predict_numpy(m_head, Xho, device)\n",
    "        acc_head = (yhat_head == yho).mean()\n",
    "\n",
    "        # Stage C: 'spatial+head'\n",
    "        m_sp = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_sp, Xcal, ycal, n_classes, mode='spatial+head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                        l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_sp = predict_numpy(m_sp, Xho, device)\n",
    "        acc_sp = (yhat_sp == yho).mean()\n",
    "\n",
    "        best_idx = np.argmax([acc_out, acc_head, acc_sp])\n",
    "        yhat_best = [yhat_out, yhat_head, yhat_sp][best_idx]\n",
    "\n",
    "        y_true_full[te_idx] = yho\n",
    "        y_pred_full[te_idx] = yhat_best\n",
    "\n",
    "    return y_true_full, y_pred_full\n",
    "\n",
    "# =========================\n",
    "# FOLDS JSON helpers\n",
    "# =========================\n",
    "def save_group_folds_json_with_indices(subject_ids_str, groups_array, n_splits, out_json_path,\n",
    "                                       created_by=\"dose_experiment\", description=None):\n",
    "    out_json_path = Path(out_json_path)\n",
    "    unique_subjects_int = sorted(np.unique(groups_array).tolist())\n",
    "    subject_ids = [f\"S{sid:03d}\" for sid in unique_subjects_int]\n",
    "\n",
    "    if len(subject_ids) < n_splits:\n",
    "        raise ValueError(f\"n_splits={n_splits} mayor que n√∫mero de sujetos={len(subject_ids)}\")\n",
    "\n",
    "    groups = np.arange(len(subject_ids))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    folds = []\n",
    "    fold_i = 0\n",
    "    for train_idx_grp, test_idx_grp in gkf.split(groups, groups, groups):\n",
    "        fold_i += 1\n",
    "        train_sids = [subject_ids[int(i)] for i in train_idx_grp]\n",
    "        test_sids  = [subject_ids[int(i)] for i in test_idx_grp]\n",
    "\n",
    "        train_sids_int = [int(s[1:]) for s in train_sids]\n",
    "        test_sids_int  = [int(s[1:]) for s in test_sids]\n",
    "\n",
    "        tr_idx = np.where(np.isin(groups_array, train_sids_int))[0].tolist()\n",
    "        te_idx = np.where(np.isin(groups_array, test_sids_int))[0].tolist()\n",
    "\n",
    "        folds.append({\n",
    "            \"fold\": int(fold_i),\n",
    "            \"train\": train_sids,\n",
    "            \"test\": test_sids,\n",
    "            \"tr_idx\": tr_idx,\n",
    "            \"te_idx\": te_idx\n",
    "        })\n",
    "\n",
    "    payload = {\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"created_by\": created_by,\n",
    "        \"description\": description if description is not None else \"\",\n",
    "        \"n_splits\": int(n_splits),\n",
    "        \"n_subjects\": len(subject_ids),\n",
    "        \"subject_ids\": subject_ids,\n",
    "        \"folds\": folds\n",
    "    }\n",
    "\n",
    "    out_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Folds JSON con √≠ndices guardado ‚Üí {out_json_path}\")\n",
    "    return out_json_path\n",
    "\n",
    "def load_group_folds_json(path_json, expected_subject_ids=None, strict_check=True):\n",
    "    path_json = Path(path_json)\n",
    "    if not path_json.exists():\n",
    "        raise FileNotFoundError(f\"No existe {path_json}\")\n",
    "    with open(path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        payload = json.load(f)\n",
    "\n",
    "    subj_json = payload.get(\"subject_ids\", [])\n",
    "    if expected_subject_ids is not None:\n",
    "        expected = sorted(list(expected_subject_ids))\n",
    "        if subj_json != expected:\n",
    "            msg = (\"Los subject_ids del JSON no coinciden con expected_subject_ids.\\n\"\n",
    "                   f\"JSON has {len(subj_json)} subjects, expected {len(expected)}.\\n\"\n",
    "                   f\"First 10 JSON: {subj_json[:10]}\\nFirst 10 expected: {expected[:10]}\")\n",
    "            if strict_check:\n",
    "                raise ValueError(msg)\n",
    "            else:\n",
    "                print(\"WARNING: \" + msg)\n",
    "    return payload\n",
    "\n",
    "# =========================\n",
    "# EXPERIMENTO\n",
    "# =========================\n",
    "def run_experiment(save_folds_json=True, folds_json_path=FOLDS_DIR, folds_json_description=\"GroupKFold folds for comparison\"):\n",
    "    \"\"\"\n",
    "    - Crea/lee JSON con folds por sujeto (incluye tr_idx/te_idx)\n",
    "    - Entrena modelo global por fold (inter-sujeto puro) con validaci√≥n interna por sujetos + ES\n",
    "    - Eval√∫a Global acc en test\n",
    "    - Realiza Fine-Tuning PROGRESIVO por sujeto (4-fold CV) y reporta acc\n",
    "    \"\"\"\n",
    "    mne.set_log_level('WARNING')\n",
    "\n",
    "    # sujetos y dataset\n",
    "    subs = subjects_available()\n",
    "    print(f\"Sujetos elegibles: {len(subs)} ‚Üí {subs[:10]}{'...' if len(subs)>10 else ''}\")\n",
    "\n",
    "    X, y, groups, chs = build_dataset_all(subs, scenario=CLASS_SCENARIO, window_mode=WINDOW_MODE)\n",
    "    N, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Listo para entrenar: N={N} | T={T} | C={C} | clases={n_classes} | sujetos={len(np.unique(groups))}\")\n",
    "\n",
    "    ds = EEGTrials(X, y, groups)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # preparar JSON folds\n",
    "    if folds_json_path is None:\n",
    "        folds_json_path = Path(\"folds\") / f\"group_folds_{N_FOLDS}splits.json\"\n",
    "    else:\n",
    "        folds_json_path = Path(folds_json_path)\n",
    "    folds_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    unique_subs = sorted(np.unique(groups).tolist())\n",
    "    subject_ids_str = [f\"S{s:03d}\" for s in unique_subs]\n",
    "\n",
    "    if not folds_json_path.exists():\n",
    "        if not save_folds_json:\n",
    "            raise FileNotFoundError(f\"Folds JSON no encontrado en {folds_json_path} y save_folds_json=False.\")\n",
    "        save_group_folds_json_with_indices(subject_ids_str, groups, n_splits=N_FOLDS,\n",
    "                                           out_json_path=folds_json_path,\n",
    "                                           created_by=\"Joel_Clasificador\",\n",
    "                                           description=folds_json_description)\n",
    "\n",
    "    payload = load_group_folds_json(folds_json_path, expected_subject_ids=subject_ids_str, strict_check=False)\n",
    "    folds = payload[\"folds\"]\n",
    "\n",
    "    # bucle por folds\n",
    "    global_folds = []\n",
    "    ft_prog_folds = []\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "\n",
    "    for f in folds:\n",
    "        fold = f[\"fold\"]\n",
    "        tr_idx = np.asarray(f.get(\"tr_idx\", []), dtype=int)\n",
    "        te_idx = np.asarray(f.get(\"te_idx\", []), dtype=int)\n",
    "\n",
    "        if tr_idx.size == 0 or te_idx.size == 0:\n",
    "            print(f\"Advertencia: fold {fold} sin √≠ndices tr/te v√°lidos. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        # ===== Split de validaci√≥n por SUJETOS dentro del set de entrenamiento =====\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=GLOBAL_VAL_SPLIT, random_state=RANDOM_STATE)\n",
    "        tr_subj_idx, va_subj_idx = next(gss.split(tr_idx, groups[tr_idx], groups[tr_idx]))\n",
    "        tr_sub_idx = tr_idx[tr_subj_idx]\n",
    "        va_idx     = tr_idx[va_subj_idx]\n",
    "\n",
    "        tr_loader = DataLoader(Subset(ds, tr_sub_idx), batch_size=BATCH_SIZE, shuffle=True,  drop_last=False)\n",
    "        va_loader = DataLoader(Subset(ds, va_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "        te_loader = DataLoader(Subset(ds, te_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "        # ===== EEGNet =====\n",
    "        model = EEGNet(n_ch=C, n_classes=n_classes, F1=8, D=2, kernel_t=64, k_sep=16,\n",
    "                       pool1_t=4, pool2_t=8, dropout=0.5).to(DEVICE)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "        def _acc(loader):\n",
    "            return evaluate_with_preds(model, loader)[2]\n",
    "\n",
    "        # ===== Entrenamiento con logging + EARLY STOPPING por val_acc =====\n",
    "        print(f\"\\n[Fold {fold}/{N_FOLDS}] Entrenando modelo global con validaci√≥n interna por sujetos...\"\n",
    "              f\" (n_train={len(tr_sub_idx)} | n_val={len(va_idx)} | n_test={len(te_idx)})\")\n",
    "\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        best_val = -1.0\n",
    "        bad = 0\n",
    "\n",
    "        for epoch in range(1, EPOCHS_GLOBAL + 1):\n",
    "            train_epoch(model, tr_loader, opt, criterion)\n",
    "\n",
    "            if epoch % LOG_EVERY == 0:\n",
    "                tr_acc = _acc(tr_loader)\n",
    "                va_acc = _acc(va_loader)\n",
    "                print(f\"  √âpoca {epoch:3d} | train_acc={tr_acc:.4f} | val_acc={va_acc:.4f}\")\n",
    "\n",
    "                if va_acc > best_val + 1e-4:\n",
    "                    best_val = va_acc\n",
    "                    best_state = copy.deepcopy(model.state_dict())\n",
    "                    bad = 0\n",
    "                else:\n",
    "                    bad += 1\n",
    "                    if bad >= GLOBAL_PATIENCE:\n",
    "                        print(f\"  Early stopping en √©poca {epoch} (mejor val_acc={best_val:.4f})\")\n",
    "                        break\n",
    "\n",
    "        # cargar mejor estado antes de evaluar en test\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "        # ===== Evaluaci√≥n global (inter-sujeto puro) =====\n",
    "        y_true, y_pred, acc_global = evaluate_with_preds(model, te_loader)\n",
    "        global_folds.append(acc_global)\n",
    "        all_true.append(y_true); all_pred.append(y_pred)\n",
    "\n",
    "        print(f\"[Fold {fold}/{N_FOLDS}] Global acc={acc_global:.4f}\")\n",
    "        print_report(y_true, y_pred, CLASS_NAMES_4C)\n",
    "\n",
    "        # ---------- Fine-tuning PROGRESIVO por sujeto con 4-fold CV ----------\n",
    "        X_te, y_te, g_te = X[te_idx], y[te_idx], groups[te_idx]\n",
    "\n",
    "        y_true_ft_all, y_pred_ft_all = [], []\n",
    "        used_subjects = 0\n",
    "        for sid in np.unique(g_te):\n",
    "            idx = np.where(g_te == sid)[0]\n",
    "            Xs, ys = X_te[idx], y_te[idx]\n",
    "\n",
    "            # Seguridad\n",
    "            if len(ys) < CALIB_CV_FOLDS or len(np.unique(ys)) < 2:\n",
    "                continue\n",
    "\n",
    "            y_true_subj, y_pred_subj = subject_cv_finetune_predict_progressive(\n",
    "                model, Xs, ys, DEVICE, n_splits=CALIB_CV_FOLDS, n_classes=n_classes\n",
    "            )\n",
    "            y_true_ft_all.append(y_true_subj)\n",
    "            y_pred_ft_all.append(y_pred_subj)\n",
    "            used_subjects += 1\n",
    "\n",
    "        if len(y_true_ft_all) > 0:\n",
    "            y_true_ft_all = np.concatenate(y_true_ft_all)\n",
    "            y_pred_ft_all = np.concatenate(y_pred_ft_all)\n",
    "            acc_ft = (y_true_ft_all == y_pred_ft_all).mean()\n",
    "            print(f\"  Fine-tuning PROGRESIVO (por sujeto, {CALIB_CV_FOLDS}-fold CV) acc={acc_ft:.4f} | sujetos={used_subjects}\")\n",
    "            print(f\"  Œî(FT-Global) = {acc_ft - acc_global:+.4f}\")\n",
    "        else:\n",
    "            acc_ft = np.nan\n",
    "            print(\"  Fine-tuning PROGRESIVO no ejecutado (sujeto(s) con muestras insuficientes).\")\n",
    "\n",
    "        ft_prog_folds.append(acc_ft)\n",
    "\n",
    "    # ---------- resultados finales ----------\n",
    "    if len(all_true) > 0:\n",
    "        all_true = np.concatenate(all_true)\n",
    "        all_pred = np.concatenate(all_pred)\n",
    "    else:\n",
    "        all_true = np.array([], dtype=int)\n",
    "        all_pred = np.array([], dtype=int)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTADOS FINALES\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Global folds:\", [f\"{a:.4f}\" for a in global_folds])\n",
    "    if len(global_folds) > 0:\n",
    "        print(f\"Global mean: {np.mean(global_folds):.4f}\")\n",
    "\n",
    "    print(\"Fine-tune PROGRESIVO folds:\", [(\"nan\" if (a is None or np.isnan(a)) else f\"{a:.4f}\") for a in ft_prog_folds])\n",
    "    if len(ft_prog_folds) > 0:\n",
    "        print(f\"Fine-tune PROGRESIVO mean: {np.nanmean(ft_prog_folds):.4f}\")\n",
    "        print(f\"Œî(FT-Global) mean: {np.nanmean(ft_prog_folds) - np.mean(global_folds):+.4f}\")\n",
    "\n",
    "    # Matriz de confusi√≥n global (sobre todos los folds)\n",
    "    if all_true.size > 0:\n",
    "        plot_confusion(all_true, all_pred, CLASS_NAMES_4C,\n",
    "                       title=\"Confusion Matrix - Global Model (All Folds)\",\n",
    "                       fname=\"confusion_global_allfolds.png\")\n",
    "        print(\"\\n‚Ü≥ Matriz de confusi√≥n guardada: confusion_global_allfolds.png\")\n",
    "\n",
    "    return {\n",
    "        \"global_folds\": global_folds,\n",
    "        \"ft_prog_folds\": ft_prog_folds,\n",
    "        \"all_true\": all_true,\n",
    "        \"all_pred\": all_pred,\n",
    "        \"folds_json_path\": str(folds_json_path)\n",
    "    }\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üß† INICIANDO EXPERIMENTO CON EEGNet + FINE-TUNING PROGRESIVO (por sujeto, 4-fold CV)\")\n",
    "    print(f\"üîß Configuraci√≥n: {CLASS_SCENARIO}, {len(EXPECTED_8)} canales, {WINDOW_MODE}\")\n",
    "    print(f\"‚öôÔ∏è  FT: epochs={FT_EPOCHS}, base_lr={FT_BASE_LR}, head_lr={FT_HEAD_LR}, L2SP={FT_L2SP}, patience={FT_PATIENCE}, CV={CALIB_CV_FOLDS}\")\n",
    "    run_experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f43e24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Usando dispositivo: cuda\n",
      "Sujetos elegibles: 103 ‚Üí [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]...\n",
      "üîß Configuraci√≥n com√∫n: escenario=4c, ventana=3s, fs=160.0\n",
      "Sujetos elegibles: 103 ‚Üí [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Construyendo dataset INTER (balanceado):   1%|          | 1/103 [00:00<00:13,  7.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Construyendo dataset INTER (balanceado):   8%|‚ñä         | 8/103 [00:01<00:13,  7.14it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1227c81c9e35>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;31m# --- MODO INTER-SUJETO (cross-subject) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m     run_inter(\n\u001b[0m\u001b[1;32m    952\u001b[0m         \u001b[0msave_folds_json\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[0mfolds_json_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFOLDS_JSON_DEFAULT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-1227c81c9e35>\u001b[0m in \u001b[0;36mrun_inter\u001b[0;34m(save_folds_json, folds_json_path, folds_json_description)\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Sujetos elegibles: {len(subs)} ‚Üí {subs[:10]}{'...' if len(subs)>10 else ''}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dataset_all_inter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscenario\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCLASS_SCENARIO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWINDOW_MODE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m     \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEEGTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-1227c81c9e35>\u001b[0m in \u001b[0;36mbuild_dataset_all_inter\u001b[0;34m(subjects, scenario, window_mode)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msdir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"S{s:03d}R{r:02d}.edf\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_trials_from_run_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscenario\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mch_template\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mchs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mch_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-1227c81c9e35>\u001b[0m in \u001b[0;36mextract_trials_from_run_common\u001b[0;34m(edf_path, scenario, window_mode)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mseg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mseg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mseg\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mseg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'EO'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/_methods.py\u001b[0m in \u001b[0;36m_std\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where, mean)\u001b[0m\n\u001b[1;32m    216\u001b[0m def _std(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False, *,\n\u001b[1;32m    217\u001b[0m          where=True, mean=None):\n\u001b[0;32m--> 218\u001b[0;31m     ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n\u001b[0m\u001b[1;32m    219\u001b[0m                keepdims=keepdims, where=where, mean=mean)\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/_methods.py\u001b[0m in \u001b[0;36m_var\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where, mean)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;31m# Note that if dtype is not of inexact type then arraymean will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;31m# not be either.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0marrmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;31m# The shape of rcount has to match arrmean to not change the shape of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;31m# out in broadcasting. Otherwise, it cannot be stored back to arrmean.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# EEGNet unificado: INTER (cross-subject) + INTRA (per-subject CV) con FT progresivo\n",
    "# Mantiene comportamientos y resultados de tus dos scripts originales.\n",
    "\n",
    "import os, re, math, random, json, itertools, copy, argparse\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import (GroupKFold, StratifiedKFold, StratifiedShuffleSplit,\n",
    "                                     GroupShuffleSplit)\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# CONFIGURACI√ìN GLOBAL\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "FOLDS_JSON_DEFAULT = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# determinismo\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "# canales / runs / escenario\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','CPz']\n",
    "MI_RUNS_LR = [4, 8, 12]\n",
    "MI_RUNS_OF = [6, 10, 14]\n",
    "BASELINE_RUNS_EO = [1]   # solo para lectura de nombres de canales si hiciera falta\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "\n",
    "CLASS_SCENARIO = '4c'\n",
    "WINDOW_MODE = '3s'  # 0‚Äì3s\n",
    "FS = 160.0\n",
    "CLASS_NAMES_4C = ['Left', 'Right', 'Both Fists', 'Both Feet']\n",
    "\n",
    "# Hiperpar√°metros compartidos\n",
    "N_FOLDS_INTER = 5\n",
    "BATCH_SIZE_INTER = 32\n",
    "EPOCHS_GLOBAL_INTER = 100\n",
    "LR_GLOBAL_INTER = 1e-3\n",
    "GLOBAL_VAL_SPLIT = 0.15\n",
    "GLOBAL_PATIENCE = 10\n",
    "LOG_EVERY = 5\n",
    "\n",
    "# Fine-tuning (compartidos)\n",
    "CALIB_CV_FOLDS = 4\n",
    "FT_EPOCHS = 30\n",
    "FT_BASE_LR = 5e-5\n",
    "FT_HEAD_LR = 1e-3\n",
    "FT_L2SP = 1e-4\n",
    "FT_PATIENCE = 5\n",
    "FT_VAL_RATIO = 0.2\n",
    "\n",
    "# INTRA espec√≠ficos (mismos valores que tu script)\n",
    "N_FOLDS_INTRA = 5\n",
    "BATCH_SIZE_GLOBAL_INTRA = 16\n",
    "EPOCHS_GLOBAL_INTRA = 100\n",
    "LR_GLOBAL_INTRA = 1e-3\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES EEG / EDF\n",
    "# =========================\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = re.sub(r'([A-Za-z])0([0-9])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])Z$', r'\\1z', s)\n",
    "    s = s.replace('fp', 'Fp').replace('FP', 'Fp')\n",
    "    s = ''.join(ch.upper() if ch != 'z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {}\n",
    "    for ch in raw.ch_names:\n",
    "        lab = normalize_label(ch)\n",
    "        lab = lab[:-1] + 'z' if lab.endswith('Z') else lab\n",
    "        lab = re.sub(r'([A-Z])Z$', r'\\1z', lab)\n",
    "        mapping[ch] = lab\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired_channels=EXPECTED_8):\n",
    "    have = [ch for ch in desired_channels if ch in raw.ch_names]\n",
    "    missing = [ch for ch in desired_channels if ch not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"Warning: faltan canales {missing} en archivo {getattr(raw,'filenames', [''])[0]}\")\n",
    "        return None\n",
    "    raw.reorder_channels([ch for ch in raw.ch_names if ch in desired_channels] +\n",
    "                         [ch for ch in raw.ch_names if ch not in desired_channels])\n",
    "    raw.pick_channels(desired_channels, ordered=True)\n",
    "    return raw\n",
    "\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(run_id:int):\n",
    "    if run_id in MI_RUNS_LR: return 'LR'\n",
    "    if run_id in MI_RUNS_OF: return 'OF'\n",
    "    if run_id in BASELINE_RUNS_EO: return 'EO'\n",
    "    return None\n",
    "\n",
    "def read_raw_edf(path: Path):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        mont = mne.channels.make_standard_montage('standard_1020')\n",
    "        raw.set_montage(mont, on_missing='ignore')\n",
    "    except Exception:\n",
    "        pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad=\"auto\")\n",
    "    raw = ensure_channels_order(raw, EXPECTED_8)\n",
    "    if raw is None: return None\n",
    "    return raw\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    if raw.annotations is None or len(raw.annotations) == 0:\n",
    "        return []\n",
    "    def _norm(s): return str(s).strip().upper().replace(' ', '')\n",
    "    res = []\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = _norm(desc)\n",
    "        if tag in ('T1','T2'):\n",
    "            res.append((float(onset), tag))\n",
    "    res.sort()\n",
    "    # deduplicaci√≥n m√≠nima\n",
    "    dedup, last_t1, last_t2 = [], -1e9, -1e9\n",
    "    for t, tag in res:\n",
    "        if tag == 'T1':\n",
    "            if (t - last_t1) >= 0.5: dedup.append((t, tag)); last_t1 = t\n",
    "        else:\n",
    "            if (t - last_t2) >= 0.5: dedup.append((t, tag)); last_t2 = t\n",
    "    return dedup\n",
    "\n",
    "# =========================\n",
    "# DATASETS (dos variantes)\n",
    "# =========================\n",
    "def extract_trials_from_run_common(edf_path: Path, scenario: str, window_mode: str):\n",
    "    subj, run = parse_subject_run(edf_path)\n",
    "    kind = run_kind(run)\n",
    "    if kind not in ('LR','OF','EO'):\n",
    "        return ([], [])\n",
    "    raw = read_raw_edf(edf_path)\n",
    "    if raw is None: return ([], [])\n",
    "    data = raw.get_data()\n",
    "    fs = raw.info['sfreq']; assert abs(fs - FS) < 1e-6\n",
    "    out = []\n",
    "\n",
    "    if kind in ('LR','OF'):\n",
    "        events = collect_events_T1T2(raw)\n",
    "        rel_start, rel_end = (0.0, 3.0) if window_mode == '3s' else (-1.0, 5.0)\n",
    "        for onset_sec, tag in events:\n",
    "            if kind == 'LR':\n",
    "                label = 0 if tag == 'T1' else 1\n",
    "            else:\n",
    "                label = 2 if tag == 'T1' else 3\n",
    "            if scenario == '2c' and label not in (0,1): continue\n",
    "            if scenario == '3c' and label not in (0,1,2): continue\n",
    "            s = int(round((raw.first_time + onset_sec + rel_start) * fs))\n",
    "            e = int(round((raw.first_time + onset_sec + rel_end) * fs))\n",
    "            if s < 0 or e > data.shape[1]: continue\n",
    "            seg = data[:, s:e].T.astype(np.float32)\n",
    "            seg = (seg - seg.mean(axis=0, keepdims=True)) / (seg.std(axis=0, keepdims=True) + 1e-6)\n",
    "            out.append((seg, label, subj))\n",
    "    elif kind == 'EO':\n",
    "        return ([], raw.ch_names)\n",
    "\n",
    "    return out, raw.ch_names\n",
    "\n",
    "# INTER: balancea a 21 ensayos por clase/sujeto (como tu script INTER)\n",
    "def build_dataset_all_inter(subjects, scenario='4c', window_mode='3s'):\n",
    "    X, y, groups, ch_template = [], [], [], None\n",
    "    for s in tqdm(subjects, desc=\"Construyendo dataset INTER (balanceado)\"):\n",
    "        sdir = DATA_RAW / f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "        trials = {0:[],1:[],2:[],3:[]}\n",
    "        for r in MI_RUNS_LR + MI_RUNS_OF:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run_common(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                trials[lab].append(seg)\n",
    "        # skip if falta alguna clase\n",
    "        if any(len(trials[k])==0 for k in (0,1,2,3)): continue\n",
    "        need = 21\n",
    "        rng = check_random_state(RANDOM_STATE + s)\n",
    "        def pick(arr):\n",
    "            if len(arr) < need:\n",
    "                idx = rng.choice(len(arr), size=need, replace=True); return [arr[i] for i in idx]\n",
    "            rng.shuffle(arr); return arr[:need]\n",
    "        for lab in (0,1,2,3):\n",
    "            for seg in pick(trials[lab]):\n",
    "                X.append(seg); y.append(lab); groups.append(s)\n",
    "    X = np.stack(X, axis=0); y = np.asarray(y, np.int64); groups = np.asarray(groups, np.int64)\n",
    "    n, T, C = X.shape\n",
    "    print(f\"[INTER] Dataset: N={n} | T={T} | C={C} | clases={len(np.unique(y))} | sujetos={len(np.unique(groups))}\")\n",
    "    return X, y, groups, ch_template\n",
    "\n",
    "# INTRA: sin balanceo artificial (como tu script INTRA)\n",
    "def build_dataset_all_intra(subjects, scenario='4c', window_mode='3s'):\n",
    "    X, y, groups = [], [], []\n",
    "    for s in subjects:\n",
    "        sdir = DATA_RAW / f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "        for r in MI_RUNS_LR + MI_RUNS_OF:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, _ = extract_trials_from_run_common(p, scenario, window_mode)\n",
    "            for seg, lab, subj in outs:\n",
    "                X.append(seg); y.append(lab); groups.append(subj)\n",
    "    X = np.stack(X, axis=0); y = np.asarray(y, np.int64); groups = np.asarray(groups, np.int64)\n",
    "    n, T, C = X.shape\n",
    "    print(f\"[INTRA] Dataset: N={n} | T={T} | C={C} | clases={len(np.unique(y))} | sujetos={len(np.unique(groups))}\")\n",
    "    return X, y, groups\n",
    "\n",
    "def subjects_available():\n",
    "    subs = []\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try: sid = int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir / f\"S{sid:03d}R{r:02d}.edf\").exists() for r in (MI_RUNS_LR + MI_RUNS_OF))\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "# =========================\n",
    "# MODELO\n",
    "# =========================\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, n_ch: int, n_classes: int,\n",
    "                 F1: int = 8, D: int = 2, kernel_t: int = 64, k_sep: int = 16,\n",
    "                 pool1_t: int = 4, pool2_t: int = 8, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.n_ch = n_ch; self.n_classes = n_classes\n",
    "        self.F1 = F1; self.D = D; self.F2 = F1 * D\n",
    "        self.kernel_t = kernel_t; self.k_sep = k_sep\n",
    "        self.pool1_t = pool1_t; self.pool2_t = pool2_t\n",
    "        self.conv_temporal = nn.Conv2d(1, F1, kernel_size=(kernel_t, 1),\n",
    "                                       padding=(kernel_t // 2, 0), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(F1); self.act = nn.ELU()\n",
    "        self.conv_depthwise = nn.Conv2d(F1, self.F2, kernel_size=(1, n_ch),\n",
    "                                        groups=F1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.F2)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=(pool1_t, 1), stride=(pool1_t, 1))\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.conv_sep_depth = nn.Conv2d(self.F2, self.F2, kernel_size=(k_sep, 1),\n",
    "                                        groups=self.F2, padding=(k_sep // 2, 0), bias=False)\n",
    "        self.conv_sep_point = nn.Conv2d(self.F2, self.F2, kernel_size=(1, 1), bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.F2)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(pool2_t, 1), stride=(pool2_t, 1))\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = None; self.out = None; self._T_in = None\n",
    "\n",
    "    def _build_head(self, T_in: int, device: torch.device):\n",
    "        T1 = T_in // self.pool1_t; T2 = T1 // self.pool2_t\n",
    "        feat_dim = self.F2 * T2 * 1\n",
    "        self.fc  = nn.Linear(feat_dim, 80, bias=True).to(device)\n",
    "        self.out = nn.Linear(80, self.n_classes, bias=True).to(device)\n",
    "        self._T_in = T_in\n",
    "\n",
    "    def ensure_head(self, T_in: int, device: torch.device):\n",
    "        if (self.fc is None) or (self.out is None) or (self._T_in != T_in):\n",
    "            self._build_head(T_in, device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, _, T, C = x.shape\n",
    "        self.ensure_head(T, x.device)\n",
    "        z = self.conv_temporal(x); z = self.bn1(z); z = self.act(z)\n",
    "        z = self.conv_depthwise(z); z = self.bn2(z); z = self.act(z)\n",
    "        z = self.pool1(z); z = self.drop1(z)\n",
    "        z = self.conv_sep_depth(z); z = self.conv_sep_point(z)\n",
    "        z = self.bn3(z); z = self.act(z)\n",
    "        z = self.pool2(z); z = self.drop2(z)\n",
    "        z = self.flatten(z)\n",
    "        z = self.fc(z); z = self.act(z)\n",
    "        return self.out(z)  # logits; CrossEntropyLoss aplica softmax internamente\n",
    "\n",
    "# =========================\n",
    "# DATASETS TORCH\n",
    "# =========================\n",
    "class EEGTrials(Dataset):\n",
    "    def __init__(self, X, y, groups):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.g = groups.astype(np.int64)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        x = np.expand_dims(x, 0)  # (1,T,C)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx])\n",
    "\n",
    "# =========================\n",
    "# ENTRENAR / EVALUAR\n",
    "# =========================\n",
    "def train_epoch(model, loader, opt, criterion):\n",
    "    model.train()\n",
    "    for xb, yb, _ in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward(); opt.step()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_with_preds(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for xb, yb, _ in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        pred = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "        y_pred.extend(pred); y_true.extend(yb.numpy().tolist())\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    return y_true, y_pred, float(acc)\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes, title, fname):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.nan_to_num(cm_norm)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title); plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    ticks = np.arange(len(classes))\n",
    "    plt.xticks(ticks, classes, rotation=45, ha='right'); plt.yticks(ticks, classes)\n",
    "    fmt = '.2f'; thresh = cm_norm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "        plt.text(j, i, format(cm_norm[i, j], fmt),\n",
    "                 ha=\"center\", color=\"white\" if cm_norm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True label'); plt.xlabel('Predicted label')\n",
    "    plt.tight_layout(); plt.savefig(fname, dpi=150, bbox_inches='tight'); plt.close()\n",
    "\n",
    "def print_report(y_true, y_pred, classes):\n",
    "    print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
    "\n",
    "# =========================\n",
    "# FINE-TUNING PROGRESIVO (compartido)\n",
    "# =========================\n",
    "def _param_groups(model, mode):\n",
    "    if mode == 'out':\n",
    "        train = list(model.out.parameters())\n",
    "    elif mode == 'head':\n",
    "        train = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "    elif mode == 'spatial+head':\n",
    "        train = (list(model.conv_depthwise.parameters()) +\n",
    "                 list(model.bn2.parameters()) +\n",
    "                 list(model.conv_sep_depth.parameters()) +\n",
    "                 list(model.conv_sep_point.parameters()) +\n",
    "                 list(model.bn3.parameters()) +\n",
    "                 list(model.fc.parameters()) +\n",
    "                 list(model.out.parameters()))\n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "    return train\n",
    "\n",
    "def _freeze_for_mode(model, mode):\n",
    "    for p in model.parameters(): p.requires_grad = False\n",
    "    if mode == 'out':\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'head':\n",
    "        for p in model.fc.parameters():  p.requires_grad = True\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'spatial+head':\n",
    "        for p in model.conv_depthwise.parameters(): p.requires_grad = True\n",
    "        for p in model.bn2.parameters():           p.requires_grad = True\n",
    "        for p in model.conv_sep_depth.parameters():p.requires_grad = True\n",
    "        for p in model.conv_sep_point.parameters():p.requires_grad = True\n",
    "        for p in model.bn3.parameters():           p.requires_grad = True\n",
    "        for p in model.fc.parameters():            p.requires_grad = True\n",
    "        for p in model.out.parameters():           p.requires_grad = True\n",
    "\n",
    "def _class_weights(y_np, n_classes):\n",
    "    counts = np.bincount(y_np, minlength=n_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    w = counts.sum() / counts\n",
    "    w = w / w.mean()\n",
    "    return torch.tensor(w, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "def _train_one_mode_inter(model, X_cal, y_cal, n_classes, mode,\n",
    "                          epochs=FT_EPOCHS, batch_size=16,\n",
    "                          head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                          l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO):\n",
    "    \"\"\"Versi√≥n INTER: early stopping por val_loss (como tu INTER).\"\"\"\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=RANDOM_STATE)\n",
    "    (tr_idx, va_idx), = sss.split(X_cal, y_cal)\n",
    "    Xtr, ytr = X_cal[tr_idx], y_cal[tr_idx]\n",
    "    Xva, yva = X_cal[va_idx], y_cal[va_idx]\n",
    "\n",
    "    ds_tr = TensorDataset(torch.from_numpy(Xtr).float().unsqueeze(1),\n",
    "                          torch.from_numpy(ytr).long())\n",
    "    ds_va = TensorDataset(torch.from_numpy(Xva).float().unsqueeze(1),\n",
    "                          torch.from_numpy(yva).long())\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    _freeze_for_mode(model, mode)\n",
    "\n",
    "    if mode == 'spatial+head':\n",
    "        base_params = (list(model.conv_depthwise.parameters()) +\n",
    "                       list(model.bn2.parameters()) +\n",
    "                       list(model.conv_sep_depth.parameters()) +\n",
    "                       list(model.conv_sep_point.parameters()) +\n",
    "                       list(model.bn3.parameters()))\n",
    "        head_params = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "        train_params = base_params + head_params\n",
    "        opt = optim.Adam([{\"params\": base_params, \"lr\": base_lr},\n",
    "                          {\"params\": head_params, \"lr\": head_lr}])\n",
    "    else:\n",
    "        train_params = _param_groups(model, mode)\n",
    "        opt = optim.Adam(train_params, lr=head_lr)\n",
    "\n",
    "    ref = [p.detach().clone().to(p.device) for p in train_params]\n",
    "    class_w = _class_weights(ytr, n_classes)\n",
    "    crit = nn.CrossEntropyLoss(weight=class_w)\n",
    "\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_val = float('inf')\n",
    "    bad = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # train\n",
    "        model.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            reg = 0.0\n",
    "            for p_cur, p_ref in zip(train_params, ref):\n",
    "                reg = reg + torch.sum((p_cur - p_ref)**2)\n",
    "            loss = loss + l2sp_lambda * reg\n",
    "            loss.backward(); opt.step()\n",
    "        # val -> val_loss\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, nval = 0.0, 0\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                loss = crit(logits, yb)\n",
    "                val_loss += loss.item() * xb.size(0); nval += xb.size(0)\n",
    "            val_loss /= max(1, nval)\n",
    "\n",
    "        if val_loss + 1e-7 < best_val:\n",
    "            best_val = val_loss; bad = 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience: break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def _train_one_mode_intra(model, X_tr, y_tr, X_va, y_va, mode, n_classes,\n",
    "                          epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                          l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, batch_size=16):\n",
    "    \"\"\"Versi√≥n INTRA: selecci√≥n por val_acc (como tu INTRA).\"\"\"\n",
    "    _freeze_for_mode(model, mode)\n",
    "    ds_tr = TensorDataset(torch.from_numpy(X_tr).float().unsqueeze(1),\n",
    "                          torch.from_numpy(y_tr).long())\n",
    "    ds_va = TensorDataset(torch.from_numpy(X_va).float().unsqueeze(1),\n",
    "                          torch.from_numpy(y_va).long())\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    if mode == 'spatial+head':\n",
    "        base_params = (list(model.conv_depthwise.parameters()) +\n",
    "                       list(model.bn2.parameters()) +\n",
    "                       list(model.conv_sep_depth.parameters()) +\n",
    "                       list(model.conv_sep_point.parameters()) +\n",
    "                       list(model.bn3.parameters()))\n",
    "        head_params = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "        train_params = base_params + head_params\n",
    "        opt = optim.Adam([{\"params\": base_params, \"lr\": base_lr},\n",
    "                          {\"params\": head_params, \"lr\": head_lr}])\n",
    "    elif mode == 'head':\n",
    "        train_params = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "        opt = optim.Adam(train_params, lr=head_lr)\n",
    "    else:  # out\n",
    "        train_params = list(model.out.parameters())\n",
    "        opt = optim.Adam(train_params, lr=head_lr)\n",
    "\n",
    "    ref = [p.detach().clone().to(p.device) for p in train_params]\n",
    "    class_w = _class_weights(y_tr, n_classes)\n",
    "    crit = nn.CrossEntropyLoss(weight=class_w)\n",
    "\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_val_acc = -1.0\n",
    "    bad = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # train\n",
    "        model.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            reg = 0.0\n",
    "            for p_cur, p_ref in zip(train_params, ref):\n",
    "                reg = reg + torch.sum((p_cur - p_ref)**2)\n",
    "            loss = loss + l2sp_lambda * reg\n",
    "            loss.backward(); opt.step()\n",
    "        # val -> val_acc\n",
    "        y_true_va, y_pred_va, acc_va = _eval_dl(model, dl_va)\n",
    "        if acc_va > best_val_acc + 1e-6:\n",
    "            best_val_acc = acc_va; best_state = copy.deepcopy(model.state_dict()); bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience: break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return copy.deepcopy(model), float(best_val_acc)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _eval_dl(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        y_pred.extend(logits.argmax(1).cpu().numpy().tolist())\n",
    "        y_true.extend(yb.numpy().tolist())\n",
    "    y_true = np.asarray(y_true, int); y_pred = np.asarray(y_pred, int)\n",
    "    acc = (y_true == y_pred).mean() if y_true.size else 0.0\n",
    "    return y_true, y_pred, float(acc)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_numpy(model, X_np):\n",
    "    model.eval()\n",
    "    xb = torch.from_numpy(X_np).float().unsqueeze(1).to(DEVICE)\n",
    "    logits = model(xb)\n",
    "    return logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "def subject_cv_finetune_predict_progressive_inter(model_global, Xs, ys, n_classes):\n",
    "    \"\"\"INTER: 4-fold en el sujeto, entrena out/head/spatial+head (val_loss) y elige por accuracy en holdout.\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=CALIB_CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    y_true_full = np.empty_like(ys); y_pred_full = np.empty_like(ys)\n",
    "    for tr_idx, te_idx in skf.split(Xs, ys):\n",
    "        Xcal, ycal = Xs[tr_idx], ys[tr_idx]\n",
    "        Xho,  yho  = Xs[te_idx], ys[te_idx]\n",
    "        # out\n",
    "        m_out = copy.deepcopy(model_global)\n",
    "        _train_one_mode_inter(m_out, Xcal, ycal, n_classes, mode='out',\n",
    "                              epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                              patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_out = predict_numpy(m_out, Xho); acc_out = (yhat_out == yho).mean()\n",
    "        # head\n",
    "        m_head = copy.deepcopy(model_global)\n",
    "        _train_one_mode_inter(m_head, Xcal, ycal, n_classes, mode='head',\n",
    "                              epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                              patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_head = predict_numpy(m_head, Xho); acc_head = (yhat_head == yho).mean()\n",
    "        # spatial+head\n",
    "        m_sp = copy.deepcopy(model_global)\n",
    "        _train_one_mode_inter(m_sp, Xcal, ycal, n_classes, mode='spatial+head',\n",
    "                              epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                              l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_sp = predict_numpy(m_sp, Xho); acc_sp = (yhat_sp == yho).mean()\n",
    "        best_idx = np.argmax([acc_out, acc_head, acc_sp])\n",
    "        yhat_best = [yhat_out, yhat_head, yhat_sp][best_idx]\n",
    "        y_true_full[te_idx] = yho; y_pred_full[te_idx] = yhat_best\n",
    "    return y_true_full, y_pred_full\n",
    "\n",
    "# =========================\n",
    "# PIPELINE INTER (cross-subject)\n",
    "# =========================\n",
    "def run_inter(save_folds_json=True, folds_json_path=FOLDS_JSON_DEFAULT,\n",
    "              folds_json_description=\"GroupKFold folds for comparison\"):\n",
    "    mne.set_log_level('WARNING')\n",
    "    subs = subjects_available()\n",
    "    print(f\"Sujetos elegibles: {len(subs)} ‚Üí {subs[:10]}{'...' if len(subs)>10 else ''}\")\n",
    "\n",
    "    X, y, groups, chs = build_dataset_all_inter(subs, scenario=CLASS_SCENARIO, window_mode=WINDOW_MODE)\n",
    "    N, T, C = X.shape; n_classes = len(np.unique(y))\n",
    "    ds = EEGTrials(X, y, groups)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    folds_json_path = Path(folds_json_path) if folds_json_path else Path(\"folds/group_folds_5.json\")\n",
    "    folds_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    subject_ids_str = [f\"S{s:03d}\" for s in sorted(np.unique(groups).tolist())]\n",
    "\n",
    "    if not folds_json_path.exists():\n",
    "        if not save_folds_json:\n",
    "            raise FileNotFoundError(f\"No existe {folds_json_path} y save_folds_json=False.\")\n",
    "        save_group_folds_json_with_indices(subject_ids_str, groups, n_splits=N_FOLDS_INTER,\n",
    "                                           out_json_path=folds_json_path,\n",
    "                                           created_by=\"EEGNet_INTER\",\n",
    "                                           description=folds_json_description)\n",
    "\n",
    "    payload = load_group_folds_json(folds_json_path, expected_subject_ids=subject_ids_str, strict_check=False)\n",
    "    folds = payload[\"folds\"]\n",
    "\n",
    "    global_folds, ft_prog_folds, all_true, all_pred = [], [], [], []\n",
    "\n",
    "    for f in folds:\n",
    "        fold = f[\"fold\"]\n",
    "        tr_idx = np.asarray(f.get(\"tr_idx\", []), dtype=int)\n",
    "        te_idx = np.asarray(f.get(\"te_idx\", []), dtype=int)\n",
    "        if tr_idx.size == 0 or te_idx.size == 0:\n",
    "            print(f\"Advertencia: fold {fold} vac√≠o. Saltando.\"); continue\n",
    "\n",
    "        # split de validaci√≥n por sujetos dentro de TRAIN\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=GLOBAL_VAL_SPLIT, random_state=RANDOM_STATE)\n",
    "        tr_subj_idx, va_subj_idx = next(gss.split(tr_idx, groups[tr_idx], groups[tr_idx]))\n",
    "        tr_sub_idx = tr_idx[tr_subj_idx]; va_idx = tr_idx[va_subj_idx]\n",
    "\n",
    "        tr_loader = DataLoader(Subset(ds, tr_sub_idx), batch_size=BATCH_SIZE_INTER, shuffle=True,  drop_last=False)\n",
    "        va_loader = DataLoader(Subset(ds, va_idx),     batch_size=BATCH_SIZE_INTER, shuffle=False, drop_last=False)\n",
    "        te_loader = DataLoader(Subset(ds, te_idx),     batch_size=BATCH_SIZE_INTER, shuffle=False, drop_last=False)\n",
    "\n",
    "        model = EEGNet(n_ch=C, n_classes=n_classes, F1=8, D=2, kernel_t=64, k_sep=16,\n",
    "                       pool1_t=4, pool2_t=8, dropout=0.5).to(DEVICE)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_GLOBAL_INTER)\n",
    "\n",
    "        def _acc(loader): return evaluate_with_preds(model, loader)[2]\n",
    "\n",
    "        print(f\"\\n[Fold {fold}/{N_FOLDS_INTER}] Entrenamiento global (val por sujetos)\")\n",
    "        best_state = copy.deepcopy(model.state_dict()); best_val = -1.0; bad = 0\n",
    "        for epoch in range(1, EPOCHS_GLOBAL_INTER + 1):\n",
    "            train_epoch(model, tr_loader, opt, criterion)\n",
    "            if epoch % LOG_EVERY == 0:\n",
    "                tr_acc = _acc(tr_loader); va_acc = _acc(va_loader)\n",
    "                print(f\"  √âpoca {epoch:3d} | train_acc={tr_acc:.4f} | val_acc={va_acc:.4f}\")\n",
    "                if va_acc > best_val + 1e-4:\n",
    "                    best_val = va_acc; best_state = copy.deepcopy(model.state_dict()); bad = 0\n",
    "                else:\n",
    "                    bad += 1\n",
    "                    if bad >= GLOBAL_PATIENCE:\n",
    "                        print(f\"  Early stopping en √©poca {epoch} (mejor val_acc={best_val:.4f})\")\n",
    "                        break\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "        # test inter-sujeto\n",
    "        y_true, y_pred, acc_global = evaluate_with_preds(model, te_loader)\n",
    "        global_folds.append(acc_global); all_true.append(y_true); all_pred.append(y_pred)\n",
    "        print(f\"[Fold {fold}] Global acc={acc_global:.4f}\")\n",
    "        print_report(y_true, y_pred, CLASS_NAMES_4C)\n",
    "\n",
    "        # FT progresivo por sujeto en TEST\n",
    "        X_te, y_te, g_te = X[te_idx], y[te_idx], groups[te_idx]\n",
    "        y_true_ft_all, y_pred_ft_all, used_subjects = [], [], 0\n",
    "        for sid in np.unique(g_te):\n",
    "            idx = np.where(g_te == sid)[0]\n",
    "            Xs, ys = X_te[idx], y_te[idx]\n",
    "            if len(ys) < CALIB_CV_FOLDS or len(np.unique(ys)) < 2:\n",
    "                continue\n",
    "            y_true_subj, y_pred_subj = subject_cv_finetune_predict_progressive_inter(\n",
    "                model, Xs, ys, n_classes)\n",
    "            y_true_ft_all.append(y_true_subj); y_pred_ft_all.append(y_pred_subj); used_subjects += 1\n",
    "        if len(y_true_ft_all) > 0:\n",
    "            y_true_ft_all = np.concatenate(y_true_ft_all)\n",
    "            y_pred_ft_all = np.concatenate(y_pred_ft_all)\n",
    "            acc_ft = (y_true_ft_all == y_pred_ft_all).mean()\n",
    "            print(f\"  FT PROGRESIVO (por sujeto, {CALIB_CV_FOLDS}-fold) acc={acc_ft:.4f} | sujetos={used_subjects}\")\n",
    "            print(f\"  Œî(FT-Global) = {acc_ft - acc_global:+.4f}\")\n",
    "        else:\n",
    "            acc_ft = np.nan\n",
    "            print(\"  FT PROGRESIVO no ejecutado (sujeto(s) insuficientes).\")\n",
    "        ft_prog_folds.append(acc_ft)\n",
    "\n",
    "    all_true = np.concatenate(all_true) if len(all_true)>0 else np.array([], int)\n",
    "    all_pred = np.concatenate(all_pred) if len(all_pred)>0 else np.array([], int)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTADOS INTER\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Global folds:\", [f\"{a:.4f}\" for a in global_folds])\n",
    "    if len(global_folds)>0: print(f\"Global mean: {np.mean(global_folds):.4f}\")\n",
    "    print(\"Fine-tune PROGRESIVO folds:\", [(\"nan\" if (a is None or np.isnan(a)) else f\"{a:.4f}\") for a in ft_prog_folds])\n",
    "    if len(ft_prog_folds)>0:\n",
    "        print(f\"Fine-tune PROGRESIVO mean: {np.nanmean(ft_prog_folds):.4f}\")\n",
    "        print(f\"Œî(FT-Global) mean: {np.nanmean(ft_prog_folds) - np.mean(global_folds):+.4f}\")\n",
    "\n",
    "    if all_true.size > 0:\n",
    "        plot_confusion(all_true, all_pred, CLASS_NAMES_4C,\n",
    "                       title=\"INTER ‚Äî Confusion Matrix (All Folds)\",\n",
    "                       fname=\"confusion_inter_allfolds.png\")\n",
    "        print(\"‚Ü≥ Matriz de confusi√≥n guardada: confusion_inter_allfolds.png\")\n",
    "\n",
    "# =========================\n",
    "# PIPELINE INTRA (per-subject CV)\n",
    "# =========================\n",
    "def train_global_model_intra(X, y, epochs=EPOCHS_GLOBAL_INTRA):\n",
    "    n_ch = X.shape[2]; n_classes = len(np.unique(y))\n",
    "    model = EEGNet(n_ch=n_ch, n_classes=n_classes, F1=8, D=2, kernel_t=64, k_sep=16,\n",
    "                   pool1_t=4, pool2_t=8, dropout=0.5).to(DEVICE)\n",
    "    ds = EEGTrials(X, y, np.zeros(len(y)))\n",
    "    dl = DataLoader(ds, batch_size=BATCH_SIZE_GLOBAL_INTRA, shuffle=True, drop_last=False)\n",
    "    crit = nn.CrossEntropyLoss(); opt = optim.Adam(model.parameters(), lr=LR_GLOBAL_INTRA)\n",
    "    for ep in range(1, epochs+1):\n",
    "        train_epoch(model, dl, opt, crit)\n",
    "        if ep % LOG_EVERY == 0:\n",
    "            y_t, y_p, acc = evaluate_with_preds(model, dl)\n",
    "            print(f\"[GLOBAL PRETRAIN] √âpoca {ep:3d}/{epochs} | train_acc={acc:.4f}\")\n",
    "    return model\n",
    "\n",
    "def run_intra(n_folds=N_FOLDS_INTRA):\n",
    "    ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    OUT_BASE = PROJ / 'models' / 'eegnet_intra_ft'\n",
    "    TAB_DIR = OUT_BASE / 'tables'; LOG_DIR = OUT_BASE / 'logs'; FIG_DIR = OUT_BASE / 'figures'\n",
    "    for d in (TAB_DIR, LOG_DIR, FIG_DIR): d.mkdir(parents=True, exist_ok=True)\n",
    "    mne.set_log_level('WARNING')\n",
    "\n",
    "    subs = subjects_available()\n",
    "    X, y, groups = build_dataset_all_intra(subs, scenario=CLASS_SCENARIO, window_mode=WINDOW_MODE)\n",
    "    print(\"üîπ Entrenando modelo GLOBAL (pretrain sobre todos los sujetos mezclados)...\")\n",
    "    model_global = train_global_model_intra(X, y, epochs=EPOCHS_GLOBAL_INTRA)\n",
    "\n",
    "    subjects = np.unique(groups)\n",
    "    rows, cm_items = [], []\n",
    "    all_true_global, all_pred_global = [], []\n",
    "\n",
    "    for sid in subjects:\n",
    "        idx = np.where(groups == sid)[0]\n",
    "        Xs, ys = X[idx], y[idx]\n",
    "        print(f\"\\n== Sujeto S{sid:03d} ==  N={len(ys)} | T={X.shape[1]} | C={X.shape[2]}\")\n",
    "        skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "        fold_accs, fold_f1s = [], []\n",
    "        y_true_full, y_pred_full = [], []\n",
    "\n",
    "        for fold_i, (tr_idx, te_idx) in enumerate(skf.split(Xs, ys), start=1):\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=FT_VAL_RATIO, random_state=RANDOM_STATE+fold_i)\n",
    "            (cal_idx, va_idx), = sss.split(Xs[tr_idx], ys[tr_idx])\n",
    "            cal_idx = tr_idx[cal_idx]; va_idx = tr_idx[va_idx]; te_idx_ = te_idx\n",
    "            Xcal, ycal = Xs[cal_idx], ys[cal_idx]\n",
    "            Xval, yval = Xs[va_idx], ys[va_idx]\n",
    "            Xtest, ytest = Xs[te_idx_], ys[te_idx_]\n",
    "            print(f\"  [Fold {fold_i}/{n_folds}] train_cal={len(ycal)} | val={len(yval)} | test={len(ytest)}\")\n",
    "\n",
    "            # clones del global\n",
    "            m_out  = copy.deepcopy(model_global).to(DEVICE)\n",
    "            m_head = copy.deepcopy(model_global).to(DEVICE)\n",
    "            m_sp   = copy.deepcopy(model_global).to(DEVICE)\n",
    "            n_classes = len(np.unique(y))\n",
    "\n",
    "            # out/head/spatial+head con selecci√≥n por val_acc\n",
    "            m_out,  accA = _train_one_mode_intra(m_out,  Xcal, ycal, Xval, yval, mode='out',\n",
    "                                                 n_classes=n_classes, epochs=FT_EPOCHS,\n",
    "                                                 head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                                                 patience=FT_PATIENCE, batch_size=16)\n",
    "            m_head, accB = _train_one_mode_intra(m_head, Xcal, ycal, Xval, yval, mode='head',\n",
    "                                                 n_classes=n_classes, epochs=FT_EPOCHS,\n",
    "                                                 head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                                                 patience=FT_PATIENCE, batch_size=16)\n",
    "            m_sp,   accC = _train_one_mode_intra(m_sp,   Xcal, ycal, Xval, yval, mode='spatial+head',\n",
    "                                                 n_classes=n_classes, epochs=FT_EPOCHS,\n",
    "                                                 head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                                                 l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, batch_size=16)\n",
    "\n",
    "            stages = [('out',m_out,accA), ('head',m_head,accB), ('spatial+head',m_sp,accC)]\n",
    "            best_name, best_model, best_val = max(stages, key=lambda t: t[2])\n",
    "\n",
    "            yhat = predict_numpy(best_model, Xtest)\n",
    "            acc = (yhat == ytest).mean()\n",
    "            f1m = f1_score(ytest, yhat, average='macro')\n",
    "            print(f\"    ‚Üí Best stage={best_name} (val_acc={best_val:.4f}) | TEST acc={acc:.4f} | f1m={f1m:.4f}\")\n",
    "\n",
    "            fold_accs.append(float(acc)); fold_f1s.append(float(f1m))\n",
    "            y_true_full.extend(ytest.tolist()); y_pred_full.extend(yhat.tolist())\n",
    "\n",
    "        acc_mu = float(np.mean(fold_accs)) if fold_accs else 0.0\n",
    "        f1_mu  = float(np.mean(fold_f1s)) if fold_f1s else 0.0\n",
    "        rows.append(dict(subject=f\"S{sid:03d}\", acc_mean=acc_mu, f1_macro_mean=f1_mu, k=n_folds))\n",
    "        print(f\"  ‚áí Sujeto S{sid:03d} | ACC={acc_mu:.3f} | F1m={f1_mu:.3f}\")\n",
    "\n",
    "        cm = confusion_matrix(y_true_full, y_pred_full, labels=list(range(len(CLASS_NAMES_4C))))\n",
    "        cm_items.append((f\"S{sid:03d}\", cm, CLASS_NAMES_4C))\n",
    "        all_true_global.extend(y_true_full); all_pred_global.extend(y_pred_full)\n",
    "\n",
    "    # resumen global y guardados (id√©ntico a tu INTRA)\n",
    "    df = pd.DataFrame(rows).sort_values(\"subject\")\n",
    "    acc_mu_glob = float(df['acc_mean'].mean()) if not df.empty else 0.0\n",
    "    acc_sd_glob = float(df['acc_mean'].std(ddof=0)) if not df.empty else 0.0\n",
    "    f1_mu_glob  = float(df['f1_macro_mean'].mean()) if not df.empty else 0.0\n",
    "    f1_sd_glob  = float(df['f1_macro_mean'].std(ddof=0)) if not df.empty else 0.0\n",
    "\n",
    "    df_glob = pd.DataFrame([{'subject':'GLOBAL','acc_mean':acc_mu_glob,'f1_macro_mean':f1_mu_glob,'k':n_folds}])\n",
    "    df_out = pd.concat([df, df_glob], ignore_index=True)\n",
    "\n",
    "    run_tag = f\"{ts}_eegnet_intra_ft_{'0to3000ms' if WINDOW_MODE=='3s' else 'custom'}\"\n",
    "    out_csv = TAB_DIR / f\"{run_tag}_metrics.csv\"\n",
    "    df_out.to_csv(out_csv, index=False); print(f\"\\nüìÑ CSV ‚Üí {out_csv}\")\n",
    "\n",
    "    out_txt = LOG_DIR / f\"{run_tag}_metrics.txt\"\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"EEGNet INTRA FT (progresivo) ‚Äî {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"k-fold por sujeto: {n_folds}\\n\\n\")\n",
    "        f.write(\"subject | acc_mean | f1_macro_mean | k\\n\")\n",
    "        f.write(\"-\"*60 + \"\\n\")\n",
    "        for _, r in df_out.iterrows():\n",
    "            f.write(f\"{r['subject']} | {r['acc_mean']:.4f} | {r['f1_macro_mean']:.4f} | {int(r['k'])}\\n\")\n",
    "        f.write(\"\\nGLOBAL:\\n\")\n",
    "        f.write(f\"ACC={acc_mu_glob:.3f}¬±{acc_sd_glob:.3f} | F1m={f1_mu_glob:.3f}¬±{f1_sd_glob:.3f}\\n\")\n",
    "    print(f\"üìù TXT ‚Üí {out_txt}\")\n",
    "\n",
    "    # mosaicos por sujeto\n",
    "    def _plot_mosaic(cm_items, per_fig=12, n_cols=4):\n",
    "        n = len(cm_items)\n",
    "        if n == 0: return\n",
    "        n_figs = int(np.ceil(n / per_fig))\n",
    "        for fig_idx in range(n_figs):\n",
    "            start, end = fig_idx*per_fig, min((fig_idx+1)*per_fig, n)\n",
    "            chunk = cm_items[start:end]\n",
    "            count = len(chunk); n_rows = int(np.ceil(count / n_cols))\n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(4.5*n_cols, 3.8*n_rows), dpi=140)\n",
    "            axes = np.atleast_2d(axes).flatten()\n",
    "            for ax_i, (label, cm_sum, classes) in enumerate(chunk):\n",
    "                ax = axes[ax_i]\n",
    "                with np.errstate(invalid='ignore'):\n",
    "                    cm_norm = cm_sum.astype('float') / cm_sum.sum(axis=1, keepdims=True)\n",
    "                cm_norm = np.nan_to_num(cm_norm)\n",
    "                ax.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues, vmin=0.0, vmax=1.0)\n",
    "                ax.set_title(label); ax.set_xticks(range(len(classes))); ax.set_yticks(range(len(classes)))\n",
    "                ax.set_xticklabels(classes, rotation=45, ha='right', fontsize=9)\n",
    "                ax.set_yticklabels(classes, fontsize=9)\n",
    "                for i, j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "                    ax.text(j, i, f\"{cm_norm[i, j]:.2f}\", ha=\"center\",\n",
    "                            color=\"white\" if cm_norm[i, j] > 0.5 else \"black\")\n",
    "                ax.set_xlabel(\"\"); ax.set_ylabel(\"\")\n",
    "            for j in range(ax_i + 1, len(axes)): axes[j].axis(\"off\")\n",
    "            fig.suptitle(f\"INTRA ‚Äî Matrices de confusi√≥n por sujeto (p√°gina {fig_idx+1}/{n_figs})\", y=0.995, fontsize=14)\n",
    "            fig.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "            out_png = (PROJ / 'models' / 'eegnet_intra_ft' / 'figures' / f\"{run_tag}_confusions_p{fig_idx+1}.png\")\n",
    "            fig.savefig(out_png); plt.close(fig); print(f\"üñºÔ∏è  Fig ‚Üí {out_png}\")\n",
    "\n",
    "    _plot_mosaic(cm_items, per_fig=12, n_cols=4)\n",
    "\n",
    "    # confusi√≥n global\n",
    "    if len(all_true_global) > 0:\n",
    "        all_true = np.array(all_true_global, int); all_pred = np.array(all_pred_global, int)\n",
    "        cmG = confusion_matrix(all_true, all_pred, labels=list(range(len(CLASS_NAMES_4C))))\n",
    "        plt.figure(figsize=(6.5,5.2), dpi=140)\n",
    "        with np.errstate(invalid='ignore'):\n",
    "            cm_norm = cmG.astype('float') / cmG.sum(axis=1, keepdims=True)\n",
    "        cm_norm = np.nan_to_num(cm_norm)\n",
    "        plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues, vmin=0, vmax=1)\n",
    "        plt.title(\"INTRA ‚Äî Matriz de confusi√≥n GLOBAL\"); plt.colorbar(fraction=0.046, pad=0.04)\n",
    "        ticks = np.arange(len(CLASS_NAMES_4C))\n",
    "        plt.xticks(ticks, CLASS_NAMES_4C, rotation=45, ha='right'); plt.yticks(ticks, CLASS_NAMES_4C)\n",
    "        for i, j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "            plt.text(j, i, f\"{cm_norm[i, j]:.2f}\",\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if cm_norm[i, j] > 0.5 else \"black\")\n",
    "        plt.tight_layout()\n",
    "        out_png = PROJ / 'models' / 'eegnet_intra_ft' / 'figures' / f\"{ts}_global_confusion.png\"\n",
    "        plt.savefig(out_png); plt.close(); print(f\"üñºÔ∏è  Global Fig ‚Üí {out_png}\")\n",
    "\n",
    "    print(f\"\\n[GLOBAL INTRA FT] ACC={acc_mu_glob:.3f}¬±{acc_sd_glob:.3f} | F1m={f1_mu_glob:.3f}¬±{f1_sd_glob:.3f}\")\n",
    "\n",
    "# =========================\n",
    "# FOLDS JSON helpers (INTER)\n",
    "# =========================\n",
    "def save_group_folds_json_with_indices(subject_ids_str, groups_array, n_splits, out_json_path,\n",
    "                                       created_by=\"eegnet_unificado\", description=None):\n",
    "    out_json_path = Path(out_json_path)\n",
    "    unique_subjects_int = sorted(np.unique(groups_array).tolist())\n",
    "    subject_ids = [f\"S{sid:03d}\" for sid in unique_subjects_int]\n",
    "    if len(subject_ids) < n_splits:\n",
    "        raise ValueError(f\"n_splits={n_splits} > n_subjects={len(subject_ids)}\")\n",
    "    groups = np.arange(len(subject_ids))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    folds = []; fold_i = 0\n",
    "    for train_idx_grp, test_idx_grp in gkf.split(groups, groups, groups):\n",
    "        fold_i += 1\n",
    "        train_sids = [subject_ids[int(i)] for i in train_idx_grp]\n",
    "        test_sids  = [subject_ids[int(i)] for i in test_idx_grp]\n",
    "        train_sids_int = [int(s[1:]) for s in train_sids]\n",
    "        test_sids_int  = [int(s[1:]) for s in test_sids]\n",
    "        tr_idx = np.where(np.isin(groups_array, train_sids_int))[0].tolist()\n",
    "        te_idx = np.where(np.isin(groups_array, test_sids_int))[0].tolist()\n",
    "        folds.append({\"fold\": int(fold_i), \"train\": train_sids, \"test\": test_sids,\n",
    "                      \"tr_idx\": tr_idx, \"te_idx\": te_idx})\n",
    "    payload = {\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"created_by\": created_by,\n",
    "        \"description\": description if description is not None else \"\",\n",
    "        \"n_splits\": int(n_splits),\n",
    "        \"n_subjects\": len(subject_ids),\n",
    "        \"subject_ids\": subject_ids,\n",
    "        \"folds\": folds\n",
    "    }\n",
    "    out_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Folds JSON con √≠ndices guardado ‚Üí {out_json_path}\")\n",
    "    return out_json_path\n",
    "\n",
    "def load_group_folds_json(path_json, expected_subject_ids=None, strict_check=True):\n",
    "    path_json = Path(path_json)\n",
    "    if not path_json.exists(): raise FileNotFoundError(f\"No existe {path_json}\")\n",
    "    with open(path_json, \"r\", encoding=\"utf-8\") as f: payload = json.load(f)\n",
    "    subj_json = payload.get(\"subject_ids\", [])\n",
    "    if expected_subject_ids is not None:\n",
    "        expected = sorted(list(expected_subject_ids))\n",
    "        if subj_json != expected:\n",
    "            msg = (\"subject_ids del JSON no coinciden.\\n\"\n",
    "                   f\"JSON has {len(subj_json)} vs expected {len(expected)}.\")\n",
    "            if strict_check: raise ValueError(msg)\n",
    "            else: print(\"WARNING:\", msg)\n",
    "    return payload\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Lista de sujetos elegibles (solo para feedback r√°pido)\n",
    "    subs = subjects_available()\n",
    "    if len(subs) == 0:\n",
    "        print(\"No hay sujetos elegibles en data/raw (o todos excluidos).\")\n",
    "    else:\n",
    "        print(f\"Sujetos elegibles: {len(subs)} ‚Üí {subs[:10]}{'...' if len(subs)>10 else ''}\")\n",
    "\n",
    "    print(f\"üîß Configuraci√≥n com√∫n: escenario={CLASS_SCENARIO}, ventana={WINDOW_MODE}, fs={FS}\")\n",
    "\n",
    "    # === Elige UNO: descomenta la l√≠nea del modo que quieras correr ===\n",
    "\n",
    "    # --- MODO INTER-SUJETO (cross-subject) ---\n",
    "    run_inter(\n",
    "        save_folds_json=True,\n",
    "        folds_json_path=FOLDS_JSON_DEFAULT,\n",
    "        folds_json_description=\"GroupKFold folds for comparison\"\n",
    "    )\n",
    "\n",
    "    # --- MODO INTRA-SUJETO (k-fold por sujeto) ---\n",
    "    # run_intra(n_folds=N_FOLDS_INTRA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1074d128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Usando dispositivo: cuda\n",
      "üß† INICIANDO EXPERIMENTO CON EEGNet + FINE-TUNING PROGRESIVO (por sujeto, 4-fold CV)\n",
      "üîß Configuraci√≥n: 4c, 8 canales, 6s\n",
      "‚öôÔ∏è  FT: epochs=30, base_lr=5e-05, head_lr=0.001, L2SP=0.0001, patience=5, CV=4\n",
      "üß≤ SupCon: enabled=False, epochs=40, batch=64, temp=0.07\n",
      "Sujetos elegibles: 103 ‚Üí [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Construyendo dataset (RAW): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:20<00:00,  5.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset construido: N=8652 | T=960 | C=8 | clases=4 | sujetos √∫nicos=103\n",
      "Listo para entrenar: N=8652 | T=960 | C=8 | clases=4 | sujetos=103\n",
      "\n",
      "[Fold 1/5] Entrenando modelo global con validaci√≥n interna por sujetos... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  √âpoca   5 | train_acc=0.4545 | val_acc=0.4203\n",
      "  √âpoca  10 | train_acc=0.4827 | val_acc=0.4359\n",
      "  √âpoca  15 | train_acc=0.4841 | val_acc=0.4295\n",
      "  √âpoca  20 | train_acc=0.4990 | val_acc=0.4606\n",
      "  √âpoca  25 | train_acc=0.4928 | val_acc=0.4487\n",
      "  √âpoca  30 | train_acc=0.5093 | val_acc=0.4670\n",
      "  √âpoca  35 | train_acc=0.5047 | val_acc=0.4625\n",
      "  √âpoca  40 | train_acc=0.5000 | val_acc=0.4734\n",
      "  √âpoca  45 | train_acc=0.5060 | val_acc=0.4634\n",
      "  √âpoca  50 | train_acc=0.5091 | val_acc=0.4615\n",
      "  √âpoca  55 | train_acc=0.4945 | val_acc=0.4570\n",
      "  √âpoca  60 | train_acc=0.5166 | val_acc=0.4725\n",
      "  √âpoca  65 | train_acc=0.5185 | val_acc=0.4588\n",
      "  √âpoca  70 | train_acc=0.5155 | val_acc=0.4689\n",
      "  √âpoca  75 | train_acc=0.5207 | val_acc=0.4789\n",
      "  √âpoca  80 | train_acc=0.5078 | val_acc=0.4634\n",
      "  √âpoca  85 | train_acc=0.5217 | val_acc=0.4570\n",
      "  √âpoca  90 | train_acc=0.5160 | val_acc=0.4625\n",
      "  √âpoca  95 | train_acc=0.5226 | val_acc=0.4734\n",
      "  √âpoca 100 | train_acc=0.5207 | val_acc=0.4707\n",
      "[Fold 1/5] Global acc=0.4473\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5196    0.4807    0.4994       441\n",
      "       Right     0.4730    0.5760    0.5194       441\n",
      "  Both Fists     0.3524    0.3764    0.3640       441\n",
      "   Both Feet     0.4511    0.3560    0.3980       441\n",
      "\n",
      "    accuracy                         0.4473      1764\n",
      "   macro avg     0.4490    0.4473    0.4452      1764\n",
      "weighted avg     0.4490    0.4473    0.4452      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5153 | sujetos=21\n",
      "  Œî(FT-Global) = +0.0680\n",
      "\n",
      "[Fold 2/5] Entrenando modelo global con validaci√≥n interna por sujetos... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  √âpoca   5 | train_acc=0.3879 | val_acc=0.3764\n",
      "  √âpoca  10 | train_acc=0.4227 | val_acc=0.4066\n",
      "  √âpoca  15 | train_acc=0.4329 | val_acc=0.4029\n",
      "  √âpoca  20 | train_acc=0.4425 | val_acc=0.4103\n",
      "  √âpoca  25 | train_acc=0.4614 | val_acc=0.4350\n",
      "  √âpoca  30 | train_acc=0.4639 | val_acc=0.4277\n",
      "  √âpoca  35 | train_acc=0.4739 | val_acc=0.4377\n",
      "  √âpoca  40 | train_acc=0.4727 | val_acc=0.4322\n",
      "  √âpoca  45 | train_acc=0.4750 | val_acc=0.4368\n",
      "  √âpoca  50 | train_acc=0.4779 | val_acc=0.4451\n",
      "  √âpoca  55 | train_acc=0.4748 | val_acc=0.4286\n",
      "  √âpoca  60 | train_acc=0.4896 | val_acc=0.4469\n",
      "  √âpoca  65 | train_acc=0.4859 | val_acc=0.4505\n",
      "  √âpoca  70 | train_acc=0.4960 | val_acc=0.4579\n",
      "  √âpoca  75 | train_acc=0.4893 | val_acc=0.4542\n",
      "  √âpoca  80 | train_acc=0.4903 | val_acc=0.4451\n",
      "  √âpoca  85 | train_acc=0.4915 | val_acc=0.4441\n",
      "  √âpoca  90 | train_acc=0.4943 | val_acc=0.4414\n",
      "  √âpoca  95 | train_acc=0.4914 | val_acc=0.4432\n",
      "  √âpoca 100 | train_acc=0.5038 | val_acc=0.4551\n",
      "[Fold 2/5] Global acc=0.4643\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5287    0.5215    0.5251       441\n",
      "       Right     0.5012    0.4785    0.4896       441\n",
      "  Both Fists     0.4131    0.3719    0.3914       441\n",
      "   Both Feet     0.4188    0.4853    0.4496       441\n",
      "\n",
      "    accuracy                         0.4643      1764\n",
      "   macro avg     0.4655    0.4643    0.4639      1764\n",
      "weighted avg     0.4655    0.4643    0.4639      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5771 | sujetos=21\n",
      "  Œî(FT-Global) = +0.1128\n",
      "\n",
      "[Fold 3/5] Entrenando modelo global con validaci√≥n interna por sujetos... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  √âpoca   5 | train_acc=0.4341 | val_acc=0.4386\n",
      "  √âpoca  10 | train_acc=0.4650 | val_acc=0.4789\n",
      "  √âpoca  15 | train_acc=0.4783 | val_acc=0.4835\n",
      "  √âpoca  20 | train_acc=0.4774 | val_acc=0.4762\n",
      "  √âpoca  25 | train_acc=0.4893 | val_acc=0.4918\n",
      "  √âpoca  30 | train_acc=0.4883 | val_acc=0.4991\n",
      "  √âpoca  35 | train_acc=0.4983 | val_acc=0.4771\n",
      "  √âpoca  40 | train_acc=0.5003 | val_acc=0.4982\n",
      "  √âpoca  45 | train_acc=0.5026 | val_acc=0.5027\n",
      "  √âpoca  50 | train_acc=0.4978 | val_acc=0.5128\n",
      "  √âpoca  55 | train_acc=0.4978 | val_acc=0.5009\n",
      "  √âpoca  60 | train_acc=0.5024 | val_acc=0.4954\n",
      "  √âpoca  65 | train_acc=0.5109 | val_acc=0.4991\n",
      "  √âpoca  70 | train_acc=0.5043 | val_acc=0.4973\n",
      "  √âpoca  75 | train_acc=0.5074 | val_acc=0.4872\n",
      "  √âpoca  80 | train_acc=0.5062 | val_acc=0.5082\n",
      "  √âpoca  85 | train_acc=0.5038 | val_acc=0.5018\n",
      "  √âpoca  90 | train_acc=0.4945 | val_acc=0.4799\n",
      "  √âpoca  95 | train_acc=0.5059 | val_acc=0.4899\n",
      "  √âpoca 100 | train_acc=0.4990 | val_acc=0.5064\n",
      "  Early stopping en √©poca 100 (mejor val_acc=0.5128)\n",
      "[Fold 3/5] Global acc=0.4320\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.4429    0.5624    0.4955       441\n",
      "       Right     0.5152    0.5397    0.5271       441\n",
      "  Both Fists     0.3719    0.2698    0.3127       441\n",
      "   Both Feet     0.3720    0.3560    0.3638       441\n",
      "\n",
      "    accuracy                         0.4320      1764\n",
      "   macro avg     0.4255    0.4320    0.4248      1764\n",
      "weighted avg     0.4255    0.4320    0.4248      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.4881 | sujetos=21\n",
      "  Œî(FT-Global) = +0.0561\n",
      "\n",
      "[Fold 4/5] Entrenando modelo global con validaci√≥n interna por sujetos... (n_train=5880 | n_val=1092 | n_test=1680)\n",
      "  √âpoca   5 | train_acc=0.4090 | val_acc=0.3755\n",
      "  √âpoca  10 | train_acc=0.4274 | val_acc=0.3947\n",
      "  √âpoca  15 | train_acc=0.4583 | val_acc=0.4139\n",
      "  √âpoca  20 | train_acc=0.4617 | val_acc=0.4093\n",
      "  √âpoca  25 | train_acc=0.4677 | val_acc=0.4130\n",
      "  √âpoca  30 | train_acc=0.4755 | val_acc=0.4084\n",
      "  √âpoca  35 | train_acc=0.4772 | val_acc=0.4212\n",
      "  √âpoca  40 | train_acc=0.4871 | val_acc=0.4277\n",
      "  √âpoca  45 | train_acc=0.4849 | val_acc=0.4414\n",
      "  √âpoca  50 | train_acc=0.4818 | val_acc=0.4414\n",
      "  √âpoca  55 | train_acc=0.4906 | val_acc=0.4551\n",
      "  √âpoca  60 | train_acc=0.4895 | val_acc=0.4487\n",
      "  √âpoca  65 | train_acc=0.4871 | val_acc=0.4386\n",
      "  √âpoca  70 | train_acc=0.4978 | val_acc=0.4588\n",
      "  √âpoca  75 | train_acc=0.4998 | val_acc=0.4505\n",
      "  √âpoca  80 | train_acc=0.4968 | val_acc=0.4643\n",
      "  √âpoca  85 | train_acc=0.4949 | val_acc=0.4533\n",
      "  √âpoca  90 | train_acc=0.4964 | val_acc=0.4496\n",
      "  √âpoca  95 | train_acc=0.4976 | val_acc=0.4744\n",
      "  √âpoca 100 | train_acc=0.5020 | val_acc=0.4551\n",
      "[Fold 4/5] Global acc=0.4494\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5450    0.5762    0.5602       420\n",
      "       Right     0.5273    0.4143    0.4640       420\n",
      "  Both Fists     0.3468    0.3690    0.3576       420\n",
      "   Both Feet     0.4009    0.4381    0.4187       420\n",
      "\n",
      "    accuracy                         0.4494      1680\n",
      "   macro avg     0.4550    0.4494    0.4501      1680\n",
      "weighted avg     0.4550    0.4494    0.4501      1680\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5571 | sujetos=20\n",
      "  Œî(FT-Global) = +0.1077\n",
      "\n",
      "[Fold 5/5] Entrenando modelo global con validaci√≥n interna por sujetos... (n_train=5880 | n_val=1092 | n_test=1680)\n",
      "  √âpoca   5 | train_acc=0.4359 | val_acc=0.4112\n",
      "  √âpoca  10 | train_acc=0.4571 | val_acc=0.4267\n",
      "  √âpoca  15 | train_acc=0.4544 | val_acc=0.4167\n",
      "  √âpoca  20 | train_acc=0.4699 | val_acc=0.4414\n",
      "  √âpoca  25 | train_acc=0.4789 | val_acc=0.4496\n",
      "  √âpoca  30 | train_acc=0.4787 | val_acc=0.4396\n",
      "  √âpoca  35 | train_acc=0.4733 | val_acc=0.4313\n",
      "  √âpoca  40 | train_acc=0.4864 | val_acc=0.4487\n",
      "  √âpoca  45 | train_acc=0.4816 | val_acc=0.4423\n",
      "  √âpoca  50 | train_acc=0.4872 | val_acc=0.4524\n",
      "  √âpoca  55 | train_acc=0.4918 | val_acc=0.4432\n",
      "  √âpoca  60 | train_acc=0.4920 | val_acc=0.4643\n",
      "  √âpoca  65 | train_acc=0.4854 | val_acc=0.4570\n",
      "  √âpoca  70 | train_acc=0.4939 | val_acc=0.4505\n",
      "  √âpoca  75 | train_acc=0.4932 | val_acc=0.4496\n",
      "  √âpoca  80 | train_acc=0.4932 | val_acc=0.4515\n",
      "  √âpoca  85 | train_acc=0.4827 | val_acc=0.4460\n",
      "  √âpoca  90 | train_acc=0.4874 | val_acc=0.4551\n",
      "  √âpoca  95 | train_acc=0.4898 | val_acc=0.4460\n",
      "  √âpoca 100 | train_acc=0.5031 | val_acc=0.4505\n",
      "[Fold 5/5] Global acc=0.5030\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5595    0.6381    0.5962       420\n",
      "       Right     0.5376    0.5952    0.5650       420\n",
      "  Both Fists     0.4382    0.3548    0.3921       420\n",
      "   Both Feet     0.4495    0.4238    0.4363       420\n",
      "\n",
      "    accuracy                         0.5030      1680\n",
      "   macro avg     0.4962    0.5030    0.4974      1680\n",
      "weighted avg     0.4962    0.5030    0.4974      1680\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5655 | sujetos=20\n",
      "  Œî(FT-Global) = +0.0625\n",
      "\n",
      "============================================================\n",
      "RESULTADOS FINALES\n",
      "============================================================\n",
      "Global folds: ['0.4473', '0.4643', '0.4320', '0.4494', '0.5030']\n",
      "Global mean: 0.4592\n",
      "Fine-tune PROGRESIVO folds: ['0.5153', '0.5771', '0.4881', '0.5571', '0.5655']\n",
      "Fine-tune PROGRESIVO mean: 0.5406\n",
      "Œî(FT-Global) mean: +0.0814\n",
      "\n",
      "‚Ü≥ Matriz de confusi√≥n guardada: confusion_global_allfolds.png\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Replicaci√≥n fiel del paper \"A Deep Learning MI-EEG Classification Model for BCIs\"\n",
    "# Dose et al., EUSIPCO 2018 ‚Äî ahora con EEGNet (Lawhern et al., 2018) en vez de ShallowConvNet\n",
    "# Protocolo: entrenamiento global inter-sujeto + FINE-TUNING PROGRESIVO por sujeto\n",
    "# (CV 4-fold, LRs discriminativos, L2-SP, early stopping con validaci√≥n)\n",
    "# + SupCon preentrenamiento contrastivo supervisado (opcional)\n",
    "\n",
    "import os, re, math, random, json, itertools, copy\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold, StratifiedShuffleSplit, GroupShuffleSplit\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# CONFIGURACI√ìN GENERAL\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "FOLDS_DIR = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dispositivo y semilla\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "# Escenario de clases\n",
    "CLASS_SCENARIO = '4c'\n",
    "WINDOW_MODE = '6s'\n",
    "FS = 160.0\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Entrenamiento global\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS_GLOBAL = 100\n",
    "LR = 1e-3\n",
    "# >>> A√±adidos para diagn√≥stico/ES global <<<\n",
    "GLOBAL_VAL_SPLIT = 0.15   # fracci√≥n de sujetos (dentro del train) para validaci√≥n\n",
    "GLOBAL_PATIENCE  = 10     # √©pocas sin mejora en val_acc\n",
    "LOG_EVERY        = 5      # log cada N √©pocas\n",
    "\n",
    "# Fine-tuning por sujeto (protocolo robusto)\n",
    "CALIB_CV_FOLDS = 4            # 4-fold CV por sujeto ~ 75/25\n",
    "FT_EPOCHS = 30                 # ES con validaci√≥n\n",
    "FT_BASE_LR = 5e-5              # convs \"base\" (temporal/depthwise) ‚Äî m√°s bajo\n",
    "FT_HEAD_LR = 1e-3              # fc+out ‚Äî m√°s alto\n",
    "FT_L2SP = 1e-4                 # regularizaci√≥n suave\n",
    "FT_PATIENCE = 5                # early stopping con validaci√≥n\n",
    "FT_VAL_RATIO = 0.2             # validaci√≥n dentro del set de calibraci√≥n\n",
    "\n",
    "# Sujetos excluidos\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "\n",
    "# Runs\n",
    "MI_RUNS_LR = [4, 8, 12]\n",
    "MI_RUNS_OF = [6, 10, 14]\n",
    "BASELINE_RUNS_EO = [1]\n",
    "\n",
    "# Canales (8 en lugar de 64)\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','CPz']\n",
    "\n",
    "# ====== SupCon (preentrenamiento contrastivo supervisado) ======\n",
    "USE_SUPCON_PRETRAIN = False      # <- pon False para desactivarlo\n",
    "SUPCON_EPOCHS = 40\n",
    "SUPCON_BATCH = 64               # intenta que sea >=64 si cabe en GPU\n",
    "SUPCON_LR = 1e-3\n",
    "SUPCON_TEMP = 0.07\n",
    "SUPCON_PROJ_DIM = 128\n",
    "SUPCON_LOG_EVERY = 5\n",
    "\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES DE CANALES\n",
    "# =========================\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = re.sub(r'([A-Za-z])0([0-9])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])Z$', r'\\1z', s)\n",
    "    s = s.replace('fp', 'Fp').replace('FP', 'Fp')\n",
    "    s = ''.join(ch.upper() if ch != 'z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {}\n",
    "    for ch in raw.ch_names:\n",
    "        lab = normalize_label(ch)\n",
    "        lab = lab[:-1] + 'z' if lab.endswith('Z') else lab\n",
    "        lab = re.sub(r'([A-Z])Z$', r'\\1z', lab)\n",
    "        mapping[ch] = lab\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired_channels=EXPECTED_8):\n",
    "    have = [ch for ch in desired_channels if ch in raw.ch_names]\n",
    "    missing = [ch for ch in desired_channels if ch not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"Warning: faltan canales {missing} en archivo {getattr(raw,'filenames', [''])[0]}\")\n",
    "        return None\n",
    "    # Reordenar y quedarse SOLO con los deseados (8)\n",
    "    raw.reorder_channels([ch for ch in raw.ch_names if ch in desired_channels] +\n",
    "                         [ch for ch in raw.ch_names if ch not in desired_channels])\n",
    "    raw.pick_channels(desired_channels, ordered=True)\n",
    "    return raw\n",
    "\n",
    "# =========================\n",
    "# LECTURA DE EDF y EVENTOS\n",
    "# =========================\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(run_id:int):\n",
    "    if run_id in MI_RUNS_LR: return 'LR'\n",
    "    if run_id in MI_RUNS_OF: return 'OF'\n",
    "    if run_id in BASELINE_RUNS_EO: return 'EO'\n",
    "    return None\n",
    "\n",
    "def read_raw_edf(path: Path):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        mont = mne.channels.make_standard_montage('standard_1020')\n",
    "        raw.set_montage(mont, on_missing='ignore')\n",
    "    except Exception:\n",
    "        pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad=\"auto\")\n",
    "    raw = ensure_channels_order(raw, EXPECTED_8)\n",
    "    if raw is None:\n",
    "        return None\n",
    "    # Band-pass opcional (desactivado)\n",
    "    #raw.filter(l_freq=8., h_freq=30., picks='eeg', method='iir', verbose=False)\n",
    "    return raw\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    if raw.annotations is None or len(raw.annotations) == 0:\n",
    "        return []\n",
    "    def _norm(s): return str(s).strip().upper().replace(' ', '')\n",
    "    res = []\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = _norm(desc)\n",
    "        if tag in ('T1','T2'):\n",
    "            res.append((float(onset), tag))\n",
    "    res.sort()\n",
    "    dedup = []\n",
    "    last_t1 = last_t2 = -1e9\n",
    "    for t, tag in res:\n",
    "        if tag == 'T1':\n",
    "            if (t - last_t1) >= 0.5: dedup.append((t, tag)); last_t1 = t\n",
    "        else:\n",
    "            if (t - last_t2) >= 0.5: dedup.append((t, tag)); last_t2 = t\n",
    "    return dedup\n",
    "\n",
    "# =========================\n",
    "# CONSTRUCCI√ìN DE DATASETS\n",
    "# =========================\n",
    "def subjects_available():\n",
    "    subs = []\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try:\n",
    "            sid = int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir / f\"S{sid:03d}R{r:02d}.edf\").exists() for r in (MI_RUNS_LR + MI_RUNS_OF))\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "def extract_trials_from_run(edf_path: Path, scenario: str, window_mode: str):\n",
    "    subj, run = parse_subject_run(edf_path)\n",
    "    kind = run_kind(run)\n",
    "    if kind not in ('LR','OF','EO'):\n",
    "        return ([], [])\n",
    "\n",
    "    raw = read_raw_edf(edf_path)\n",
    "    if raw is None:\n",
    "        return ([], [])\n",
    "\n",
    "    data = raw.get_data()\n",
    "    fs = raw.info['sfreq']\n",
    "    assert abs(fs - FS) < 1e-6\n",
    "\n",
    "    out = []\n",
    "\n",
    "    if kind in ('LR','OF'):\n",
    "        events = collect_events_T1T2(raw)\n",
    "        if window_mode == '3s':\n",
    "            rel_start, rel_end = 0.0, 3.0\n",
    "        else:\n",
    "            rel_start, rel_end = -1.0, 5.0\n",
    "\n",
    "        for onset_sec, tag in events:\n",
    "            if kind == 'LR':\n",
    "                if tag == 'T1': label = 'L'\n",
    "                elif tag == 'T2': label = 'R'\n",
    "                else: continue\n",
    "            else:\n",
    "                if tag == 'T1': label = 'BFISTS'\n",
    "                elif tag == 'T2': label = 'BFEET'\n",
    "                else: continue\n",
    "\n",
    "            if scenario == '2c' and label not in ('L','R'):\n",
    "                continue\n",
    "            if scenario == '3c' and label not in ('L','R','BFISTS'):\n",
    "                continue\n",
    "            if scenario == '4c' and label not in ('L','R','BFISTS','BFEET'):\n",
    "                continue\n",
    "\n",
    "            s = int(round((raw.first_time + onset_sec + rel_start) * fs))\n",
    "            e = int(round((raw.first_time + onset_sec + rel_end) * fs))\n",
    "            if s < 0 or e > data.shape[1]:\n",
    "                continue\n",
    "\n",
    "            seg = data[:, s:e].T.astype(np.float32)\n",
    "            # Normalizaci√≥n por √©poca canal-a-canal (z-score)\n",
    "            seg = (seg - seg.mean(axis=0, keepdims=True)) / (seg.std(axis=0, keepdims=True) + 1e-6)\n",
    "\n",
    "            if label == 'L':       y = 0\n",
    "            elif label == 'R':     y = 1\n",
    "            elif label == 'BFISTS':y = 2\n",
    "            elif label == 'BFEET': y = 3\n",
    "            else: continue\n",
    "\n",
    "            out.append((seg, y, subj))\n",
    "\n",
    "    elif kind == 'EO':\n",
    "        return ([], raw.ch_names)\n",
    "\n",
    "    return out, raw.ch_names\n",
    "\n",
    "def build_dataset_all(subjects, scenario='4c', window_mode='3s'):\n",
    "    X, y, groups = [], [], []\n",
    "    ch_template = None\n",
    "\n",
    "    for s in tqdm(subjects, desc=\"Construyendo dataset (RAW)\"):\n",
    "        sdir = DATA_RAW / f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "\n",
    "        trials_L, trials_R, trials_FISTS, trials_FEET = [], [], [], []\n",
    "\n",
    "        for r in MI_RUNS_LR:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 0: trials_L.append(seg)\n",
    "                elif lab == 1: trials_R.append(seg)\n",
    "\n",
    "        for r in MI_RUNS_OF:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 2: trials_FISTS.append(seg)\n",
    "                elif lab == 3: trials_FEET.append(seg)\n",
    "\n",
    "        need_per_class = 21\n",
    "        def pick(trials, n, rng):\n",
    "            if len(trials) < n:\n",
    "                idx = rng.choice(len(trials), size=n, replace=True)\n",
    "                return [trials[i] for i in idx]\n",
    "            rng.shuffle(trials)\n",
    "            return trials[:n]\n",
    "\n",
    "        rng = check_random_state(RANDOM_STATE + s)\n",
    "        if len(trials_L)==0 or len(trials_R)==0 or len(trials_FISTS)==0 or len(trials_FEET)==0:\n",
    "            continue\n",
    "\n",
    "        Lp  = pick(trials_L,     need_per_class, rng)\n",
    "        Rp  = pick(trials_R,     need_per_class, rng)\n",
    "        FIp = pick(trials_FISTS, need_per_class, rng)\n",
    "        FEp = pick(trials_FEET,  need_per_class, rng)\n",
    "\n",
    "        pack = [(Lp, 0), (Rp, 1), (FIp, 2), (FEp, 3)]\n",
    "        for segs, lab in pack:\n",
    "            for seg in segs:\n",
    "                X.append(seg)\n",
    "                y.append(lab)\n",
    "                groups.append(s)\n",
    "\n",
    "    X = np.stack(X, axis=0)\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "    groups = np.asarray(groups, dtype=np.int64)\n",
    "\n",
    "    n, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Dataset construido: N={n} | T={T} | C={C} | clases={n_classes} | sujetos √∫nicos={len(np.unique(groups))}\")\n",
    "    return X, y, groups, ch_template\n",
    "\n",
    "# =========================\n",
    "# EEGNet (Lawhern et al., 2018) adaptado a (B,1,T,C)\n",
    "# =========================\n",
    "class EEGNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Entrada: x de forma (B, 1, T, C)  [T=tiempo, C=canales]\n",
    "    Bloques:\n",
    "      1) Temporal conv     : Conv2d(1 -> F1, (kernel_t,1), padding 'same'), BN, ELU\n",
    "      2) Depthwise (espacial): Conv2d(F1 -> F1*D, (1,C), groups=F1, BN, ELU, AvgPool(4,1), Dropout\n",
    "      3) Separable temporal: Depthwise temporal (k_sep,1) groups=F1*D + Pointwise 1x1 a F2, BN, ELU, AvgPool(8,1), Dropout\n",
    "      4) FC -> OUT\n",
    "    \"\"\"\n",
    "    def __init__(self, n_ch: int, n_classes: int,\n",
    "                 F1: int = 8, D: int = 2, kernel_t: int = 64, k_sep: int = 16,\n",
    "                 pool1_t: int = 4, pool2_t: int = 8, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.n_ch = n_ch\n",
    "        self.n_classes = n_classes\n",
    "        self.F1 = F1\n",
    "        self.D = D\n",
    "        self.F2 = F1 * D\n",
    "        self.kernel_t = kernel_t\n",
    "        self.k_sep = k_sep\n",
    "        self.pool1_t = pool1_t\n",
    "        self.pool2_t = pool2_t\n",
    "\n",
    "        # Bloque 1: temporal\n",
    "        self.conv_temporal = nn.Conv2d(1, F1, kernel_size=(kernel_t, 1),\n",
    "                                       padding=(kernel_t // 2, 0), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(F1)\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "        # Bloque 2: depthwise (espacial)\n",
    "        self.conv_depthwise = nn.Conv2d(F1, self.F2, kernel_size=(1, n_ch),\n",
    "                                        groups=F1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.F2)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=(pool1_t, 1), stride=(pool1_t, 1))\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "\n",
    "        # Bloque 3: separable temporal (depthwise temporal + pointwise)\n",
    "        self.conv_sep_depth = nn.Conv2d(self.F2, self.F2, kernel_size=(k_sep, 1),\n",
    "                                        groups=self.F2, padding=(k_sep // 2, 0), bias=False)\n",
    "        self.conv_sep_point = nn.Conv2d(self.F2, self.F2, kernel_size=(1, 1), bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.F2)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(pool2_t, 1), stride=(pool2_t, 1))\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Cabeza din√°mica\n",
    "        self.fc = None\n",
    "        self.out = None\n",
    "        self._T_in = None\n",
    "\n",
    "    def _build_head(self, T_in: int, device: torch.device):\n",
    "        # Con padding 'same' en temporal y separable temporal,\n",
    "        # el tama√±o temporal se reduce por los pools:\n",
    "        T1 = T_in // self.pool1_t\n",
    "        T2 = T1 // self.pool2_t\n",
    "        feat_dim = self.F2 * T2 * 1  # ancho=1 tras conv_depthwise (kernel (1,C))\n",
    "        self.fc = nn.Linear(feat_dim, 80, bias=True).to(device)\n",
    "        self.out = nn.Linear(80, self.n_classes, bias=True).to(device)\n",
    "        self._T_in = T_in\n",
    "\n",
    "    def ensure_head(self, T_in: int, device: torch.device):\n",
    "        if (self.fc is None) or (self.out is None) or (self._T_in != T_in):\n",
    "            self._build_head(T_in, device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B,1,T,C)\n",
    "        B, _, T, C = x.shape\n",
    "        self.ensure_head(T, x.device)\n",
    "\n",
    "        z = self.conv_temporal(x)\n",
    "        z = self.bn1(z); z = self.act(z)\n",
    "\n",
    "        z = self.conv_depthwise(z)   # (B, F2, T, 1)\n",
    "        z = self.bn2(z); z = self.act(z)\n",
    "        z = self.pool1(z)\n",
    "        z = self.drop1(z)\n",
    "\n",
    "        z = self.conv_sep_depth(z)\n",
    "        z = self.conv_sep_point(z)\n",
    "        z = self.bn3(z); z = self.act(z)\n",
    "        z = self.pool2(z)\n",
    "        z = self.drop2(z)\n",
    "\n",
    "        z = self.flatten(z)\n",
    "        z = self.fc(z); z = self.act(z)\n",
    "        z = self.out(z)\n",
    "        return z\n",
    "\n",
    "    def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Devuelve el embedding pre-logits tras fc+ELU (dim=80).\n",
    "        x: (B,1,T,C)\n",
    "        \"\"\"\n",
    "        B, _, T, C = x.shape\n",
    "        self.ensure_head(T, x.device)\n",
    "\n",
    "        z = self.conv_temporal(x); z = self.bn1(z); z = self.act(z)\n",
    "        z = self.conv_depthwise(z); z = self.bn2(z); z = self.act(z)\n",
    "        z = self.pool1(z); z = self.drop1(z)\n",
    "        z = self.conv_sep_depth(z); z = self.conv_sep_point(z)\n",
    "        z = self.bn3(z); z = self.act(z)\n",
    "        z = self.pool2(z); z = self.drop2(z)\n",
    "        z = self.flatten(z)\n",
    "        z = self.fc(z); z = self.act(z)   # embedding 80-D\n",
    "        return z\n",
    "\n",
    "# =========================\n",
    "# TORCH DATASET\n",
    "# =========================\n",
    "class EEGTrials(Dataset):\n",
    "    def __init__(self, X, y, groups):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.g = groups.astype(np.int64)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        x = np.expand_dims(x, 0)                 # (1, T, C)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx])\n",
    "\n",
    "CLASS_NAMES_4C = ['Left', 'Right', 'Both Fists', 'Both Feet']\n",
    "\n",
    "# ====== Augmentations EEG para SupCon ======\n",
    "class EEGAugment(torch.nn.Module):\n",
    "    def __init__(self, p_jitter=0.5, max_jitter=16,  # ~100 ms a 160 Hz\n",
    "                 p_noise=0.8, noise_std=0.02,\n",
    "                 p_tmask=0.4, tmask_max=32,          # ~200 ms\n",
    "                 p_cdrop=0.2, cdrop_max=1):          # drop 0-1 canales\n",
    "        super().__init__()\n",
    "        self.p_jitter = p_jitter\n",
    "        self.max_jitter = max_jitter\n",
    "        self.p_noise = p_noise\n",
    "        self.noise_std = noise_std\n",
    "        self.p_tmask = p_tmask\n",
    "        self.tmask_max = tmask_max\n",
    "        self.p_cdrop = p_cdrop\n",
    "        self.cdrop_max = cdrop_max\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Acepta:\n",
    "          - (1, T, C)  -> ejemplo √∫nico (3D)\n",
    "          - (B, 1, T, C) -> batch (4D)\n",
    "        Devuelve con la misma dimensionalidad de entrada.\n",
    "        \"\"\"\n",
    "        original_3d = False\n",
    "        if x.dim() == 3:           # (1, T, C)\n",
    "            original_3d = True\n",
    "            x = x.unsqueeze(0)     # -> (B=1, 1, T, C)\n",
    "        elif x.dim() != 4:\n",
    "            raise ValueError(f\"EEGAugment espera 3D o 4D, recibido {x.dim()}D\")\n",
    "\n",
    "        B, _, T, C = x.shape\n",
    "        out = x.clone()\n",
    "\n",
    "        # Jitter temporal\n",
    "        if torch.rand(1).item() < self.p_jitter and T > 2:\n",
    "            shift = int(torch.randint(-self.max_jitter, self.max_jitter + 1, (1,)).item())\n",
    "            if shift > 0:\n",
    "                out[:, :, shift:, :] = out[:, :, :-shift, :].clone()\n",
    "            elif shift < 0:\n",
    "                out[:, :, :shift, :] = out[:, :, -shift:, :].clone()\n",
    "\n",
    "        # Ruido gaussiano leve\n",
    "        if torch.rand(1).item() < self.p_noise:\n",
    "            out = out + torch.randn_like(out) * self.noise_std\n",
    "\n",
    "        # Time masking corto\n",
    "        if torch.rand(1).item() < self.p_tmask and T > 4:\n",
    "            w = int(torch.randint(1, self.tmask_max + 1, (1,)).item())\n",
    "            s = int(torch.randint(0, max(1, T - w), (1,)).item())\n",
    "            out[:, :, s:s + w, :] = 0.0\n",
    "\n",
    "        # Channel dropout\n",
    "        if torch.rand(1).item() < self.p_cdrop and C > 1:\n",
    "            k = int(torch.randint(1, self.cdrop_max + 1, (1,)).item())\n",
    "            ch = torch.randperm(C)[:k]\n",
    "            out[:, :, :, ch] = 0.0\n",
    "\n",
    "        if original_3d:\n",
    "            out = out.squeeze(0)   # vuelve a (1, T, C)\n",
    "        return out\n",
    "\n",
    "class ContrastiveTrials(Dataset):\n",
    "    \"\"\" Devuelve dos vistas aumentadas + etiqueta \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.aug = EEGAugment()\n",
    "\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]               # (T,C)\n",
    "        y = self.y[idx]\n",
    "        x = np.expand_dims(x, 0)      # (1,T,C)\n",
    "        x = torch.from_numpy(x)\n",
    "        v1 = self.aug(x.clone())\n",
    "        v2 = self.aug(x.clone())\n",
    "        return v1, v2, torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# ====== Proyecci√≥n + p√©rdida SupCon ======\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, in_dim=80, proj_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, in_dim, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_dim, proj_dim, bias=True)\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        z = self.net(z)\n",
    "        z = torch.nn.functional.normalize(z, dim=-1)\n",
    "        return z\n",
    "\n",
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Supervised Contrastive Loss (Khosla et al.)\n",
    "    features: (B, n_views, D) normalizadas\n",
    "    labels  : (B,)\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.tau = temperature\n",
    "\n",
    "    def forward(self, features, labels):\n",
    "        device = features.device\n",
    "        B, V, D = features.shape\n",
    "        feat = features.view(B*V, D)\n",
    "        labels = labels.view(B)\n",
    "        labels = labels.contiguous().view(-1, 1)                    # (B,1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(device)        # (B,B)\n",
    "\n",
    "        sim = torch.div(torch.matmul(feat, feat.T), self.tau)       # (BV,BV)\n",
    "        logits_mask = torch.ones_like(sim) - torch.eye(B*V, device=device)\n",
    "        sim = sim * logits_mask\n",
    "\n",
    "        mask = mask.repeat(V, V)                                     # (BV,BV)\n",
    "\n",
    "        exp_sim = torch.exp(sim) * logits_mask\n",
    "        log_prob = sim - torch.log(exp_sim.sum(dim=1, keepdim=True) + 1e-12)\n",
    "\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(dim=1) / (mask.sum(dim=1) + 1e-12)\n",
    "        loss = - mean_log_prob_pos.mean()\n",
    "        return loss\n",
    "\n",
    "def supcon_pretrain(model: EEGNet, X_tr: np.ndarray, y_tr: np.ndarray,\n",
    "                    epochs: int = SUPCON_EPOCHS, batch_size: int = SUPCON_BATCH,\n",
    "                    lr: float = SUPCON_LR, temperature: float = SUPCON_TEMP,\n",
    "                    proj_dim: int = SUPCON_PROJ_DIM, log_every: int = SUPCON_LOG_EVERY):\n",
    "    \"\"\"\n",
    "    Preentrena el backbone de EEGNet con SupCon usando etiquetas (positivos = misma clase).\n",
    "    Usa dos vistas augmentadas por ensayo.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    # asegurar construcci√≥n de cabeza (necesitamos conocer T)\n",
    "    with torch.no_grad():\n",
    "        dummy = torch.from_numpy(X_tr[:2]).float().unsqueeze(1).to(DEVICE)\n",
    "        _ = model(dummy)\n",
    "\n",
    "    proj = ProjectionHead(in_dim=80, proj_dim=proj_dim).to(DEVICE)\n",
    "    crit = SupConLoss(temperature=temperature)\n",
    "    ds = ContrastiveTrials(X_tr, y_tr)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    # Entrenamos backbone + proyector\n",
    "    opt = optim.Adam(list(model.parameters()) + list(proj.parameters()), lr=lr)\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        running = 0.0\n",
    "        n = 0\n",
    "        for v1, v2, yb in dl:\n",
    "            v1 = v1.to(DEVICE)  # (B,1,T,C)\n",
    "            v2 = v2.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            z1 = model.forward_features(v1)         # (B,80)\n",
    "            z2 = model.forward_features(v2)         # (B,80)\n",
    "            p1 = proj(z1)                           # (B,D)\n",
    "            p2 = proj(z2)                           # (B,D)\n",
    "            feats = torch.stack([p1, p2], dim=1)    # (B,2,D)\n",
    "\n",
    "            loss = crit(feats, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            bs = yb.size(0)\n",
    "            running += loss.item() * bs\n",
    "            n += bs\n",
    "\n",
    "        if ep % log_every == 0:\n",
    "            print(f\"[SupCon] Epoch {ep:03d}/{epochs} | loss={running/max(1,n):.4f}\")\n",
    "\n",
    "    print(\"[SupCon] Preentrenamiento contrastivo completado (backbone inicializado).\")\n",
    "\n",
    "# =========================\n",
    "# ENTRENAMIENTO / EVALUACI√ìN\n",
    "# =========================\n",
    "def train_epoch(model, loader, opt, criterion):\n",
    "    model.train()\n",
    "    for xb, yb, _ in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_with_preds(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for xb, yb, _ in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        pred = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "        y_pred.extend(pred)\n",
    "        y_true.extend(yb.numpy().tolist())\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    return y_true, y_pred, float(acc)\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes, title, fname):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.nan_to_num(cm_norm)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm_norm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "        plt.text(j, i, format(cm_norm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm_norm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def print_report(y_true, y_pred, classes):\n",
    "    print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
    "\n",
    "# =========================\n",
    "# FINE-TUNING PROGRESIVO por sujeto (con validaci√≥n interna)\n",
    "# =========================\n",
    "def _param_groups(model, mode):\n",
    "    \"\"\"\n",
    "    En EEGNet:\n",
    "      - 'out'            : solo capa final (model.out)\n",
    "      - 'head'           : fc + out\n",
    "      - 'spatial+head'   : depthwise + separable + fc + out  (temporal queda congelada)\n",
    "    \"\"\"\n",
    "    if mode == 'out':\n",
    "        train = list(model.out.parameters())\n",
    "    elif mode == 'head':\n",
    "        train = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "    elif mode == 'spatial+head':\n",
    "        train = (list(model.conv_depthwise.parameters()) +\n",
    "                 list(model.bn2.parameters()) +\n",
    "                 list(model.conv_sep_depth.parameters()) +\n",
    "                 list(model.conv_sep_point.parameters()) +\n",
    "                 list(model.bn3.parameters()) +\n",
    "                 list(model.fc.parameters()) +\n",
    "                 list(model.out.parameters()))\n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "    return train\n",
    "\n",
    "def _freeze_for_mode(model, mode):\n",
    "    # Congelamos todo\n",
    "    for p in model.parameters(): p.requires_grad = False\n",
    "    # Siempre mantenemos CONGELADO el bloque temporal en este protocolo\n",
    "    # (conv_temporal + bn1)\n",
    "    if mode == 'out':\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'head':\n",
    "        for p in model.fc.parameters():  p.requires_grad = True\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'spatial+head':\n",
    "        for p in model.conv_depthwise.parameters(): p.requires_grad = True\n",
    "        for p in model.bn2.parameters():           p.requires_grad = True\n",
    "        for p in model.conv_sep_depth.parameters():p.requires_grad = True\n",
    "        for p in model.conv_sep_point.parameters():p.requires_grad = True\n",
    "        for p in model.bn3.parameters():           p.requires_grad = True\n",
    "        for p in model.fc.parameters():            p.requires_grad = True\n",
    "        for p in model.out.parameters():           p.requires_grad = True\n",
    "\n",
    "def _class_weights(y_np, n_classes):\n",
    "    counts = np.bincount(y_np, minlength=n_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    weights = counts.sum() / counts\n",
    "    weights = weights / weights.mean()\n",
    "    return torch.tensor(weights, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "def _train_one_mode(model, X_cal, y_cal, n_classes, mode,\n",
    "                    epochs=FT_EPOCHS, batch_size=16,\n",
    "                    head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                    l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO):\n",
    "    \"\"\"\n",
    "    Entrena en 'mode' con early stopping sobre un conjunto de validaci√≥n interno.\n",
    "    Devuelve el modelo con los mejores pesos (por val loss).\n",
    "    \"\"\"\n",
    "    # Split Cal -> (train_cal, val_cal)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=RANDOM_STATE)\n",
    "    (tr_idx, va_idx), = sss.split(X_cal, y_cal)\n",
    "    Xtr, ytr = X_cal[tr_idx], y_cal[tr_idx]\n",
    "    Xva, yva = X_cal[va_idx], y_cal[va_idx]\n",
    "\n",
    "    ds_tr = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xtr).float().unsqueeze(1),\n",
    "        torch.from_numpy(ytr).long()\n",
    "    )\n",
    "    ds_va = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xva).float().unsqueeze(1),\n",
    "        torch.from_numpy(yva).long()\n",
    "    )\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    _freeze_for_mode(model, mode)\n",
    "\n",
    "    if mode == 'spatial+head':\n",
    "        # grupos con LR discriminativos\n",
    "        base_params = (list(model.conv_depthwise.parameters()) +\n",
    "                       list(model.bn2.parameters()) +\n",
    "                       list(model.conv_sep_depth.parameters()) +\n",
    "                       list(model.conv_sep_point.parameters()) +\n",
    "                       list(model.bn3.parameters()))\n",
    "        head_params = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "        train_params = base_params + head_params\n",
    "        opt = optim.Adam([\n",
    "            {\"params\": base_params, \"lr\": base_lr},\n",
    "            {\"params\": head_params, \"lr\": head_lr},\n",
    "        ])\n",
    "    else:\n",
    "        train_params = _param_groups(model, mode)\n",
    "        opt = optim.Adam(train_params, lr=head_lr)\n",
    "\n",
    "    ref = [p.detach().clone().to(p.device) for p in train_params]\n",
    "    class_w = _class_weights(ytr, n_classes)\n",
    "    crit = nn.CrossEntropyLoss(weight=class_w)\n",
    "\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_val = float('inf')\n",
    "    bad = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # --- train ---\n",
    "        model.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            # L2-SP hacia referencia de los par√°metros que estamos entrenando\n",
    "            reg = 0.0\n",
    "            for p_cur, p_ref in zip(train_params, ref):\n",
    "                reg = reg + torch.sum((p_cur - p_ref)**2)\n",
    "            loss = loss + l2sp_lambda * reg\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # --- val ---\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            nval = 0\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                loss = crit(logits, yb)\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "                nval += xb.size(0)\n",
    "            val_loss /= max(1, nval)\n",
    "\n",
    "        if val_loss + 1e-7 < best_val:\n",
    "            best_val = val_loss\n",
    "            bad = 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_numpy(model, X_np, device):\n",
    "    model.eval()\n",
    "    xb = torch.from_numpy(X_np).float().unsqueeze(1).to(device)  # (N,1,T,C)\n",
    "    logits = model(xb)\n",
    "    return logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "def subject_cv_finetune_predict_progressive(model_global, Xs, ys, device,\n",
    "                                            n_splits=CALIB_CV_FOLDS, n_classes=4):\n",
    "    \"\"\"\n",
    "    Para un sujeto: 4-fold StratifiedKFold.\n",
    "      - En cada fold: 'out' ‚Üí 'head' ‚Üí 'spatial+head' (temporal congelado)\n",
    "      - Se elige el mejor en el split de holdout del sujeto.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    y_true_full = np.empty_like(ys)\n",
    "    y_pred_full = np.empty_like(ys)\n",
    "\n",
    "    for tr_idx, te_idx in skf.split(Xs, ys):\n",
    "        Xcal, ycal = Xs[tr_idx], ys[tr_idx]\n",
    "        Xho,  yho  = Xs[te_idx], ys[te_idx]\n",
    "\n",
    "        # Stage A: 'out'\n",
    "        m_out = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_out, Xcal, ycal, n_classes, mode='out',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_out = predict_numpy(m_out, Xho, device)\n",
    "\n",
    "        # Stage B: 'head'\n",
    "        m_head = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_head, Xcal, ycal, n_classes, mode='head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_head = predict_numpy(m_head, Xho, device)\n",
    "\n",
    "        # Stage C: 'spatial+head'\n",
    "        m_sp = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_sp, Xcal, ycal, n_classes, mode='spatial+head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                        l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_sp = predict_numpy(m_sp, Xho, device)\n",
    "\n",
    "        # mejor de las tres\n",
    "        accs = [ (yhat_out == yho).mean(), (yhat_head == yho).mean(), (yhat_sp == yho).mean() ]\n",
    "        best_idx = int(np.argmax(accs))\n",
    "        yhat_best = [yhat_out, yhat_head, yhat_sp][best_idx]\n",
    "\n",
    "        y_true_full[te_idx] = yho\n",
    "        y_pred_full[te_idx] = yhat_best\n",
    "\n",
    "    return y_true_full, y_pred_full\n",
    "\n",
    "# =========================\n",
    "# FOLDS JSON helpers\n",
    "# =========================\n",
    "def save_group_folds_json_with_indices(subject_ids_str, groups_array, n_splits, out_json_path,\n",
    "                                       created_by=\"dose_experiment\", description=None):\n",
    "    out_json_path = Path(out_json_path)\n",
    "    unique_subjects_int = sorted(np.unique(groups_array).tolist())\n",
    "    subject_ids = [f\"S{sid:03d}\" for sid in unique_subjects_int]\n",
    "\n",
    "    if len(subject_ids) < n_splits:\n",
    "        raise ValueError(f\"n_splits={n_splits} mayor que n√∫mero de sujetos={len(subject_ids)}\")\n",
    "\n",
    "    groups = np.arange(len(subject_ids))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    folds = []\n",
    "    fold_i = 0\n",
    "    for train_idx_grp, test_idx_grp in gkf.split(groups, groups, groups):\n",
    "        fold_i += 1\n",
    "        train_sids = [subject_ids[int(i)] for i in train_idx_grp]\n",
    "        test_sids  = [subject_ids[int(i)] for i in test_idx_grp]\n",
    "\n",
    "        train_sids_int = [int(s[1:]) for s in train_sids]\n",
    "        test_sids_int  = [int(s[1:]) for s in test_sids]\n",
    "\n",
    "        tr_idx = np.where(np.isin(groups_array, train_sids_int))[0].tolist()\n",
    "        te_idx = np.where(np.isin(groups_array, test_sids_int))[0].tolist()\n",
    "\n",
    "        folds.append({\n",
    "            \"fold\": int(fold_i),\n",
    "            \"train\": train_sids,\n",
    "            \"test\": test_sids,\n",
    "            \"tr_idx\": tr_idx,\n",
    "            \"te_idx\": te_idx\n",
    "        })\n",
    "\n",
    "    payload = {\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"created_by\": created_by,\n",
    "        \"description\": description if description is not None else \"\",\n",
    "        \"n_splits\": int(n_splits),\n",
    "        \"n_subjects\": len(subject_ids),\n",
    "        \"subject_ids\": subject_ids,\n",
    "        \"folds\": folds\n",
    "    }\n",
    "\n",
    "    out_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Folds JSON con √≠ndices guardado ‚Üí {out_json_path}\")\n",
    "    return out_json_path\n",
    "\n",
    "def load_group_folds_json(path_json, expected_subject_ids=None, strict_check=True):\n",
    "    path_json = Path(path_json)\n",
    "    if not path_json.exists():\n",
    "        raise FileNotFoundError(f\"No existe {path_json}\")\n",
    "    with open(path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        payload = json.load(f)\n",
    "\n",
    "    subj_json = payload.get(\"subject_ids\", [])\n",
    "    if expected_subject_ids is not None:\n",
    "        expected = sorted(list(expected_subject_ids))\n",
    "        if subj_json != expected:\n",
    "            msg = (\"Los subject_ids del JSON no coinciden con expected_subject_ids.\\n\"\n",
    "                   f\"JSON has {len(subj_json)} subjects, expected {len(expected)}.\\n\"\n",
    "                   f\"First 10 JSON: {subj_json[:10]}\\nFirst 10 expected: {expected[:10]}\")\n",
    "            if strict_check:\n",
    "                raise ValueError(msg)\n",
    "            else:\n",
    "                print(\"WARNING: \" + msg)\n",
    "    return payload\n",
    "\n",
    "# =========================\n",
    "# EXPERIMENTO\n",
    "# =========================\n",
    "def run_experiment(save_folds_json=True, folds_json_path=FOLDS_DIR, folds_json_description=\"GroupKFold folds for comparison\"):\n",
    "    \"\"\"\n",
    "    - Crea/lee JSON con folds por sujeto (incluye tr_idx/te_idx)\n",
    "    - Entrena modelo global por fold (inter-sujeto puro) con validaci√≥n interna por sujetos + ES\n",
    "    - Eval√∫a Global acc en test\n",
    "    - Realiza Fine-Tuning PROGRESIVO por sujeto (4-fold CV) y reporta acc\n",
    "    \"\"\"\n",
    "    mne.set_log_level('WARNING')\n",
    "\n",
    "    # sujetos y dataset\n",
    "    subs = subjects_available()\n",
    "    print(f\"Sujetos elegibles: {len(subs)} ‚Üí {subs[:10]}{'...' if len(subs)>10 else ''}\")\n",
    "\n",
    "    X, y, groups, chs = build_dataset_all(subs, scenario=CLASS_SCENARIO, window_mode=WINDOW_MODE)\n",
    "    N, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Listo para entrenar: N={N} | T={T} | C={C} | clases={n_classes} | sujetos={len(np.unique(groups))}\")\n",
    "\n",
    "    ds = EEGTrials(X, y, groups)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # preparar JSON folds\n",
    "    if folds_json_path is None:\n",
    "        folds_json_path = Path(\"folds\") / f\"group_folds_{N_FOLDS}splits.json\"\n",
    "    else:\n",
    "        folds_json_path = Path(folds_json_path)\n",
    "    folds_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    unique_subs = sorted(np.unique(groups).tolist())\n",
    "    subject_ids_str = [f\"S{s:03d}\" for s in unique_subs]\n",
    "\n",
    "    if not folds_json_path.exists():\n",
    "        if not save_folds_json:\n",
    "            raise FileNotFoundError(f\"Folds JSON no encontrado en {folds_json_path} y save_folds_json=False.\")\n",
    "        save_group_folds_json_with_indices(subject_ids_str, groups, n_splits=N_FOLDS,\n",
    "                                           out_json_path=folds_json_path,\n",
    "                                           created_by=\"Joel_Clasificador\",\n",
    "                                           description=folds_json_description)\n",
    "\n",
    "    payload = load_group_folds_json(folds_json_path, expected_subject_ids=subject_ids_str, strict_check=False)\n",
    "    folds = payload[\"folds\"]\n",
    "\n",
    "    # bucle por folds\n",
    "    global_folds = []\n",
    "    ft_prog_folds = []\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "\n",
    "    for f in folds:\n",
    "        fold = f[\"fold\"]\n",
    "        tr_idx = np.asarray(f.get(\"tr_idx\", []), dtype=int)\n",
    "        te_idx = np.asarray(f.get(\"te_idx\", []), dtype=int)\n",
    "\n",
    "        if tr_idx.size == 0 or te_idx.size == 0:\n",
    "            print(f\"Advertencia: fold {fold} sin √≠ndices tr/te v√°lidos. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        # ===== Split de validaci√≥n por SUJETOS dentro del set de entrenamiento =====\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=GLOBAL_VAL_SPLIT, random_state=RANDOM_STATE)\n",
    "        tr_subj_idx, va_subj_idx = next(gss.split(tr_idx, groups[tr_idx], groups[tr_idx]))\n",
    "        tr_sub_idx = tr_idx[tr_subj_idx]\n",
    "        va_idx     = tr_idx[va_subj_idx]\n",
    "\n",
    "        tr_loader = DataLoader(Subset(ds, tr_sub_idx), batch_size=BATCH_SIZE, shuffle=True,  drop_last=False)\n",
    "        va_loader = DataLoader(Subset(ds, va_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "        te_loader = DataLoader(Subset(ds, te_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "        # ===== EEGNet =====\n",
    "        model = EEGNet(n_ch=C, n_classes=n_classes, F1=8, D=2, kernel_t=64, k_sep=16,\n",
    "                       pool1_t=4, pool2_t=8, dropout=0.5).to(DEVICE)\n",
    "\n",
    "        # ===== PREENTRENAMIENTO SUPCON (opcional) =====\n",
    "        if USE_SUPCON_PRETRAIN:\n",
    "            X_tr_sup = X[tr_sub_idx]\n",
    "            y_tr_sup = y[tr_sub_idx]\n",
    "            print(f\"[Fold {fold}/{N_FOLDS}] SupCon pretrain on {len(X_tr_sup)} trials...\")\n",
    "            supcon_pretrain(model, X_tr_sup, y_tr_sup,\n",
    "                            epochs=SUPCON_EPOCHS, batch_size=SUPCON_BATCH,\n",
    "                            lr=SUPCON_LR, temperature=SUPCON_TEMP, proj_dim=SUPCON_PROJ_DIM,\n",
    "                            log_every=SUPCON_LOG_EVERY)\n",
    "\n",
    "        # Optimizador para entrenamiento supervisado\n",
    "        opt = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "        def _acc(loader):\n",
    "            return evaluate_with_preds(model, loader)[2]\n",
    "\n",
    "        # ===== Entrenamiento con logging + EARLY STOPPING por val_acc =====\n",
    "        print(f\"\\n[Fold {fold}/{N_FOLDS}] Entrenando modelo global con validaci√≥n interna por sujetos...\"\n",
    "              f\" (n_train={len(tr_sub_idx)} | n_val={len(va_idx)} | n_test={len(te_idx)})\")\n",
    "\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        best_val = -1.0\n",
    "        bad = 0\n",
    "\n",
    "        for epoch in range(1, EPOCHS_GLOBAL + 1):\n",
    "            train_epoch(model, tr_loader, opt, criterion)\n",
    "\n",
    "            if epoch % LOG_EVERY == 0:\n",
    "                tr_acc = _acc(tr_loader)\n",
    "                va_acc = _acc(va_loader)\n",
    "                print(f\"  √âpoca {epoch:3d} | train_acc={tr_acc:.4f} | val_acc={va_acc:.4f}\")\n",
    "\n",
    "                if va_acc > best_val + 1e-4:\n",
    "                    best_val = va_acc\n",
    "                    best_state = copy.deepcopy(model.state_dict())\n",
    "                    bad = 0\n",
    "                else:\n",
    "                    bad += 1\n",
    "                    if bad >= GLOBAL_PATIENCE:\n",
    "                        print(f\"  Early stopping en √©poca {epoch} (mejor val_acc={best_val:.4f})\")\n",
    "                        break\n",
    "\n",
    "        # cargar mejor estado antes de evaluar en test\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "        # ===== Evaluaci√≥n global (inter-sujeto puro) =====\n",
    "        y_true, y_pred, acc_global = evaluate_with_preds(model, te_loader)\n",
    "        global_folds.append(acc_global)\n",
    "        all_true.append(y_true); all_pred.append(y_pred)\n",
    "\n",
    "        print(f\"[Fold {fold}/{N_FOLDS}] Global acc={acc_global:.4f}\")\n",
    "        print_report(y_true, y_pred, CLASS_NAMES_4C)\n",
    "\n",
    "        # ---------- Fine-tuning PROGRESIVO por sujeto con 4-fold CV ----------\n",
    "        X_te, y_te, g_te = X[te_idx], y[te_idx], groups[te_idx]\n",
    "\n",
    "        y_true_ft_all, y_pred_ft_all = [], []\n",
    "        used_subjects = 0\n",
    "        for sid in np.unique(g_te):\n",
    "            idx = np.where(g_te == sid)[0]\n",
    "            Xs, ys = X_te[idx], y_te[idx]\n",
    "\n",
    "            # Seguridad\n",
    "            if len(ys) < CALIB_CV_FOLDS or len(np.unique(ys)) < 2:\n",
    "                continue\n",
    "\n",
    "            y_true_subj, y_pred_subj = subject_cv_finetune_predict_progressive(\n",
    "                model, Xs, ys, DEVICE, n_splits=CALIB_CV_FOLDS, n_classes=n_classes\n",
    "            )\n",
    "            y_true_ft_all.append(y_true_subj)\n",
    "            y_pred_ft_all.append(y_pred_subj)\n",
    "            used_subjects += 1\n",
    "\n",
    "        if len(y_true_ft_all) > 0:\n",
    "            y_true_ft_all = np.concatenate(y_true_ft_all)\n",
    "            y_pred_ft_all = np.concatenate(y_pred_ft_all)\n",
    "            acc_ft = (y_true_ft_all == y_pred_ft_all).mean()\n",
    "            print(f\"  Fine-tuning PROGRESIVO (por sujeto, {CALIB_CV_FOLDS}-fold CV) acc={acc_ft:.4f} | sujetos={used_subjects}\")\n",
    "            print(f\"  Œî(FT-Global) = {acc_ft - acc_global:+.4f}\")\n",
    "        else:\n",
    "            acc_ft = np.nan\n",
    "            print(\"  Fine-tuning PROGRESIVO no ejecutado (sujeto(s) con muestras insuficientes).\")\n",
    "\n",
    "        ft_prog_folds.append(acc_ft)\n",
    "\n",
    "    # ---------- resultados finales ----------\n",
    "    if len(all_true) > 0:\n",
    "        all_true = np.concatenate(all_true)\n",
    "        all_pred = np.concatenate(all_pred)\n",
    "    else:\n",
    "        all_true = np.array([], dtype=int)\n",
    "        all_pred = np.array([], dtype=int)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTADOS FINALES\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Global folds:\", [f\"{a:.4f}\" for a in global_folds])\n",
    "    if len(global_folds) > 0:\n",
    "        print(f\"Global mean: {np.mean(global_folds):.4f}\")\n",
    "\n",
    "    print(\"Fine-tune PROGRESIVO folds:\", [(\"nan\" if (a is None or np.isnan(a)) else f\"{a:.4f}\") for a in ft_prog_folds])\n",
    "    if len(ft_prog_folds) > 0:\n",
    "        print(f\"Fine-tune PROGRESIVO mean: {np.nanmean(ft_prog_folds):.4f}\")\n",
    "        print(f\"Œî(FT-Global) mean: {np.nanmean(ft_prog_folds) - np.mean(global_folds):+.4f}\")\n",
    "\n",
    "    # Matriz de confusi√≥n global (sobre todos los folds)\n",
    "    if all_true.size > 0:\n",
    "        plot_confusion(all_true, all_pred, CLASS_NAMES_4C,\n",
    "                       title=\"Confusion Matrix - Global Model (All Folds)\",\n",
    "                       fname=\"confusion_global_allfolds.png\")\n",
    "        print(\"\\n‚Ü≥ Matriz de confusi√≥n guardada: confusion_global_allfolds.png\")\n",
    "\n",
    "    return {\n",
    "        \"global_folds\": global_folds,\n",
    "        \"ft_prog_folds\": ft_prog_folds,\n",
    "        \"all_true\": all_true,\n",
    "        \"all_pred\": all_pred,\n",
    "        \"folds_json_path\": str(folds_json_path)\n",
    "    }\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üß† INICIANDO EXPERIMENTO CON EEGNet + FINE-TUNING PROGRESIVO (por sujeto, 4-fold CV)\")\n",
    "    print(f\"üîß Configuraci√≥n: {CLASS_SCENARIO}, {len(EXPECTED_8)} canales, {WINDOW_MODE}\")\n",
    "    print(f\"‚öôÔ∏è  FT: epochs={FT_EPOCHS}, base_lr={FT_BASE_LR}, head_lr={FT_HEAD_LR}, L2SP={FT_L2SP}, patience={FT_PATIENCE}, CV={CALIB_CV_FOLDS}\")\n",
    "    print(f\"üß≤ SupCon: enabled={USE_SUPCON_PRETRAIN}, epochs={SUPCON_EPOCHS}, batch={SUPCON_BATCH}, temp={SUPCON_TEMP}\")\n",
    "    run_experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11d50cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Usando dispositivo: cuda\n",
      "üß† INICIANDO EXPERIMENTO CON EEGNet + FINE-TUNING PROGRESIVO (por sujeto, 4-fold CV)\n",
      "üîß Configuraci√≥n: 4c, 8 canales, 6s\n",
      "‚öôÔ∏è  FT: epochs=30, base_lr=5e-05, head_lr=0.001, L2SP=0.0001, patience=5, CV=4\n",
      "Sujetos elegibles: 103 ‚Üí [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Construyendo dataset (RAW):   0%|          | 0/103 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Construyendo dataset (RAW): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [01:14<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset construido: N=8652 | T=960 | C=8 | clases=4 | sujetos √∫nicos=103\n",
      "Listo para entrenar: N=8652 | T=960 | C=8 | clases=4 | sujetos=103\n",
      "\n",
      "[Fold 1/5] Entrenando modelo global... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  √âpoca   1 | train_acc=0.4275 | val_acc=0.4084 | LR=0.01000\n",
      "  √âpoca   5 | train_acc=0.4463 | val_acc=0.4112 | LR=0.01000\n",
      "  √âpoca  10 | train_acc=0.4940 | val_acc=0.4533 | LR=0.01000\n",
      "  √âpoca  15 | train_acc=0.5059 | val_acc=0.4789 | LR=0.01000\n",
      "  √âpoca  20 | train_acc=0.5059 | val_acc=0.4478 | LR=0.00100\n",
      "  √âpoca  25 | train_acc=0.5236 | val_acc=0.4762 | LR=0.00100\n",
      "  √âpoca  30 | train_acc=0.5254 | val_acc=0.4853 | LR=0.00100\n",
      "  √âpoca  35 | train_acc=0.5162 | val_acc=0.4615 | LR=0.00100\n",
      "  √âpoca  40 | train_acc=0.5264 | val_acc=0.4789 | LR=0.00100\n",
      "  √âpoca  45 | train_acc=0.5267 | val_acc=0.4716 | LR=0.00100\n",
      "  √âpoca  50 | train_acc=0.5317 | val_acc=0.4771 | LR=0.00010\n",
      "  √âpoca  55 | train_acc=0.5283 | val_acc=0.4881 | LR=0.00010\n",
      "  √âpoca  60 | train_acc=0.5202 | val_acc=0.4661 | LR=0.00010\n",
      "  √âpoca  65 | train_acc=0.5297 | val_acc=0.4808 | LR=0.00010\n",
      "  √âpoca  70 | train_acc=0.5271 | val_acc=0.4817 | LR=0.00010\n",
      "  √âpoca  75 | train_acc=0.5233 | val_acc=0.4789 | LR=0.00010\n",
      "  √âpoca  80 | train_acc=0.5131 | val_acc=0.4661 | LR=0.00010\n",
      "  √âpoca  85 | train_acc=0.5233 | val_acc=0.4853 | LR=0.00010\n",
      "  √âpoca  90 | train_acc=0.5302 | val_acc=0.4908 | LR=0.00010\n",
      "  √âpoca  95 | train_acc=0.5278 | val_acc=0.4826 | LR=0.00010\n",
      "  √âpoca 100 | train_acc=0.5057 | val_acc=0.4634 | LR=0.00010\n",
      "[Fold 1/5] Global acc=0.4518\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5218    0.4875    0.5041       441\n",
      "       Right     0.4979    0.5442    0.5200       441\n",
      "  Both Fists     0.3491    0.4195    0.3811       441\n",
      "   Both Feet     0.4618    0.3560    0.4020       441\n",
      "\n",
      "    accuracy                         0.4518      1764\n",
      "   macro avg     0.4576    0.4518    0.4518      1764\n",
      "weighted avg     0.4576    0.4518    0.4518      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5266 | sujetos=21\n",
      "  Œî(FT-Global) = +0.0748\n",
      "\n",
      "[Fold 2/5] Entrenando modelo global... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  √âpoca   1 | train_acc=0.3999 | val_acc=0.3581 | LR=0.01000\n",
      "  √âpoca   5 | train_acc=0.4436 | val_acc=0.4304 | LR=0.01000\n",
      "  √âpoca  10 | train_acc=0.4786 | val_acc=0.4313 | LR=0.01000\n",
      "  √âpoca  15 | train_acc=0.4840 | val_acc=0.4267 | LR=0.01000\n",
      "  √âpoca  20 | train_acc=0.4791 | val_acc=0.4350 | LR=0.00100\n",
      "  √âpoca  25 | train_acc=0.4890 | val_acc=0.4570 | LR=0.00100\n",
      "  √âpoca  30 | train_acc=0.4997 | val_acc=0.4524 | LR=0.00100\n",
      "  √âpoca  35 | train_acc=0.4955 | val_acc=0.4496 | LR=0.00100\n",
      "  √âpoca  40 | train_acc=0.5035 | val_acc=0.4579 | LR=0.00100\n",
      "  √âpoca  45 | train_acc=0.5021 | val_acc=0.4615 | LR=0.00100\n",
      "  √âpoca  50 | train_acc=0.5019 | val_acc=0.4551 | LR=0.00010\n",
      "  √âpoca  55 | train_acc=0.5003 | val_acc=0.4524 | LR=0.00010\n",
      "  √âpoca  60 | train_acc=0.4964 | val_acc=0.4579 | LR=0.00010\n",
      "  √âpoca  65 | train_acc=0.5057 | val_acc=0.4460 | LR=0.00010\n",
      "  √âpoca  70 | train_acc=0.4786 | val_acc=0.4332 | LR=0.00010\n",
      "  √âpoca  75 | train_acc=0.5009 | val_acc=0.4542 | LR=0.00010\n",
      "  √âpoca  80 | train_acc=0.5014 | val_acc=0.4542 | LR=0.00010\n",
      "  √âpoca  85 | train_acc=0.4997 | val_acc=0.4423 | LR=0.00010\n",
      "  √âpoca  90 | train_acc=0.5026 | val_acc=0.4643 | LR=0.00010\n",
      "  √âpoca  95 | train_acc=0.4981 | val_acc=0.4524 | LR=0.00010\n",
      "  √âpoca 100 | train_acc=0.5064 | val_acc=0.4588 | LR=0.00010\n",
      "[Fold 2/5] Global acc=0.5011\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5619    0.6485    0.6021       441\n",
      "       Right     0.5126    0.5986    0.5523       441\n",
      "  Both Fists     0.4498    0.3152    0.3707       441\n",
      "   Both Feet     0.4524    0.4422    0.4472       441\n",
      "\n",
      "    accuracy                         0.5011      1764\n",
      "   macro avg     0.4942    0.5011    0.4931      1764\n",
      "weighted avg     0.4942    0.5011    0.4931      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5675 | sujetos=21\n",
      "  Œî(FT-Global) = +0.0663\n",
      "\n",
      "[Fold 3/5] Entrenando modelo global... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  √âpoca   1 | train_acc=0.4275 | val_acc=0.4258 | LR=0.01000\n",
      "  √âpoca   5 | train_acc=0.4722 | val_acc=0.4359 | LR=0.01000\n",
      "  √âpoca  10 | train_acc=0.4934 | val_acc=0.4625 | LR=0.01000\n",
      "  √âpoca  15 | train_acc=0.4988 | val_acc=0.4606 | LR=0.01000\n",
      "  √âpoca  20 | train_acc=0.4960 | val_acc=0.4625 | LR=0.00100\n",
      "  √âpoca  25 | train_acc=0.5128 | val_acc=0.4771 | LR=0.00100\n",
      "  √âpoca  30 | train_acc=0.5109 | val_acc=0.4844 | LR=0.00100\n",
      "  √âpoca  35 | train_acc=0.5143 | val_acc=0.4844 | LR=0.00100\n",
      "  √âpoca  40 | train_acc=0.5119 | val_acc=0.4780 | LR=0.00100\n",
      "  √âpoca  45 | train_acc=0.5150 | val_acc=0.4799 | LR=0.00100\n",
      "  √âpoca  50 | train_acc=0.5204 | val_acc=0.4799 | LR=0.00010\n",
      "  √âpoca  55 | train_acc=0.5138 | val_acc=0.4863 | LR=0.00010\n",
      "  √âpoca  60 | train_acc=0.5214 | val_acc=0.4753 | LR=0.00010\n",
      "  √âpoca  65 | train_acc=0.5086 | val_acc=0.4725 | LR=0.00010\n",
      "  √âpoca  70 | train_acc=0.5186 | val_acc=0.4844 | LR=0.00010\n",
      "  √âpoca  75 | train_acc=0.5209 | val_acc=0.4808 | LR=0.00010\n",
      "  √âpoca  80 | train_acc=0.5186 | val_acc=0.4835 | LR=0.00010\n",
      "  √âpoca  85 | train_acc=0.5198 | val_acc=0.4808 | LR=0.00010\n",
      "  √âpoca  90 | train_acc=0.5204 | val_acc=0.4817 | LR=0.00010\n",
      "  √âpoca  95 | train_acc=0.5109 | val_acc=0.4744 | LR=0.00010\n",
      "  √âpoca 100 | train_acc=0.5131 | val_acc=0.4863 | LR=0.00010\n",
      "[Fold 3/5] Global acc=0.4439\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5254    0.4921    0.5082       441\n",
      "       Right     0.5536    0.4218    0.4788       441\n",
      "  Both Fists     0.3558    0.3469    0.3513       441\n",
      "   Both Feet     0.3880    0.5147    0.4425       441\n",
      "\n",
      "    accuracy                         0.4439      1764\n",
      "   macro avg     0.4557    0.4439    0.4452      1764\n",
      "weighted avg     0.4557    0.4439    0.4452      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5130 | sujetos=21\n",
      "  Œî(FT-Global) = +0.0692\n",
      "\n",
      "[Fold 4/5] Entrenando modelo global... (n_train=5880 | n_val=1092 | n_test=1680)\n",
      "  √âpoca   1 | train_acc=0.4077 | val_acc=0.3892 | LR=0.01000\n",
      "  √âpoca   5 | train_acc=0.4543 | val_acc=0.4267 | LR=0.01000\n",
      "  √âpoca  10 | train_acc=0.4830 | val_acc=0.4505 | LR=0.01000\n",
      "  √âpoca  15 | train_acc=0.4702 | val_acc=0.4423 | LR=0.01000\n",
      "  √âpoca  20 | train_acc=0.4954 | val_acc=0.4625 | LR=0.00100\n",
      "  √âpoca  25 | train_acc=0.5102 | val_acc=0.4560 | LR=0.00100\n",
      "  √âpoca  30 | train_acc=0.5119 | val_acc=0.4551 | LR=0.00100\n",
      "  √âpoca  35 | train_acc=0.5228 | val_acc=0.4560 | LR=0.00100\n",
      "  √âpoca  40 | train_acc=0.5214 | val_acc=0.4615 | LR=0.00100\n",
      "  √âpoca  45 | train_acc=0.5177 | val_acc=0.4570 | LR=0.00100\n",
      "  √âpoca  50 | train_acc=0.5189 | val_acc=0.4597 | LR=0.00010\n",
      "  √âpoca  55 | train_acc=0.5134 | val_acc=0.4643 | LR=0.00010\n",
      "  √âpoca  60 | train_acc=0.5197 | val_acc=0.4570 | LR=0.00010\n",
      "  √âpoca  65 | train_acc=0.5207 | val_acc=0.4597 | LR=0.00010\n",
      "  √âpoca  70 | train_acc=0.5221 | val_acc=0.4707 | LR=0.00010\n",
      "  √âpoca  75 | train_acc=0.5223 | val_acc=0.4716 | LR=0.00010\n",
      "  √âpoca  80 | train_acc=0.5192 | val_acc=0.4661 | LR=0.00010\n",
      "  √âpoca  85 | train_acc=0.5240 | val_acc=0.4689 | LR=0.00010\n",
      "  √âpoca  90 | train_acc=0.5240 | val_acc=0.4588 | LR=0.00010\n",
      "  √âpoca  95 | train_acc=0.5156 | val_acc=0.4716 | LR=0.00010\n",
      "  √âpoca 100 | train_acc=0.5221 | val_acc=0.4689 | LR=0.00010\n",
      "[Fold 4/5] Global acc=0.4935\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5650    0.6000    0.5820       420\n",
      "       Right     0.4940    0.5905    0.5380       420\n",
      "  Both Fists     0.4264    0.3381    0.3772       420\n",
      "   Both Feet     0.4687    0.4452    0.4567       420\n",
      "\n",
      "    accuracy                         0.4935      1680\n",
      "   macro avg     0.4885    0.4935    0.4884      1680\n",
      "weighted avg     0.4885    0.4935    0.4884      1680\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5714 | sujetos=20\n",
      "  Œî(FT-Global) = +0.0780\n",
      "\n",
      "[Fold 5/5] Entrenando modelo global... (n_train=5880 | n_val=1092 | n_test=1680)\n",
      "  √âpoca   1 | train_acc=0.4321 | val_acc=0.4350 | LR=0.01000\n",
      "  √âpoca   5 | train_acc=0.4624 | val_acc=0.4707 | LR=0.01000\n",
      "  √âpoca  10 | train_acc=0.4871 | val_acc=0.4908 | LR=0.01000\n",
      "  √âpoca  15 | train_acc=0.4886 | val_acc=0.4670 | LR=0.01000\n",
      "  √âpoca  20 | train_acc=0.4957 | val_acc=0.4908 | LR=0.00100\n",
      "  √âpoca  25 | train_acc=0.5092 | val_acc=0.4918 | LR=0.00100\n",
      "  √âpoca  30 | train_acc=0.5117 | val_acc=0.4890 | LR=0.00100\n",
      "  √âpoca  35 | train_acc=0.5104 | val_acc=0.4826 | LR=0.00100\n",
      "  √âpoca  40 | train_acc=0.5109 | val_acc=0.4927 | LR=0.00100\n",
      "  √âpoca  45 | train_acc=0.5112 | val_acc=0.4817 | LR=0.00100\n",
      "  √âpoca  50 | train_acc=0.5116 | val_acc=0.4908 | LR=0.00010\n",
      "  √âpoca  55 | train_acc=0.5143 | val_acc=0.4973 | LR=0.00010\n",
      "  √âpoca  60 | train_acc=0.5134 | val_acc=0.4826 | LR=0.00010\n",
      "  √âpoca  65 | train_acc=0.5124 | val_acc=0.4918 | LR=0.00010\n",
      "  √âpoca  70 | train_acc=0.5134 | val_acc=0.4936 | LR=0.00010\n",
      "  √âpoca  75 | train_acc=0.5104 | val_acc=0.4844 | LR=0.00010\n",
      "  √âpoca  80 | train_acc=0.5112 | val_acc=0.4908 | LR=0.00010\n",
      "  √âpoca  85 | train_acc=0.5133 | val_acc=0.4963 | LR=0.00010\n",
      "  √âpoca  90 | train_acc=0.5116 | val_acc=0.4872 | LR=0.00010\n",
      "  √âpoca  95 | train_acc=0.5145 | val_acc=0.4927 | LR=0.00010\n",
      "  √âpoca 100 | train_acc=0.5143 | val_acc=0.4954 | LR=0.00010\n",
      "[Fold 5/5] Global acc=0.5363\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5833    0.6833    0.6294       420\n",
      "       Right     0.5899    0.5857    0.5878       420\n",
      "  Both Fists     0.4527    0.3762    0.4109       420\n",
      "   Both Feet     0.4976    0.5000    0.4988       420\n",
      "\n",
      "    accuracy                         0.5363      1680\n",
      "   macro avg     0.5309    0.5363    0.5317      1680\n",
      "weighted avg     0.5309    0.5363    0.5317      1680\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.6006 | sujetos=20\n",
      "  Œî(FT-Global) = +0.0643\n",
      "\n",
      "============================================================\n",
      "RESULTADOS FINALES\n",
      "============================================================\n",
      "Global folds: ['0.4518', '0.5011', '0.4439', '0.4935', '0.5363']\n",
      "Global mean: 0.4853\n",
      "Fine-tune PROGRESIVO folds: ['0.5266', '0.5675', '0.5130', '0.5714', '0.6006']\n",
      "Fine-tune PROGRESIVO mean: 0.5558\n",
      "Œî(FT-Global) mean: +0.0705\n",
      "\n",
      "‚Ü≥ Matriz de confusi√≥n guardada: confusion_global_allfolds.png\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# EEGNet + protocolo global + fine-tuning progresivo por sujeto\n",
    "# Cambios clave:\n",
    "# - BN: momentum=0.99, eps=1e-3\n",
    "# - Dropout: 0.25 (bloque 1), 0.5 (bloque 2)\n",
    "# - Max-norm (p=2, max=2.0) en conv_depthwise, conv_sep_point, fc, out\n",
    "# - Adam(lr=1e-2) + MultiStepLR([20,50], gamma=0.1) en global\n",
    "# - Sin weight decay\n",
    "# - 8 canales con FCz\n",
    "# - Flag para apagar/encender z-score por √©poca\n",
    "# - Batch por defecto 32 (puedes probar 64)\n",
    "\n",
    "import os, re, math, random, json, itertools, copy\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold, StratifiedShuffleSplit, GroupShuffleSplit\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# CONFIGURACI√ìN GENERAL\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "FOLDS_DIR = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dispositivo y semilla\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "# Escenario de clases\n",
    "CLASS_SCENARIO = '4c'\n",
    "\n",
    "# Ventana por defecto 3s (prueba tambi√©n '6s' para [-1,5]s)\n",
    "WINDOW_MODE = '6s'\n",
    "FS = 160.0\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Entrenamiento global\n",
    "BATCH_SIZE = 32          # ‚á† recomendado 32; si la GPU aguanta, prueba 64\n",
    "EPOCHS_GLOBAL = 100\n",
    "LR_INIT = 1e-2           # LR alto + BN + max-norm\n",
    "SCHED_MILESTONES = [20, 50]\n",
    "SCHED_GAMMA = 0.1\n",
    "\n",
    "# Validaci√≥n/ES global\n",
    "GLOBAL_VAL_SPLIT = 0.15\n",
    "GLOBAL_PATIENCE  = 10\n",
    "LOG_EVERY        = 5\n",
    "\n",
    "# Fine-tuning por sujeto (protocolo robusto)\n",
    "CALIB_CV_FOLDS = 4\n",
    "FT_EPOCHS = 30\n",
    "FT_BASE_LR = 5e-5\n",
    "FT_HEAD_LR = 1e-3\n",
    "FT_L2SP = 1e-4\n",
    "FT_PATIENCE = 5\n",
    "FT_VAL_RATIO = 0.2\n",
    "\n",
    "# Normalizaci√≥n por √©poca canal-a-canal (z-score)\n",
    "NORM_EPOCH_ZSCORE = True\n",
    "\n",
    "# Sujetos excluidos\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "\n",
    "# Runs\n",
    "MI_RUNS_LR = [4, 8, 12]\n",
    "MI_RUNS_OF = [6, 10, 14]\n",
    "BASELINE_RUNS_EO = [1]\n",
    "\n",
    "# Canales (8 con FCz)\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','FCz']\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES DE CANALES\n",
    "# =========================\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = re.sub(r'([A-Za-z])0([0-9])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])Z$', r'\\1z', s)\n",
    "    s = s.replace('fp', 'Fp').replace('FP', 'Fp')\n",
    "    s = ''.join(ch.upper() if ch != 'z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {}\n",
    "    for ch in raw.ch_names:\n",
    "        lab = normalize_label(ch)\n",
    "        lab = lab[:-1] + 'z' if lab.endswith('Z') else lab\n",
    "        lab = re.sub(r'([A-Z])Z$', r'\\1z', lab)\n",
    "        mapping[ch] = lab\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired_channels=EXPECTED_8):\n",
    "    missing = [ch for ch in desired_channels if ch not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"Warning: faltan canales {missing} en archivo {getattr(raw,'filenames', [''])[0]}\")\n",
    "        return None\n",
    "    raw.reorder_channels([ch for ch in raw.ch_names if ch in desired_channels] +\n",
    "                         [ch for ch in raw.ch_names if ch not in desired_channels])\n",
    "    raw.pick_channels(desired_channels, ordered=True)\n",
    "    return raw\n",
    "\n",
    "# =========================\n",
    "# LECTURA DE EDF y EVENTOS\n",
    "# =========================\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(run_id:int):\n",
    "    if run_id in MI_RUNS_LR: return 'LR'\n",
    "    if run_id in MI_RUNS_OF: return 'OF'\n",
    "    if run_id in BASELINE_RUNS_EO: return 'EO'\n",
    "    return None\n",
    "\n",
    "def read_raw_edf(path: Path):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        mont = mne.channels.make_standard_montage('standard_1020')\n",
    "        raw.set_montage(mont, on_missing='ignore')\n",
    "    except Exception:\n",
    "        pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad=\"auto\")\n",
    "    raw = ensure_channels_order(raw, EXPECTED_8)\n",
    "    if raw is None:\n",
    "        return None\n",
    "    # Notch a 60 Hz (o 50 Hz seg√∫n tu red) con spectrum_fit\n",
    "    raw.notch_filter(freqs=[60.0], picks='eeg', method='spectrum_fit', phase='zero')\n",
    "    return raw\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    if raw.annotations is None or len(raw.annotations) == 0:\n",
    "        return []\n",
    "    def _norm(s): return str(s).strip().upper().replace(' ', '')\n",
    "    res = []\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = _norm(desc)\n",
    "        if tag in ('T1','T2'):\n",
    "            res.append((float(onset), tag))\n",
    "    res.sort()\n",
    "    dedup = []\n",
    "    last_t1 = last_t2 = -1e9\n",
    "    for t, tag in res:\n",
    "        if tag == 'T1':\n",
    "            if (t - last_t1) >= 0.5: dedup.append((t, tag)); last_t1 = t\n",
    "        else:\n",
    "            if (t - last_t2) >= 0.5: dedup.append((t, tag)); last_t2 = t\n",
    "    return dedup\n",
    "\n",
    "# =========================\n",
    "# CONSTRUCCI√ìN DE DATASETS\n",
    "# =========================\n",
    "def subjects_available():\n",
    "    subs = []\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try:\n",
    "            sid = int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir / f\"S{sid:03d}R{r:02d}.edf\").exists() for r in (MI_RUNS_LR + MI_RUNS_OF))\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "def extract_trials_from_run(edf_path: Path, scenario: str, window_mode: str):\n",
    "    subj, run = parse_subject_run(edf_path)\n",
    "    kind = run_kind(run)\n",
    "    if kind not in ('LR','OF','EO'):\n",
    "        return ([], [])\n",
    "\n",
    "    raw = read_raw_edf(edf_path)\n",
    "    if raw is None:\n",
    "        return ([], [])\n",
    "\n",
    "    data = raw.get_data()\n",
    "    fs = raw.info['sfreq']\n",
    "    assert abs(fs - FS) < 1e-6\n",
    "\n",
    "    out = []\n",
    "\n",
    "    if kind in ('LR','OF'):\n",
    "        events = collect_events_T1T2(raw)\n",
    "        if window_mode == '3s':\n",
    "            rel_start, rel_end = 0.0, 3.0\n",
    "        else:\n",
    "            rel_start, rel_end = -1.0, 5.0\n",
    "\n",
    "        for onset_sec, tag in events:\n",
    "            if kind == 'LR':\n",
    "                if tag == 'T1': label = 'L'\n",
    "                elif tag == 'T2': label = 'R'\n",
    "                else: continue\n",
    "            else:\n",
    "                if tag == 'T1': label = 'BFISTS'\n",
    "                elif tag == 'T2': label = 'BFEET'\n",
    "                else: continue\n",
    "\n",
    "            if scenario == '2c' and label not in ('L','R'):\n",
    "                continue\n",
    "            if scenario == '3c' and label not in ('L','R','BFISTS'):\n",
    "                continue\n",
    "            if scenario == '4c' and label not in ('L','R','BFISTS','BFEET'):\n",
    "                continue\n",
    "\n",
    "            s = int(round((raw.first_time + onset_sec + rel_start) * fs))\n",
    "            e = int(round((raw.first_time + onset_sec + rel_end) * fs))\n",
    "            if s < 0 or e > data.shape[1]:\n",
    "                continue\n",
    "\n",
    "            seg = data[:, s:e].T.astype(np.float32)\n",
    "            if NORM_EPOCH_ZSCORE:\n",
    "                seg = (seg - seg.mean(axis=0, keepdims=True)) / (seg.std(axis=0, keepdims=True) + 1e-6)\n",
    "\n",
    "            if label == 'L':       y = 0\n",
    "            elif label == 'R':     y = 1\n",
    "            elif label == 'BFISTS':y = 2\n",
    "            elif label == 'BFEET': y = 3\n",
    "            else: continue\n",
    "\n",
    "            out.append((seg, y, subj))\n",
    "\n",
    "    elif kind == 'EO':\n",
    "        return ([], raw.ch_names)\n",
    "\n",
    "    return out, raw.ch_names\n",
    "\n",
    "def build_dataset_all(subjects, scenario='4c', window_mode='3s'):\n",
    "    X, y, groups = [], [], []\n",
    "    ch_template = None\n",
    "\n",
    "    for s in tqdm(subjects, desc=\"Construyendo dataset (RAW)\"):\n",
    "        sdir = DATA_RAW / f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "\n",
    "        trials_L, trials_R, trials_FISTS, trials_FEET = [], [], [], []\n",
    "\n",
    "        for r in MI_RUNS_LR:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 0: trials_L.append(seg)\n",
    "                elif lab == 1: trials_R.append(seg)\n",
    "\n",
    "        for r in MI_RUNS_OF:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 2: trials_FISTS.append(seg)\n",
    "                elif lab == 3: trials_FEET.append(seg)\n",
    "\n",
    "        need_per_class = 21\n",
    "        def pick(trials, n, rng):\n",
    "            if len(trials) < n:\n",
    "                idx = rng.choice(len(trials), size=n, replace=True)\n",
    "                return [trials[i] for i in idx]\n",
    "            rng.shuffle(trials)\n",
    "            return trials[:n]\n",
    "\n",
    "        rng = check_random_state(RANDOM_STATE + s)\n",
    "        if len(trials_L)==0 or len(trials_R)==0 or len(trials_FISTS)==0 or len(trials_FEET)==0:\n",
    "            continue\n",
    "\n",
    "        Lp  = pick(trials_L,     need_per_class, rng)\n",
    "        Rp  = pick(trials_R,     need_per_class, rng)\n",
    "        FIp = pick(trials_FISTS, need_per_class, rng)\n",
    "        FEp = pick(trials_FEET,  need_per_class, rng)\n",
    "\n",
    "        pack = [(Lp, 0), (Rp, 1), (FIp, 2), (FEp, 3)]\n",
    "        for segs, lab in pack:\n",
    "            for seg in segs:\n",
    "                X.append(seg)\n",
    "                y.append(lab)\n",
    "                groups.append(s)\n",
    "\n",
    "    X = np.stack(X, axis=0)\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "    groups = np.asarray(groups, dtype=np.int64)\n",
    "\n",
    "    n, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Dataset construido: N={n} | T={T} | C={C} | clases={n_classes} | sujetos √∫nicos={len(np.unique(groups))}\")\n",
    "    return X, y, groups, ch_template\n",
    "\n",
    "# =========================\n",
    "# EEGNet (Lawhern et al., 2018) adaptado a (B,1,T,C)\n",
    "# =========================\n",
    "class EEGNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Entrada: x de forma (B, 1, T, C)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_ch: int, n_classes: int,\n",
    "                 F1: int = 8, D: int = 2, kernel_t: int = 64, k_sep: int = 16,\n",
    "                 pool1_t: int = 4, pool2_t: int = 8,\n",
    "                 drop1_p: float = 0.25, drop2_p: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.n_ch = n_ch\n",
    "        self.n_classes = n_classes\n",
    "        self.F1 = F1\n",
    "        self.D = D\n",
    "        self.F2 = F1 * D\n",
    "        self.kernel_t = kernel_t\n",
    "        self.k_sep = k_sep\n",
    "        self.pool1_t = pool1_t\n",
    "        self.pool2_t = pool2_t\n",
    "\n",
    "        # Bloque 1: temporal\n",
    "        self.conv_temporal = nn.Conv2d(1, F1, kernel_size=(kernel_t, 1),\n",
    "                                       padding=(kernel_t // 2, 0), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(F1, momentum=0.99, eps=1e-3)\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "        # Bloque 2: depthwise (espacial)\n",
    "        self.conv_depthwise = nn.Conv2d(F1, self.F2, kernel_size=(1, n_ch),\n",
    "                                        groups=F1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.F2, momentum=0.99, eps=1e-3)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=(pool1_t, 1), stride=(pool1_t, 1))\n",
    "        self.drop1 = nn.Dropout(drop1_p)\n",
    "\n",
    "        # Bloque 3: separable temporal\n",
    "        self.conv_sep_depth = nn.Conv2d(self.F2, self.F2, kernel_size=(k_sep, 1),\n",
    "                                        groups=self.F2, padding=(k_sep // 2, 0), bias=False)\n",
    "        self.conv_sep_point = nn.Conv2d(self.F2, self.F2, kernel_size=(1, 1), bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.F2, momentum=0.99, eps=1e-3)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(pool2_t, 1), stride=(pool2_t, 1))\n",
    "        self.drop2 = nn.Dropout(drop2_p)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Cabeza din√°mica\n",
    "        self.fc = None\n",
    "        self.out = None\n",
    "        self._T_in = None\n",
    "\n",
    "    def _build_head(self, T_in: int, device: torch.device):\n",
    "        T1 = T_in // self.pool1_t\n",
    "        T2 = T1 // self.pool2_t\n",
    "        feat_dim = self.F2 * T2 * 1\n",
    "        self.fc = nn.Linear(feat_dim, 80, bias=True).to(device)\n",
    "        self.out = nn.Linear(80, self.n_classes, bias=True).to(device)\n",
    "        self._T_in = T_in\n",
    "\n",
    "    def ensure_head(self, T_in: int, device: torch.device):\n",
    "        if (self.fc is None) or (self.out is None) or (self._T_in != T_in):\n",
    "            self._build_head(T_in, device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, _, T, C = x.shape\n",
    "        self.ensure_head(T, x.device)\n",
    "\n",
    "        z = self.conv_temporal(x)\n",
    "        z = self.bn1(z); z = self.act(z)\n",
    "\n",
    "        z = self.conv_depthwise(z)   # (B, F2, T, 1)\n",
    "        z = self.bn2(z); z = self.act(z)\n",
    "        z = self.pool1(z)\n",
    "        z = self.drop1(z)\n",
    "\n",
    "        z = self.conv_sep_depth(z)\n",
    "        z = self.conv_sep_point(z)\n",
    "        z = self.bn3(z); z = self.act(z)\n",
    "        z = self.pool2(z)\n",
    "        z = self.drop2(z)\n",
    "\n",
    "        z = self.flatten(z)\n",
    "        z = self.fc(z); z = self.act(z)\n",
    "        z = self.out(z)\n",
    "        return z\n",
    "\n",
    "# =========================\n",
    "# TORCH DATASET\n",
    "# =========================\n",
    "class EEGTrials(Dataset):\n",
    "    def __init__(self, X, y, groups):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.g = groups.astype(np.int64)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        x = np.expand_dims(x, 0)                 # (1, T, C)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx])\n",
    "\n",
    "CLASS_NAMES_4C = ['Left', 'Right', 'Both Fists', 'Both Feet']\n",
    "\n",
    "# =========================\n",
    "# UTIL: MAX-NORM\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def apply_max_norm(model, max_value=2.0, p=2.0):\n",
    "    \"\"\"\n",
    "    Aplica max-norm (||W||_p <= max_value) a capas clave de EEGNet.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    if hasattr(model, 'conv_depthwise'): layers.append(model.conv_depthwise)\n",
    "    if hasattr(model, 'conv_sep_point'): layers.append(model.conv_sep_point)\n",
    "    if hasattr(model, 'fc'):             layers.append(model.fc)\n",
    "    if hasattr(model, 'out'):            layers.append(model.out)\n",
    "\n",
    "    for layer in layers:\n",
    "        if hasattr(layer, 'weight') and layer.weight is not None:\n",
    "            w = layer.weight.data\n",
    "            # normalizamos por filtros (dim=0)\n",
    "            norms = w.view(w.size(0), -1).norm(p=p, dim=1, keepdim=True)\n",
    "            desired = torch.clamp(norms, max=max_value)\n",
    "            w.view(w.size(0), -1).mul_(desired / (1e-8 + norms))\n",
    "\n",
    "# =========================\n",
    "# ENTRENAMIENTO / EVALUACI√ìN\n",
    "# =========================\n",
    "def train_epoch(model, loader, opt, criterion, maxnorm=None):\n",
    "    model.train()\n",
    "    for xb, yb, _ in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if maxnorm is not None:\n",
    "            apply_max_norm(model, max_value=maxnorm, p=2.0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_with_preds(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for xb, yb, _ in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        pred = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "        y_pred.extend(pred)\n",
    "        y_true.extend(yb.numpy().tolist())\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    return y_true, y_pred, float(acc)\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes, title, fname):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.nan_to_num(cm_norm)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm_norm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "        plt.text(j, i, format(cm_norm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm_norm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def print_report(y_true, y_pred, classes):\n",
    "    print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
    "\n",
    "# =========================\n",
    "# FINE-TUNING PROGRESIVO por sujeto (con validaci√≥n interna)\n",
    "# =========================\n",
    "def _param_groups(model, mode):\n",
    "    if mode == 'out':\n",
    "        train = list(model.out.parameters())\n",
    "    elif mode == 'head':\n",
    "        train = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "    elif mode == 'spatial+head':\n",
    "        train = (list(model.conv_depthwise.parameters()) +\n",
    "                 list(model.bn2.parameters()) +\n",
    "                 list(model.conv_sep_depth.parameters()) +\n",
    "                 list(model.conv_sep_point.parameters()) +\n",
    "                 list(model.bn3.parameters()) +\n",
    "                 list(model.fc.parameters()) +\n",
    "                 list(model.out.parameters()))\n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "    return train\n",
    "\n",
    "def _freeze_for_mode(model, mode):\n",
    "    for p in model.parameters(): p.requires_grad = False\n",
    "    if mode == 'out':\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'head':\n",
    "        for p in model.fc.parameters():  p.requires_grad = True\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'spatial+head':\n",
    "        for p in model.conv_depthwise.parameters(): p.requires_grad = True\n",
    "        for p in model.bn2.parameters():           p.requires_grad = True\n",
    "        for p in model.conv_sep_depth.parameters():p.requires_grad = True\n",
    "        for p in model.conv_sep_point.parameters():p.requires_grad = True\n",
    "        for p in model.bn3.parameters():           p.requires_grad = True\n",
    "        for p in model.fc.parameters():            p.requires_grad = True\n",
    "        for p in model.out.parameters():           p.requires_grad = True\n",
    "\n",
    "def _class_weights(y_np, n_classes):\n",
    "    counts = np.bincount(y_np, minlength=n_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    weights = counts.sum() / counts\n",
    "    weights = weights / weights.mean()\n",
    "    return torch.tensor(weights, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "def _train_one_mode(model, X_cal, y_cal, n_classes, mode,\n",
    "                    epochs=FT_EPOCHS, batch_size=32,\n",
    "                    head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                    l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=RANDOM_STATE)\n",
    "    (tr_idx, va_idx), = sss.split(X_cal, y_cal)\n",
    "    Xtr, ytr = X_cal[tr_idx], y_cal[tr_idx]\n",
    "    Xva, yva = X_cal[va_idx], y_cal[va_idx]\n",
    "\n",
    "    ds_tr = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xtr).float().unsqueeze(1),\n",
    "        torch.from_numpy(ytr).long()\n",
    "    )\n",
    "    ds_va = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xva).float().unsqueeze(1),\n",
    "        torch.from_numpy(yva).long()\n",
    "    )\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    _freeze_for_mode(model, mode)\n",
    "\n",
    "    if mode == 'spatial+head':\n",
    "        base_params = (list(model.conv_depthwise.parameters()) +\n",
    "                       list(model.bn2.parameters()) +\n",
    "                       list(model.conv_sep_depth.parameters()) +\n",
    "                       list(model.conv_sep_point.parameters()) +\n",
    "                       list(model.bn3.parameters()))\n",
    "        head_params = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "        train_params = base_params + head_params\n",
    "        opt = optim.Adam([\n",
    "            {\"params\": base_params, \"lr\": base_lr},\n",
    "            {\"params\": head_params, \"lr\": head_lr},\n",
    "        ])\n",
    "    else:\n",
    "        train_params = _param_groups(model, mode)\n",
    "        opt = optim.Adam(train_params, lr=head_lr)\n",
    "\n",
    "    ref = [p.detach().clone().to(p.device) for p in train_params]\n",
    "    class_w = _class_weights(ytr, n_classes)\n",
    "    crit = nn.CrossEntropyLoss(weight=class_w)\n",
    "\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_val = float('inf')\n",
    "    bad = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            reg = 0.0\n",
    "            for p_cur, p_ref in zip(train_params, ref):\n",
    "                reg = reg + torch.sum((p_cur - p_ref)**2)\n",
    "            loss = loss + l2sp_lambda * reg\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            apply_max_norm(model, max_value=2.0, p=2.0)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            nval = 0\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                loss = crit(logits, yb)\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "                nval += xb.size(0)\n",
    "            val_loss /= max(1, nval)\n",
    "\n",
    "        if val_loss + 1e-7 < best_val:\n",
    "            best_val = val_loss\n",
    "            bad = 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_numpy(model, X_np, device):\n",
    "    model.eval()\n",
    "    xb = torch.from_numpy(X_np).float().unsqueeze(1).to(device)\n",
    "    logits = model(xb)\n",
    "    return logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "def subject_cv_finetune_predict_progressive(model_global, Xs, ys, device,\n",
    "                                            n_splits=CALIB_CV_FOLDS, n_classes=4):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    y_true_full = np.empty_like(ys)\n",
    "    y_pred_full = np.empty_like(ys)\n",
    "\n",
    "    for tr_idx, te_idx in skf.split(Xs, ys):\n",
    "        Xcal, ycal = Xs[tr_idx], ys[tr_idx]\n",
    "        Xho,  yho  = Xs[te_idx], ys[te_idx]\n",
    "\n",
    "        m_out = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_out, Xcal, ycal, n_classes, mode='out',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_out = predict_numpy(m_out, Xho, device)\n",
    "        acc_out = (yhat_out == yho).mean()\n",
    "\n",
    "        m_head = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_head, Xcal, ycal, n_classes, mode='head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_head = predict_numpy(m_head, Xho, device)\n",
    "        acc_head = (yhat_head == yho).mean()\n",
    "\n",
    "        m_sp = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_sp, Xcal, ycal, n_classes, mode='spatial+head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                        l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_sp = predict_numpy(m_sp, Xho, device)\n",
    "        acc_sp = (yhat_sp == yho).mean()\n",
    "\n",
    "        best_idx = np.argmax([acc_out, acc_head, acc_sp])\n",
    "        yhat_best = [yhat_out, yhat_head, yhat_sp][best_idx]\n",
    "\n",
    "        y_true_full[te_idx] = yho\n",
    "        y_pred_full[te_idx] = yhat_best\n",
    "\n",
    "    return y_true_full, y_pred_full\n",
    "\n",
    "# =========================\n",
    "# FOLDS JSON helpers\n",
    "# =========================\n",
    "def save_group_folds_json_with_indices(subject_ids_str, groups_array, n_splits, out_json_path,\n",
    "                                       created_by=\"dose_experiment\", description=None):\n",
    "    out_json_path = Path(out_json_path)\n",
    "    unique_subjects_int = sorted(np.unique(groups_array).tolist())\n",
    "    subject_ids = [f\"S{sid:03d}\" for sid in unique_subjects_int]\n",
    "\n",
    "    if len(subject_ids) < n_splits:\n",
    "        raise ValueError(f\"n_splits={n_splits} mayor que n√∫mero de sujetos={len(subject_ids)}\")\n",
    "\n",
    "    groups = np.arange(len(subject_ids))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    folds = []\n",
    "    fold_i = 0\n",
    "    for train_idx_grp, test_idx_grp in gkf.split(groups, groups, groups):\n",
    "        fold_i += 1\n",
    "        train_sids = [subject_ids[int(i)] for i in train_idx_grp]\n",
    "        test_sids  = [subject_ids[int(i)] for i in test_idx_grp]\n",
    "\n",
    "        train_sids_int = [int(s[1:]) for s in train_sids]\n",
    "        test_sids_int  = [int(s[1:]) for s in test_sids]\n",
    "\n",
    "        tr_idx = np.where(np.isin(groups_array, train_sids_int))[0].tolist()\n",
    "        te_idx = np.where(np.isin(groups_array, test_sids_int))[0].tolist()\n",
    "\n",
    "        folds.append({\n",
    "            \"fold\": int(fold_i),\n",
    "            \"train\": train_sids,\n",
    "            \"test\": test_sids,\n",
    "            \"tr_idx\": tr_idx,\n",
    "            \"te_idx\": te_idx\n",
    "        })\n",
    "\n",
    "    payload = {\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"created_by\": created_by,\n",
    "        \"description\": description if description is not None else \"\",\n",
    "        \"n_splits\": int(n_splits),\n",
    "        \"n_subjects\": len(subject_ids),\n",
    "        \"subject_ids\": subject_ids,\n",
    "        \"folds\": folds\n",
    "    }\n",
    "\n",
    "    out_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Folds JSON con √≠ndices guardado ‚Üí {out_json_path}\")\n",
    "    return out_json_path\n",
    "\n",
    "def load_group_folds_json(path_json, expected_subject_ids=None, strict_check=True):\n",
    "    path_json = Path(path_json)\n",
    "    if not path_json.exists():\n",
    "        raise FileNotFoundError(f\"No existe {path_json}\")\n",
    "    with open(path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        payload = json.load(f)\n",
    "\n",
    "    subj_json = payload.get(\"subject_ids\", [])\n",
    "    if expected_subject_ids is not None:\n",
    "        expected = sorted(list(expected_subject_ids))\n",
    "        if subj_json != expected:\n",
    "            msg = (\"Los subject_ids del JSON no coinciden con expected_subject_ids.\\n\"\n",
    "                   f\"JSON has {len(subj_json)} subjects, expected {len(expected)}.\\n\"\n",
    "                   f\"First 10 JSON: {subj_json[:10]}\\nFirst 10 expected: {expected[:10]}\")\n",
    "            if strict_check:\n",
    "                raise ValueError(msg)\n",
    "            else:\n",
    "                print(\"WARNING: \" + msg)\n",
    "    return payload\n",
    "\n",
    "# =========================\n",
    "# EXPERIMENTO\n",
    "# =========================\n",
    "def run_experiment(save_folds_json=True, folds_json_path=FOLDS_DIR, folds_json_description=\"GroupKFold folds for comparison\"):\n",
    "    mne.set_log_level('WARNING')\n",
    "\n",
    "    subs = subjects_available()\n",
    "    print(f\"Sujetos elegibles: {len(subs)} ‚Üí {subs[:10]}{'...' if len(subs)>10 else ''}\")\n",
    "\n",
    "    X, y, groups, chs = build_dataset_all(subs, scenario=CLASS_SCENARIO, window_mode=WINDOW_MODE)\n",
    "    N, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Listo para entrenar: N={N} | T={T} | C={C} | clases={n_classes} | sujetos={len(np.unique(groups))}\")\n",
    "\n",
    "    ds = EEGTrials(X, y, groups)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # preparar JSON folds\n",
    "    if folds_json_path is None:\n",
    "        folds_json_path = Path(\"folds\") / f\"group_folds_{N_FOLDS}splits.json\"\n",
    "    else:\n",
    "        folds_json_path = Path(folds_json_path)\n",
    "    folds_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    unique_subs = sorted(np.unique(groups).tolist())\n",
    "    subject_ids_str = [f\"S{s:03d}\" for s in unique_subs]\n",
    "\n",
    "    if not folds_json_path.exists():\n",
    "        if not save_folds_json:\n",
    "            raise FileNotFoundError(f\"Folds JSON no encontrado en {folds_json_path} y save_folds_json=False.\")\n",
    "        save_group_folds_json_with_indices(subject_ids_str, groups, n_splits=N_FOLDS,\n",
    "                                           out_json_path=folds_json_path,\n",
    "                                           created_by=\"Joel_Clasificador\",\n",
    "                                           description=folds_json_description)\n",
    "\n",
    "    payload = load_group_folds_json(folds_json_path, expected_subject_ids=subject_ids_str, strict_check=False)\n",
    "    folds = payload[\"folds\"]\n",
    "\n",
    "    # bucle por folds\n",
    "    global_folds = []\n",
    "    ft_prog_folds = []\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "\n",
    "    for f in folds:\n",
    "        fold = f[\"fold\"]\n",
    "        tr_idx = np.asarray(f.get(\"tr_idx\", []), dtype=int)\n",
    "        te_idx = np.asarray(f.get(\"te_idx\", []), dtype=int)\n",
    "\n",
    "        if tr_idx.size == 0 or te_idx.size == 0:\n",
    "            print(f\"Advertencia: fold {fold} sin √≠ndices tr/te v√°lidos. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        # ===== Split de validaci√≥n por sujetos =====\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=GLOBAL_VAL_SPLIT, random_state=RANDOM_STATE)\n",
    "        tr_subj_idx, va_subj_idx = next(gss.split(tr_idx, groups[tr_idx], groups[tr_idx]))\n",
    "        tr_sub_idx = tr_idx[tr_subj_idx]\n",
    "        va_idx     = tr_idx[va_subj_idx]\n",
    "\n",
    "        tr_loader = DataLoader(Subset(ds, tr_sub_idx), batch_size=BATCH_SIZE, shuffle=True,  drop_last=False)\n",
    "        va_loader = DataLoader(Subset(ds, va_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "        te_loader = DataLoader(Subset(ds, te_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "        # ===== EEGNet =====\n",
    "        model = EEGNet(n_ch=C, n_classes=n_classes, F1=8, D=2, kernel_t=64, k_sep=16,\n",
    "                       pool1_t=4, pool2_t=8, drop1_p=0.25, drop2_p=0.5).to(DEVICE)\n",
    "\n",
    "        # Adam sin weight decay + scheduler tipo paper\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_INIT)\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(opt, milestones=SCHED_MILESTONES, gamma=SCHED_GAMMA)\n",
    "\n",
    "        def _acc(loader):\n",
    "            return evaluate_with_preds(model, loader)[2]\n",
    "\n",
    "        # ===== Entrenamiento global con ES por val_acc + max-norm =====\n",
    "        print(f\"\\n[Fold {fold}/{N_FOLDS}] Entrenando modelo global...\"\n",
    "              f\" (n_train={len(tr_sub_idx)} | n_val={len(va_idx)} | n_test={len(te_idx)})\")\n",
    "\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        best_val = -1.0\n",
    "        bad = 0\n",
    "\n",
    "        for epoch in range(1, EPOCHS_GLOBAL + 1):\n",
    "            train_epoch(model, tr_loader, opt, criterion, maxnorm=2.0)\n",
    "            scheduler.step()\n",
    "\n",
    "            if epoch % LOG_EVERY == 0 or epoch in (1, 20, 50, 100):\n",
    "                tr_acc = _acc(tr_loader)\n",
    "                va_acc = _acc(va_loader)\n",
    "                print(f\"  √âpoca {epoch:3d} | train_acc={tr_acc:.4f} | val_acc={va_acc:.4f} | LR={opt.param_groups[0]['lr']:.5f}\")\n",
    "\n",
    "                if va_acc > best_val + 1e-4:\n",
    "                    best_val = va_acc\n",
    "                    best_state = copy.deepcopy(model.state_dict())\n",
    "                    bad = 0\n",
    "                else:\n",
    "                    bad += 1\n",
    "                    if bad >= GLOBAL_PATIENCE:\n",
    "                        print(f\"  Early stopping en √©poca {epoch} (mejor val_acc={best_val:.4f})\")\n",
    "                        break\n",
    "\n",
    "        # cargar mejor estado antes de evaluar en test\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "        # ===== Evaluaci√≥n global (inter-sujeto puro) =====\n",
    "        y_true, y_pred, acc_global = evaluate_with_preds(model, te_loader)\n",
    "        global_folds.append(acc_global)\n",
    "        all_true.append(y_true); all_pred.append(y_pred)\n",
    "\n",
    "        print(f\"[Fold {fold}/{N_FOLDS}] Global acc={acc_global:.4f}\")\n",
    "        print_report(y_true, y_pred, CLASS_NAMES_4C)\n",
    "\n",
    "        # ---------- Fine-tuning PROGRESIVO por sujeto con 4-fold CV ----------\n",
    "        X_te, y_te, g_te = X[te_idx], y[te_idx], groups[te_idx]\n",
    "\n",
    "        y_true_ft_all, y_pred_ft_all = [], []\n",
    "        used_subjects = 0\n",
    "        for sid in np.unique(g_te):\n",
    "            idx = np.where(g_te == sid)[0]\n",
    "            Xs, ys = X_te[idx], y_te[idx]\n",
    "\n",
    "            if len(ys) < CALIB_CV_FOLDS or len(np.unique(ys)) < 2:\n",
    "                continue\n",
    "\n",
    "            y_true_subj, y_pred_subj = subject_cv_finetune_predict_progressive(\n",
    "                model, Xs, ys, DEVICE, n_splits=CALIB_CV_FOLDS, n_classes=n_classes\n",
    "            )\n",
    "            y_true_ft_all.append(y_true_subj)\n",
    "            y_pred_ft_all.append(y_pred_subj)\n",
    "            used_subjects += 1\n",
    "\n",
    "        if len(y_true_ft_all) > 0:\n",
    "            y_true_ft_all = np.concatenate(y_true_ft_all)\n",
    "            y_pred_ft_all = np.concatenate(y_pred_ft_all)\n",
    "            acc_ft = (y_true_ft_all == y_pred_ft_all).mean()\n",
    "            print(f\"  Fine-tuning PROGRESIVO (por sujeto, {CALIB_CV_FOLDS}-fold CV) acc={acc_ft:.4f} | sujetos={used_subjects}\")\n",
    "            print(f\"  Œî(FT-Global) = {acc_ft - acc_global:+.4f}\")\n",
    "        else:\n",
    "            acc_ft = np.nan\n",
    "            print(\"  Fine-tuning PROGRESIVO no ejecutado (sujeto(s) con muestras insuficientes).\")\n",
    "\n",
    "        ft_prog_folds.append(acc_ft)\n",
    "\n",
    "    # ---------- resultados finales ----------\n",
    "    if len(all_true) > 0:\n",
    "        all_true = np.concatenate(all_true)\n",
    "        all_pred = np.concatenate(all_pred)\n",
    "    else:\n",
    "        all_true = np.array([], dtype=int)\n",
    "        all_pred = np.array([], dtype=int)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTADOS FINALES\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Global folds:\", [f\"{a:.4f}\" for a in global_folds])\n",
    "    if len(global_folds) > 0:\n",
    "        print(f\"Global mean: {np.mean(global_folds):.4f}\")\n",
    "\n",
    "    print(\"Fine-tune PROGRESIVO folds:\", [(\"nan\" if (a is None or np.isnan(a)) else f\"{a:.4f}\") for a in ft_prog_folds])\n",
    "    if len(ft_prog_folds) > 0:\n",
    "        print(f\"Fine-tune PROGRESIVO mean: {np.nanmean(ft_prog_folds):.4f}\")\n",
    "        print(f\"Œî(FT-Global) mean: {np.nanmean(ft_prog_folds) - np.mean(global_folds):+.4f}\")\n",
    "\n",
    "    if all_true.size > 0:\n",
    "        plot_confusion(all_true, all_pred, CLASS_NAMES_4C,\n",
    "                       title=\"Confusion Matrix - Global Model (All Folds)\",\n",
    "                       fname=\"confusion_global_allfolds.png\")\n",
    "        print(\"\\n‚Ü≥ Matriz de confusi√≥n guardada: confusion_global_allfolds.png\")\n",
    "\n",
    "    return {\n",
    "        \"global_folds\": global_folds,\n",
    "        \"ft_prog_folds\": ft_prog_folds,\n",
    "        \"all_true\": all_true,\n",
    "        \"all_pred\": all_pred,\n",
    "        \"folds_json_path\": str(folds_json_path)\n",
    "    }\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üß† INICIANDO EXPERIMENTO CON EEGNet + FINE-TUNING PROGRESIVO (por sujeto, 4-fold CV)\")\n",
    "    print(f\"üîß Configuraci√≥n: {CLASS_SCENARIO}, {len(EXPECTED_8)} canales, {WINDOW_MODE}\")\n",
    "    print(f\"‚öôÔ∏è  FT: epochs={FT_EPOCHS}, base_lr={FT_BASE_LR}, head_lr={FT_HEAD_LR}, L2SP={FT_L2SP}, patience={FT_PATIENCE}, CV={CALIB_CV_FOLDS}\")\n",
    "    run_experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c81561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Usando dispositivo: cuda\n",
      "üß† INICIANDO EXPERIMENTO CON EEGNet (capacidad‚Üë) + CE ponderada + smoothing + aug OF focalizado\n",
      "üîß Configuraci√≥n: 4c, 8 canales, 6s\n",
      "‚öôÔ∏è  FT: epochs=30, base_lr=5e-05, head_lr=0.001, L2SP=0.0001, patience=5, CV=4\n",
      "Sujetos elegibles: 103 ‚Üí [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Construyendo dataset (RAW): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:37<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset construido: N=8652 | T=960 | C=8 | clases=4 | sujetos √∫nicos=103\n",
      "Listo para entrenar: N=8652 | T=960 | C=8 | clases=4 | sujetos=103\n",
      "\n",
      "[Fold 1/5] Entrenando modelo global... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  √âpoca   1 | train_acc=0.4707 | val_acc=0.4441 | LR=0.01000\n",
      "  √âpoca   5 | train_acc=0.5000 | val_acc=0.4496 | LR=0.00655\n",
      "  √âpoca  10 | train_acc=0.5502 | val_acc=0.4844 | LR=0.00024\n",
      "  √âpoca  15 | train_acc=0.5200 | val_acc=0.4734 | LR=0.00905\n",
      "  √âpoca  20 | train_acc=0.5148 | val_acc=0.4689 | LR=0.00578\n",
      "  Early stopping en √©poca 22 (mejor val_acc=0.4844)\n",
      "‚Ü≥ Curva de entrenamiento guardada: training_curve_fold1.png\n",
      "[Fold 1/5] Global acc=0.4649\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.6081    0.4785    0.5355       441\n",
      "       Right     0.5249    0.5261    0.5255       441\n",
      "  Both Fists     0.3494    0.3946    0.3706       441\n",
      "   Both Feet     0.4256    0.4603    0.4423       441\n",
      "\n",
      "    accuracy                         0.4649      1764\n",
      "   macro avg     0.4770    0.4649    0.4685      1764\n",
      "weighted avg     0.4770    0.4649    0.4685      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5363 | sujetos=21\n",
      "  Œî(FT-Global) = +0.0714\n",
      "\n",
      "[Fold 2/5] Entrenando modelo global... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  √âpoca   1 | train_acc=0.4310 | val_acc=0.4176 | LR=0.01000\n",
      "  √âpoca   5 | train_acc=0.4950 | val_acc=0.4542 | LR=0.00655\n",
      "  √âpoca  10 | train_acc=0.5045 | val_acc=0.4606 | LR=0.00024\n",
      "  √âpoca  15 | train_acc=0.5216 | val_acc=0.4487 | LR=0.00905\n",
      "  Early stopping en √©poca 15 (mejor val_acc=0.4661)\n",
      "‚Ü≥ Curva de entrenamiento guardada: training_curve_fold2.png\n",
      "[Fold 2/5] Global acc=0.5170\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5232    0.6644    0.5854       441\n",
      "       Right     0.5562    0.6735    0.6092       441\n",
      "  Both Fists     0.4492    0.3311    0.3812       441\n",
      "   Both Feet     0.5101    0.3991    0.4478       441\n",
      "\n",
      "    accuracy                         0.5170      1764\n",
      "   macro avg     0.5097    0.5170    0.5059      1764\n",
      "weighted avg     0.5097    0.5170    0.5059      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5612 | sujetos=21\n",
      "  Œî(FT-Global) = +0.0442\n",
      "\n",
      "[Fold 3/5] Entrenando modelo global... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  √âpoca   1 | train_acc=0.4626 | val_acc=0.4725 | LR=0.01000\n",
      "  √âpoca   5 | train_acc=0.5021 | val_acc=0.5000 | LR=0.00655\n",
      "  √âpoca  10 | train_acc=0.5342 | val_acc=0.4973 | LR=0.00024\n",
      "  √âpoca  15 | train_acc=0.5419 | val_acc=0.5027 | LR=0.00905\n",
      "  Early stopping en √©poca 19 (mejor val_acc=0.5192)\n",
      "‚Ü≥ Curva de entrenamiento guardada: training_curve_fold3.png\n",
      "[Fold 3/5] Global acc=0.4626\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5279    0.4717    0.4982       441\n",
      "       Right     0.5349    0.4694    0.5000       441\n",
      "  Both Fists     0.3915    0.4580    0.4222       441\n",
      "   Both Feet     0.4261    0.4512    0.4383       441\n",
      "\n",
      "    accuracy                         0.4626      1764\n",
      "   macro avg     0.4701    0.4626    0.4647      1764\n",
      "weighted avg     0.4701    0.4626    0.4647      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5323 | sujetos=21\n",
      "  Œî(FT-Global) = +0.0697\n",
      "\n",
      "[Fold 4/5] Entrenando modelo global... (n_train=5880 | n_val=1092 | n_test=1680)\n",
      "  √âpoca   1 | train_acc=0.4541 | val_acc=0.4451 | LR=0.01000\n",
      "  √âpoca   5 | train_acc=0.4990 | val_acc=0.4817 | LR=0.00655\n",
      "  √âpoca  10 | train_acc=0.5340 | val_acc=0.5101 | LR=0.00024\n",
      "  √âpoca  15 | train_acc=0.5119 | val_acc=0.4789 | LR=0.00905\n",
      "  √âpoca  20 | train_acc=0.5395 | val_acc=0.5009 | LR=0.00578\n",
      "  Early stopping en √©poca 22 (mejor val_acc=0.5101)\n",
      "‚Ü≥ Curva de entrenamiento guardada: training_curve_fold4.png\n",
      "[Fold 4/5] Global acc=0.4815\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5748    0.5214    0.5468       420\n",
      "       Right     0.5747    0.4762    0.5208       420\n",
      "  Both Fists     0.4024    0.4857    0.4401       420\n",
      "   Both Feet     0.4189    0.4429    0.4306       420\n",
      "\n",
      "    accuracy                         0.4815      1680\n",
      "   macro avg     0.4927    0.4815    0.4846      1680\n",
      "weighted avg     0.4927    0.4815    0.4846      1680\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5780 | sujetos=20\n",
      "  Œî(FT-Global) = +0.0964\n",
      "\n",
      "[Fold 5/5] Entrenando modelo global... (n_train=5880 | n_val=1092 | n_test=1680)\n",
      "  √âpoca   1 | train_acc=0.4600 | val_acc=0.4423 | LR=0.01000\n",
      "  √âpoca   5 | train_acc=0.4859 | val_acc=0.4799 | LR=0.00655\n",
      "  √âpoca  10 | train_acc=0.5318 | val_acc=0.4863 | LR=0.00024\n",
      "  √âpoca  15 | train_acc=0.5053 | val_acc=0.4835 | LR=0.00905\n",
      "  √âpoca  20 | train_acc=0.5282 | val_acc=0.4936 | LR=0.00578\n",
      "  √âpoca  25 | train_acc=0.5347 | val_acc=0.5027 | LR=0.00206\n",
      "  √âpoca  30 | train_acc=0.5437 | val_acc=0.5000 | LR=0.00006\n",
      "  √âpoca  35 | train_acc=0.5233 | val_acc=0.5046 | LR=0.00976\n",
      "  √âpoca  40 | train_acc=0.5219 | val_acc=0.4982 | LR=0.00880\n",
      "  √âpoca  45 | train_acc=0.5466 | val_acc=0.5009 | LR=0.00727\n",
      "  Early stopping en √©poca 49 (mejor val_acc=0.5110)\n",
      "‚Ü≥ Curva de entrenamiento guardada: training_curve_fold5.png\n",
      "[Fold 5/5] Global acc=0.5327\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.6141    0.6214    0.6178       420\n",
      "       Right     0.5809    0.5643    0.5725       420\n",
      "  Both Fists     0.4543    0.4262    0.4398       420\n",
      "   Both Feet     0.4812    0.5190    0.4994       420\n",
      "\n",
      "    accuracy                         0.5327      1680\n",
      "   macro avg     0.5326    0.5327    0.5324      1680\n",
      "weighted avg     0.5326    0.5327    0.5324      1680\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.6161 | sujetos=20\n",
      "  Œî(FT-Global) = +0.0833\n",
      "\n",
      "============================================================\n",
      "RESULTADOS FINALES\n",
      "============================================================\n",
      "Global folds: ['0.4649', '0.5170', '0.4626', '0.4815', '0.5327']\n",
      "Global mean: 0.4917\n",
      "Fine-tune PROGRESIVO folds: ['0.5363', '0.5612', '0.5323', '0.5780', '0.6161']\n",
      "Fine-tune PROGRESIVO mean: 0.5648\n",
      "Œî(FT-Global) mean: +0.0730\n",
      "\n",
      "‚Ü≥ Matriz de confusi√≥n guardada: confusion_global_allfolds.png\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# EEGNet + protocolo global + fine-tuning progresivo (mejoras BFISTS)\n",
    "# Cambios: CE ponderada + label smoothing, cutout focalizado para OF,\n",
    "# F1=16 (m√°s capacidad), paciencia=12, SGDR, jitter+noise, curvas de entrenamiento,\n",
    "# notch adaptativo seg√∫n SNR y sampler balanceado sujeto+clase.\n",
    "\n",
    "import os, re, math, random, json, itertools, copy\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold, StratifiedShuffleSplit, GroupShuffleSplit\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# CONFIGURACI√ìN GENERAL\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "FOLDS_DIR = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dispositivo y semilla\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE); np.random.seed(RANDOM_STATE); random.seed(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "# Escenario y ventana\n",
    "CLASS_SCENARIO = '4c'\n",
    "WINDOW_MODE = '6s'   # 6s como acordamos\n",
    "FS = 160.0\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Entrenamiento global\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS_GLOBAL = 100\n",
    "LR_INIT = 1e-2\n",
    "SGDR_T0 = 10\n",
    "SGDR_Tmult = 2\n",
    "\n",
    "# Validaci√≥n/ES global\n",
    "GLOBAL_VAL_SPLIT = 0.15\n",
    "GLOBAL_PATIENCE  = 12   # ‚Üê m√°s laxo\n",
    "LOG_EVERY        = 5\n",
    "\n",
    "# Fine-tuning por sujeto\n",
    "CALIB_CV_FOLDS = 4\n",
    "FT_EPOCHS = 30\n",
    "FT_BASE_LR = 5e-5\n",
    "FT_HEAD_LR = 1e-3\n",
    "FT_L2SP = 1e-4\n",
    "FT_PATIENCE = 5\n",
    "FT_VAL_RATIO = 0.2\n",
    "\n",
    "# Normalizaci√≥n por √©poca canal-a-canal (z-score)\n",
    "NORM_EPOCH_ZSCORE = True\n",
    "\n",
    "# Sujetos excluidos\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "\n",
    "# Runs\n",
    "MI_RUNS_LR = [4, 8, 12]\n",
    "MI_RUNS_OF = [6, 10, 14]\n",
    "BASELINE_RUNS_EO = [1]\n",
    "\n",
    "# Canales (8 con FCz)\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','FCz']\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES DE CANALES\n",
    "# =========================\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = re.sub(r'([A-Za-z])0([0-9])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])Z$', r'\\1z', s)\n",
    "    s = s.replace('fp', 'Fp').replace('FP', 'Fp')\n",
    "    s = ''.join(ch.upper() if ch != 'z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {}\n",
    "    for ch in raw.ch_names:\n",
    "        lab = normalize_label(ch)\n",
    "        lab = lab[:-1] + 'z' if lab.endswith('Z') else lab\n",
    "        lab = re.sub(r'([A-Z])Z$', r'\\1z', lab)\n",
    "        mapping[ch] = lab\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired_channels=EXPECTED_8):\n",
    "    missing = [ch for ch in desired_channels if ch not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"Warning: faltan canales {missing} en archivo {getattr(raw,'filenames', [''])[0]}\")\n",
    "        return None\n",
    "    raw.reorder_channels([ch for ch in raw.ch_names if ch in desired_channels] +\n",
    "                         [ch for ch in raw.ch_names if ch not in desired_channels])\n",
    "    raw.pick_channels(desired_channels, ordered=True)\n",
    "    return raw\n",
    "\n",
    "# =========================\n",
    "# LECTURA DE EDF y EVENTOS\n",
    "# =========================\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(run_id:int):\n",
    "    if run_id in MI_RUNS_LR: return 'LR'\n",
    "    if run_id in MI_RUNS_OF: return 'OF'\n",
    "    if run_id in BASELINE_RUNS_EO: return 'EO'\n",
    "    return None\n",
    "\n",
    "# --- Notch adaptativo: lee CSV si existe (subject, run, snr50_db, snr60_db)\n",
    "_SNR_TABLE = None\n",
    "def _load_snr_table():\n",
    "    global _SNR_TABLE\n",
    "    if _SNR_TABLE is not None:\n",
    "        return _SNR_TABLE\n",
    "    csv_path = PROJ / 'reports' / 'psd_mains' / 'psd_mains_summary.csv'\n",
    "    if csv_path.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            _SNR_TABLE = df\n",
    "        except Exception as e:\n",
    "            print(f\"[SNR] No se pudo leer {csv_path}: {e}\")\n",
    "            _SNR_TABLE = None\n",
    "    return _SNR_TABLE\n",
    "\n",
    "def _decide_notch(subject, run, th_db=10.0):\n",
    "    df = _load_snr_table()\n",
    "    if df is None:  # default\n",
    "        return 60.0\n",
    "    row = df[(df['subject']==subject) & (df['run']==run)]\n",
    "    if row.empty:\n",
    "        return 60.0\n",
    "    snr50 = float(row['snr50_db'].iloc[0])\n",
    "    snr60 = float(row['snr60_db'].iloc[0])\n",
    "    if snr60 >= th_db and snr60 >= snr50: return 60.0\n",
    "    if snr50 >= th_db and snr50 >  snr60: return 50.0\n",
    "    return None  # no notch si no sobresale\n",
    "\n",
    "def read_raw_edf(path: Path):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        mont = mne.channels.make_standard_montage('standard_1020')\n",
    "        raw.set_montage(mont, on_missing='ignore')\n",
    "    except Exception:\n",
    "        pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad=\"auto\")\n",
    "    raw = ensure_channels_order(raw, EXPECTED_8)\n",
    "    if raw is None:\n",
    "        return None\n",
    "\n",
    "    # Notch adaptativo (por SNR); si no hay CSV, por defecto 60 Hz\n",
    "    sid, rid = parse_subject_run(path)\n",
    "    notch = _decide_notch(sid, rid)\n",
    "    if notch is not None:\n",
    "        raw.notch_filter(freqs=[float(notch)], picks='eeg', method='spectrum_fit', phase='zero')\n",
    "\n",
    "    return raw\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    if raw.annotations is None or len(raw.annotations) == 0:\n",
    "        return []\n",
    "    def _norm(s): return str(s).strip().upper().replace(' ', '')\n",
    "    res = []\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = _norm(desc)\n",
    "        if tag in ('T1','T2'):\n",
    "            res.append((float(onset), tag))\n",
    "    res.sort()\n",
    "    dedup = []\n",
    "    last_t1 = last_t2 = -1e9\n",
    "    for t, tag in res:\n",
    "        if tag == 'T1':\n",
    "            if (t - last_t1) >= 0.5: dedup.append((t, tag)); last_t1 = t\n",
    "        else:\n",
    "            if (t - last_t2) >= 0.5: dedup.append((t, tag)); last_t2 = t\n",
    "    return dedup\n",
    "\n",
    "# =========================\n",
    "# CONSTRUCCI√ìN DE DATASETS\n",
    "# =========================\n",
    "def subjects_available():\n",
    "    subs = []\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try:\n",
    "            sid = int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir / f\"S{sid:03d}R{r:02d}.edf\").exists() for r in (MI_RUNS_LR + MI_RUNS_OF))\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "def extract_trials_from_run(edf_path: Path, scenario: str, window_mode: str):\n",
    "    subj, run = parse_subject_run(edf_path)\n",
    "    kind = run_kind(run)\n",
    "    if kind not in ('LR','OF','EO'):\n",
    "        return ([], [])\n",
    "\n",
    "    raw = read_raw_edf(edf_path)\n",
    "    if raw is None:\n",
    "        return ([], [])\n",
    "\n",
    "    data = raw.get_data()\n",
    "    fs = raw.info['sfreq']\n",
    "    assert abs(fs - FS) < 1e-6\n",
    "\n",
    "    out = []\n",
    "\n",
    "    if kind in ('LR','OF'):\n",
    "        events = collect_events_T1T2(raw)\n",
    "        if window_mode == '3s':\n",
    "            rel_start, rel_end = 0.0, 3.0\n",
    "        else:\n",
    "            rel_start, rel_end = -1.0, 5.0\n",
    "\n",
    "        for onset_sec, tag in events:\n",
    "            if kind == 'LR':\n",
    "                if tag == 'T1': label = 'L'\n",
    "                elif tag == 'T2': label = 'R'\n",
    "                else: continue\n",
    "            else:\n",
    "                if tag == 'T1': label = 'BFISTS'\n",
    "                elif tag == 'T2': label = 'BFEET'\n",
    "                else: continue\n",
    "\n",
    "            if scenario == '2c' and label not in ('L','R'): continue\n",
    "            if scenario == '3c' and label not in ('L','R','BFISTS'): continue\n",
    "            if scenario == '4c' and label not in ('L','R','BFISTS','BFEET'): continue\n",
    "\n",
    "            s = int(round((raw.first_time + onset_sec + rel_start) * fs))\n",
    "            e = int(round((raw.first_time + onset_sec + rel_end) * fs))\n",
    "            if s < 0 or e > data.shape[1]:\n",
    "                continue\n",
    "\n",
    "            seg = data[:, s:e].T.astype(np.float32)\n",
    "            if NORM_EPOCH_ZSCORE:\n",
    "                seg = (seg - seg.mean(axis=0, keepdims=True)) / (seg.std(axis=0, keepdims=True) + 1e-6)\n",
    "\n",
    "            if label == 'L':       y = 0\n",
    "            elif label == 'R':     y = 1\n",
    "            elif label == 'BFISTS':y = 2\n",
    "            elif label == 'BFEET': y = 3\n",
    "            else: continue\n",
    "\n",
    "            out.append((seg, y, subj))\n",
    "\n",
    "    elif kind == 'EO':\n",
    "        return ([], raw.ch_names)\n",
    "\n",
    "    return out, raw.ch_names\n",
    "\n",
    "def build_dataset_all(subjects, scenario='4c', window_mode='3s'):\n",
    "    X, y, groups = [], [], []\n",
    "    ch_template = None\n",
    "\n",
    "    for s in tqdm(subjects, desc=\"Construyendo dataset (RAW)\"):\n",
    "        sdir = DATA_RAW / f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "\n",
    "        trials_L, trials_R, trials_FISTS, trials_FEET = [], [], [], []\n",
    "\n",
    "        for r in MI_RUNS_LR:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 0: trials_L.append(seg)\n",
    "                elif lab == 1: trials_R.append(seg)\n",
    "\n",
    "        for r in MI_RUNS_OF:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 2: trials_FISTS.append(seg)\n",
    "                elif lab == 3: trials_FEET.append(seg)\n",
    "\n",
    "        need_per_class = 21\n",
    "        def pick(trials, n, rng):\n",
    "            if len(trials) < n:\n",
    "                idx = rng.choice(len(trials), size=n, replace=True)\n",
    "                return [trials[i] for i in idx]\n",
    "            rng.shuffle(trials)\n",
    "            return trials[:n]\n",
    "\n",
    "        rng = check_random_state(RANDOM_STATE + s)\n",
    "        if len(trials_L)==0 or len(trials_R)==0 or len(trials_FISTS)==0 or len(trials_FEET)==0:\n",
    "            continue\n",
    "\n",
    "        Lp  = pick(trials_L,     need_per_class, rng)\n",
    "        Rp  = pick(trials_R,     need_per_class, rng)\n",
    "        FIp = pick(trials_FISTS, need_per_class, rng)\n",
    "        FEp = pick(trials_FEET,  need_per_class, rng)\n",
    "\n",
    "        pack = [(Lp, 0), (Rp, 1), (FIp, 2), (FEp, 3)]\n",
    "        for segs, lab in pack:\n",
    "            for seg in segs:\n",
    "                X.append(seg); y.append(lab); groups.append(s)\n",
    "\n",
    "    X = np.stack(X, axis=0)\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "    groups = np.asarray(groups, dtype=np.int64)\n",
    "\n",
    "    n, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Dataset construido: N={n} | T={T} | C={C} | clases={n_classes} | sujetos √∫nicos={len(np.unique(groups))}\")\n",
    "    return X, y, groups, ch_template\n",
    "\n",
    "# =========================\n",
    "# AUGMENTS (solo train)\n",
    "# =========================\n",
    "def do_time_jitter(x, max_ms=50, fs=160.0):\n",
    "    # x: (B,1,T,C) torch.float\n",
    "    max_shift = int(round(max_ms/1000.0 * fs))\n",
    "    if max_shift <= 0: return x\n",
    "    B,_,T,C = x.shape\n",
    "    shifts = torch.randint(low=-max_shift, high=max_shift+1, size=(B,), device=x.device)\n",
    "    out = torch.empty_like(x)\n",
    "    for i,s in enumerate(shifts):\n",
    "        if s==0: out[i] = x[i]; continue\n",
    "        if s>0:\n",
    "            out[i,:,s:,:] = x[i,:,:T-s,:]\n",
    "            out[i,:,:s,:] = 0\n",
    "        else:\n",
    "            s = -s\n",
    "            out[i,:,:T-s,:] = x[i,:,s:,:]\n",
    "            out[i,:,T-s:,:] = 0\n",
    "    return out\n",
    "\n",
    "def do_gaussian_noise(x, sigma=0.01):\n",
    "    if sigma<=0: return x\n",
    "    return x + sigma*torch.randn_like(x)\n",
    "\n",
    "def do_temporal_cutout_masked(x, y, classes_mask={2,3}, min_ms=20, max_ms=40, fs=160.0):\n",
    "    # Aplica cutout corto SOLO a clases en classes_mask (OF: BFISTS=2, BFEET=3)\n",
    "    B,_,T,C = x.shape\n",
    "    Lmin = int(round(min_ms/1000.0*fs))\n",
    "    Lmax = int(round(max_ms/1000.0*fs))\n",
    "    if Lmin<=0 or Lmax<=0 or Lmin>Lmax: return x\n",
    "    out = x.clone()\n",
    "    for i in range(B):\n",
    "        if int(y[i].item()) in classes_mask:\n",
    "            L = random.randint(Lmin, Lmax)\n",
    "            if L < T:\n",
    "                s = random.randint(0, T-L)\n",
    "                out[i,:,s:s+L,:] = 0\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# EEGNet (Lawhern) (B,1,T,C)\n",
    "# =========================\n",
    "class EEGNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Entrada: x de forma (B, 1, T, C)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_ch: int, n_classes: int,\n",
    "                 F1: int = 16, D: int = 2, kernel_t: int = 64, k_sep: int = 16,\n",
    "                 pool1_t: int = 4, pool2_t: int = 8,\n",
    "                 drop1_p: float = 0.25, drop2_p: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.n_ch = n_ch\n",
    "        self.n_classes = n_classes\n",
    "        self.F1 = F1\n",
    "        self.D = D\n",
    "        self.F2 = F1 * D\n",
    "        self.kernel_t = kernel_t\n",
    "        self.k_sep = k_sep\n",
    "        self.pool1_t = pool1_t\n",
    "        self.pool2_t = pool2_t\n",
    "\n",
    "        # Bloque 1: temporal\n",
    "        self.conv_temporal = nn.Conv2d(1, F1, kernel_size=(kernel_t, 1),\n",
    "                                       padding=(kernel_t // 2, 0), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(F1, momentum=0.99, eps=1e-3)\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "        # Bloque 2: depthwise (espacial)\n",
    "        self.conv_depthwise = nn.Conv2d(F1, self.F2, kernel_size=(1, n_ch),\n",
    "                                        groups=F1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.F2, momentum=0.99, eps=1e-3)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=(pool1_t, 1), stride=(pool1_t, 1))\n",
    "        self.drop1 = nn.Dropout(drop1_p)\n",
    "\n",
    "        # Bloque 3: separable temporal\n",
    "        self.conv_sep_depth = nn.Conv2d(self.F2, self.F2, kernel_size=(k_sep, 1),\n",
    "                                        groups=self.F2, padding=(k_sep // 2, 0), bias=False)\n",
    "        self.conv_sep_point = nn.Conv2d(self.F2, self.F2, kernel_size=(1, 1), bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.F2, momentum=0.99, eps=1e-3)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(pool2_t, 1), stride=(pool2_t, 1))\n",
    "        self.drop2 = nn.Dropout(drop2_p)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Cabeza din√°mica\n",
    "        self.fc = None\n",
    "        self.out = None\n",
    "        self._T_in = None\n",
    "\n",
    "    def _build_head(self, T_in: int, device: torch.device):\n",
    "        T1 = T_in // self.pool1_t\n",
    "        T2 = T1 // self.pool2_t\n",
    "        feat_dim = self.F2 * T2 * 1\n",
    "        self.fc = nn.Linear(feat_dim, 80, bias=True).to(device)\n",
    "        self.out = nn.Linear(80, self.n_classes, bias=True).to(device)\n",
    "        self._T_in = T_in\n",
    "\n",
    "    def ensure_head(self, T_in: int, device: torch.device):\n",
    "        if (self.fc is None) or (self.out is None) or (self._T_in != T_in):\n",
    "            self._build_head(T_in, device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, _, T, C = x.shape\n",
    "        self.ensure_head(T, x.device)\n",
    "\n",
    "        z = self.conv_temporal(x)\n",
    "        z = self.bn1(z); z = self.act(z)\n",
    "\n",
    "        z = self.conv_depthwise(z)   # (B, F2, T, 1)\n",
    "        z = self.bn2(z); z = self.act(z)\n",
    "        z = self.pool1(z)\n",
    "        z = self.drop1(z)\n",
    "\n",
    "        z = self.conv_sep_depth(z)\n",
    "        z = self.conv_sep_point(z)\n",
    "        z = self.bn3(z); z = self.act(z)\n",
    "        z = self.pool2(z)\n",
    "        z = self.drop2(z)\n",
    "\n",
    "        z = self.flatten(z)\n",
    "        z = self.fc(z); z = self.act(z)\n",
    "        z = self.out(z)\n",
    "        return z\n",
    "\n",
    "# =========================\n",
    "# TORCH DATASET\n",
    "# =========================\n",
    "class EEGTrials(Dataset):\n",
    "    def __init__(self, X, y, groups):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.g = groups.astype(np.int64)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        x = np.expand_dims(x, 0)                 # (1, T, C)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx])\n",
    "\n",
    "CLASS_NAMES_4C = ['Left', 'Right', 'Both Fists', 'Both Feet']\n",
    "\n",
    "# =========================\n",
    "# UTIL: MAX-NORM\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def apply_max_norm(model, max_value=2.0, p=2.0):\n",
    "    layers = []\n",
    "    if hasattr(model, 'conv_depthwise'): layers.append(model.conv_depthwise)\n",
    "    if hasattr(model, 'conv_sep_point'): layers.append(model.conv_sep_point)\n",
    "    if hasattr(model, 'fc'):             layers.append(model.fc)\n",
    "    if hasattr(model, 'out'):            layers.append(model.out)\n",
    "    for layer in layers:\n",
    "        if hasattr(layer, 'weight') and layer.weight is not None:\n",
    "            w = layer.weight.data\n",
    "            norms = w.view(w.size(0), -1).norm(p=p, dim=1, keepdim=True)\n",
    "            desired = torch.clamp(norms, max=max_value)\n",
    "            w.view(w.size(0), -1).mul_(desired / (1e-8 + norms))\n",
    "\n",
    "# =========================\n",
    "# ENTRENAMIENTO / EVALUACI√ìN\n",
    "# =========================\n",
    "def build_weighted_sampler(y, groups):\n",
    "    # pesos inversos por clase y por sujeto ‚Üí balance en cada batch\n",
    "    y = np.asarray(y); groups = np.asarray(groups)\n",
    "    class_counts = np.bincount(y, minlength=len(np.unique(y))).astype(float)\n",
    "    class_w = 1.0 / class_counts[y]\n",
    "    subj_vals, subj_counts = np.unique(groups, return_counts=True)\n",
    "    subj_map = {s:c for s,c in zip(subj_vals, subj_counts)}\n",
    "    subj_w = np.array([1.0/subj_map[g] for g in groups], dtype=float)\n",
    "    w = class_w * subj_w\n",
    "    w = w / w.mean()\n",
    "    w_t = torch.from_numpy(w).float()\n",
    "    sampler = WeightedRandomSampler(weights=w_t, num_samples=len(w_t), replacement=True)\n",
    "    return sampler\n",
    "\n",
    "def make_class_weighted_criterion(y_indices, n_classes):\n",
    "    from collections import Counter\n",
    "    cnt = Counter(y_indices.tolist())\n",
    "    total = sum(cnt.values())\n",
    "    weights = torch.tensor(\n",
    "        [total / max(1, cnt.get(c, 0)) for c in range(n_classes)],\n",
    "        dtype=torch.float32, device=DEVICE\n",
    "    )\n",
    "    weights = weights / weights.mean()\n",
    "    # CE con smoothing\n",
    "    return nn.CrossEntropyLoss(weight=weights, label_smoothing=0.05)\n",
    "\n",
    "def train_epoch(model, loader, opt, criterion, do_aug=True, fs=160.0, maxnorm=None):\n",
    "    model.train()\n",
    "    for xb, yb, _ in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "\n",
    "        # ---- AUGMENTS sin mixup ----\n",
    "        if do_aug:\n",
    "            xb = do_time_jitter(xb, max_ms=50, fs=fs)\n",
    "            xb = do_gaussian_noise(xb, sigma=0.01)\n",
    "            xb = do_temporal_cutout_masked(xb, yb, classes_mask={2,3}, min_ms=20, max_ms=40, fs=fs)\n",
    "\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if maxnorm is not None:\n",
    "            apply_max_norm(model, max_value=maxnorm, p=2.0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_with_preds(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for xb, yb, _ in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        pred = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "        y_pred.extend(pred)\n",
    "        y_true.extend(yb.numpy().tolist())\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    return y_true, y_pred, float(acc)\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes, title, fname):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.nan_to_num(cm_norm)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm_norm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "        plt.text(j, i, format(cm_norm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm_norm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label'); plt.xlabel('Predicted label')\n",
    "    plt.tight_layout(); plt.savefig(fname, dpi=150, bbox_inches='tight'); plt.close()\n",
    "\n",
    "def plot_training_curves(history, fname):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(history['train_acc'], label='train_acc')\n",
    "    plt.plot(history['val_acc'], label='val_acc')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
    "    plt.title('Training curve'); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(fname, dpi=150); plt.close()\n",
    "\n",
    "def print_report(y_true, y_pred, classes):\n",
    "    print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
    "\n",
    "# =========================\n",
    "# FINE-TUNING PROGRESIVO por sujeto (con validaci√≥n interna)\n",
    "# =========================\n",
    "def _param_groups(model, mode):\n",
    "    if mode == 'out':\n",
    "        train = list(model.out.parameters())\n",
    "    elif mode == 'head':\n",
    "        train = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "    elif mode == 'spatial+head':\n",
    "        train = (list(model.conv_depthwise.parameters()) +\n",
    "                 list(model.bn2.parameters()) +\n",
    "                 list(model.conv_sep_depth.parameters()) +\n",
    "                 list(model.conv_sep_point.parameters()) +\n",
    "                 list(model.bn3.parameters()) +\n",
    "                 list(model.fc.parameters()) +\n",
    "                 list(model.out.parameters()))\n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "    return train\n",
    "\n",
    "def _freeze_for_mode(model, mode):\n",
    "    for p in model.parameters(): p.requires_grad = False\n",
    "    if mode == 'out':\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'head':\n",
    "        for p in model.fc.parameters():  p.requires_grad = True\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'spatial+head':\n",
    "        for p in model.conv_depthwise.parameters(): p.requires_grad = True\n",
    "        for p in model.bn2.parameters():           p.requires_grad = True\n",
    "        for p in model.conv_sep_depth.parameters():p.requires_grad = True\n",
    "        for p in model.conv_sep_point.parameters():p.requires_grad = True\n",
    "        for p in model.bn3.parameters():           p.requires_grad = True\n",
    "        for p in model.fc.parameters():            p.requires_grad = True\n",
    "        for p in model.out.parameters():           p.requires_grad = True\n",
    "\n",
    "def _class_weights(y_np, n_classes):\n",
    "    counts = np.bincount(y_np, minlength=n_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    weights = counts.sum() / counts\n",
    "    weights = weights / weights.mean()\n",
    "    return torch.tensor(weights, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "def _train_one_mode(model, X_cal, y_cal, n_classes, mode,\n",
    "                    epochs=FT_EPOCHS, batch_size=32,\n",
    "                    head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                    l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=RANDOM_STATE)\n",
    "    (tr_idx, va_idx), = sss.split(X_cal, y_cal)\n",
    "    Xtr, ytr = X_cal[tr_idx], y_cal[tr_idx]\n",
    "    Xva, yva = X_cal[va_idx], y_cal[va_idx]\n",
    "\n",
    "    ds_tr = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xtr).float().unsqueeze(1),\n",
    "        torch.from_numpy(ytr).long()\n",
    "    )\n",
    "    ds_va = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xva).float().unsqueeze(1),\n",
    "        torch.from_numpy(yva).long()\n",
    "    )\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    _freeze_for_mode(model, mode)\n",
    "\n",
    "    if mode == 'spatial+head':\n",
    "        base_params = (list(model.conv_depthwise.parameters()) +\n",
    "                       list(model.bn2.parameters()) +\n",
    "                       list(model.conv_sep_depth.parameters()) +\n",
    "                       list(model.conv_sep_point.parameters()) +\n",
    "                       list(model.bn3.parameters()))\n",
    "        head_params = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "        train_params = base_params + head_params\n",
    "        opt = optim.Adam([\n",
    "            {\"params\": base_params, \"lr\": base_lr},\n",
    "            {\"params\": head_params, \"lr\": head_lr},\n",
    "        ])\n",
    "    else:\n",
    "        train_params = _param_groups(model, mode)\n",
    "        opt = optim.Adam(train_params, lr=head_lr)\n",
    "\n",
    "    ref = [p.detach().clone().to(p.device) for p in train_params]\n",
    "    class_w = _class_weights(ytr, n_classes)\n",
    "    crit = nn.CrossEntropyLoss(weight=class_w, label_smoothing=0.05)\n",
    "\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_val = float('inf'); bad = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            reg = 0.0\n",
    "            for p_cur, p_ref in zip(train_params, ref):\n",
    "                reg = reg + torch.sum((p_cur - p_ref)**2)\n",
    "            loss = loss + l2sp_lambda * reg\n",
    "            loss.backward(); opt.step()\n",
    "            apply_max_norm(model, max_value=2.0, p=2.0)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0; nval = 0\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                loss = crit(logits, yb)\n",
    "                val_loss += loss.item() * xb.size(0); nval += xb.size(0)\n",
    "            val_loss /= max(1, nval)\n",
    "\n",
    "        if val_loss + 1e-7 < best_val:\n",
    "            best_val = val_loss; bad = 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience: break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_numpy(model, X_np, device):\n",
    "    model.eval()\n",
    "    xb = torch.from_numpy(X_np).float().unsqueeze(1).to(device)\n",
    "    logits = model(xb)\n",
    "    return logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "def subject_cv_finetune_predict_progressive(model_global, Xs, ys, device,\n",
    "                                            n_splits=CALIB_CV_FOLDS, n_classes=4):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    y_true_full = np.empty_like(ys); y_pred_full = np.empty_like(ys)\n",
    "\n",
    "    for tr_idx, te_idx in skf.split(Xs, ys):\n",
    "        Xcal, ycal = Xs[tr_idx], ys[tr_idx]\n",
    "        Xho,  yho  = Xs[te_idx], ys[te_idx]\n",
    "\n",
    "        m_out = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_out, Xcal, ycal, n_classes, mode='out',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_out = predict_numpy(m_out, Xho, device)\n",
    "\n",
    "        m_head = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_head, Xcal, ycal, n_classes, mode='head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_head = predict_numpy(m_head, Xho, device)\n",
    "\n",
    "        m_sp = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_sp, Xcal, ycal, n_classes, mode='spatial+head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                        l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_sp = predict_numpy(m_sp, Xho, device)\n",
    "\n",
    "        best_idx = np.argmax([\n",
    "            (yhat_out == yho).mean(),\n",
    "            (yhat_head == yho).mean(),\n",
    "            (yhat_sp == yho).mean()\n",
    "        ])\n",
    "        yhat_best = [yhat_out, yhat_head, yhat_sp][best_idx]\n",
    "\n",
    "        y_true_full[te_idx] = yho; y_pred_full[te_idx] = yhat_best\n",
    "\n",
    "    return y_true_full, y_pred_full\n",
    "\n",
    "# =========================\n",
    "# FOLDS JSON helpers\n",
    "# =========================\n",
    "def save_group_folds_json_with_indices(subject_ids_str, groups_array, n_splits, out_json_path,\n",
    "                                       created_by=\"dose_experiment\", description=None):\n",
    "    out_json_path = Path(out_json_path)\n",
    "    unique_subjects_int = sorted(np.unique(groups_array).tolist())\n",
    "    subject_ids = [f\"S{sid:03d}\" for sid in unique_subjects_int]\n",
    "\n",
    "    if len(subject_ids) < n_splits:\n",
    "        raise ValueError(f\"n_splits={n_splits} mayor que n√∫mero de sujetos={len(subject_ids)}\")\n",
    "\n",
    "    groups = np.arange(len(subject_ids))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    folds = []\n",
    "    fold_i = 0\n",
    "    for train_idx_grp, test_idx_grp in gkf.split(groups, groups, groups):\n",
    "        fold_i += 1\n",
    "        train_sids = [subject_ids[int(i)] for i in train_idx_grp]\n",
    "        test_sids  = [subject_ids[int(i)] for i in test_idx_grp]\n",
    "\n",
    "        train_sids_int = [int(s[1:]) for s in train_sids]\n",
    "        test_sids_int  = [int(s[1:]) for s in test_sids]\n",
    "\n",
    "        tr_idx = np.where(np.isin(groups_array, train_sids_int))[0].tolist()\n",
    "        te_idx = np.where(np.isin(groups_array, test_sids_int))[0].tolist()\n",
    "\n",
    "        folds.append({\n",
    "            \"fold\": int(fold_i),\n",
    "            \"train\": train_sids,\n",
    "            \"test\": test_sids,\n",
    "            \"tr_idx\": tr_idx,\n",
    "            \"te_idx\": te_idx\n",
    "        })\n",
    "\n",
    "    payload = {\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"created_by\": created_by,\n",
    "        \"description\": description if description is not None else \"\",\n",
    "        \"n_splits\": int(n_splits),\n",
    "        \"n_subjects\": len(subject_ids),\n",
    "        \"subject_ids\": subject_ids,\n",
    "        \"folds\": folds\n",
    "    }\n",
    "\n",
    "    out_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Folds JSON con √≠ndices guardado ‚Üí {out_json_path}\")\n",
    "    return out_json_path\n",
    "\n",
    "def load_group_folds_json(path_json, expected_subject_ids=None, strict_check=True):\n",
    "    path_json = Path(path_json)\n",
    "    if not path_json.exists():\n",
    "        raise FileNotFoundError(f\"No existe {path_json}\")\n",
    "    with open(path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        payload = json.load(f)\n",
    "\n",
    "    subj_json = payload.get(\"subject_ids\", [])\n",
    "    if expected_subject_ids is not None:\n",
    "        expected = sorted(list(expected_subject_ids))\n",
    "        if subj_json != expected:\n",
    "            msg = (\"Los subject_ids del JSON no coinciden con expected_subject_ids.\\n\"\n",
    "                   f\"JSON has {len(subj_json)} subjects, expected {len(expected)}.\\n\"\n",
    "                   f\"First 10 JSON: {subj_json[:10]}\\nFirst 10 expected: {expected[:10]}\")\n",
    "            if strict_check:\n",
    "                raise ValueError(msg)\n",
    "            else:\n",
    "                print(\"WARNING: \" + msg)\n",
    "    return payload\n",
    "\n",
    "# =========================\n",
    "# EXPERIMENTO\n",
    "# =========================\n",
    "def run_experiment(save_folds_json=True, folds_json_path=FOLDS_DIR, folds_json_description=\"GroupKFold folds for comparison\"):\n",
    "    mne.set_log_level('WARNING')\n",
    "\n",
    "    subs = subjects_available()\n",
    "    print(f\"Sujetos elegibles: {len(subs)} ‚Üí {subs[:10]}{'...' if len(subs)>10 else ''}\")\n",
    "\n",
    "    X, y, groups, chs = build_dataset_all(subs, scenario=CLASS_SCENARIO, window_mode=WINDOW_MODE)\n",
    "    N, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Listo para entrenar: N={N} | T={T} | C={C} | clases={n_classes} | sujetos={len(np.unique(groups))}\")\n",
    "\n",
    "    ds = EEGTrials(X, y, groups)\n",
    "\n",
    "    # preparar JSON folds\n",
    "    if folds_json_path is None:\n",
    "        folds_json_path = Path(\"folds\") / f\"group_folds_{N_FOLDS}splits.json\"\n",
    "    else:\n",
    "        folds_json_path = Path(folds_json_path)\n",
    "    folds_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    unique_subs = sorted(np.unique(groups).tolist())\n",
    "    subject_ids_str = [f\"S{s:03d}\" for s in unique_subs]\n",
    "\n",
    "    if not folds_json_path.exists():\n",
    "        if not save_folds_json:\n",
    "            raise FileNotFoundError(f\"Folds JSON no encontrado en {folds_json_path} y save_folds_json=False.\")\n",
    "        save_group_folds_json_with_indices(subject_ids_str, groups, n_splits=N_FOLDS,\n",
    "                                           out_json_path=folds_json_path,\n",
    "                                           created_by=\"Joel_Clasificador\",\n",
    "                                           description=folds_json_description)\n",
    "\n",
    "    payload = load_group_folds_json(folds_json_path, expected_subject_ids=subject_ids_str, strict_check=False)\n",
    "    folds = payload[\"folds\"]\n",
    "\n",
    "    # bucle por folds\n",
    "    global_folds = []\n",
    "    ft_prog_folds = []\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "\n",
    "    for f in folds:\n",
    "        fold = f[\"fold\"]\n",
    "        tr_idx = np.asarray(f.get(\"tr_idx\", []), dtype=int)\n",
    "        te_idx = np.asarray(f.get(\"te_idx\", []), dtype=int)\n",
    "\n",
    "        if tr_idx.size == 0 or te_idx.size == 0:\n",
    "            print(f\"Advertencia: fold {fold} sin √≠ndices tr/te v√°lidos. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        # ===== Split de validaci√≥n por sujetos =====\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=GLOBAL_VAL_SPLIT, random_state=RANDOM_STATE)\n",
    "        tr_subj_idx, va_subj_idx = next(gss.split(tr_idx, groups[tr_idx], groups[tr_idx]))\n",
    "        tr_sub_idx = tr_idx[tr_subj_idx]\n",
    "        va_idx     = tr_idx[va_subj_idx]\n",
    "\n",
    "        # Sampler balanceado para train\n",
    "        sampler = build_weighted_sampler(y[tr_sub_idx], groups[tr_sub_idx])\n",
    "\n",
    "        tr_loader = DataLoader(Subset(ds, tr_sub_idx), batch_size=BATCH_SIZE, sampler=sampler, drop_last=False)\n",
    "        va_loader = DataLoader(Subset(ds, va_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "        te_loader = DataLoader(Subset(ds, te_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "        # ===== Modelo =====\n",
    "        model = EEGNet(n_ch=C, n_classes=n_classes,\n",
    "                       F1=16, D=2, kernel_t=64, k_sep=16,\n",
    "                       pool1_t=4, pool2_t=8, drop1_p=0.25, drop2_p=0.5).to(DEVICE)\n",
    "\n",
    "        # Opt y scheduler SGDR\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_INIT)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=SGDR_T0, T_mult=SGDR_Tmult)\n",
    "\n",
    "        # Criterio ponderado (en funci√≥n del split de train del fold)\n",
    "        criterion = make_class_weighted_criterion(torch.tensor(y[tr_sub_idx]), n_classes)\n",
    "\n",
    "        # M√©trica r√°pida\n",
    "        def _acc(loader):\n",
    "            return evaluate_with_preds(model, loader)[2]\n",
    "\n",
    "        # Historia para curvas\n",
    "        history = {'train_acc': [], 'val_acc': []}\n",
    "\n",
    "        # ===== Entrenamiento global =====\n",
    "        print(f\"\\n[Fold {fold}/{N_FOLDS}] Entrenando modelo global...\"\n",
    "              f\" (n_train={len(tr_sub_idx)} | n_val={len(va_idx)} | n_test={len(te_idx)})\")\n",
    "\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        best_val = -1.0\n",
    "        bad = 0\n",
    "\n",
    "        for epoch in range(1, EPOCHS_GLOBAL + 1):\n",
    "            train_epoch(model, tr_loader, opt, criterion,\n",
    "                        do_aug=True, fs=FS, maxnorm=2.0)\n",
    "            scheduler.step(epoch-1 + 1e-8)  # tick suave\n",
    "\n",
    "            # eval\n",
    "            tr_acc = _acc(tr_loader)\n",
    "            va_acc = _acc(va_loader)\n",
    "            history['train_acc'].append(tr_acc)\n",
    "            history['val_acc'].append(va_acc)\n",
    "\n",
    "            if (epoch % LOG_EVERY == 0) or epoch in (1, 10, 20, 50, 100):\n",
    "                cur_lr = opt.param_groups[0]['lr']\n",
    "                print(f\"  √âpoca {epoch:3d} | train_acc={tr_acc:.4f} | val_acc={va_acc:.4f} | LR={cur_lr:.5f}\")\n",
    "\n",
    "            if va_acc > best_val + 1e-4:\n",
    "                best_val = va_acc\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "                bad = 0\n",
    "            else:\n",
    "                bad += 1\n",
    "                if bad >= GLOBAL_PATIENCE:\n",
    "                    print(f\"  Early stopping en √©poca {epoch} (mejor val_acc={best_val:.4f})\")\n",
    "                    break\n",
    "\n",
    "        # guardar curva de entrenamiento\n",
    "        curve_path = f\"training_curve_fold{fold}.png\"\n",
    "        plot_training_curves(history, curve_path)\n",
    "        print(f\"‚Ü≥ Curva de entrenamiento guardada: {curve_path}\")\n",
    "\n",
    "        # cargar mejor estado antes de evaluar en test\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "        # ===== Evaluaci√≥n global (inter-sujeto puro) =====\n",
    "        y_true, y_pred, acc_global = evaluate_with_preds(model, te_loader)\n",
    "        global_folds.append(acc_global)\n",
    "        all_true.append(y_true); all_pred.append(y_pred)\n",
    "\n",
    "        print(f\"[Fold {fold}/{N_FOLDS}] Global acc={acc_global:.4f}\")\n",
    "        print_report(y_true, y_pred, CLASS_NAMES_4C)\n",
    "\n",
    "        # ---------- Fine-tuning PROGRESIVO por sujeto con 4-fold CV ----------\n",
    "        X_te, y_te, g_te = X[te_idx], y[te_idx], groups[te_idx]\n",
    "\n",
    "        y_true_ft_all, y_pred_ft_all = [], []\n",
    "        used_subjects = 0\n",
    "        for sid in np.unique(g_te):\n",
    "            idx = np.where(g_te == sid)[0]\n",
    "            Xs, ys = X_te[idx], y_te[idx]\n",
    "\n",
    "            if len(ys) < CALIB_CV_FOLDS or len(np.unique(ys)) < 2:\n",
    "                continue\n",
    "\n",
    "            y_true_subj, y_pred_subj = subject_cv_finetune_predict_progressive(\n",
    "                model, Xs, ys, DEVICE, n_splits=CALIB_CV_FOLDS, n_classes=n_classes\n",
    "            )\n",
    "            y_true_ft_all.append(y_true_subj)\n",
    "            y_pred_ft_all.append(y_pred_subj)\n",
    "            used_subjects += 1\n",
    "\n",
    "        if len(y_true_ft_all) > 0:\n",
    "            y_true_ft_all = np.concatenate(y_true_ft_all)\n",
    "            y_pred_ft_all = np.concatenate(y_pred_ft_all)\n",
    "            acc_ft = (y_true_ft_all == y_pred_ft_all).mean()\n",
    "            print(f\"  Fine-tuning PROGRESIVO (por sujeto, {CALIB_CV_FOLDS}-fold CV) acc={acc_ft:.4f} | sujetos={used_subjects}\")\n",
    "            print(f\"  Œî(FT-Global) = {acc_ft - acc_global:+.4f}\")\n",
    "        else:\n",
    "            acc_ft = np.nan\n",
    "            print(\"  Fine-tuning PROGRESIVO no ejecutado (sujeto(s) con muestras insuficientes).\")\n",
    "\n",
    "        ft_prog_folds.append(acc_ft)\n",
    "\n",
    "    # ---------- resultados finales ----------\n",
    "    if len(all_true) > 0:\n",
    "        all_true = np.concatenate(all_true)\n",
    "        all_pred = np.concatenate(all_pred)\n",
    "    else:\n",
    "        all_true = np.array([], dtype=int)\n",
    "        all_pred = np.array([], dtype=int)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTADOS FINALES\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Global folds:\", [f\"{a:.4f}\" for a in global_folds])\n",
    "    if len(global_folds) > 0:\n",
    "        print(f\"Global mean: {np.mean(global_folds):.4f}\")\n",
    "\n",
    "    print(\"Fine-tune PROGRESIVO folds:\", [(\"nan\" if (a is None or np.isnan(a)) else f\"{a:.4f}\") for a in ft_prog_folds])\n",
    "    if len(ft_prog_folds) > 0:\n",
    "        print(f\"Fine-tune PROGRESIVO mean: {np.nanmean(ft_prog_folds):.4f}\")\n",
    "        print(f\"Œî(FT-Global) mean: {np.nanmean(ft_prog_folds) - np.mean(global_folds):+.4f}\")\n",
    "\n",
    "    if all_true.size > 0:\n",
    "        plot_confusion(all_true, all_pred, CLASS_NAMES_4C,\n",
    "                       title=\"Confusion Matrix - Global Model (All Folds)\",\n",
    "                       fname=\"confusion_global_allfolds.png\")\n",
    "        print(\"\\n‚Ü≥ Matriz de confusi√≥n guardada: confusion_global_allfolds.png\")\n",
    "\n",
    "    return {\n",
    "        \"global_folds\": global_folds,\n",
    "        \"ft_prog_folds\": ft_prog_folds,\n",
    "        \"all_true\": all_true,\n",
    "        \"all_pred\": all_pred,\n",
    "        \"folds_json_path\": str(folds_json_path)\n",
    "    }\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üß† INICIANDO EXPERIMENTO CON EEGNet (capacidad‚Üë) + CE ponderada + smoothing + aug OF focalizado\")\n",
    "    print(f\"üîß Configuraci√≥n: {CLASS_SCENARIO}, {len(EXPECTED_8)} canales, {WINDOW_MODE}\")\n",
    "    print(f\"‚öôÔ∏è  FT: epochs={FT_EPOCHS}, base_lr={FT_BASE_LR}, head_lr={FT_HEAD_LR}, L2SP={FT_L2SP}, patience={FT_PATIENCE}, CV={CALIB_CV_FOLDS}\")\n",
    "    run_experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa5653c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Usando dispositivo: cuda\n",
      "üß† INICIANDO EXPERIMENTO CON EEGNet + FINE-TUNING PROGRESIVO (augments + SGDR + tweaks)\n",
      "üîß Configuraci√≥n: 4c, 8 canales, 6s\n",
      "‚öôÔ∏è  FT: epochs=30, base_lr=5e-05, head_lr=0.001, L2SP=0.0001, patience=5, CV=4\n",
      "Sujetos elegibles: 103 ‚Üí [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Construyendo dataset (RAW):   0%|          | 0/103 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Construyendo dataset (RAW): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:37<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset construido: N=8652 | T=960 | C=8 | clases=4 | sujetos √∫nicos=103\n",
      "Listo para entrenar: N=8652 | T=960 | C=8 | clases=4 | sujetos=103\n",
      "\n",
      "[Fold 1/5] Entrenando modelo global... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  √âpoca   1 | train_acc=0.4605 | val_acc=0.4304 | LR=0.01000\n",
      "  √âpoca   5 | train_acc=0.5138 | val_acc=0.4643 | LR=0.00250\n",
      "  √âpoca  10 | train_acc=0.5226 | val_acc=0.4744 | LR=0.00854\n",
      "  √âpoca  15 | train_acc=0.5514 | val_acc=0.4872 | LR=0.00250\n",
      "  √âpoca  20 | train_acc=0.5371 | val_acc=0.4698 | LR=0.00996\n",
      "  √âpoca  25 | train_acc=0.5229 | val_acc=0.4799 | LR=0.00854\n",
      "  Early stopping en √©poca 26 (mejor val_acc=0.4881)\n",
      "‚Ü≥ Curva de entrenamiento guardada: training_curve_fold1.png\n",
      "[Fold 1/5] Global acc=0.4575\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5516    0.4603    0.5019       441\n",
      "       Right     0.5642    0.4785    0.5178       441\n",
      "  Both Fists     0.3540    0.5057    0.4164       441\n",
      "   Both Feet     0.4337    0.3855    0.4082       441\n",
      "\n",
      "    accuracy                         0.4575      1764\n",
      "   macro avg     0.4759    0.4575    0.4611      1764\n",
      "weighted avg     0.4759    0.4575    0.4611      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5130 | sujetos=21\n",
      "  Œî(FT-Global) = +0.0556\n",
      "\n",
      "[Fold 2/5] Entrenando modelo global... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  √âpoca   1 | train_acc=0.4155 | val_acc=0.4029 | LR=0.01000\n",
      "  √âpoca   5 | train_acc=0.4707 | val_acc=0.4359 | LR=0.00250\n",
      "  √âpoca  10 | train_acc=0.5117 | val_acc=0.4560 | LR=0.00854\n",
      "  √âpoca  15 | train_acc=0.5133 | val_acc=0.4588 | LR=0.00250\n",
      "  √âpoca  20 | train_acc=0.5312 | val_acc=0.4734 | LR=0.00996\n",
      "  √âpoca  25 | train_acc=0.5345 | val_acc=0.4762 | LR=0.00854\n",
      "  √âpoca  30 | train_acc=0.5561 | val_acc=0.4808 | LR=0.00565\n",
      "  √âpoca  35 | train_acc=0.5866 | val_acc=0.4881 | LR=0.00250\n",
      "  √âpoca  40 | train_acc=0.5714 | val_acc=0.4597 | LR=0.00038\n",
      "  Early stopping en √©poca 43 (mejor val_acc=0.4908)\n",
      "‚Ü≥ Curva de entrenamiento guardada: training_curve_fold2.png\n",
      "[Fold 2/5] Global acc=0.5351\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.6128    0.6281    0.6204       441\n",
      "       Right     0.5934    0.5692    0.5810       441\n",
      "  Both Fists     0.4307    0.5215    0.4718       441\n",
      "   Both Feet     0.5239    0.4218    0.4673       441\n",
      "\n",
      "    accuracy                         0.5351      1764\n",
      "   macro avg     0.5402    0.5351    0.5351      1764\n",
      "weighted avg     0.5402    0.5351    0.5351      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.6173 | sujetos=21\n",
      "  Œî(FT-Global) = +0.0822\n",
      "\n",
      "[Fold 3/5] Entrenando modelo global... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  √âpoca   1 | train_acc=0.4432 | val_acc=0.4332 | LR=0.01000\n",
      "  √âpoca   5 | train_acc=0.5350 | val_acc=0.5165 | LR=0.00250\n",
      "  √âpoca  10 | train_acc=0.5147 | val_acc=0.5375 | LR=0.00854\n",
      "  √âpoca  15 | train_acc=0.5336 | val_acc=0.5385 | LR=0.00250\n",
      "  √âpoca  20 | train_acc=0.5450 | val_acc=0.5375 | LR=0.00996\n",
      "  √âpoca  25 | train_acc=0.5212 | val_acc=0.5128 | LR=0.00854\n",
      "  Early stopping en √©poca 29 (mejor val_acc=0.5421)\n",
      "‚Ü≥ Curva de entrenamiento guardada: training_curve_fold3.png\n",
      "[Fold 3/5] Global acc=0.4711\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5245    0.5102    0.5172       441\n",
      "       Right     0.5631    0.5261    0.5440       441\n",
      "  Both Fists     0.3818    0.4762    0.4238       441\n",
      "   Both Feet     0.4397    0.3719    0.4029       441\n",
      "\n",
      "    accuracy                         0.4711      1764\n",
      "   macro avg     0.4773    0.4711    0.4720      1764\n",
      "weighted avg     0.4773    0.4711    0.4720      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5323 | sujetos=21\n",
      "  Œî(FT-Global) = +0.0612\n",
      "\n",
      "[Fold 4/5] Entrenando modelo global... (n_train=5880 | n_val=1092 | n_test=1680)\n",
      "  √âpoca   1 | train_acc=0.4437 | val_acc=0.4313 | LR=0.01000\n",
      "  √âpoca   5 | train_acc=0.5162 | val_acc=0.4853 | LR=0.00250\n",
      "  √âpoca  10 | train_acc=0.5167 | val_acc=0.4789 | LR=0.00854\n",
      "  √âpoca  15 | train_acc=0.5304 | val_acc=0.4991 | LR=0.00250\n",
      "  √âpoca  20 | train_acc=0.5204 | val_acc=0.4927 | LR=0.00996\n",
      "  √âpoca  25 | train_acc=0.5369 | val_acc=0.4863 | LR=0.00854\n",
      "  Early stopping en √©poca 26 (mejor val_acc=0.5192)\n",
      "‚Ü≥ Curva de entrenamiento guardada: training_curve_fold4.png\n",
      "[Fold 4/5] Global acc=0.5107\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5940    0.5643    0.5788       420\n",
      "       Right     0.5891    0.4643    0.5193       420\n",
      "  Both Fists     0.4256    0.5786    0.4904       420\n",
      "   Both Feet     0.4828    0.4357    0.4581       420\n",
      "\n",
      "    accuracy                         0.5107      1680\n",
      "   macro avg     0.5229    0.5107    0.5116      1680\n",
      "weighted avg     0.5229    0.5107    0.5116      1680\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5774 | sujetos=20\n",
      "  Œî(FT-Global) = +0.0667\n",
      "\n",
      "[Fold 5/5] Entrenando modelo global... (n_train=5880 | n_val=1092 | n_test=1680)\n",
      "  √âpoca   1 | train_acc=0.4199 | val_acc=0.4167 | LR=0.01000\n",
      "  √âpoca   5 | train_acc=0.4830 | val_acc=0.4643 | LR=0.00250\n",
      "  √âpoca  10 | train_acc=0.5071 | val_acc=0.4588 | LR=0.00854\n",
      "  √âpoca  15 | train_acc=0.5031 | val_acc=0.4835 | LR=0.00250\n",
      "  √âpoca  20 | train_acc=0.5122 | val_acc=0.4643 | LR=0.00996\n",
      "  Early stopping en √©poca 24 (mejor val_acc=0.4954)\n",
      "‚Ü≥ Curva de entrenamiento guardada: training_curve_fold5.png\n",
      "[Fold 5/5] Global acc=0.5190\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.6141    0.5190    0.5626       420\n",
      "       Right     0.5718    0.4643    0.5125       420\n",
      "  Both Fists     0.4377    0.5524    0.4884       420\n",
      "   Both Feet     0.5000    0.5405    0.5195       420\n",
      "\n",
      "    accuracy                         0.5190      1680\n",
      "   macro avg     0.5309    0.5190    0.5207      1680\n",
      "weighted avg     0.5309    0.5190    0.5207      1680\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5863 | sujetos=20\n",
      "  Œî(FT-Global) = +0.0673\n",
      "\n",
      "============================================================\n",
      "RESULTADOS FINALES\n",
      "============================================================\n",
      "Global folds: ['0.4575', '0.5351', '0.4711', '0.5107', '0.5190']\n",
      "Global mean: 0.4987\n",
      "Fine-tune PROGRESIVO folds: ['0.5130', '0.6173', '0.5323', '0.5774', '0.5863']\n",
      "Fine-tune PROGRESIVO mean: 0.5653\n",
      "Œî(FT-Global) mean: +0.0666\n",
      "\n",
      "‚Ü≥ Matriz de confusi√≥n guardada: confusion_global_allfolds.png\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# EEGNet + protocolo global + fine-tuning progresivo por sujeto (augments + SGDR + tweaks)\n",
    "# Cambios clave:\n",
    "# 1) Capacidad ‚Üë: F1=24, fc=128\n",
    "# 2) Loss ponderada por clase (+20% a Both Fists) con soporte para mixup (WeightedSoftCrossEntropy)\n",
    "# 3) Cutout focalizado (30‚Äì60 ms) solo para clases OF (2,3)\n",
    "# 4) SGDR con T0=6, Tmult=2\n",
    "# 5) TTA (n=5, jitter ¬±25 ms) en evaluaci√≥n\n",
    "\n",
    "import os, re, math, random, json, itertools, copy\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold, StratifiedShuffleSplit, GroupShuffleSplit\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# CONFIGURACI√ìN GENERAL\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "FOLDS_DIR = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dispositivo y semilla\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE); np.random.seed(RANDOM_STATE); random.seed(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "# Escenario y ventana\n",
    "CLASS_SCENARIO = '4c'\n",
    "WINDOW_MODE = '6s'   # mantienes 6s\n",
    "FS = 160.0\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Entrenamiento global\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS_GLOBAL = 100\n",
    "LR_INIT = 1e-2\n",
    "SGDR_T0 = 6      # ‚Üì ciclos cortos al inicio\n",
    "SGDR_Tmult = 2\n",
    "\n",
    "# Validaci√≥n/ES global\n",
    "GLOBAL_VAL_SPLIT = 0.15\n",
    "GLOBAL_PATIENCE  = 10\n",
    "LOG_EVERY        = 5\n",
    "\n",
    "# Fine-tuning por sujeto (protocolo robusto)\n",
    "CALIB_CV_FOLDS = 4\n",
    "FT_EPOCHS = 30\n",
    "FT_BASE_LR = 5e-5\n",
    "FT_HEAD_LR = 1e-3\n",
    "FT_L2SP = 1e-4\n",
    "FT_PATIENCE = 5\n",
    "FT_VAL_RATIO = 0.2\n",
    "\n",
    "# Normalizaci√≥n por √©poca canal-a-canal (z-score)\n",
    "NORM_EPOCH_ZSCORE = True\n",
    "\n",
    "# Sujetos excluidos\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "\n",
    "# Runs\n",
    "MI_RUNS_LR = [4, 8, 12]\n",
    "MI_RUNS_OF = [6, 10, 14]\n",
    "BASELINE_RUNS_EO = [1]\n",
    "\n",
    "# Canales (8 con FCz)\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','FCz']\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES DE CANALES\n",
    "# =========================\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = re.sub(r'([A-Za-z])0([0-9])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])Z$', r'\\1z', s)\n",
    "    s = s.replace('fp', 'Fp').replace('FP', 'Fp')\n",
    "    s = ''.join(ch.upper() if ch != 'z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {}\n",
    "    for ch in raw.ch_names:\n",
    "        lab = normalize_label(ch)\n",
    "        lab = lab[:-1] + 'z' if lab.endswith('Z') else lab\n",
    "        lab = re.sub(r'([A-Z])Z$', r'\\1z', lab)\n",
    "        mapping[ch] = lab\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired_channels=EXPECTED_8):\n",
    "    missing = [ch for ch in desired_channels if ch not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"Warning: faltan canales {missing} en archivo {getattr(raw,'filenames', [''])[0]}\")\n",
    "        return None\n",
    "    raw.reorder_channels([ch for ch in raw.ch_names if ch in desired_channels] +\n",
    "                         [ch for ch in raw.ch_names if ch not in desired_channels])\n",
    "    raw.pick_channels(desired_channels, ordered=True)\n",
    "    return raw\n",
    "\n",
    "# =========================\n",
    "# LECTURA DE EDF y EVENTOS\n",
    "# =========================\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(run_id:int):\n",
    "    if run_id in MI_RUNS_LR: return 'LR'\n",
    "    if run_id in MI_RUNS_OF: return 'OF'\n",
    "    if run_id in BASELINE_RUNS_EO: return 'EO'\n",
    "    return None\n",
    "\n",
    "# --- Notch adaptativo: lee CSV si existe (subject, run, snr50_db, snr60_db)\n",
    "_SNR_TABLE = None\n",
    "def _load_snr_table():\n",
    "    global _SNR_TABLE\n",
    "    if _SNR_TABLE is not None:\n",
    "        return _SNR_TABLE\n",
    "    csv_path = PROJ / 'reports' / 'psd_mains' / 'psd_mains_summary.csv'\n",
    "    if csv_path.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            _SNR_TABLE = df\n",
    "        except Exception as e:\n",
    "            print(f\"[SNR] No se pudo leer {csv_path}: {e}\")\n",
    "            _SNR_TABLE = None\n",
    "    return _SNR_TABLE\n",
    "\n",
    "def _decide_notch(subject, run, th_db=10.0):\n",
    "    df = _load_snr_table()\n",
    "    if df is None:  # default\n",
    "        return 60.0\n",
    "    row = df[(df['subject']==subject) & (df['run']==run)]\n",
    "    if row.empty:\n",
    "        return 60.0\n",
    "    snr50 = float(row['snr50_db'].iloc[0]); snr60 = float(row['snr60_db'].iloc[0])\n",
    "    if snr60 >= th_db and snr60 >= snr50: return 60.0\n",
    "    if snr50 >= th_db and snr50 >  snr60: return 50.0\n",
    "    return None  # no notch si no sobresale\n",
    "\n",
    "def read_raw_edf(path: Path):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        mont = mne.channels.make_standard_montage('standard_1020')\n",
    "        raw.set_montage(mont, on_missing='ignore')\n",
    "    except Exception:\n",
    "        pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad=\"auto\")\n",
    "    raw = ensure_channels_order(raw, EXPECTED_8)\n",
    "    if raw is None:\n",
    "        return None\n",
    "\n",
    "    # Notch adaptativo (por SNR); si no hay CSV, por defecto 60 Hz\n",
    "    sid, rid = parse_subject_run(path)\n",
    "    notch = _decide_notch(sid, rid)\n",
    "    if notch is not None:\n",
    "        raw.notch_filter(freqs=[float(notch)], picks='eeg', method='spectrum_fit', phase='zero')\n",
    "\n",
    "    return raw\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    if raw.annotations is None or len(raw.annotations) == 0:\n",
    "        return []\n",
    "    def _norm(s): return str(s).strip().upper().replace(' ', '')\n",
    "    res = []\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = _norm(desc)\n",
    "        if tag in ('T1','T2'):\n",
    "            res.append((float(onset), tag))\n",
    "    res.sort()\n",
    "    dedup = []\n",
    "    last_t1 = last_t2 = -1e9\n",
    "    for t, tag in res:\n",
    "        if tag == 'T1':\n",
    "            if (t - last_t1) >= 0.5: dedup.append((t, tag)); last_t1 = t\n",
    "        else:\n",
    "            if (t - last_t2) >= 0.5: dedup.append((t, tag)); last_t2 = t\n",
    "    return dedup\n",
    "\n",
    "# =========================\n",
    "# CONSTRUCCI√ìN DE DATASETS\n",
    "# =========================\n",
    "def subjects_available():\n",
    "    subs = []\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try: sid = int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir / f\"S{sid:03d}R{r:02d}.edf\").exists() for r in (MI_RUNS_LR + MI_RUNS_OF))\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "def extract_trials_from_run(edf_path: Path, scenario: str, window_mode: str):\n",
    "    subj, run = parse_subject_run(edf_path)\n",
    "    kind = run_kind(run)\n",
    "    if kind not in ('LR','OF','EO'):\n",
    "        return ([], [])\n",
    "\n",
    "    raw = read_raw_edf(edf_path)\n",
    "    if raw is None:\n",
    "        return ([], [])\n",
    "\n",
    "    data = raw.get_data()\n",
    "    fs = raw.info['sfreq']\n",
    "    assert abs(fs - FS) < 1e-6\n",
    "\n",
    "    out = []\n",
    "\n",
    "    if kind in ('LR','OF'):\n",
    "        events = collect_events_T1T2(raw)\n",
    "        if window_mode == '3s':\n",
    "            rel_start, rel_end = 0.0, 3.0\n",
    "        else:\n",
    "            rel_start, rel_end = -1.0, 5.0\n",
    "\n",
    "        for onset_sec, tag in events:\n",
    "            if kind == 'LR':\n",
    "                if tag == 'T1': label = 'L'\n",
    "                elif tag == 'T2': label = 'R'\n",
    "                else: continue\n",
    "            else:\n",
    "                if tag == 'T1': label = 'BFISTS'\n",
    "                elif tag == 'T2': label = 'BFEET'\n",
    "                else: continue\n",
    "\n",
    "            if scenario == '2c' and label not in ('L','R'): continue\n",
    "            if scenario == '3c' and label not in ('L','R','BFISTS'): continue\n",
    "            if scenario == '4c' and label not in ('L','R','BFISTS','BFEET'): continue\n",
    "\n",
    "            s = int(round((raw.first_time + onset_sec + rel_start) * fs))\n",
    "            e = int(round((raw.first_time + onset_sec + rel_end) * fs))\n",
    "            if s < 0 or e > data.shape[1]:\n",
    "                continue\n",
    "\n",
    "            seg = data[:, s:e].T.astype(np.float32)\n",
    "            if NORM_EPOCH_ZSCORE:\n",
    "                seg = (seg - seg.mean(axis=0, keepdims=True)) / (seg.std(axis=0, keepdims=True) + 1e-6)\n",
    "\n",
    "            if label == 'L':       y = 0\n",
    "            elif label == 'R':     y = 1\n",
    "            elif label == 'BFISTS':y = 2\n",
    "            elif label == 'BFEET': y = 3\n",
    "            else: continue\n",
    "\n",
    "            out.append((seg, y, subj))\n",
    "\n",
    "    elif kind == 'EO':\n",
    "        return ([], raw.ch_names)\n",
    "\n",
    "    return out, raw.ch_names\n",
    "\n",
    "def build_dataset_all(subjects, scenario='4c', window_mode='3s'):\n",
    "    X, y, groups = [], [], []\n",
    "    ch_template = None\n",
    "\n",
    "    for s in tqdm(subjects, desc=\"Construyendo dataset (RAW)\"):\n",
    "        sdir = DATA_RAW / f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "\n",
    "        trials_L, trials_R, trials_FISTS, trials_FEET = [], [], [], []\n",
    "\n",
    "        for r in MI_RUNS_LR:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 0: trials_L.append(seg)\n",
    "                elif lab == 1: trials_R.append(seg)\n",
    "\n",
    "        for r in MI_RUNS_OF:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 2: trials_FISTS.append(seg)\n",
    "                elif lab == 3: trials_FEET.append(seg)\n",
    "\n",
    "        need_per_class = 21\n",
    "        def pick(trials, n, rng):\n",
    "            if len(trials) < n:\n",
    "                idx = rng.choice(len(trials), size=n, replace=True)\n",
    "                return [trials[i] for i in idx]\n",
    "            rng.shuffle(trials)\n",
    "            return trials[:n]\n",
    "\n",
    "        rng = check_random_state(RANDOM_STATE + s)\n",
    "        if len(trials_L)==0 or len(trials_R)==0 or len(trials_FISTS)==0 or len(trials_FEET)==0:\n",
    "            continue\n",
    "\n",
    "        Lp  = pick(trials_L,     need_per_class, rng)\n",
    "        Rp  = pick(trials_R,     need_per_class, rng)\n",
    "        FIp = pick(trials_FISTS, need_per_class, rng)\n",
    "        FEp = pick(trials_FEET,  need_per_class, rng)\n",
    "\n",
    "        pack = [(Lp, 0), (Rp, 1), (FIp, 2), (FEp, 3)]\n",
    "        for segs, lab in pack:\n",
    "            for seg in segs:\n",
    "                X.append(seg); y.append(lab); groups.append(s)\n",
    "\n",
    "    X = np.stack(X, axis=0)\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "    groups = np.asarray(groups, dtype=np.int64)\n",
    "\n",
    "    n, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Dataset construido: N={n} | T={T} | C={C} | clases={n_classes} | sujetos √∫nicos={len(np.unique(groups))}\")\n",
    "    return X, y, groups, ch_template\n",
    "\n",
    "# =========================\n",
    "# AUGMENTS (solo train)\n",
    "# =========================\n",
    "def do_time_jitter(x, max_ms=50, fs=160.0):\n",
    "    # x: (B,1,T,C) torch.float\n",
    "    max_shift = int(round(max_ms/1000.0 * fs))\n",
    "    if max_shift <= 0: return x\n",
    "    B,_,T,C = x.shape\n",
    "    shifts = torch.randint(low=-max_shift, high=max_shift+1, size=(B,), device=x.device)\n",
    "    out = torch.empty_like(x)\n",
    "    for i,s in enumerate(shifts):\n",
    "        if s==0: out[i] = x[i]; continue\n",
    "        if s>0:\n",
    "            out[i,:,s:,:] = x[i,:,:T-s,:]\n",
    "            out[i,:,:s,:] = 0\n",
    "        else:\n",
    "            s = -s\n",
    "            out[i,:,:T-s,:] = x[i,:,s:,:]\n",
    "            out[i,:,T-s:,:] = 0\n",
    "    return out\n",
    "\n",
    "def do_gaussian_noise(x, sigma=0.01):\n",
    "    if sigma<=0: return x\n",
    "    return x + sigma*torch.randn_like(x)\n",
    "\n",
    "def do_temporal_cutout(x, min_ms=100, max_ms=150, fs=160.0):\n",
    "    B,_,T,C = x.shape\n",
    "    Lmin = int(round(min_ms/1000.0*fs))\n",
    "    Lmax = int(round(max_ms/1000.0*fs))\n",
    "    if Lmin<=0 or Lmax<=0 or Lmin>Lmax: return x\n",
    "    out = x.clone()\n",
    "    for i in range(B):\n",
    "        L = random.randint(Lmin, Lmax)\n",
    "        if L>=T: continue\n",
    "        s = random.randint(0, T-L)\n",
    "        out[i,:,s:s+L,:] = 0\n",
    "    return out\n",
    "\n",
    "def do_temporal_cutout_masked(x, y, classes_mask={2,3}, min_ms=30, max_ms=60, fs=160.0):\n",
    "    # Aplica cutout solo si y pertenece a classes_mask\n",
    "    B,_,T,C = x.shape\n",
    "    if B == 0: return x\n",
    "    Lmin = int(round(min_ms/1000.0*fs))\n",
    "    Lmax = int(round(max_ms/1000.0*fs))\n",
    "    if Lmin<=0 or Lmax<=0 or Lmin>Lmax: return x\n",
    "    out = x.clone()\n",
    "    y_np = y.detach().cpu().numpy()\n",
    "    for i in range(B):\n",
    "        if int(y_np[i]) not in classes_mask: \n",
    "            continue\n",
    "        L = random.randint(Lmin, Lmax)\n",
    "        if L>=T: continue\n",
    "        s = random.randint(0, T-L)\n",
    "        out[i,:,s:s+L,:] = 0\n",
    "    return out\n",
    "\n",
    "def mixup_batch(x, y, n_classes, alpha=0.2):\n",
    "    if alpha<=0:\n",
    "        y_onehot = torch.nn.functional.one_hot(y, num_classes=n_classes).float()\n",
    "        return x, y_onehot, 1.0\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    perm = torch.randperm(x.size(0), device=x.device)\n",
    "    x_mix = lam*x + (1-lam)*x[perm]\n",
    "    y_a = torch.nn.functional.one_hot(y, num_classes=n_classes).float()\n",
    "    y_b = y_a[perm]\n",
    "    y_mix = lam*y_a + (1-lam)*y_b\n",
    "    return x_mix, y_mix, lam\n",
    "\n",
    "class WeightedSoftCrossEntropy(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-entropy suave (targets en probas, p.ej. mixup) con pesos por clase.\n",
    "    \"\"\"\n",
    "    def __init__(self, class_weights=None, label_smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.register_buffer('w', None if class_weights is None else class_weights.clone().float())\n",
    "        self.ls = float(label_smoothing)\n",
    "\n",
    "    def forward(self, logits, target_probs):\n",
    "        # label smoothing: mezcla target_probs con uniforme\n",
    "        if self.ls > 0:\n",
    "            K = logits.size(1)\n",
    "            target_probs = (1-self.ls)*target_probs + self.ls*(1.0/K)\n",
    "        logp = torch.log_softmax(logits, dim=1)  # (B,C)\n",
    "        loss_per_class = -(target_probs * logp)  # (B,C)\n",
    "        if self.w is not None:\n",
    "            loss_per_class = loss_per_class * self.w.unsqueeze(0)  # pesar por clase\n",
    "        loss = loss_per_class.sum(dim=1).mean()\n",
    "        return loss\n",
    "\n",
    "# =========================\n",
    "# EEGNet (Lawhern et al., 2018) adaptado a (B,1,T,C) + ChannelDropout\n",
    "# =========================\n",
    "class ChannelDropout(nn.Module):\n",
    "    def __init__(self, p=0.1):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    def forward(self, x):\n",
    "        if not self.training or self.p<=0: return x\n",
    "        B,_,T,C = x.shape\n",
    "        mask = (torch.rand(B,1,1,C, device=x.device) > self.p).float()\n",
    "        return x * mask\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Entrada: x de forma (B, 1, T, C)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_ch: int, n_classes: int,\n",
    "                 F1: int = 24, D: int = 2, kernel_t: int = 64, k_sep: int = 16,\n",
    "                 pool1_t: int = 4, pool2_t: int = 6,\n",
    "                 drop1_p: float = 0.35, drop2_p: float = 0.6,\n",
    "                 chdrop_p: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.n_ch = n_ch\n",
    "        self.n_classes = n_classes\n",
    "        self.F1 = F1\n",
    "        self.D = D\n",
    "        self.F2 = F1 * D\n",
    "        self.kernel_t = kernel_t\n",
    "        self.k_sep = k_sep\n",
    "        self.pool1_t = pool1_t\n",
    "        self.pool2_t = pool2_t\n",
    "\n",
    "        self.chdrop = ChannelDropout(p=chdrop_p)\n",
    "\n",
    "        # Bloque 1: temporal\n",
    "        self.conv_temporal = nn.Conv2d(1, F1, kernel_size=(kernel_t, 1),\n",
    "                                       padding=(kernel_t // 2, 0), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(F1, momentum=0.99, eps=1e-3)\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "        # Bloque 2: depthwise (espacial)\n",
    "        self.conv_depthwise = nn.Conv2d(F1, self.F2, kernel_size=(1, n_ch),\n",
    "                                        groups=F1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.F2, momentum=0.99, eps=1e-3)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=(pool1_t, 1), stride=(pool1_t, 1))\n",
    "        self.drop1 = nn.Dropout(drop1_p)\n",
    "\n",
    "        # Bloque 3: separable temporal\n",
    "        self.conv_sep_depth = nn.Conv2d(self.F2, self.F2, kernel_size=(k_sep, 1),\n",
    "                                        groups=self.F2, padding=(k_sep // 2, 0), bias=False)\n",
    "        self.conv_sep_point = nn.Conv2d(self.F2, self.F2, kernel_size=(1, 1), bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.F2, momentum=0.99, eps=1e-3)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(pool2_t, 1), stride=(pool2_t, 1))\n",
    "        self.drop2 = nn.Dropout(drop2_p)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Cabeza din√°mica\n",
    "        self.fc = None\n",
    "        self.out = None\n",
    "        self._T_in = None\n",
    "\n",
    "    def _build_head(self, T_in: int, device: torch.device):\n",
    "        T1 = T_in // self.pool1_t\n",
    "        T2 = T1 // self.pool2_t\n",
    "        feat_dim = self.F2 * T2 * 1\n",
    "        self.fc  = nn.Linear(feat_dim, 128, bias=True).to(device)\n",
    "        self.out = nn.Linear(128, self.n_classes, bias=True).to(device)\n",
    "        self._T_in = T_in\n",
    "\n",
    "    def ensure_head(self, T_in: int, device: torch.device):\n",
    "        if (self.fc is None) or (self.out is None) or (self._T_in != T_in):\n",
    "            self._build_head(T_in, device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, _, T, C = x.shape\n",
    "        self.ensure_head(T, x.device)\n",
    "\n",
    "        x = self.chdrop(x)  # ChannelDropout\n",
    "\n",
    "        z = self.conv_temporal(x)\n",
    "        z = self.bn1(z); z = self.act(z)\n",
    "\n",
    "        z = self.conv_depthwise(z)   # (B, F2, T, 1)\n",
    "        z = self.bn2(z); z = self.act(z)\n",
    "        z = self.pool1(z)\n",
    "        z = self.drop1(z)\n",
    "\n",
    "        z = self.conv_sep_depth(z)\n",
    "        z = self.conv_sep_point(z)\n",
    "        z = self.bn3(z); z = self.act(z)\n",
    "        z = self.pool2(z)\n",
    "        z = self.drop2(z)\n",
    "\n",
    "        z = self.flatten(z)\n",
    "        z = self.fc(z); z = self.act(z)\n",
    "        z = self.out(z)\n",
    "        return z\n",
    "\n",
    "# =========================\n",
    "# TORCH DATASET\n",
    "# =========================\n",
    "class EEGTrials(Dataset):\n",
    "    def __init__(self, X, y, groups):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.g = groups.astype(np.int64)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        x = np.expand_dims(x, 0)                 # (1, T, C)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx])\n",
    "\n",
    "CLASS_NAMES_4C = ['Left', 'Right', 'Both Fists', 'Both Feet']\n",
    "\n",
    "# =========================\n",
    "# UTIL: MAX-NORM\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def apply_max_norm(model, max_value=2.0, p=2.0):\n",
    "    layers = []\n",
    "    if hasattr(model, 'conv_depthwise'): layers.append(model.conv_depthwise)\n",
    "    if hasattr(model, 'conv_sep_point'): layers.append(model.conv_sep_point)\n",
    "    if hasattr(model, 'fc'):             layers.append(model.fc)\n",
    "    if hasattr(model, 'out'):            layers.append(model.out)\n",
    "    for layer in layers:\n",
    "        if hasattr(layer, 'weight') and layer.weight is not None:\n",
    "            w = layer.weight.data\n",
    "            norms = w.view(w.size(0), -1).norm(p=p, dim=1, keepdim=True)\n",
    "            desired = torch.clamp(norms, max=max_value)\n",
    "            w.view(w.size(0), -1).mul_(desired / (1e-8 + norms))\n",
    "\n",
    "# =========================\n",
    "# ENTRENAMIENTO / EVALUACI√ìN\n",
    "# =========================\n",
    "def build_weighted_sampler(y, groups):\n",
    "    # pesos inversos por clase y por sujeto ‚Üí promueve balance en cada batch\n",
    "    y = np.asarray(y); groups = np.asarray(groups)\n",
    "    class_counts = np.bincount(y, minlength=len(np.unique(y))).astype(float)\n",
    "    class_w = 1.0 / class_counts[y]\n",
    "    subj_vals, subj_counts = np.unique(groups, return_counts=True)\n",
    "    subj_map = {s:c for s,c in zip(subj_vals, subj_counts)}\n",
    "    subj_w = np.array([1.0/subj_map[g] for g in groups], dtype=float)\n",
    "    w = class_w * subj_w\n",
    "    w = w / w.mean()\n",
    "    w_t = torch.from_numpy(w).float()\n",
    "    sampler = WeightedRandomSampler(weights=w_t, num_samples=len(w_t), replacement=True)\n",
    "    return sampler\n",
    "\n",
    "def make_class_weight_tensor(y_indices, n_classes, boost_bfists=1.20):\n",
    "    # pesos inversos por clase, normalizados; clase 2 (*Both Fists*) con boost\n",
    "    counts = np.bincount(y_indices, minlength=n_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    w = counts.sum() / counts\n",
    "    w = w / w.mean()\n",
    "    w[2] *= boost_bfists\n",
    "    w = w / w.mean()\n",
    "    return torch.tensor(w, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "def train_epoch(model, loader, opt, criterion_soft, n_classes,\n",
    "                do_aug=True, fs=160.0, maxnorm=None):\n",
    "    model.train()\n",
    "    for xb, yb, _ in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "\n",
    "        # ---- AUGMENTS (orden: jitter ‚Üí noise ‚Üí cutout focalizado ‚Üí mixup) ----\n",
    "        if do_aug:\n",
    "            xb = do_time_jitter(xb, max_ms=50, fs=fs)\n",
    "            xb = do_gaussian_noise(xb, sigma=0.01)\n",
    "            xb = do_temporal_cutout_masked(xb, yb, classes_mask={2,3}, min_ms=30, max_ms=60, fs=fs)\n",
    "            xb, yt, _ = mixup_batch(xb, yb, n_classes=n_classes, alpha=0.2)\n",
    "            logits = model(xb)\n",
    "            loss = criterion_soft(logits, yt)\n",
    "        else:\n",
    "            logits = model(xb)\n",
    "            # yb como √≠ndices ‚Üí convertir a one-hot para criterio suave\n",
    "            yt = torch.nn.functional.one_hot(yb, num_classes=n_classes).float()\n",
    "            loss = criterion_soft(logits, yt)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if maxnorm is not None:\n",
    "            apply_max_norm(model, max_value=maxnorm, p=2.0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _predict_tta(model, xb, n=5, fs=160.0):\n",
    "    outs = []\n",
    "    for _ in range(n):\n",
    "        xj = do_time_jitter(xb, max_ms=25, fs=fs)\n",
    "        outs.append(model(xj))\n",
    "    return torch.stack(outs, dim=0).mean(dim=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_with_preds(model, loader, use_tta=True, tta_n=5):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for xb, yb, _ in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        if use_tta:\n",
    "            logits = _predict_tta(model, xb, n=tta_n, fs=FS)\n",
    "        else:\n",
    "            logits = model(xb)\n",
    "        pred = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "        y_pred.extend(pred)\n",
    "        y_true.extend(yb.numpy().tolist())\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    return y_true, y_pred, float(acc)\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes, title, fname):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.nan_to_num(cm_norm)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm_norm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "        plt.text(j, i, format(cm_norm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm_norm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label'); plt.xlabel('Predicted label')\n",
    "    plt.tight_layout(); plt.savefig(fname, dpi=150, bbox_inches='tight'); plt.close()\n",
    "\n",
    "def plot_training_curves(history, fname):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(history['train_acc'], label='train_acc')\n",
    "    plt.plot(history['val_acc'], label='val_acc')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
    "    plt.title('Training curve'); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(fname, dpi=150); plt.close()\n",
    "\n",
    "def print_report(y_true, y_pred, classes):\n",
    "    print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
    "\n",
    "# =========================\n",
    "# FINE-TUNING PROGRESIVO por sujeto (con validaci√≥n interna)\n",
    "# =========================\n",
    "def _param_groups(model, mode):\n",
    "    if mode == 'out':\n",
    "        train = list(model.out.parameters())\n",
    "    elif mode == 'head':\n",
    "        train = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "    elif mode == 'spatial+head':\n",
    "        train = (list(model.conv_depthwise.parameters()) +\n",
    "                 list(model.bn2.parameters()) +\n",
    "                 list(model.conv_sep_depth.parameters()) +\n",
    "                 list(model.conv_sep_point.parameters()) +\n",
    "                 list(model.bn3.parameters()) +\n",
    "                 list(model.fc.parameters()) +\n",
    "                 list(model.out.parameters()))\n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "    return train\n",
    "\n",
    "def _freeze_for_mode(model, mode):\n",
    "    for p in model.parameters(): p.requires_grad = False\n",
    "    if mode == 'out':\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'head':\n",
    "        for p in model.fc.parameters():  p.requires_grad = True\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'spatial+head':\n",
    "        for p in model.conv_depthwise.parameters(): p.requires_grad = True\n",
    "        for p in model.bn2.parameters():           p.requires_grad = True\n",
    "        for p in model.conv_sep_depth.parameters():p.requires_grad = True\n",
    "        for p in model.conv_sep_point.parameters():p.requires_grad = True\n",
    "        for p in model.bn3.parameters():           p.requires_grad = True\n",
    "        for p in model.fc.parameters():            p.requires_grad = True\n",
    "        for p in model.out.parameters():           p.requires_grad = True\n",
    "\n",
    "def _class_weights(y_np, n_classes):\n",
    "    counts = np.bincount(y_np, minlength=n_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    weights = counts.sum() / counts\n",
    "    weights = weights / weights.mean()\n",
    "    return torch.tensor(weights, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "def _train_one_mode(model, X_cal, y_cal, n_classes, mode,\n",
    "                    epochs=FT_EPOCHS, batch_size=32,\n",
    "                    head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                    l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=RANDOM_STATE)\n",
    "    (tr_idx, va_idx), = sss.split(X_cal, y_cal)\n",
    "    Xtr, ytr = X_cal[tr_idx], y_cal[tr_idx]\n",
    "    Xva, yva = X_cal[va_idx], y_cal[va_idx]\n",
    "\n",
    "    ds_tr = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xtr).float().unsqueeze(1),\n",
    "        torch.from_numpy(ytr).long()\n",
    "    )\n",
    "    ds_va = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xva).float().unsqueeze(1),\n",
    "        torch.from_numpy(yva).long()\n",
    "    )\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    _freeze_for_mode(model, mode)\n",
    "\n",
    "    if mode == 'spatial+head':\n",
    "        base_params = (list(model.conv_depthwise.parameters()) +\n",
    "                       list(model.bn2.parameters()) +\n",
    "                       list(model.conv_sep_depth.parameters()) +\n",
    "                       list(model.conv_sep_point.parameters()) +\n",
    "                       list(model.bn3.parameters()))\n",
    "        head_params = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "        train_params = base_params + head_params\n",
    "        opt = optim.Adam([\n",
    "            {\"params\": base_params, \"lr\": base_lr},\n",
    "            {\"params\": head_params, \"lr\": head_lr},\n",
    "        ])\n",
    "    else:\n",
    "        train_params = _param_groups(model, mode)\n",
    "        opt = optim.Adam(train_params, lr=head_lr)\n",
    "\n",
    "    ref = [p.detach().clone().to(p.device) for p in train_params]\n",
    "    class_w = _class_weights(ytr, n_classes)\n",
    "    crit = nn.CrossEntropyLoss(weight=class_w)\n",
    "\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_val = float('inf'); bad = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            reg = 0.0\n",
    "            for p_cur, p_ref in zip(train_params, ref):\n",
    "                reg = reg + torch.sum((p_cur - p_ref)**2)\n",
    "            loss = loss + l2sp_lambda * reg\n",
    "            loss.backward(); opt.step()\n",
    "            apply_max_norm(model, max_value=2.0, p=2.0)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0; nval = 0\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                loss = crit(logits, yb)\n",
    "                val_loss += loss.item() * xb.size(0); nval += xb.size(0)\n",
    "            val_loss /= max(1, nval)\n",
    "\n",
    "        if val_loss + 1e-7 < best_val:\n",
    "            best_val = val_loss; bad = 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience: break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_numpy(model, X_np, device):\n",
    "    model.eval()\n",
    "    xb = torch.from_numpy(X_np).float().unsqueeze(1).to(device)\n",
    "    logits = model(xb)\n",
    "    return logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "def subject_cv_finetune_predict_progressive(model_global, Xs, ys, device,\n",
    "                                            n_splits=CALIB_CV_FOLDS, n_classes=4):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    y_true_full = np.empty_like(ys); y_pred_full = np.empty_like(ys)\n",
    "\n",
    "    for tr_idx, te_idx in skf.split(Xs, ys):\n",
    "        Xcal, ycal = Xs[tr_idx], ys[tr_idx]\n",
    "        Xho,  yho  = Xs[te_idx], ys[te_idx]\n",
    "\n",
    "        m_out = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_out, Xcal, ycal, n_classes, mode='out',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_out = predict_numpy(m_out, Xho, device); acc_out = (yhat_out == yho).mean()\n",
    "\n",
    "        m_head = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_head, Xcal, ycal, n_classes, mode='head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_head = predict_numpy(m_head, Xho, device); acc_head = (yhat_head == yho).mean()\n",
    "\n",
    "        m_sp = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_sp, Xcal, ycal, n_classes, mode='spatial+head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                        l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_sp = predict_numpy(m_sp, Xho, device); acc_sp = (yhat_sp == yho).mean()\n",
    "\n",
    "        best_idx = np.argmax([acc_out, acc_head, acc_sp])\n",
    "        yhat_best = [yhat_out, yhat_head, yhat_sp][best_idx]\n",
    "\n",
    "        y_true_full[te_idx] = yho; y_pred_full[te_idx] = yhat_best\n",
    "\n",
    "    return y_true_full, y_pred_full\n",
    "\n",
    "# =========================\n",
    "# FOLDS JSON helpers\n",
    "# =========================\n",
    "def save_group_folds_json_with_indices(subject_ids_str, groups_array, n_splits, out_json_path,\n",
    "                                       created_by=\"dose_experiment\", description=None):\n",
    "    out_json_path = Path(out_json_path)\n",
    "    unique_subjects_int = sorted(np.unique(groups_array).tolist())\n",
    "    subject_ids = [f\"S{sid:03d}\" for sid in unique_subjects_int]\n",
    "\n",
    "    if len(subject_ids) < n_splits:\n",
    "        raise ValueError(f\"n_splits={n_splits} mayor que n√∫mero de sujetos={len(subject_ids)}\")\n",
    "\n",
    "    groups = np.arange(len(subject_ids))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    folds = []\n",
    "    fold_i = 0\n",
    "    for train_idx_grp, test_idx_grp in gkf.split(groups, groups, groups):\n",
    "        fold_i += 1\n",
    "        train_sids = [subject_ids[int(i)] for i in train_idx_grp]\n",
    "        test_sids  = [subject_ids[int(i)] for i in test_idx_grp]\n",
    "\n",
    "        train_sids_int = [int(s[1:]) for s in train_sids]\n",
    "        test_sids_int  = [int(s[1:]) for s in test_sids]\n",
    "\n",
    "        tr_idx = np.where(np.isin(groups_array, train_sids_int))[0].tolist()\n",
    "        te_idx = np.where(np.isin(groups_array, test_sids_int))[0].tolist()\n",
    "\n",
    "        folds.append({\n",
    "            \"fold\": int(fold_i),\n",
    "            \"train\": train_sids,\n",
    "            \"test\": test_sids,\n",
    "            \"tr_idx\": tr_idx,\n",
    "            \"te_idx\": te_idx\n",
    "        })\n",
    "\n",
    "    payload = {\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"created_by\": created_by,\n",
    "        \"description\": description if description is not None else \"\",\n",
    "        \"n_splits\": int(n_splits),\n",
    "        \"n_subjects\": len(subject_ids),\n",
    "        \"subject_ids\": subject_ids,\n",
    "        \"folds\": folds\n",
    "    }\n",
    "\n",
    "    out_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Folds JSON con √≠ndices guardado ‚Üí {out_json_path}\")\n",
    "    return out_json_path\n",
    "\n",
    "def load_group_folds_json(path_json, expected_subject_ids=None, strict_check=True):\n",
    "    path_json = Path(path_json)\n",
    "    if not path_json.exists():\n",
    "        raise FileNotFoundError(f\"No existe {path_json}\")\n",
    "    with open(path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        payload = json.load(f)\n",
    "\n",
    "    subj_json = payload.get(\"subject_ids\", [])\n",
    "    if expected_subject_ids is not None:\n",
    "        expected = sorted(list(expected_subject_ids))\n",
    "        if subj_json != expected:\n",
    "            msg = (\"Los subject_ids del JSON no coinciden con expected_subject_ids.\\n\"\n",
    "                   f\"JSON has {len(subj_json)} subjects, expected {len(expected)}.\\n\"\n",
    "                   f\"First 10 JSON: {subj_json[:10]}\\nFirst 10 expected: {expected[:10]}\")\n",
    "            if strict_check:\n",
    "                raise ValueError(msg)\n",
    "            else:\n",
    "                print(\"WARNING: \" + msg)\n",
    "    return payload\n",
    "\n",
    "# =========================\n",
    "# EXPERIMENTO\n",
    "# =========================\n",
    "def run_experiment(save_folds_json=True, folds_json_path=FOLDS_DIR, folds_json_description=\"GroupKFold folds for comparison\"):\n",
    "    mne.set_log_level('WARNING')\n",
    "\n",
    "    subs = subjects_available()\n",
    "    print(f\"Sujetos elegibles: {len(subs)} ‚Üí {subs[:10]}{'...' if len(subs)>10 else ''}\")\n",
    "\n",
    "    X, y, groups, chs = build_dataset_all(subs, scenario=CLASS_SCENARIO, window_mode=WINDOW_MODE)\n",
    "    N, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Listo para entrenar: N={N} | T={T} | C={C} | clases={n_classes} | sujetos={len(np.unique(groups))}\")\n",
    "\n",
    "    ds = EEGTrials(X, y, groups)\n",
    "\n",
    "    # preparar JSON folds\n",
    "    if folds_json_path is None:\n",
    "        folds_json_path = Path(\"folds\") / f\"group_folds_{N_FOLDS}splits.json\"\n",
    "    else:\n",
    "        folds_json_path = Path(folds_json_path)\n",
    "    folds_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    unique_subs = sorted(np.unique(groups).tolist())\n",
    "    subject_ids_str = [f\"S{s:03d}\" for s in unique_subs]\n",
    "\n",
    "    if not folds_json_path.exists():\n",
    "        if not save_folds_json:\n",
    "            raise FileNotFoundError(f\"Folds JSON no encontrado en {folds_json_path} y save_folds_json=False.\")\n",
    "        save_group_folds_json_with_indices(subject_ids_str, groups, n_splits=N_FOLDS,\n",
    "                                           out_json_path=folds_json_path,\n",
    "                                           created_by=\"Joel_Clasificador\",\n",
    "                                           description=folds_json_description)\n",
    "\n",
    "    payload = load_group_folds_json(folds_json_path, expected_subject_ids=subject_ids_str, strict_check=False)\n",
    "    folds = payload[\"folds\"]\n",
    "\n",
    "    # bucle por folds\n",
    "    global_folds = []\n",
    "    ft_prog_folds = []\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "\n",
    "    for f in folds:\n",
    "        fold = f[\"fold\"]\n",
    "        tr_idx = np.asarray(f.get(\"tr_idx\", []), dtype=int)\n",
    "        te_idx = np.asarray(f.get(\"te_idx\", []), dtype=int)\n",
    "\n",
    "        if tr_idx.size == 0 or te_idx.size == 0:\n",
    "            print(f\"Advertencia: fold {fold} sin √≠ndices tr/te v√°lidos. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        # ===== Split de validaci√≥n por sujetos =====\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=GLOBAL_VAL_SPLIT, random_state=RANDOM_STATE)\n",
    "        tr_subj_idx, va_subj_idx = next(gss.split(tr_idx, groups[tr_idx], groups[tr_idx]))\n",
    "        tr_sub_idx = tr_idx[tr_subj_idx]\n",
    "        va_idx     = tr_idx[va_subj_idx]\n",
    "\n",
    "        # Sampler balanceado para train\n",
    "        sampler = build_weighted_sampler(y[tr_sub_idx], groups[tr_sub_idx])\n",
    "\n",
    "        tr_loader = DataLoader(Subset(ds, tr_sub_idx), batch_size=BATCH_SIZE, sampler=sampler, drop_last=False)\n",
    "        va_loader = DataLoader(Subset(ds, va_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "        te_loader = DataLoader(Subset(ds, te_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "        # ===== EEGNet =====\n",
    "        model = EEGNet(n_ch=C, n_classes=n_classes,\n",
    "                       F1=24, D=2, kernel_t=64, k_sep=16,\n",
    "                       pool1_t=4, pool2_t=6, drop1_p=0.35, drop2_p=0.6,\n",
    "                       chdrop_p=0.1).to(DEVICE)\n",
    "\n",
    "        # Opt y scheduler SGDR\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_INIT)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=SGDR_T0, T_mult=SGDR_Tmult)\n",
    "\n",
    "        # M√©trica r√°pida\n",
    "        def _acc(loader):\n",
    "            return evaluate_with_preds(model, loader, use_tta=True, tta_n=5)[2]\n",
    "\n",
    "        # Historia para curvas\n",
    "        history = {'train_acc': [], 'val_acc': []}\n",
    "\n",
    "        # ---- Criterio suave con ponderaci√≥n por clase (+20% BFISTS) ----\n",
    "        class_weights = make_class_weight_tensor(y[tr_sub_idx], n_classes, boost_bfists=1.20)\n",
    "        criterion_soft = WeightedSoftCrossEntropy(class_weights, label_smoothing=0.05)\n",
    "\n",
    "        # ===== Entrenamiento global =====\n",
    "        print(f\"\\n[Fold {fold}/{N_FOLDS}] Entrenando modelo global...\"\n",
    "              f\" (n_train={len(tr_sub_idx)} | n_val={len(va_idx)} | n_test={len(te_idx)})\")\n",
    "\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        best_val = -1.0\n",
    "        bad = 0\n",
    "\n",
    "        for epoch in range(1, EPOCHS_GLOBAL + 1):\n",
    "            train_epoch(model, tr_loader, opt, criterion_soft, n_classes=n_classes,\n",
    "                        do_aug=True, fs=FS, maxnorm=2.0)\n",
    "            scheduler.step(epoch-1 + 1e-8)  # tick suave\n",
    "\n",
    "            # eval\n",
    "            tr_acc = _acc(tr_loader)\n",
    "            va_acc = _acc(va_loader)\n",
    "            history['train_acc'].append(tr_acc)\n",
    "            history['val_acc'].append(va_acc)\n",
    "\n",
    "            if (epoch % LOG_EVERY == 0) or epoch in (1, 10, 20, 50, 100):\n",
    "                cur_lr = opt.param_groups[0]['lr']\n",
    "                print(f\"  √âpoca {epoch:3d} | train_acc={tr_acc:.4f} | val_acc={va_acc:.4f} | LR={cur_lr:.5f}\")\n",
    "\n",
    "            if va_acc > best_val + 1e-4:\n",
    "                best_val = va_acc\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "                bad = 0\n",
    "            else:\n",
    "                bad += 1\n",
    "                if bad >= GLOBAL_PATIENCE:\n",
    "                    print(f\"  Early stopping en √©poca {epoch} (mejor val_acc={best_val:.4f})\")\n",
    "                    break\n",
    "\n",
    "        # guardar curva de entrenamiento\n",
    "        curve_path = f\"training_curve_fold{fold}.png\"\n",
    "        plot_training_curves(history, curve_path)\n",
    "        print(f\"‚Ü≥ Curva de entrenamiento guardada: {curve_path}\")\n",
    "\n",
    "        # cargar mejor estado antes de evaluar en test\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "        # ===== Evaluaci√≥n global (inter-sujeto puro) =====\n",
    "        y_true, y_pred, acc_global = evaluate_with_preds(model, te_loader, use_tta=True, tta_n=5)\n",
    "        global_folds.append(acc_global)\n",
    "        all_true.append(y_true); all_pred.append(y_pred)\n",
    "\n",
    "        print(f\"[Fold {fold}/{N_FOLDS}] Global acc={acc_global:.4f}\")\n",
    "        print_report(y_true, y_pred, CLASS_NAMES_4C)\n",
    "\n",
    "        # ---------- Fine-tuning PROGRESIVO por sujeto con 4-fold CV ----------\n",
    "        X_te, y_te, g_te = X[te_idx], y[te_idx], groups[te_idx]\n",
    "\n",
    "        y_true_ft_all, y_pred_ft_all = [], []\n",
    "        used_subjects = 0\n",
    "        for sid in np.unique(g_te):\n",
    "            idx = np.where(g_te == sid)[0]\n",
    "            Xs, ys = X_te[idx], y_te[idx]\n",
    "\n",
    "            if len(ys) < CALIB_CV_FOLDS or len(np.unique(ys)) < 2:\n",
    "                continue\n",
    "\n",
    "            y_true_subj, y_pred_subj = subject_cv_finetune_predict_progressive(\n",
    "                model, Xs, ys, DEVICE, n_splits=CALIB_CV_FOLDS, n_classes=n_classes\n",
    "            )\n",
    "            y_true_ft_all.append(y_true_subj)\n",
    "            y_pred_ft_all.append(y_pred_subj)\n",
    "            used_subjects += 1\n",
    "\n",
    "        if len(y_true_ft_all) > 0:\n",
    "            y_true_ft_all = np.concatenate(y_true_ft_all)\n",
    "            y_pred_ft_all = np.concatenate(y_pred_ft_all)\n",
    "            acc_ft = (y_true_ft_all == y_pred_ft_all).mean()\n",
    "            print(f\"  Fine-tuning PROGRESIVO (por sujeto, {CALIB_CV_FOLDS}-fold CV) acc={acc_ft:.4f} | sujetos={used_subjects}\")\n",
    "            print(f\"  Œî(FT-Global) = {acc_ft - acc_global:+.4f}\")\n",
    "        else:\n",
    "            acc_ft = np.nan\n",
    "            print(\"  Fine-tuning PROGRESIVO no ejecutado (sujeto(s) con muestras insuficientes).\")\n",
    "\n",
    "        ft_prog_folds.append(acc_ft)\n",
    "\n",
    "    # ---------- resultados finales ----------\n",
    "    if len(all_true) > 0:\n",
    "        all_true = np.concatenate(all_true)\n",
    "        all_pred = np.concatenate(all_pred)\n",
    "    else:\n",
    "        all_true = np.array([], dtype=int)\n",
    "        all_pred = np.array([], dtype=int)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTADOS FINALES\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Global folds:\", [f\"{a:.4f}\" for a in global_folds])\n",
    "    if len(global_folds) > 0:\n",
    "        print(f\"Global mean: {np.mean(global_folds):.4f}\")\n",
    "\n",
    "    print(\"Fine-tune PROGRESIVO folds:\", [(\"nan\" if (a is None or np.isnan(a)) else f\"{a:.4f}\") for a in ft_prog_folds])\n",
    "    if len(ft_prog_folds) > 0:\n",
    "        print(f\"Fine-tune PROGRESIVO mean: {np.nanmean(ft_prog_folds):.4f}\")\n",
    "        print(f\"Œî(FT-Global) mean: {np.nanmean(ft_prog_folds) - np.mean(global_folds):+.4f}\")\n",
    "\n",
    "    if all_true.size > 0:\n",
    "        plot_confusion(all_true, all_pred, CLASS_NAMES_4C,\n",
    "                       title=\"Confusion Matrix - Global Model (All Folds)\",\n",
    "                       fname=\"confusion_global_allfolds.png\")\n",
    "        print(\"\\n‚Ü≥ Matriz de confusi√≥n guardada: confusion_global_allfolds.png\")\n",
    "\n",
    "    return {\n",
    "        \"global_folds\": global_folds,\n",
    "        \"ft_prog_folds\": ft_prog_folds,\n",
    "        \"all_true\": all_true,\n",
    "        \"all_pred\": all_pred,\n",
    "        \"folds_json_path\": str(folds_json_path)\n",
    "    }\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üß† INICIANDO EXPERIMENTO CON EEGNet + FINE-TUNING PROGRESIVO (augments + SGDR + tweaks)\")\n",
    "    print(f\"üîß Configuraci√≥n: {CLASS_SCENARIO}, {len(EXPECTED_8)} canales, {WINDOW_MODE}\")\n",
    "    print(f\"‚öôÔ∏è  FT: epochs={FT_EPOCHS}, base_lr={FT_BASE_LR}, head_lr={FT_HEAD_LR}, L2SP={FT_L2SP}, patience={FT_PATIENCE}, CV={CALIB_CV_FOLDS}\")\n",
    "    run_experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5670f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Usando dispositivo: cuda\n",
      "üß† INICIANDO EXPERIMENTO CON EEGNet + FINE-TUNING PROGRESIVO (augments + SGDR + tweaks++)\n",
      "üîß Configuraci√≥n: 4c, 8 canales, 6s\n",
      "‚öôÔ∏è  FT: epochs=30, base_lr=5e-05, head_lr=0.001, L2SP=0.0001, patience=5, CV=4\n",
      "Sujetos elegibles: 103 ‚Üí [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Construyendo dataset (RAW): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:37<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset construido: N=8652 | T=960 | C=8 | clases=4 | sujetos √∫nicos=103\n",
      "Listo para entrenar: N=8652 | T=960 | C=8 | clases=4 | sujetos=103\n",
      "\n",
      "[Fold 1/5] Entrenando modelo global... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  √âpoca   1 | train_acc=0.4441 | val_acc=0.4176 | LR=0.01000\n",
      "  √âpoca   5 | train_acc=0.5048 | val_acc=0.4597 | LR=0.00257\n",
      "  √âpoca  10 | train_acc=0.5097 | val_acc=0.4579 | LR=0.00855\n",
      "  √âpoca  15 | train_acc=0.5409 | val_acc=0.4771 | LR=0.00257\n",
      "  √âpoca  20 | train_acc=0.5228 | val_acc=0.4789 | LR=0.00996\n",
      "  √âpoca  25 | train_acc=0.5223 | val_acc=0.4679 | LR=0.00855\n",
      "  Early stopping en √©poca 26 (mejor val_acc=0.4799)\n",
      "‚Ü≥ Curva de entrenamiento guardada: training_curve_fold1.png\n",
      "[Fold 1/5] Global acc=0.4467\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5895    0.4331    0.4993       441\n",
      "       Right     0.5511    0.4036    0.4660       441\n",
      "  Both Fists     0.3551    0.4444    0.3948       441\n",
      "   Both Feet     0.3947    0.5057    0.4433       441\n",
      "\n",
      "    accuracy                         0.4467      1764\n",
      "   macro avg     0.4726    0.4467    0.4509      1764\n",
      "weighted avg     0.4726    0.4467    0.4509      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5057 | sujetos=21\n",
      "  Œî(FT-Global) = +0.0590\n",
      "\n",
      "[Fold 2/5] Entrenando modelo global... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  √âpoca   1 | train_acc=0.4163 | val_acc=0.4029 | LR=0.01000\n",
      "  √âpoca   5 | train_acc=0.4634 | val_acc=0.4332 | LR=0.00257\n",
      "  √âpoca  10 | train_acc=0.4941 | val_acc=0.4670 | LR=0.00855\n",
      "  √âpoca  15 | train_acc=0.5050 | val_acc=0.4652 | LR=0.00257\n",
      "  √âpoca  20 | train_acc=0.5169 | val_acc=0.4679 | LR=0.00996\n",
      "  √âpoca  25 | train_acc=0.4824 | val_acc=0.4460 | LR=0.00855\n",
      "  √âpoca  30 | train_acc=0.5280 | val_acc=0.4890 | LR=0.00570\n",
      "  √âpoca  35 | train_acc=0.5257 | val_acc=0.4670 | LR=0.00257\n",
      "  √âpoca  40 | train_acc=0.5473 | val_acc=0.4670 | LR=0.00048\n",
      "  √âpoca  45 | train_acc=0.5342 | val_acc=0.4789 | LR=0.00996\n",
      "  √âpoca  50 | train_acc=0.5416 | val_acc=0.4725 | LR=0.00949\n",
      "  Early stopping en √©poca 54 (mejor val_acc=0.4936)\n",
      "‚Ü≥ Curva de entrenamiento guardada: training_curve_fold2.png\n",
      "[Fold 2/5] Global acc=0.5374\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.6574    0.5873    0.6204       441\n",
      "       Right     0.6000    0.5782    0.5889       441\n",
      "  Both Fists     0.4272    0.5193    0.4688       441\n",
      "   Both Feet     0.5012    0.4649    0.4824       441\n",
      "\n",
      "    accuracy                         0.5374      1764\n",
      "   macro avg     0.5465    0.5374    0.5401      1764\n",
      "weighted avg     0.5465    0.5374    0.5401      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.6043 | sujetos=21\n",
      "  Œî(FT-Global) = +0.0669\n",
      "\n",
      "[Fold 3/5] Entrenando modelo global... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  √âpoca   1 | train_acc=0.4187 | val_acc=0.3956 | LR=0.01000\n",
      "  √âpoca   5 | train_acc=0.4934 | val_acc=0.4927 | LR=0.00257\n",
      "  √âpoca  10 | train_acc=0.5288 | val_acc=0.4982 | LR=0.00855\n",
      "  √âpoca  15 | train_acc=0.5223 | val_acc=0.5201 | LR=0.00257\n",
      "  √âpoca  20 | train_acc=0.5141 | val_acc=0.4991 | LR=0.00996\n",
      "  √âpoca  25 | train_acc=0.5418 | val_acc=0.5027 | LR=0.00855\n",
      "  √âpoca  30 | train_acc=0.5397 | val_acc=0.5201 | LR=0.00570\n",
      "  √âpoca  35 | train_acc=0.5345 | val_acc=0.5238 | LR=0.00257\n",
      "  √âpoca  40 | train_acc=0.5588 | val_acc=0.5110 | LR=0.00048\n",
      "  √âpoca  45 | train_acc=0.5399 | val_acc=0.5165 | LR=0.00996\n",
      "  √âpoca  50 | train_acc=0.5430 | val_acc=0.5293 | LR=0.00949\n",
      "  √âpoca  55 | train_acc=0.5400 | val_acc=0.5110 | LR=0.00855\n",
      "  Early stopping en √©poca 56 (mejor val_acc=0.5366)\n",
      "‚Ü≥ Curva de entrenamiento guardada: training_curve_fold3.png\n",
      "[Fold 3/5] Global acc=0.4683\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5211    0.4762    0.4976       441\n",
      "       Right     0.5988    0.4603    0.5205       441\n",
      "  Both Fists     0.3802    0.4535    0.4137       441\n",
      "   Both Feet     0.4294    0.4830    0.4546       441\n",
      "\n",
      "    accuracy                         0.4683      1764\n",
      "   macro avg     0.4824    0.4683    0.4716      1764\n",
      "weighted avg     0.4824    0.4683    0.4716      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5306 | sujetos=21\n",
      "  Œî(FT-Global) = +0.0624\n",
      "\n",
      "[Fold 4/5] Entrenando modelo global... (n_train=5880 | n_val=1092 | n_test=1680)\n",
      "  √âpoca   1 | train_acc=0.4049 | val_acc=0.3892 | LR=0.01000\n",
      "  √âpoca   5 | train_acc=0.4762 | val_acc=0.4460 | LR=0.00257\n",
      "  √âpoca  10 | train_acc=0.5145 | val_acc=0.4753 | LR=0.00855\n",
      "  √âpoca  15 | train_acc=0.5141 | val_acc=0.4844 | LR=0.00257\n",
      "  √âpoca  20 | train_acc=0.5323 | val_acc=0.4808 | LR=0.00996\n",
      "  √âpoca  25 | train_acc=0.5327 | val_acc=0.4927 | LR=0.00855\n",
      "  Early stopping en √©poca 29 (mejor val_acc=0.5092)\n",
      "‚Ü≥ Curva de entrenamiento guardada: training_curve_fold4.png\n",
      "[Fold 4/5] Global acc=0.4815\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5987    0.4262    0.4979       420\n",
      "       Right     0.6028    0.4119    0.4894       420\n",
      "  Both Fists     0.3981    0.4976    0.4423       420\n",
      "   Both Feet     0.4359    0.5905    0.5015       420\n",
      "\n",
      "    accuracy                         0.4815      1680\n",
      "   macro avg     0.5088    0.4815    0.4828      1680\n",
      "weighted avg     0.5088    0.4815    0.4828      1680\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5744 | sujetos=20\n",
      "  Œî(FT-Global) = +0.0929\n",
      "\n",
      "[Fold 5/5] Entrenando modelo global... (n_train=5880 | n_val=1092 | n_test=1680)\n",
      "  √âpoca   1 | train_acc=0.4267 | val_acc=0.4341 | LR=0.01000\n",
      "  √âpoca   5 | train_acc=0.4947 | val_acc=0.4734 | LR=0.00257\n",
      "  √âpoca  10 | train_acc=0.4966 | val_acc=0.4899 | LR=0.00855\n",
      "  √âpoca  15 | train_acc=0.5218 | val_acc=0.5027 | LR=0.00257\n",
      "  √âpoca  20 | train_acc=0.5207 | val_acc=0.4890 | LR=0.00996\n",
      "  √âpoca  25 | train_acc=0.5298 | val_acc=0.4762 | LR=0.00855\n",
      "  √âpoca  30 | train_acc=0.5497 | val_acc=0.4991 | LR=0.00570\n",
      "  Early stopping en √©poca 30 (mejor val_acc=0.5027)\n",
      "‚Ü≥ Curva de entrenamiento guardada: training_curve_fold5.png\n",
      "[Fold 5/5] Global acc=0.5339\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5902    0.6310    0.6099       420\n",
      "       Right     0.5444    0.5833    0.5632       420\n",
      "  Both Fists     0.4871    0.4500    0.4678       420\n",
      "   Both Feet     0.5038    0.4714    0.4871       420\n",
      "\n",
      "    accuracy                         0.5339      1680\n",
      "   macro avg     0.5314    0.5339    0.5320      1680\n",
      "weighted avg     0.5314    0.5339    0.5320      1680\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5827 | sujetos=20\n",
      "  Œî(FT-Global) = +0.0488\n",
      "\n",
      "============================================================\n",
      "RESULTADOS FINALES\n",
      "============================================================\n",
      "Global folds: ['0.4467', '0.5374', '0.4683', '0.4815', '0.5339']\n",
      "Global mean: 0.4936\n",
      "Fine-tune PROGRESIVO folds: ['0.5057', '0.6043', '0.5306', '0.5744', '0.5827']\n",
      "Fine-tune PROGRESIVO mean: 0.5595\n",
      "Œî(FT-Global) mean: +0.0660\n",
      "\n",
      "‚Ü≥ Matriz de confusi√≥n guardada: confusion_global_allfolds.png\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# EEGNet + protocolo global + fine-tuning progresivo por sujeto (augments + SGDR + tweaks++)\n",
    "\n",
    "import os, re, math, random, json, itertools, copy\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold, StratifiedShuffleSplit, GroupShuffleSplit\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# CONFIGURACI√ìN GENERAL\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "FOLDS_DIR = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dispositivo y semilla\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE); np.random.seed(RANDOM_STATE); random.seed(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "# Escenario y ventana\n",
    "CLASS_SCENARIO = '4c'\n",
    "WINDOW_MODE = '6s'   # 6s\n",
    "FS = 160.0\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Entrenamiento global\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS_GLOBAL = 100\n",
    "LR_INIT = 1e-2\n",
    "# LR_INIT = 1.5e-2  # ‚Üê opcional para probar con batch=64\n",
    "SGDR_T0 = 6\n",
    "SGDR_Tmult = 2\n",
    "SGDR_ETA_MIN = 1e-4  # nuevo: no caer tan bajo entre reinicios\n",
    "\n",
    "# Validaci√≥n/ES global\n",
    "GLOBAL_VAL_SPLIT = 0.15\n",
    "GLOBAL_PATIENCE  = 15      # ‚Üë paciencia\n",
    "MIN_EPOCHS_BEFORE_ES = 12  # no cortar antes de completar ~2 ciclos de T0=6\n",
    "LOG_EVERY        = 5\n",
    "\n",
    "# Fine-tuning por sujeto\n",
    "CALIB_CV_FOLDS = 4\n",
    "FT_EPOCHS = 30\n",
    "FT_BASE_LR = 5e-5\n",
    "FT_HEAD_LR = 1e-3\n",
    "FT_L2SP = 1e-4\n",
    "FT_PATIENCE = 5\n",
    "FT_VAL_RATIO = 0.2\n",
    "\n",
    "# Normalizaci√≥n por √©poca canal-a-canal (z-score)\n",
    "NORM_EPOCH_ZSCORE = True\n",
    "\n",
    "# Sujetos excluidos\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "\n",
    "# Runs\n",
    "MI_RUNS_LR = [4, 8, 12]\n",
    "MI_RUNS_OF = [6, 10, 14]\n",
    "BASELINE_RUNS_EO = [1]\n",
    "\n",
    "# Canales (8 con FCz)\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','FCz']\n",
    "\n",
    "# Augments\n",
    "MIXUP_ALPHA = 0.25         # ‚Üë respecto a 0.20\n",
    "CUTOUT_MASKED_MS = (30,60) # igual\n",
    "TIME_JITTER_MS = 50\n",
    "TTA_N = 7                  # ‚Üë TTA en evaluaci√≥n\n",
    "TTA_JITTER_MS = 25\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES DE CANALES\n",
    "# =========================\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = re.sub(r'([A-Za-z])0([0-9])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])Z$', r'\\1z', s)\n",
    "    s = s.replace('fp', 'Fp').replace('FP', 'Fp')\n",
    "    s = ''.join(ch.upper() if ch != 'z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {}\n",
    "    for ch in raw.ch_names:\n",
    "        lab = normalize_label(ch)\n",
    "        lab = lab[:-1] + 'z' if lab.endswith('Z') else lab\n",
    "        lab = re.sub(r'([A-Z])Z$', r'\\1z', lab)\n",
    "        mapping[ch] = lab\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired_channels=EXPECTED_8):\n",
    "    missing = [ch for ch in desired_channels if ch not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"Warning: faltan canales {missing} en archivo {getattr(raw,'filenames', [''])[0]}\")\n",
    "        return None\n",
    "    raw.reorder_channels([ch for ch in raw.ch_names if ch in desired_channels] +\n",
    "                         [ch for ch in raw.ch_names if ch not in desired_channels])\n",
    "    raw.pick_channels(desired_channels, ordered=True)\n",
    "    return raw\n",
    "\n",
    "# =========================\n",
    "# LECTURA DE EDF y EVENTOS\n",
    "# =========================\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(run_id:int):\n",
    "    if run_id in MI_RUNS_LR: return 'LR'\n",
    "    if run_id in MI_RUNS_OF: return 'OF'\n",
    "    if run_id in BASELINE_RUNS_EO: return 'EO'\n",
    "    return None\n",
    "\n",
    "# --- Notch adaptativo: lee CSV si existe (subject, run, snr50_db, snr60_db)\n",
    "_SNR_TABLE = None\n",
    "def _load_snr_table():\n",
    "    global _SNR_TABLE\n",
    "    if _SNR_TABLE is not None:\n",
    "        return _SNR_TABLE\n",
    "    csv_path = PROJ / 'reports' / 'psd_mains' / 'psd_mains_summary.csv'\n",
    "    if csv_path.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            _SNR_TABLE = df\n",
    "        except Exception as e:\n",
    "            print(f\"[SNR] No se pudo leer {csv_path}: {e}\")\n",
    "            _SNR_TABLE = None\n",
    "    return _SNR_TABLE\n",
    "\n",
    "def _decide_notch(subject, run, th_db=10.0):\n",
    "    df = _load_snr_table()\n",
    "    if df is None:\n",
    "        return 60.0\n",
    "    row = df[(df['subject']==subject) & (df['run']==run)]\n",
    "    if row.empty:\n",
    "        return 60.0\n",
    "    snr50 = float(row['snr50_db'].iloc[0]); snr60 = float(row['snr60_db'].iloc[0])\n",
    "    if snr60 >= th_db and snr60 >= snr50: return 60.0\n",
    "    if snr50 >= th_db and snr50 >  snr60: return 50.0\n",
    "    return None  # no notch\n",
    "\n",
    "def read_raw_edf(path: Path):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        mont = mne.channels.make_standard_montage('standard_1020')\n",
    "        raw.set_montage(mont, on_missing='ignore')\n",
    "    except Exception:\n",
    "        pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad=\"auto\")\n",
    "    raw = ensure_channels_order(raw, EXPECTED_8)\n",
    "    if raw is None:\n",
    "        return None\n",
    "\n",
    "    sid, rid = parse_subject_run(path)\n",
    "    notch = _decide_notch(sid, rid)\n",
    "    if notch is not None:\n",
    "        raw.notch_filter(freqs=[float(notch)], picks='eeg', method='spectrum_fit', phase='zero')\n",
    "\n",
    "    return raw\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    if raw.annotations is None or len(raw.annotations) == 0:\n",
    "        return []\n",
    "    def _norm(s): return str(s).strip().upper().replace(' ', '')\n",
    "    res = []\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = _norm(desc)\n",
    "        if tag in ('T1','T2'):\n",
    "            res.append((float(onset), tag))\n",
    "    res.sort()\n",
    "    dedup = []\n",
    "    last_t1 = last_t2 = -1e9\n",
    "    for t, tag in res:\n",
    "        if tag == 'T1':\n",
    "            if (t - last_t1) >= 0.5: dedup.append((t, tag)); last_t1 = t\n",
    "        else:\n",
    "            if (t - last_t2) >= 0.5: dedup.append((t, tag)); last_t2 = t\n",
    "    return dedup\n",
    "\n",
    "# =========================\n",
    "# CONSTRUCCI√ìN DE DATASETS\n",
    "# =========================\n",
    "def subjects_available():\n",
    "    subs = []\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try: sid = int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir / f\"S{sid:03d}R{r:02d}.edf\").exists() for r in (MI_RUNS_LR + MI_RUNS_OF))\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "def extract_trials_from_run(edf_path: Path, scenario: str, window_mode: str):\n",
    "    subj, run = parse_subject_run(edf_path)\n",
    "    kind = run_kind(run)\n",
    "    if kind not in ('LR','OF','EO'):\n",
    "        return ([], [])\n",
    "\n",
    "    raw = read_raw_edf(edf_path)\n",
    "    if raw is None:\n",
    "        return ([], [])\n",
    "\n",
    "    data = raw.get_data()\n",
    "    fs = raw.info['sfreq']\n",
    "    assert abs(fs - FS) < 1e-6\n",
    "\n",
    "    out = []\n",
    "\n",
    "    if kind in ('LR','OF'):\n",
    "        events = collect_events_T1T2(raw)\n",
    "        if window_mode == '3s':\n",
    "            rel_start, rel_end = 0.0, 3.0\n",
    "        else:\n",
    "            rel_start, rel_end = -1.0, 5.0\n",
    "\n",
    "        for onset_sec, tag in events:\n",
    "            if kind == 'LR':\n",
    "                if tag == 'T1': label = 'L'\n",
    "                elif tag == 'T2': label = 'R'\n",
    "                else: continue\n",
    "            else:\n",
    "                if tag == 'T1': label = 'BFISTS'\n",
    "                elif tag == 'T2': label = 'BFEET'\n",
    "                else: continue\n",
    "\n",
    "            if scenario == '2c' and label not in ('L','R'): continue\n",
    "            if scenario == '3c' and label not in ('L','R','BFISTS'): continue\n",
    "            if scenario == '4c' and label not in ('L','R','BFISTS','BFEET'): continue\n",
    "\n",
    "            s = int(round((raw.first_time + onset_sec + rel_start) * fs))\n",
    "            e = int(round((raw.first_time + onset_sec + rel_end) * fs))\n",
    "            if s < 0 or e > data.shape[1]:\n",
    "                continue\n",
    "\n",
    "            seg = data[:, s:e].T.astype(np.float32)\n",
    "            if NORM_EPOCH_ZSCORE:\n",
    "                seg = (seg - seg.mean(axis=0, keepdims=True)) / (seg.std(axis=0, keepdims=True) + 1e-6)\n",
    "\n",
    "            if label == 'L':       y = 0\n",
    "            elif label == 'R':     y = 1\n",
    "            elif label == 'BFISTS':y = 2\n",
    "            elif label == 'BFEET': y = 3\n",
    "            else: continue\n",
    "\n",
    "            out.append((seg, y, subj))\n",
    "\n",
    "    elif kind == 'EO':\n",
    "        return ([], raw.ch_names)\n",
    "\n",
    "    return out, raw.ch_names\n",
    "\n",
    "def build_dataset_all(subjects, scenario='4c', window_mode='3s'):\n",
    "    X, y, groups = [], [], []\n",
    "    ch_template = None\n",
    "\n",
    "    for s in tqdm(subjects, desc=\"Construyendo dataset (RAW)\"):\n",
    "        sdir = DATA_RAW / f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "\n",
    "        trials_L, trials_R, trials_FISTS, trials_FEET = [], [], [], []\n",
    "\n",
    "        for r in MI_RUNS_LR:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 0: trials_L.append(seg)\n",
    "                elif lab == 1: trials_R.append(seg)\n",
    "\n",
    "        for r in MI_RUNS_OF:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 2: trials_FISTS.append(seg)\n",
    "                elif lab == 3: trials_FEET.append(seg)\n",
    "\n",
    "        need_per_class = 21\n",
    "        def pick(trials, n, rng):\n",
    "            if len(trials) < n:\n",
    "                idx = rng.choice(len(trials), size=n, replace=True)\n",
    "                return [trials[i] for i in idx]\n",
    "            rng.shuffle(trials)\n",
    "            return trials[:n]\n",
    "\n",
    "        rng = check_random_state(RANDOM_STATE + s)\n",
    "        if len(trials_L)==0 or len(trials_R)==0 or len(trials_FISTS)==0 or len(trials_FEET)==0:\n",
    "            continue\n",
    "\n",
    "        Lp  = pick(trials_L,     need_per_class, rng)\n",
    "        Rp  = pick(trials_R,     need_per_class, rng)\n",
    "        FIp = pick(trials_FISTS, need_per_class, rng)\n",
    "        FEp = pick(trials_FEET,  need_per_class, rng)\n",
    "\n",
    "        pack = [(Lp, 0), (Rp, 1), (FIp, 2), (FEp, 3)]\n",
    "        for segs, lab in pack:\n",
    "            for seg in segs:\n",
    "                X.append(seg); y.append(lab); groups.append(s)\n",
    "\n",
    "    X = np.stack(X, axis=0)\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "    groups = np.asarray(groups, dtype=np.int64)\n",
    "\n",
    "    n, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Dataset construido: N={n} | T={T} | C={C} | clases={n_classes} | sujetos √∫nicos={len(np.unique(groups))}\")\n",
    "    return X, y, groups, ch_template\n",
    "\n",
    "# =========================\n",
    "# AUGMENTS (solo train)\n",
    "# =========================\n",
    "def do_time_jitter(x, max_ms=50, fs=160.0):\n",
    "    max_shift = int(round(max_ms/1000.0 * fs))\n",
    "    if max_shift <= 0: return x\n",
    "    B,_,T,C = x.shape\n",
    "    shifts = torch.randint(low=-max_shift, high=max_shift+1, size=(B,), device=x.device)\n",
    "    out = torch.empty_like(x)\n",
    "    for i,s in enumerate(shifts):\n",
    "        if s==0: out[i] = x[i]; continue\n",
    "        if s>0:\n",
    "            out[i,:,s:,:] = x[i,:,:T-s,:]\n",
    "            out[i,:,:s,:] = 0\n",
    "        else:\n",
    "            s = -s\n",
    "            out[i,:,:T-s,:] = x[i,:,s:,:]\n",
    "            out[i,:,T-s:,:] = 0\n",
    "    return out\n",
    "\n",
    "def do_gaussian_noise(x, sigma=0.01):\n",
    "    if sigma<=0: return x\n",
    "    return x + sigma*torch.randn_like(x)\n",
    "\n",
    "def do_temporal_cutout_masked(x, y, classes_mask={2,3}, min_ms=30, max_ms=60, fs=160.0):\n",
    "    B,_,T,C = x.shape\n",
    "    if B == 0: return x\n",
    "    Lmin = int(round(min_ms/1000.0*fs))\n",
    "    Lmax = int(round(max_ms/1000.0*fs))\n",
    "    if Lmin<=0 or Lmax<=0 or Lmin>Lmax: return x\n",
    "    out = x.clone()\n",
    "    y_np = y.detach().cpu().numpy()\n",
    "    for i in range(B):\n",
    "        if int(y_np[i]) not in classes_mask:\n",
    "            continue\n",
    "        L = random.randint(Lmin, Lmax)\n",
    "        if L>=T: continue\n",
    "        s = random.randint(0, T-L)\n",
    "        out[i,:,s:s+L,:] = 0\n",
    "    return out\n",
    "\n",
    "def mixup_batch(x, y, n_classes, alpha=0.25):\n",
    "    if alpha<=0:\n",
    "        y_onehot = torch.nn.functional.one_hot(y, num_classes=n_classes).float()\n",
    "        return x, y_onehot, 1.0\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    perm = torch.randperm(x.size(0), device=x.device)\n",
    "    x_mix = lam*x + (1-lam)*x[perm]\n",
    "    y_a = torch.nn.functional.one_hot(y, num_classes=n_classes).float()\n",
    "    y_b = y_a[perm]\n",
    "    y_mix = lam*y_a + (1-lam)*y_b\n",
    "    return x_mix, y_mix, lam\n",
    "\n",
    "class WeightedSoftCrossEntropy(nn.Module):\n",
    "    def __init__(self, class_weights=None, label_smoothing=0.10):  # ‚Üë smoothing\n",
    "        super().__init__()\n",
    "        self.register_buffer('w', None if class_weights is None else class_weights.clone().float())\n",
    "        self.ls = float(label_smoothing)\n",
    "\n",
    "    def forward(self, logits, target_probs):\n",
    "        if self.ls > 0:\n",
    "            K = logits.size(1)\n",
    "            target_probs = (1-self.ls)*target_probs + self.ls*(1.0/K)\n",
    "        logp = torch.log_softmax(logits, dim=1)\n",
    "        loss_per_class = -(target_probs * logp)\n",
    "        if self.w is not None:\n",
    "            loss_per_class = loss_per_class * self.w.unsqueeze(0)\n",
    "        loss = loss_per_class.sum(dim=1).mean()\n",
    "        return loss\n",
    "\n",
    "# =========================\n",
    "# EEGNet (Lawhern et al., 2018) adaptado a (B,1,T,C) + ChannelDropout\n",
    "# =========================\n",
    "class ChannelDropout(nn.Module):\n",
    "    def __init__(self, p=0.15):  # ‚Üë 0.10 ‚Üí 0.15\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    def forward(self, x):\n",
    "        if not self.training or self.p<=0: return x\n",
    "        B,_,T,C = x.shape\n",
    "        mask = (torch.rand(B,1,1,C, device=x.device) > self.p).float()\n",
    "        return x * mask\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, n_ch: int, n_classes: int,\n",
    "                 F1: int = 24, D: int = 2, kernel_t: int = 64, k_sep: int = 16,\n",
    "                 pool1_t: int = 4, pool2_t: int = 6,\n",
    "                 drop1_p: float = 0.40, drop2_p: float = 0.60,  # drop1 ‚Üë\n",
    "                 chdrop_p: float = 0.15):\n",
    "        super().__init__()\n",
    "        self.n_ch = n_ch\n",
    "        self.n_classes = n_classes\n",
    "        self.F1 = F1\n",
    "        self.D = D\n",
    "        self.F2 = F1 * D\n",
    "        self.kernel_t = kernel_t\n",
    "        self.k_sep = k_sep\n",
    "        self.pool1_t = pool1_t\n",
    "        self.pool2_t = pool2_t\n",
    "\n",
    "        self.chdrop = ChannelDropout(p=chdrop_p)\n",
    "\n",
    "        self.conv_temporal = nn.Conv2d(1, F1, kernel_size=(kernel_t, 1),\n",
    "                                       padding=(kernel_t // 2, 0), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(F1, momentum=0.99, eps=1e-3)\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "        self.conv_depthwise = nn.Conv2d(F1, self.F2, kernel_size=(1, n_ch),\n",
    "                                        groups=F1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.F2, momentum=0.99, eps=1e-3)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=(pool1_t, 1), stride=(pool1_t, 1))\n",
    "        self.drop1 = nn.Dropout(drop1_p)\n",
    "\n",
    "        self.conv_sep_depth = nn.Conv2d(self.F2, self.F2, kernel_size=(k_sep, 1),\n",
    "                                        groups=self.F2, padding=(k_sep // 2, 0), bias=False)\n",
    "        self.conv_sep_point = nn.Conv2d(self.F2, self.F2, kernel_size=(1, 1), bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.F2, momentum=0.99, eps=1e-3)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(pool2_t, 1), stride=(pool2_t, 1))\n",
    "        self.drop2 = nn.Dropout(drop2_p)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = None\n",
    "        self.out = None\n",
    "        self._T_in = None\n",
    "\n",
    "    def _build_head(self, T_in: int, device: torch.device):\n",
    "        T1 = T_in // self.pool1_t\n",
    "        T2 = T1 // self.pool2_t\n",
    "        feat_dim = self.F2 * T2 * 1\n",
    "        self.fc  = nn.Linear(feat_dim, 128, bias=True).to(device)\n",
    "        self.out = nn.Linear(128, self.n_classes, bias=True).to(device)\n",
    "        self._T_in = T_in\n",
    "\n",
    "    def ensure_head(self, T_in: int, device: torch.device):\n",
    "        if (self.fc is None) or (self.out is None) or (self._T_in != T_in):\n",
    "            self._build_head(T_in, device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, _, T, C = x.shape\n",
    "        self.ensure_head(T, x.device)\n",
    "\n",
    "        x = self.chdrop(x)\n",
    "\n",
    "        z = self.conv_temporal(x)\n",
    "        z = self.bn1(z); z = self.act(z)\n",
    "\n",
    "        z = self.conv_depthwise(z)\n",
    "        z = self.bn2(z); z = self.act(z)\n",
    "        z = self.pool1(z)\n",
    "        z = self.drop1(z)\n",
    "\n",
    "        z = self.conv_sep_depth(z)\n",
    "        z = self.conv_sep_point(z)\n",
    "        z = self.bn3(z); z = self.act(z)\n",
    "        z = self.pool2(z)\n",
    "        z = self.drop2(z)\n",
    "\n",
    "        z = self.flatten(z)\n",
    "        z = self.fc(z); z = self.act(z)\n",
    "        z = self.out(z)\n",
    "        return z\n",
    "\n",
    "# =========================\n",
    "# TORCH DATASET\n",
    "# =========================\n",
    "class EEGTrials(Dataset):\n",
    "    def __init__(self, X, y, groups):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.g = groups.astype(np.int64)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        x = np.expand_dims(x, 0)                 # (1, T, C)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx])\n",
    "\n",
    "CLASS_NAMES_4C = ['Left', 'Right', 'Both Fists', 'Both Feet']\n",
    "\n",
    "# =========================\n",
    "# UTIL: MAX-NORM\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def apply_max_norm(model, max_value=2.0, p=2.0):\n",
    "    layers = []\n",
    "    if hasattr(model, 'conv_depthwise'): layers.append(model.conv_depthwise)\n",
    "    if hasattr(model, 'conv_sep_point'): layers.append(model.conv_sep_point)\n",
    "    if hasattr(model, 'fc'):             layers.append(model.fc)\n",
    "    if hasattr(model, 'out'):            layers.append(model.out)\n",
    "    for layer in layers:\n",
    "        if hasattr(layer, 'weight') and layer.weight is not None:\n",
    "            w = layer.weight.data\n",
    "            norms = w.view(w.size(0), -1).norm(p=p, dim=1, keepdim=True)\n",
    "            desired = torch.clamp(norms, max=max_value)\n",
    "            w.view(w.size(0), -1).mul_(desired / (1e-8 + norms))\n",
    "\n",
    "# =========================\n",
    "# ENTRENAMIENTO / EVALUACI√ìN\n",
    "# =========================\n",
    "def build_weighted_sampler(y, groups):\n",
    "    y = np.asarray(y); groups = np.asarray(groups)\n",
    "    class_counts = np.bincount(y, minlength=len(np.unique(y))).astype(float)\n",
    "    class_w = 1.0 / class_counts[y]\n",
    "    subj_vals, subj_counts = np.unique(groups, return_counts=True)\n",
    "    subj_map = {s:c for s,c in zip(subj_vals, subj_counts)}\n",
    "    subj_w = np.array([1.0/subj_map[g] for g in groups], dtype=float)\n",
    "    w = class_w * subj_w\n",
    "    w = w / w.mean()\n",
    "    w_t = torch.from_numpy(w).float()\n",
    "    sampler = WeightedRandomSampler(weights=w_t, num_samples=len(w_t), replacement=True)\n",
    "    return sampler\n",
    "\n",
    "def make_class_weight_tensor(y_indices, n_classes, boost_bfists=1.20, boost_bfeet=1.10):\n",
    "    counts = np.bincount(y_indices, minlength=n_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    w = counts.sum() / counts\n",
    "    w = w / w.mean()\n",
    "    w[2] *= boost_bfists  # Both Fists\n",
    "    w[3] *= boost_bfeet   # Both Feet (nuevo)\n",
    "    w = w / w.mean()\n",
    "    return torch.tensor(w, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "def train_epoch(model, loader, opt, criterion_soft, n_classes,\n",
    "                do_aug=True, fs=160.0, maxnorm=None):\n",
    "    model.train()\n",
    "    for xb, yb, _ in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "\n",
    "        if do_aug:\n",
    "            xb = do_time_jitter(xb, max_ms=TIME_JITTER_MS, fs=fs)\n",
    "            xb = do_gaussian_noise(xb, sigma=0.01)\n",
    "            xb = do_temporal_cutout_masked(xb, yb, classes_mask={2,3},\n",
    "                                           min_ms=CUTOUT_MASKED_MS[0], max_ms=CUTOUT_MASKED_MS[1], fs=fs)\n",
    "            xb, yt, _ = mixup_batch(xb, yb, n_classes=n_classes, alpha=MIXUP_ALPHA)\n",
    "            logits = model(xb)\n",
    "            loss = criterion_soft(logits, yt)\n",
    "        else:\n",
    "            logits = model(xb)\n",
    "            yt = torch.nn.functional.one_hot(yb, num_classes=n_classes).float()\n",
    "            loss = criterion_soft(logits, yt)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if maxnorm is not None:\n",
    "            apply_max_norm(model, max_value=maxnorm, p=2.0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _predict_tta(model, xb, n=TTA_N, fs=160.0):\n",
    "    outs = []\n",
    "    for _ in range(n):\n",
    "        xj = do_time_jitter(xb, max_ms=TTA_JITTER_MS, fs=fs)\n",
    "        outs.append(model(xj))\n",
    "    return torch.stack(outs, dim=0).mean(dim=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_with_preds(model, loader, use_tta=True, tta_n=TTA_N):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for xb, yb, _ in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        if use_tta:\n",
    "            logits = _predict_tta(model, xb, n=tta_n, fs=FS)\n",
    "        else:\n",
    "            logits = model(xb)\n",
    "        pred = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "        y_pred.extend(pred)\n",
    "        y_true.extend(yb.numpy().tolist())\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    return y_true, y_pred, float(acc)\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes, title, fname):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.nan_to_num(cm_norm)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm_norm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "        plt.text(j, i, format(cm_norm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm_norm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label'); plt.xlabel('Predicted label')\n",
    "    plt.tight_layout(); plt.savefig(fname, dpi=150, bbox_inches='tight'); plt.close()\n",
    "\n",
    "def plot_training_curves(history, fname):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(history['train_acc'], label='train_acc')\n",
    "    plt.plot(history['val_acc'], label='val_acc')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
    "    plt.title('Training curve'); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(fname, dpi=150); plt.close()\n",
    "\n",
    "def print_report(y_true, y_pred, classes):\n",
    "    print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
    "\n",
    "# =========================\n",
    "# FINE-TUNING PROGRESIVO por sujeto (con validaci√≥n interna)\n",
    "# =========================\n",
    "def _param_groups(model, mode):\n",
    "    if mode == 'out':\n",
    "        train = list(model.out.parameters())\n",
    "    elif mode == 'head':\n",
    "        train = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "    elif mode == 'spatial+head':\n",
    "        train = (list(model.conv_depthwise.parameters()) +\n",
    "                 list(model.bn2.parameters()) +\n",
    "                 list(model.conv_sep_depth.parameters()) +\n",
    "                 list(model.conv_sep_point.parameters()) +\n",
    "                 list(model.bn3.parameters()) +\n",
    "                 list(model.fc.parameters()) +\n",
    "                 list(model.out.parameters()))\n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "    return train\n",
    "\n",
    "def _freeze_for_mode(model, mode):\n",
    "    for p in model.parameters(): p.requires_grad = False\n",
    "    if mode == 'out':\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'head':\n",
    "        for p in model.fc.parameters():  p.requires_grad = True\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'spatial+head':\n",
    "        for p in model.conv_depthwise.parameters(): p.requires_grad = True\n",
    "        for p in model.bn2.parameters():           p.requires_grad = True\n",
    "        for p in model.conv_sep_depth.parameters():p.requires_grad = True\n",
    "        for p in model.conv_sep_point.parameters():p.requires_grad = True\n",
    "        for p in model.bn3.parameters():           p.requires_grad = True\n",
    "        for p in model.fc.parameters():            p.requires_grad = True\n",
    "        for p in model.out.parameters():           p.requires_grad = True\n",
    "\n",
    "def _class_weights(y_np, n_classes):\n",
    "    counts = np.bincount(y_np, minlength=n_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    weights = counts.sum() / counts\n",
    "    weights = weights / weights.mean()\n",
    "    return torch.tensor(weights, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "def _train_one_mode(model, X_cal, y_cal, n_classes, mode,\n",
    "                    epochs=FT_EPOCHS, batch_size=32,\n",
    "                    head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                    l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=RANDOM_STATE)\n",
    "    (tr_idx, va_idx), = sss.split(X_cal, y_cal)\n",
    "    Xtr, ytr = X_cal[tr_idx], y_cal[tr_idx]\n",
    "    Xva, yva = X_cal[va_idx], y_cal[va_idx]\n",
    "\n",
    "    ds_tr = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xtr).float().unsqueeze(1),\n",
    "        torch.from_numpy(ytr).long()\n",
    "    )\n",
    "    ds_va = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xva).float().unsqueeze(1),\n",
    "        torch.from_numpy(yva).long()\n",
    "    )\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    _freeze_for_mode(model, mode)\n",
    "\n",
    "    if mode == 'spatial+head':\n",
    "        base_params = (list(model.conv_depthwise.parameters()) +\n",
    "                       list(model.bn2.parameters()) +\n",
    "                       list(model.conv_sep_depth.parameters()) +\n",
    "                       list(model.conv_sep_point.parameters()) +\n",
    "                       list(model.bn3.parameters()))\n",
    "        head_params = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "        train_params = base_params + head_params\n",
    "        opt = optim.Adam([\n",
    "            {\"params\": base_params, \"lr\": base_lr},\n",
    "            {\"params\": head_params, \"lr\": head_lr},\n",
    "        ])\n",
    "    else:\n",
    "        train_params = _param_groups(model, mode)\n",
    "        opt = optim.Adam(train_params, lr=head_lr)\n",
    "\n",
    "    ref = [p.detach().clone().to(p.device) for p in train_params]\n",
    "    class_w = _class_weights(ytr, n_classes)\n",
    "    crit = nn.CrossEntropyLoss(weight=class_w)\n",
    "\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_val = float('inf'); bad = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            reg = 0.0\n",
    "            for p_cur, p_ref in zip(train_params, ref):\n",
    "                reg = reg + torch.sum((p_cur - p_ref)**2)\n",
    "            loss = loss + l2sp_lambda * reg\n",
    "            loss.backward(); opt.step()\n",
    "            apply_max_norm(model, max_value=2.0, p=2.0)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0; nval = 0\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                loss = crit(logits, yb)\n",
    "                val_loss += loss.item() * xb.size(0); nval += xb.size(0)\n",
    "            val_loss /= max(1, nval)\n",
    "\n",
    "        if val_loss + 1e-7 < best_val:\n",
    "            best_val = val_loss; bad = 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience: break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_numpy(model, X_np, device):\n",
    "    model.eval()\n",
    "    xb = torch.from_numpy(X_np).float().unsqueeze(1).to(device)\n",
    "    logits = model(xb)\n",
    "    return logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "def subject_cv_finetune_predict_progressive(model_global, Xs, ys, device,\n",
    "                                            n_splits=CALIB_CV_FOLDS, n_classes=4):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    y_true_full = np.empty_like(ys); y_pred_full = np.empty_like(ys)\n",
    "\n",
    "    for tr_idx, te_idx in skf.split(Xs, ys):\n",
    "        Xcal, ycal = Xs[tr_idx], ys[tr_idx]\n",
    "        Xho,  yho  = Xs[te_idx], ys[te_idx]\n",
    "\n",
    "        m_out = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_out, Xcal, ycal, n_classes, mode='out',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_out = predict_numpy(m_out, Xho, device)\n",
    "\n",
    "        m_head = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_head, Xcal, ycal, n_classes, mode='head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_head = predict_numpy(m_head, Xho, device)\n",
    "\n",
    "        m_sp = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_sp, Xcal, ycal, n_classes, mode='spatial+head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                        l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_sp = predict_numpy(m_sp, Xho, device)\n",
    "\n",
    "        accs = [ (yhat_out == yho).mean(), (yhat_head == yho).mean(), (yhat_sp == yho).mean() ]\n",
    "        best_idx = int(np.argmax(accs))\n",
    "        yhat_best = [yhat_out, yhat_head, yhat_sp][best_idx]\n",
    "\n",
    "        y_true_full[te_idx] = yho; y_pred_full[te_idx] = yhat_best\n",
    "\n",
    "    return y_true_full, y_pred_full\n",
    "\n",
    "# =========================\n",
    "# FOLDS JSON helpers\n",
    "# =========================\n",
    "def save_group_folds_json_with_indices(subject_ids_str, groups_array, n_splits, out_json_path,\n",
    "                                       created_by=\"dose_experiment\", description=None):\n",
    "    out_json_path = Path(out_json_path)\n",
    "    unique_subjects_int = sorted(np.unique(groups_array).tolist())\n",
    "    subject_ids = [f\"S{sid:03d}\" for sid in unique_subjects_int]\n",
    "\n",
    "    if len(subject_ids) < n_splits:\n",
    "        raise ValueError(f\"n_splits={n_splits} mayor que n√∫mero de sujetos={len(subject_ids)}\")\n",
    "\n",
    "    groups = np.arange(len(subject_ids))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    folds = []\n",
    "    fold_i = 0\n",
    "    for train_idx_grp, test_idx_grp in gkf.split(groups, groups, groups):\n",
    "        fold_i += 1\n",
    "        train_sids = [subject_ids[int(i)] for i in train_idx_grp]\n",
    "        test_sids  = [subject_ids[int(i)] for i in test_idx_grp]\n",
    "\n",
    "        train_sids_int = [int(s[1:]) for s in train_sids]\n",
    "        test_sids_int  = [int(s[1:]) for s in test_sids]\n",
    "\n",
    "        tr_idx = np.where(np.isin(groups_array, train_sids_int))[0].tolist()\n",
    "        te_idx = np.where(np.isin(groups_array, test_sids_int))[0].tolist()\n",
    "\n",
    "        folds.append({\n",
    "            \"fold\": int(fold_i),\n",
    "            \"train\": train_sids,\n",
    "            \"test\": test_sids,\n",
    "            \"tr_idx\": tr_idx,\n",
    "            \"te_idx\": te_idx\n",
    "        })\n",
    "\n",
    "    payload = {\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"created_by\": created_by,\n",
    "        \"description\": description if description is not None else \"\",\n",
    "        \"n_splits\": int(n_splits),\n",
    "        \"n_subjects\": len(subject_ids),\n",
    "        \"subject_ids\": subject_ids,\n",
    "        \"folds\": folds\n",
    "    }\n",
    "\n",
    "    out_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Folds JSON con √≠ndices guardado ‚Üí {out_json_path}\")\n",
    "    return out_json_path\n",
    "\n",
    "def load_group_folds_json(path_json, expected_subject_ids=None, strict_check=True):\n",
    "    path_json = Path(path_json)\n",
    "    if not path_json.exists():\n",
    "        raise FileNotFoundError(f\"No existe {path_json}\")\n",
    "    with open(path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        payload = json.load(f)\n",
    "\n",
    "    subj_json = payload.get(\"subject_ids\", [])\n",
    "    if expected_subject_ids is not None:\n",
    "        expected = sorted(list(expected_subject_ids))\n",
    "        if subj_json != expected:\n",
    "            msg = (\"Los subject_ids del JSON no coinciden con expected_subject_ids.\\n\"\n",
    "                   f\"JSON has {len(subj_json)} subjects, expected {len(expected)}.\\n\"\n",
    "                   f\"First 10 JSON: {subj_json[:10]}\\nFirst 10 expected: {expected[:10]}\")\n",
    "            if strict_check:\n",
    "                raise ValueError(msg)\n",
    "            else:\n",
    "                print(\"WARNING: \" + msg)\n",
    "    return payload\n",
    "\n",
    "# =========================\n",
    "# EXPERIMENTO\n",
    "# =========================\n",
    "def run_experiment(save_folds_json=True, folds_json_path=FOLDS_DIR, folds_json_description=\"GroupKFold folds for comparison\"):\n",
    "    mne.set_log_level('WARNING')\n",
    "\n",
    "    subs = subjects_available()\n",
    "    print(f\"Sujetos elegibles: {len(subs)} ‚Üí {subs[:10]}{'...' if len(subs)>10 else ''}\")\n",
    "\n",
    "    X, y, groups, chs = build_dataset_all(subs, scenario=CLASS_SCENARIO, window_mode=WINDOW_MODE)\n",
    "    N, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Listo para entrenar: N={N} | T={T} | C={C} | clases={n_classes} | sujetos={len(np.unique(groups))}\")\n",
    "\n",
    "    ds = EEGTrials(X, y, groups)\n",
    "\n",
    "    # preparar JSON folds\n",
    "    if folds_json_path is None:\n",
    "        folds_json_path = Path(\"folds\") / f\"group_folds_{N_FOLDS}splits.json\"\n",
    "    else:\n",
    "        folds_json_path = Path(folds_json_path)\n",
    "    folds_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    unique_subs = sorted(np.unique(groups).tolist())\n",
    "    subject_ids_str = [f\"S{s:03d}\" for s in unique_subs]\n",
    "\n",
    "    if not folds_json_path.exists():\n",
    "        if not save_folds_json:\n",
    "            raise FileNotFoundError(f\"Folds JSON no encontrado en {folds_json_path} y save_folds_json=False.\")\n",
    "        save_group_folds_json_with_indices(subject_ids_str, groups, n_splits=N_FOLDS,\n",
    "                                           out_json_path=folds_json_path,\n",
    "                                           created_by=\"Joel_Clasificador\",\n",
    "                                           description=folds_json_description)\n",
    "\n",
    "    payload = load_group_folds_json(folds_json_path, expected_subject_ids=subject_ids_str, strict_check=False)\n",
    "    folds = payload[\"folds\"]\n",
    "\n",
    "    # bucle por folds\n",
    "    global_folds = []\n",
    "    ft_prog_folds = []\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "\n",
    "    for f in folds:\n",
    "        fold = f[\"fold\"]\n",
    "        tr_idx = np.asarray(f.get(\"tr_idx\", []), dtype=int)\n",
    "        te_idx = np.asarray(f.get(\"te_idx\", []), dtype=int)\n",
    "\n",
    "        if tr_idx.size == 0 or te_idx.size == 0:\n",
    "            print(f\"Advertencia: fold {fold} sin √≠ndices tr/te v√°lidos. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        # ===== Split de validaci√≥n por sujetos =====\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=GLOBAL_VAL_SPLIT, random_state=RANDOM_STATE)\n",
    "        tr_subj_idx, va_subj_idx = next(gss.split(tr_idx, groups[tr_idx], groups[tr_idx]))\n",
    "        tr_sub_idx = tr_idx[tr_subj_idx]\n",
    "        va_idx     = tr_idx[va_subj_idx]\n",
    "\n",
    "        # Sampler balanceado para train\n",
    "        sampler = build_weighted_sampler(y[tr_sub_idx], groups[tr_sub_idx])\n",
    "\n",
    "        tr_loader = DataLoader(Subset(ds, tr_sub_idx), batch_size=BATCH_SIZE, sampler=sampler, drop_last=False)\n",
    "        va_loader = DataLoader(Subset(ds, va_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "        te_loader = DataLoader(Subset(ds, te_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "        # ===== EEGNet =====\n",
    "        model = EEGNet(n_ch=C, n_classes=n_classes,\n",
    "                       F1=24, D=2, kernel_t=64, k_sep=16,\n",
    "                       pool1_t=4, pool2_t=6, drop1_p=0.40, drop2_p=0.60,\n",
    "                       chdrop_p=0.15).to(DEVICE)\n",
    "\n",
    "        # Opt y scheduler SGDR (con eta_min)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_INIT)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            opt, T_0=SGDR_T0, T_mult=SGDR_Tmult, eta_min=SGDR_ETA_MIN\n",
    "        )\n",
    "\n",
    "        # M√©trica r√°pida\n",
    "        def _acc(loader):\n",
    "            return evaluate_with_preds(model, loader, use_tta=True, tta_n=TTA_N)[2]\n",
    "\n",
    "        # Historia para curvas\n",
    "        history = {'train_acc': [], 'val_acc': []}\n",
    "\n",
    "        # ---- Criterio suave con ponderaci√≥n por clase (+20% Fists, +10% Feet) ----\n",
    "        class_weights = make_class_weight_tensor(y[tr_sub_idx], n_classes, boost_bfists=1.20, boost_bfeet=1.10)\n",
    "        criterion_soft = WeightedSoftCrossEntropy(class_weights, label_smoothing=0.10)\n",
    "\n",
    "        # ===== Entrenamiento global =====\n",
    "        print(f\"\\n[Fold {fold}/{N_FOLDS}] Entrenando modelo global...\"\n",
    "              f\" (n_train={len(tr_sub_idx)} | n_val={len(va_idx)} | n_test={len(te_idx)})\")\n",
    "\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        best_val = -1.0\n",
    "        bad = 0\n",
    "\n",
    "        for epoch in range(1, EPOCHS_GLOBAL + 1):\n",
    "            train_epoch(model, tr_loader, opt, criterion_soft, n_classes=n_classes,\n",
    "                        do_aug=True, fs=FS, maxnorm=2.0)\n",
    "            scheduler.step(epoch-1 + 1e-8)\n",
    "\n",
    "            # eval\n",
    "            tr_acc = _acc(tr_loader)\n",
    "            va_acc = _acc(va_loader)\n",
    "            history['train_acc'].append(tr_acc)\n",
    "            history['val_acc'].append(va_acc)\n",
    "\n",
    "            if (epoch % LOG_EVERY == 0) or epoch in (1, 10, 20, 50, 100):\n",
    "                cur_lr = opt.param_groups[0]['lr']\n",
    "                print(f\"  √âpoca {epoch:3d} | train_acc={tr_acc:.4f} | val_acc={va_acc:.4f} | LR={cur_lr:.5f}\")\n",
    "\n",
    "            # early stopping \"amable\" con SGDR\n",
    "            improved = (va_acc > best_val + 1e-4)\n",
    "            if improved:\n",
    "                best_val = va_acc\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "                bad = 0\n",
    "            else:\n",
    "                bad += 1\n",
    "                if (epoch >= MIN_EPOCHS_BEFORE_ES) and (bad >= GLOBAL_PATIENCE):\n",
    "                    print(f\"  Early stopping en √©poca {epoch} (mejor val_acc={best_val:.4f})\")\n",
    "                    break\n",
    "\n",
    "        # guardar curva de entrenamiento\n",
    "        curve_path = f\"training_curve_fold{fold}.png\"\n",
    "        plot_training_curves(history, curve_path)\n",
    "        print(f\"‚Ü≥ Curva de entrenamiento guardada: {curve_path}\")\n",
    "\n",
    "        # cargar mejor estado antes de evaluar en test\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "        # ===== Evaluaci√≥n global =====\n",
    "        y_true, y_pred, acc_global = evaluate_with_preds(model, te_loader, use_tta=True, tta_n=TTA_N)\n",
    "        global_folds.append(acc_global)\n",
    "        all_true.append(y_true); all_pred.append(y_pred)\n",
    "\n",
    "        print(f\"[Fold {fold}/{N_FOLDS}] Global acc={acc_global:.4f}\")\n",
    "        print_report(y_true, y_pred, CLASS_NAMES_4C)\n",
    "\n",
    "        # ---------- Fine-tuning PROGRESIVO por sujeto con 4-fold CV ----------\n",
    "        X_te, y_te, g_te = X[te_idx], y[te_idx], groups[te_idx]\n",
    "\n",
    "        y_true_ft_all, y_pred_ft_all = [], []\n",
    "        used_subjects = 0\n",
    "        for sid in np.unique(g_te):\n",
    "            idx = np.where(g_te == sid)[0]\n",
    "            Xs, ys = X_te[idx], y_te[idx]\n",
    "\n",
    "            if len(ys) < CALIB_CV_FOLDS or len(np.unique(ys)) < 2:\n",
    "                continue\n",
    "\n",
    "            y_true_subj, y_pred_subj = subject_cv_finetune_predict_progressive(\n",
    "                model, Xs, ys, DEVICE, n_splits=CALIB_CV_FOLDS, n_classes=n_classes\n",
    "            )\n",
    "            y_true_ft_all.append(y_true_subj)\n",
    "            y_pred_ft_all.append(y_pred_subj)\n",
    "            used_subjects += 1\n",
    "\n",
    "        if len(y_true_ft_all) > 0:\n",
    "            y_true_ft_all = np.concatenate(y_true_ft_all)\n",
    "            y_pred_ft_all = np.concatenate(y_pred_ft_all)\n",
    "            acc_ft = (y_true_ft_all == y_pred_ft_all).mean()\n",
    "            print(f\"  Fine-tuning PROGRESIVO (por sujeto, {CALIB_CV_FOLDS}-fold CV) acc={acc_ft:.4f} | sujetos={used_subjects}\")\n",
    "            print(f\"  Œî(FT-Global) = {acc_ft - acc_global:+.4f}\")\n",
    "        else:\n",
    "            acc_ft = np.nan\n",
    "            print(\"  Fine-tuning PROGRESIVO no ejecutado (sujeto(s) con muestras insuficientes).\")\n",
    "\n",
    "        ft_prog_folds.append(acc_ft)\n",
    "\n",
    "    # ---------- resultados finales ----------\n",
    "    if len(all_true) > 0:\n",
    "        all_true = np.concatenate(all_true)\n",
    "        all_pred = np.concatenate(all_pred)\n",
    "    else:\n",
    "        all_true = np.array([], dtype=int)\n",
    "        all_pred = np.array([], dtype=int)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTADOS FINALES\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Global folds:\", [f\"{a:.4f}\" for a in global_folds])\n",
    "    if len(global_folds) > 0:\n",
    "        print(f\"Global mean: {np.mean(global_folds):.4f}\")\n",
    "\n",
    "    print(\"Fine-tune PROGRESIVO folds:\", [(\"nan\" if (a is None or np.isnan(a)) else f\"{a:.4f}\") for a in ft_prog_folds])\n",
    "    if len(ft_prog_folds) > 0:\n",
    "        print(f\"Fine-tune PROGRESIVO mean: {np.nanmean(ft_prog_folds):.4f}\")\n",
    "        print(f\"Œî(FT-Global) mean: {np.nanmean(ft_prog_folds) - np.mean(global_folds):+.4f}\")\n",
    "\n",
    "    if all_true.size > 0:\n",
    "        plot_confusion(all_true, all_pred, CLASS_NAMES_4C,\n",
    "                       title=\"Confusion Matrix - Global Model (All Folds)\",\n",
    "                       fname=\"confusion_global_allfolds.png\")\n",
    "        print(\"\\n‚Ü≥ Matriz de confusi√≥n guardada: confusion_global_allfolds.png\")\n",
    "\n",
    "    return {\n",
    "        \"global_folds\": global_folds,\n",
    "        \"ft_prog_folds\": ft_prog_folds,\n",
    "        \"all_true\": all_true,\n",
    "        \"all_pred\": all_pred,\n",
    "        \"folds_json_path\": str(folds_json_path)\n",
    "    }\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üß† INICIANDO EXPERIMENTO CON EEGNet + FINE-TUNING PROGRESIVO (augments + SGDR + tweaks++)\")\n",
    "    print(f\"üîß Configuraci√≥n: {CLASS_SCENARIO}, {len(EXPECTED_8)} canales, {WINDOW_MODE}\")\n",
    "    print(f\"‚öôÔ∏è  FT: epochs={FT_EPOCHS}, base_lr={FT_BASE_LR}, head_lr={FT_HEAD_LR}, L2SP={FT_L2SP}, patience={FT_PATIENCE}, CV={CALIB_CV_FOLDS}\")\n",
    "    run_experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4b70299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Usando dispositivo: cuda\n",
      "üß† INICIANDO EXPERIMENTO CON EEGNet + FINE-TUNING PROGRESIVO (Variant-B)\n",
      "üîß Configuraci√≥n: 4c, 8 canales, 6s\n",
      "‚öôÔ∏è  FT: epochs=30, base_lr=5e-05, head_lr=0.001, L2SP=0.0001, patience=5, CV=4\n",
      "Sujetos elegibles: 103 ‚Üí [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Construyendo dataset (RAW): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:38<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset construido: N=8652 | T=960 | C=8 | clases=4 | sujetos √∫nicos=103\n",
      "Listo para entrenar: N=8652 | T=960 | C=8 | clases=4 | sujetos=103\n",
      "\n",
      "[Fold 1/5] Entrenando modelo global... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  √âpoca   1 | train_acc=0.4651 | val_acc=0.4469 | LR=0.00500\n",
      "  √âpoca   5 | train_acc=0.5104 | val_acc=0.4643 | LR=0.00250\n",
      "  √âpoca  10 | train_acc=0.5119 | val_acc=0.4753 | LR=0.00854\n",
      "  √âpoca  15 | train_acc=0.5400 | val_acc=0.4936 | LR=0.00250\n",
      "  √âpoca  20 | train_acc=0.5376 | val_acc=0.4698 | LR=0.00996\n",
      "  Early stopping en √©poca 24 (mejor val_acc=0.5101)\n",
      "‚Ü≥ Curva de entrenamiento guardada: training_curve_fold1.png\n",
      "[Fold 1/5] Global acc=0.4756\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5802    0.4263    0.4915       441\n",
      "       Right     0.5622    0.4921    0.5248       441\n",
      "  Both Fists     0.3862    0.5079    0.4388       441\n",
      "   Both Feet     0.4430    0.4762    0.4590       441\n",
      "\n",
      "    accuracy                         0.4756      1764\n",
      "   macro avg     0.4929    0.4756    0.4785      1764\n",
      "weighted avg     0.4929    0.4756    0.4785      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5045 | sujetos=21\n",
      "  Œî(FT-Global) = +0.0289\n",
      "\n",
      "[Fold 2/5] Entrenando modelo global... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  √âpoca   1 | train_acc=0.4241 | val_acc=0.4194 | LR=0.00500\n",
      "  √âpoca   5 | train_acc=0.4722 | val_acc=0.4386 | LR=0.00250\n",
      "  √âpoca  10 | train_acc=0.4945 | val_acc=0.4460 | LR=0.00854\n",
      "  √âpoca  15 | train_acc=0.5067 | val_acc=0.4670 | LR=0.00250\n",
      "  √âpoca  20 | train_acc=0.5349 | val_acc=0.4716 | LR=0.00996\n",
      "  √âpoca  25 | train_acc=0.5252 | val_acc=0.4780 | LR=0.00854\n",
      "  √âpoca  30 | train_acc=0.5243 | val_acc=0.5009 | LR=0.00565\n",
      "  √âpoca  35 | train_acc=0.5400 | val_acc=0.4936 | LR=0.00250\n",
      "  √âpoca  40 | train_acc=0.5393 | val_acc=0.4982 | LR=0.00038\n",
      "  √âpoca  45 | train_acc=0.5409 | val_acc=0.4918 | LR=0.00996\n",
      "  Early stopping en √©poca 46 (mejor val_acc=0.5027)\n",
      "‚Ü≥ Curva de entrenamiento guardada: training_curve_fold2.png\n",
      "[Fold 2/5] Global acc=0.5306\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.6101    0.5465    0.5766       441\n",
      "       Right     0.5837    0.5850    0.5844       441\n",
      "  Both Fists     0.4382    0.5306    0.4800       441\n",
      "   Both Feet     0.5165    0.4603    0.4868       441\n",
      "\n",
      "    accuracy                         0.5306      1764\n",
      "   macro avg     0.5371    0.5306    0.5319      1764\n",
      "weighted avg     0.5371    0.5306    0.5319      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5918 | sujetos=21\n",
      "  Œî(FT-Global) = +0.0612\n",
      "\n",
      "[Fold 3/5] Entrenando modelo global... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  √âpoca   1 | train_acc=0.4522 | val_acc=0.4304 | LR=0.00500\n",
      "  √âpoca   5 | train_acc=0.4902 | val_acc=0.4872 | LR=0.00250\n",
      "  √âpoca  10 | train_acc=0.5055 | val_acc=0.5092 | LR=0.00854\n",
      "  √âpoca  15 | train_acc=0.5229 | val_acc=0.5128 | LR=0.00250\n",
      "  √âpoca  20 | train_acc=0.5257 | val_acc=0.5330 | LR=0.00996\n",
      "  √âpoca  25 | train_acc=0.5350 | val_acc=0.5101 | LR=0.00854\n",
      "  √âpoca  30 | train_acc=0.5454 | val_acc=0.5275 | LR=0.00565\n",
      "  √âpoca  35 | train_acc=0.5602 | val_acc=0.5211 | LR=0.00250\n",
      "  √âpoca  40 | train_acc=0.5447 | val_acc=0.5293 | LR=0.00038\n",
      "  √âpoca  45 | train_acc=0.5468 | val_acc=0.5192 | LR=0.00996\n",
      "  √âpoca  50 | train_acc=0.5495 | val_acc=0.5211 | LR=0.00948\n",
      "  Early stopping en √©poca 53 (mejor val_acc=0.5421)\n",
      "‚Ü≥ Curva de entrenamiento guardada: training_curve_fold3.png\n",
      "[Fold 3/5] Global acc=0.4858\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5283    0.4875    0.5071       441\n",
      "       Right     0.5827    0.5034    0.5401       441\n",
      "  Both Fists     0.4249    0.5261    0.4701       441\n",
      "   Both Feet     0.4372    0.4263    0.4317       441\n",
      "\n",
      "    accuracy                         0.4858      1764\n",
      "   macro avg     0.4933    0.4858    0.4873      1764\n",
      "weighted avg     0.4933    0.4858    0.4873      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5289 | sujetos=21\n",
      "  Œî(FT-Global) = +0.0431\n",
      "\n",
      "[Fold 4/5] Entrenando modelo global... (n_train=5880 | n_val=1092 | n_test=1680)\n",
      "  √âpoca   1 | train_acc=0.4257 | val_acc=0.4048 | LR=0.00500\n",
      "  √âpoca   5 | train_acc=0.4968 | val_acc=0.4643 | LR=0.00250\n",
      "  √âpoca  10 | train_acc=0.4935 | val_acc=0.4661 | LR=0.00854\n",
      "  √âpoca  15 | train_acc=0.5228 | val_acc=0.4863 | LR=0.00250\n",
      "  √âpoca  20 | train_acc=0.5145 | val_acc=0.4698 | LR=0.00996\n",
      "  √âpoca  25 | train_acc=0.5160 | val_acc=0.4725 | LR=0.00854\n",
      "  Early stopping en √©poca 26 (mejor val_acc=0.4881)\n",
      "‚Ü≥ Curva de entrenamiento guardada: training_curve_fold4.png\n",
      "[Fold 4/5] Global acc=0.4685\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.6258    0.4619    0.5315       420\n",
      "       Right     0.5687    0.4238    0.4857       420\n",
      "  Both Fists     0.3613    0.6762    0.4710       420\n",
      "   Both Feet     0.4834    0.3119    0.3792       420\n",
      "\n",
      "    accuracy                         0.4685      1680\n",
      "   macro avg     0.5098    0.4685    0.4668      1680\n",
      "weighted avg     0.5098    0.4685    0.4668      1680\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5565 | sujetos=20\n",
      "  Œî(FT-Global) = +0.0881\n",
      "\n",
      "[Fold 5/5] Entrenando modelo global... (n_train=5880 | n_val=1092 | n_test=1680)\n",
      "  √âpoca   1 | train_acc=0.4259 | val_acc=0.4322 | LR=0.00500\n",
      "  √âpoca   5 | train_acc=0.4869 | val_acc=0.4780 | LR=0.00250\n",
      "  √âpoca  10 | train_acc=0.5027 | val_acc=0.4918 | LR=0.00854\n",
      "  √âpoca  15 | train_acc=0.5206 | val_acc=0.4890 | LR=0.00250\n",
      "  √âpoca  20 | train_acc=0.4794 | val_acc=0.4533 | LR=0.00996\n",
      "  √âpoca  25 | train_acc=0.5037 | val_acc=0.4789 | LR=0.00854\n",
      "  Early stopping en √©poca 29 (mejor val_acc=0.4963)\n",
      "‚Ü≥ Curva de entrenamiento guardada: training_curve_fold5.png\n",
      "[Fold 5/5] Global acc=0.5304\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.6320    0.5643    0.5962       420\n",
      "       Right     0.5773    0.5690    0.5731       420\n",
      "  Both Fists     0.4447    0.5548    0.4936       420\n",
      "   Both Feet     0.4959    0.4333    0.4625       420\n",
      "\n",
      "    accuracy                         0.5304      1680\n",
      "   macro avg     0.5375    0.5304    0.5314      1680\n",
      "weighted avg     0.5375    0.5304    0.5314      1680\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5673 | sujetos=20\n",
      "  Œî(FT-Global) = +0.0369\n",
      "\n",
      "============================================================\n",
      "RESULTADOS FINALES\n",
      "============================================================\n",
      "Global folds: ['0.4756', '0.5306', '0.4858', '0.4685', '0.5304']\n",
      "Global mean: 0.4982\n",
      "Fine-tune PROGRESIVO folds: ['0.5045', '0.5918', '0.5289', '0.5565', '0.5673']\n",
      "Fine-tune PROGRESIVO mean: 0.5498\n",
      "Œî(FT-Global) mean: +0.0516\n",
      "\n",
      "‚Ü≥ Matriz de confusi√≥n guardada: confusion_global_allfolds.png\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# EEGNet + protocolo global + fine-tuning progresivo por sujeto (augments + SGDR + tweaks)\n",
    "# Variant-B:\n",
    "# - F1=24 (igual), fc=256\n",
    "# - kernel_t=80 (‚Üë contexto temporal), pool2_t=5 (‚Üì submuestreo)\n",
    "# - Dropout: drop1=0.40, drop2=0.60, chdrop=0.15\n",
    "# - Loss ponderada por clase (+20% a Both Fists) con soporte mixup (WeightedSoftCrossEntropy)\n",
    "# - Mixup alpha=0.30\n",
    "# - SGDR con T0=6, Tmult=2 + warmup lineal 2 √©pocas\n",
    "# - TTA (n=5, jitter ¬±25 ms) en evaluaci√≥n\n",
    "\n",
    "import os, re, math, random, json, itertools, copy\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold, StratifiedShuffleSplit, GroupShuffleSplit\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# CONFIGURACI√ìN GENERAL\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "FOLDS_DIR = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dispositivo y semilla\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE); np.random.seed(RANDOM_STATE); random.seed(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "# Escenario y ventana\n",
    "CLASS_SCENARIO = '4c'\n",
    "WINDOW_MODE = '6s'   # 6s\n",
    "FS = 160.0\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Entrenamiento global\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS_GLOBAL = 100\n",
    "LR_INIT = 1e-2\n",
    "SGDR_T0 = 6\n",
    "SGDR_Tmult = 2\n",
    "WARMUP_EPOCHS = 2  # <-- warmup lineal antes de SGDR\n",
    "\n",
    "# Validaci√≥n/ES global\n",
    "GLOBAL_VAL_SPLIT = 0.15\n",
    "GLOBAL_PATIENCE  = 10\n",
    "LOG_EVERY        = 5\n",
    "\n",
    "# Fine-tuning por sujeto (protocolo robusto)\n",
    "CALIB_CV_FOLDS = 4\n",
    "FT_EPOCHS = 30\n",
    "FT_BASE_LR = 5e-5\n",
    "FT_HEAD_LR = 1e-3\n",
    "FT_L2SP = 1e-4\n",
    "FT_PATIENCE = 5\n",
    "FT_VAL_RATIO = 0.2\n",
    "\n",
    "# Normalizaci√≥n por √©poca canal-a-canal (z-score)\n",
    "NORM_EPOCH_ZSCORE = True\n",
    "\n",
    "# Sujetos excluidos\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "\n",
    "# Runs\n",
    "MI_RUNS_LR = [4, 8, 12]\n",
    "MI_RUNS_OF = [6, 10, 14]\n",
    "BASELINE_RUNS_EO = [1]\n",
    "\n",
    "# Canales (8 con FCz)\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','FCz']\n",
    "\n",
    "# Augments\n",
    "MIXUP_ALPHA = 0.30  # Variant-B (antes 0.2‚Äì0.25)\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES DE CANALES\n",
    "# =========================\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = re.sub(r'([A-Za-z])0([0-9])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])Z$', r'\\1z', s)\n",
    "    s = s.replace('fp', 'Fp').replace('FP', 'Fp')\n",
    "    s = ''.join(ch.upper() if ch != 'z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {}\n",
    "    for ch in raw.ch_names:\n",
    "        lab = normalize_label(ch)\n",
    "        lab = lab[:-1] + 'z' if lab.endswith('Z') else lab\n",
    "        lab = re.sub(r'([A-Z])Z$', r'\\1z', lab)\n",
    "        mapping[ch] = lab\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired_channels=EXPECTED_8):\n",
    "    missing = [ch for ch in desired_channels if ch not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"Warning: faltan canales {missing} en archivo {getattr(raw,'filenames', [''])[0]}\")\n",
    "        return None\n",
    "    raw.reorder_channels([ch for ch in raw.ch_names if ch in desired_channels] +\n",
    "                         [ch for ch in raw.ch_names if ch not in desired_channels])\n",
    "    raw.pick_channels(desired_channels, ordered=True)\n",
    "    return raw\n",
    "\n",
    "# =========================\n",
    "# LECTURA DE EDF y EVENTOS\n",
    "# =========================\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(run_id:int):\n",
    "    if run_id in MI_RUNS_LR: return 'LR'\n",
    "    if run_id in MI_RUNS_OF: return 'OF'\n",
    "    if run_id in BASELINE_RUNS_EO: return 'EO'\n",
    "    return None\n",
    "\n",
    "# --- Notch adaptativo: lee CSV si existe (subject, run, snr50_db, snr60_db)\n",
    "_SNR_TABLE = None\n",
    "def _load_snr_table():\n",
    "    global _SNR_TABLE\n",
    "    if _SNR_TABLE is not None:\n",
    "        return _SNR_TABLE\n",
    "    csv_path = PROJ / 'reports' / 'psd_mains' / 'psd_mains_summary.csv'\n",
    "    if csv_path.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            _SNR_TABLE = df\n",
    "        except Exception as e:\n",
    "            print(f\"[SNR] No se pudo leer {csv_path}: {e}\")\n",
    "            _SNR_TABLE = None\n",
    "    return _SNR_TABLE\n",
    "\n",
    "def _decide_notch(subject, run, th_db=10.0):\n",
    "    df = _load_snr_table()\n",
    "    if df is None:  # default\n",
    "        return 60.0\n",
    "    row = df[(df['subject']==subject) & (df['run']==run)]\n",
    "    if row.empty:\n",
    "        return 60.0\n",
    "    snr50 = float(row['snr50_db'].iloc[0]); snr60 = float(row['snr60_db'].iloc[0])\n",
    "    if snr60 >= th_db and snr60 >= snr50: return 60.0\n",
    "    if snr50 >= th_db and snr50 >  snr60: return 50.0\n",
    "    return None  # no notch si no sobresale\n",
    "\n",
    "def read_raw_edf(path: Path):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        mont = mne.channels.make_standard_montage('standard_1020')\n",
    "        raw.set_montage(mont, on_missing='ignore')\n",
    "    except Exception:\n",
    "        pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad=\"auto\")\n",
    "    raw = ensure_channels_order(raw, EXPECTED_8)\n",
    "    if raw is None:\n",
    "        return None\n",
    "\n",
    "    # Notch adaptativo (por SNR); si no hay CSV, por defecto 60 Hz\n",
    "    sid, rid = parse_subject_run(path)\n",
    "    notch = _decide_notch(sid, rid)\n",
    "    if notch is not None:\n",
    "        raw.notch_filter(freqs=[float(notch)], picks='eeg', method='spectrum_fit', phase='zero')\n",
    "\n",
    "    return raw\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    if raw.annotations is None or len(raw.annotations) == 0:\n",
    "        return []\n",
    "    def _norm(s): return str(s).strip().upper().replace(' ', '')\n",
    "    res = []\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = _norm(desc)\n",
    "        if tag in ('T1','T2'):\n",
    "            res.append((float(onset), tag))\n",
    "    res.sort()\n",
    "    dedup = []\n",
    "    last_t1 = last_t2 = -1e9\n",
    "    for t, tag in res:\n",
    "        if tag == 'T1':\n",
    "            if (t - last_t1) >= 0.5: dedup.append((t, tag)); last_t1 = t\n",
    "        else:\n",
    "            if (t - last_t2) >= 0.5: dedup.append((t, tag)); last_t2 = t\n",
    "    return dedup\n",
    "\n",
    "# =========================\n",
    "# CONSTRUCCI√ìN DE DATASETS\n",
    "# =========================\n",
    "def subjects_available():\n",
    "    subs = []\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try: sid = int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir / f\"S{sid:03d}R{r:02d}.edf\").exists() for r in (MI_RUNS_LR + MI_RUNS_OF))\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "def extract_trials_from_run(edf_path: Path, scenario: str, window_mode: str):\n",
    "    subj, run = parse_subject_run(edf_path)\n",
    "    kind = run_kind(run)\n",
    "    if kind not in ('LR','OF','EO'):\n",
    "        return ([], [])\n",
    "\n",
    "    raw = read_raw_edf(edf_path)\n",
    "    if raw is None:\n",
    "        return ([], [])\n",
    "\n",
    "    data = raw.get_data()\n",
    "    fs = raw.info['sfreq']\n",
    "    assert abs(fs - FS) < 1e-6\n",
    "\n",
    "    out = []\n",
    "\n",
    "    if kind in ('LR','OF'):\n",
    "        events = collect_events_T1T2(raw)\n",
    "        if window_mode == '3s':\n",
    "            rel_start, rel_end = 0.0, 3.0\n",
    "        else:\n",
    "            rel_start, rel_end = -1.0, 5.0\n",
    "\n",
    "        for onset_sec, tag in events:\n",
    "            if kind == 'LR':\n",
    "                if tag == 'T1': label = 'L'\n",
    "                elif tag == 'T2': label = 'R'\n",
    "                else: continue\n",
    "            else:\n",
    "                if tag == 'T1': label = 'BFISTS'\n",
    "                elif tag == 'T2': label = 'BFEET'\n",
    "                else: continue\n",
    "\n",
    "            if scenario == '2c' and label not in ('L','R'): continue\n",
    "            if scenario == '3c' and label not in ('L','R','BFISTS'): continue\n",
    "            if scenario == '4c' and label not in ('L','R','BFISTS','BFEET'): continue\n",
    "\n",
    "            s = int(round((raw.first_time + onset_sec + rel_start) * fs))\n",
    "            e = int(round((raw.first_time + onset_sec + rel_end) * fs))\n",
    "            if s < 0 or e > data.shape[1]:\n",
    "                continue\n",
    "\n",
    "            seg = data[:, s:e].T.astype(np.float32)\n",
    "            if NORM_EPOCH_ZSCORE:\n",
    "                seg = (seg - seg.mean(axis=0, keepdims=True)) / (seg.std(axis=0, keepdims=True) + 1e-6)\n",
    "\n",
    "            if label == 'L':       y = 0\n",
    "            elif label == 'R':     y = 1\n",
    "            elif label == 'BFISTS':y = 2\n",
    "            elif label == 'BFEET': y = 3\n",
    "            else: continue\n",
    "\n",
    "            out.append((seg, y, subj))\n",
    "\n",
    "    elif kind == 'EO':\n",
    "        return ([], raw.ch_names)\n",
    "\n",
    "    return out, raw.ch_names\n",
    "\n",
    "def build_dataset_all(subjects, scenario='4c', window_mode='3s'):\n",
    "    X, y, groups = [], [], []\n",
    "    ch_template = None\n",
    "\n",
    "    for s in tqdm(subjects, desc=\"Construyendo dataset (RAW)\"):\n",
    "        sdir = DATA_RAW / f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "\n",
    "        trials_L, trials_R, trials_FISTS, trials_FEET = [], [], [], []\n",
    "\n",
    "        for r in MI_RUNS_LR:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 0: trials_L.append(seg)\n",
    "                elif lab == 1: trials_R.append(seg)\n",
    "\n",
    "        for r in MI_RUNS_OF:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 2: trials_FISTS.append(seg)\n",
    "                elif lab == 3: trials_FEET.append(seg)\n",
    "\n",
    "        need_per_class = 21\n",
    "        def pick(trials, n, rng):\n",
    "            if len(trials) < n:\n",
    "                idx = rng.choice(len(trials), size=n, replace=True)\n",
    "                return [trials[i] for i in idx]\n",
    "            rng.shuffle(trials)\n",
    "            return trials[:n]\n",
    "\n",
    "        rng = check_random_state(RANDOM_STATE + s)\n",
    "        if len(trials_L)==0 or len(trials_R)==0 or len(trials_FISTS)==0 or len(trials_FEET)==0:\n",
    "            continue\n",
    "\n",
    "        Lp  = pick(trials_L,     need_per_class, rng)\n",
    "        Rp  = pick(trials_R,     need_per_class, rng)\n",
    "        FIp = pick(trials_FISTS, need_per_class, rng)\n",
    "        FEp = pick(trials_FEET,  need_per_class, rng)\n",
    "\n",
    "        pack = [(Lp, 0), (Rp, 1), (FIp, 2), (FEp, 3)]\n",
    "        for segs, lab in pack:\n",
    "            for seg in segs:\n",
    "                X.append(seg); y.append(lab); groups.append(s)\n",
    "\n",
    "    X = np.stack(X, axis=0)\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "    groups = np.asarray(groups, dtype=np.int64)\n",
    "\n",
    "    n, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Dataset construido: N={n} | T={T} | C={C} | clases={n_classes} | sujetos √∫nicos={len(np.unique(groups))}\")\n",
    "    return X, y, groups, ch_template\n",
    "\n",
    "# =========================\n",
    "# AUGMENTS (solo train)\n",
    "# =========================\n",
    "def do_time_jitter(x, max_ms=50, fs=160.0):\n",
    "    # x: (B,1,T,C) torch.float\n",
    "    max_shift = int(round(max_ms/1000.0 * fs))\n",
    "    if max_shift <= 0: return x\n",
    "    B,_,T,C = x.shape\n",
    "    shifts = torch.randint(low=-max_shift, high=max_shift+1, size=(B,), device=x.device)\n",
    "    out = torch.empty_like(x)\n",
    "    for i,s in enumerate(shifts):\n",
    "        if s==0: out[i] = x[i]; continue\n",
    "        if s>0:\n",
    "            out[i,:,s:,:] = x[i,:,:T-s,:]\n",
    "            out[i,:,:s,:] = 0\n",
    "        else:\n",
    "            s = -s\n",
    "            out[i,:,:T-s,:] = x[i,:,s:,:]\n",
    "            out[i,:,T-s:,:] = 0\n",
    "    return out\n",
    "\n",
    "def do_gaussian_noise(x, sigma=0.01):\n",
    "    if sigma<=0: return x\n",
    "    return x + sigma*torch.randn_like(x)\n",
    "\n",
    "def do_temporal_cutout(x, min_ms=100, max_ms=150, fs=160.0):\n",
    "    B,_,T,C = x.shape\n",
    "    Lmin = int(round(min_ms/1000.0*fs))\n",
    "    Lmax = int(round(max_ms/1000.0*fs))\n",
    "    if Lmin<=0 or Lmax<=0 or Lmin>Lmax: return x\n",
    "    out = x.clone()\n",
    "    for i in range(B):\n",
    "        L = random.randint(Lmin, Lmax)\n",
    "        if L>=T: continue\n",
    "        s = random.randint(0, T-L)\n",
    "        out[i,:,s:s+L,:] = 0\n",
    "    return out\n",
    "\n",
    "def do_temporal_cutout_masked(x, y, classes_mask={2,3}, min_ms=30, max_ms=60, fs=160.0):\n",
    "    # Aplica cutout solo si y pertenece a classes_mask\n",
    "    B,_,T,C = x.shape\n",
    "    if B == 0: return x\n",
    "    Lmin = int(round(min_ms/1000.0*fs))\n",
    "    Lmax = int(round(max_ms/1000.0*fs))\n",
    "    if Lmin<=0 or Lmax<=0 or Lmin>Lmax: return x\n",
    "    out = x.clone()\n",
    "    y_np = y.detach().cpu().numpy()\n",
    "    for i in range(B):\n",
    "        if int(y_np[i]) not in classes_mask:\n",
    "            continue\n",
    "        L = random.randint(Lmin, Lmax)\n",
    "        if L>=T: continue\n",
    "        s = random.randint(0, T-L)\n",
    "        out[i,:,s:s+L,:] = 0\n",
    "    return out\n",
    "\n",
    "def mixup_batch(x, y, n_classes, alpha=MIXUP_ALPHA):\n",
    "    if alpha<=0:\n",
    "        y_onehot = torch.nn.functional.one_hot(y, num_classes=n_classes).float()\n",
    "        return x, y_onehot, 1.0\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    perm = torch.randperm(x.size(0), device=x.device)\n",
    "    x_mix = lam*x + (1-lam)*x[perm]\n",
    "    y_a = torch.nn.functional.one_hot(y, num_classes=n_classes).float()\n",
    "    y_b = y_a[perm]\n",
    "    y_mix = lam*y_a + (1-lam)*y_b\n",
    "    return x_mix, y_mix, lam\n",
    "\n",
    "class WeightedSoftCrossEntropy(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-entropy suave (targets en probas, p.ej. mixup) con pesos por clase.\n",
    "    \"\"\"\n",
    "    def __init__(self, class_weights=None, label_smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.register_buffer('w', None if class_weights is None else class_weights.clone().float())\n",
    "        self.ls = float(label_smoothing)\n",
    "\n",
    "    def forward(self, logits, target_probs):\n",
    "        # label smoothing: mezcla target_probs con uniforme\n",
    "        if self.ls > 0:\n",
    "            K = logits.size(1)\n",
    "            target_probs = (1-self.ls)*target_probs + self.ls*(1.0/K)\n",
    "        logp = torch.log_softmax(logits, dim=1)  # (B,C)\n",
    "        loss_per_class = -(target_probs * logp)  # (B,C)\n",
    "        if self.w is not None:\n",
    "            loss_per_class = loss_per_class * self.w.unsqueeze(0)  # pesar por clase\n",
    "        loss = loss_per_class.sum(dim=1).mean()\n",
    "        return loss\n",
    "\n",
    "# =========================\n",
    "# EEGNet (Lawhern et al., 2018) adaptado a (B,1,T,C) + ChannelDropout\n",
    "# =========================\n",
    "class ChannelDropout(nn.Module):\n",
    "    def __init__(self, p=0.15):  # Variant-B: 0.15\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    def forward(self, x):\n",
    "        if not self.training or self.p<=0: return x\n",
    "        B,_,T,C = x.shape\n",
    "        mask = (torch.rand(B,1,1,C, device=x.device) > self.p).float()\n",
    "        return x * mask\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Entrada: x de forma (B, 1, T, C)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_ch: int, n_classes: int,\n",
    "                 F1: int = 24, D: int = 2, kernel_t: int = 80, k_sep: int = 16,\n",
    "                 pool1_t: int = 4, pool2_t: int = 5,\n",
    "                 drop1_p: float = 0.40, drop2_p: float = 0.60,\n",
    "                 chdrop_p: float = 0.15):\n",
    "        super().__init__()\n",
    "        self.n_ch = n_ch\n",
    "        self.n_classes = n_classes\n",
    "        self.F1 = F1\n",
    "        self.D = D\n",
    "        self.F2 = F1 * D\n",
    "        self.kernel_t = kernel_t\n",
    "        self.k_sep = k_sep\n",
    "        self.pool1_t = pool1_t\n",
    "        self.pool2_t = pool2_t\n",
    "\n",
    "        self.chdrop = ChannelDropout(p=chdrop_p)\n",
    "\n",
    "        # Bloque 1: temporal\n",
    "        self.conv_temporal = nn.Conv2d(1, F1, kernel_size=(kernel_t, 1),\n",
    "                                       padding=(kernel_t // 2, 0), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(F1, momentum=0.99, eps=1e-3)\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "        # Bloque 2: depthwise (espacial)\n",
    "        self.conv_depthwise = nn.Conv2d(F1, self.F2, kernel_size=(1, n_ch),\n",
    "                                        groups=F1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.F2, momentum=0.99, eps=1e-3)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=(pool1_t, 1), stride=(pool1_t, 1))\n",
    "        self.drop1 = nn.Dropout(drop1_p)\n",
    "\n",
    "        # Bloque 3: separable temporal\n",
    "        self.conv_sep_depth = nn.Conv2d(self.F2, self.F2, kernel_size=(k_sep, 1),\n",
    "                                        groups=self.F2, padding=(k_sep // 2, 0), bias=False)\n",
    "        self.conv_sep_point = nn.Conv2d(self.F2, self.F2, kernel_size=(1, 1), bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.F2, momentum=0.99, eps=1e-3)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(pool2_t, 1), stride=(pool2_t, 1))\n",
    "        self.drop2 = nn.Dropout(drop2_p)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Cabeza din√°mica\n",
    "        self.fc = None\n",
    "        self.out = None\n",
    "        self._T_in = None\n",
    "\n",
    "    def _build_head(self, T_in: int, device: torch.device):\n",
    "        T1 = T_in // self.pool1_t\n",
    "        T2 = T1 // self.pool2_t\n",
    "        feat_dim = self.F2 * T2 * 1\n",
    "        self.fc  = nn.Linear(feat_dim, 256, bias=True).to(device)  # Variant-B: 256\n",
    "        self.out = nn.Linear(256, self.n_classes, bias=True).to(device)\n",
    "        self._T_in = T_in\n",
    "\n",
    "    def ensure_head(self, T_in: int, device: torch.device):\n",
    "        if (self.fc is None) or (self.out is None) or (self._T_in != T_in):\n",
    "            self._build_head(T_in, device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, _, T, C = x.shape\n",
    "        self.ensure_head(T, x.device)\n",
    "\n",
    "        x = self.chdrop(x)  # ChannelDropout\n",
    "\n",
    "        z = self.conv_temporal(x)\n",
    "        z = self.bn1(z); z = self.act(z)\n",
    "\n",
    "        z = self.conv_depthwise(z)   # (B, F2, T, 1)\n",
    "        z = self.bn2(z); z = self.act(z)\n",
    "        z = self.pool1(z)\n",
    "        z = self.drop1(z)\n",
    "\n",
    "        z = self.conv_sep_depth(z)\n",
    "        z = self.conv_sep_point(z)\n",
    "        z = self.bn3(z); z = self.act(z)\n",
    "        z = self.pool2(z)\n",
    "        z = self.drop2(z)\n",
    "\n",
    "        z = self.flatten(z)\n",
    "        z = self.fc(z); z = self.act(z)\n",
    "        z = self.out(z)\n",
    "        return z\n",
    "\n",
    "# =========================\n",
    "# TORCH DATASET\n",
    "# =========================\n",
    "class EEGTrials(Dataset):\n",
    "    def __init__(self, X, y, groups):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.g = groups.astype(np.int64)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        x = np.expand_dims(x, 0)                 # (1, T, C)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx])\n",
    "\n",
    "CLASS_NAMES_4C = ['Left', 'Right', 'Both Fists', 'Both Feet']\n",
    "\n",
    "# =========================\n",
    "# UTIL: MAX-NORM\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def apply_max_norm(model, max_value=2.0, p=2.0):\n",
    "    layers = []\n",
    "    if hasattr(model, 'conv_depthwise'): layers.append(model.conv_depthwise)\n",
    "    if hasattr(model, 'conv_sep_point'): layers.append(model.conv_sep_point)\n",
    "    if hasattr(model, 'fc'):             layers.append(model.fc)\n",
    "    if hasattr(model, 'out'):            layers.append(model.out)\n",
    "    for layer in layers:\n",
    "        if hasattr(layer, 'weight') and layer.weight is not None:\n",
    "            w = layer.weight.data\n",
    "            norms = w.view(w.size(0), -1).norm(p=p, dim=1, keepdim=True)\n",
    "            desired = torch.clamp(norms, max=max_value)\n",
    "            w.view(w.size(0), -1).mul_(desired / (1e-8 + norms))\n",
    "\n",
    "# =========================\n",
    "# ENTRENAMIENTO / EVALUACI√ìN\n",
    "# =========================\n",
    "def build_weighted_sampler(y, groups):\n",
    "    # pesos inversos por clase y por sujeto ‚Üí promueve balance en cada batch\n",
    "    y = np.asarray(y); groups = np.asarray(groups)\n",
    "    class_counts = np.bincount(y, minlength=len(np.unique(y))).astype(float)\n",
    "    class_w = 1.0 / class_counts[y]\n",
    "    subj_vals, subj_counts = np.unique(groups, return_counts=True)\n",
    "    subj_map = {s:c for s,c in zip(subj_vals, subj_counts)}\n",
    "    subj_w = np.array([1.0/subj_map[g] for g in groups], dtype=float)\n",
    "    w = class_w * subj_w\n",
    "    w = w / w.mean()\n",
    "    w_t = torch.from_numpy(w).float()\n",
    "    sampler = WeightedRandomSampler(weights=w_t, num_samples=len(w_t), replacement=True)\n",
    "    return sampler\n",
    "\n",
    "def make_class_weight_tensor(y_indices, n_classes, boost_bfists=1.20):\n",
    "    # pesos inversos por clase, normalizados; clase 2 (*Both Fists*) con boost\n",
    "    counts = np.bincount(y_indices, minlength=n_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    w = counts.sum() / counts\n",
    "    w = w / w.mean()\n",
    "    w[2] *= boost_bfists\n",
    "    w = w / w.mean()\n",
    "    return torch.tensor(w, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "def train_epoch(model, loader, opt, criterion_soft, n_classes,\n",
    "                do_aug=True, fs=160.0, maxnorm=None):\n",
    "    model.train()\n",
    "    for xb, yb, _ in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "\n",
    "        # ---- AUGMENTS (orden: jitter ‚Üí noise ‚Üí cutout focalizado ‚Üí mixup) ----\n",
    "        if do_aug:\n",
    "            xb = do_time_jitter(xb, max_ms=50, fs=fs)\n",
    "            xb = do_gaussian_noise(xb, sigma=0.01)\n",
    "            xb = do_temporal_cutout_masked(xb, yb, classes_mask={2,3}, min_ms=30, max_ms=60, fs=fs)\n",
    "            xb, yt, _ = mixup_batch(xb, yb, n_classes=n_classes, alpha=MIXUP_ALPHA)\n",
    "            logits = model(xb)\n",
    "            loss = criterion_soft(logits, yt)\n",
    "        else:\n",
    "            logits = model(xb)\n",
    "            # yb como √≠ndices ‚Üí convertir a one-hot para criterio suave\n",
    "            yt = torch.nn.functional.one_hot(yb, num_classes=n_classes).float()\n",
    "            loss = criterion_soft(logits, yt)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if maxnorm is not None:\n",
    "            apply_max_norm(model, max_value=maxnorm, p=2.0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _predict_tta(model, xb, n=5, fs=160.0):\n",
    "    outs = []\n",
    "    for _ in range(n):\n",
    "        xj = do_time_jitter(xb, max_ms=25, fs=fs)\n",
    "        outs.append(model(xj))\n",
    "    return torch.stack(outs, dim=0).mean(dim=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_with_preds(model, loader, use_tta=True, tta_n=5):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for xb, yb, _ in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        if use_tta:\n",
    "            logits = _predict_tta(model, xb, n=tta_n, fs=FS)\n",
    "        else:\n",
    "            logits = model(xb)\n",
    "        pred = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "        y_pred.extend(pred)\n",
    "        y_true.extend(yb.numpy().tolist())\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    return y_true, y_pred, float(acc)\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes, title, fname):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.nan_to_num(cm_norm)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm_norm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "        plt.text(j, i, format(cm_norm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm_norm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label'); plt.xlabel('Predicted label')\n",
    "    plt.tight_layout(); plt.savefig(fname, dpi=150, bbox_inches='tight'); plt.close()\n",
    "\n",
    "def plot_training_curves(history, fname):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(history['train_acc'], label='train_acc')\n",
    "    plt.plot(history['val_acc'], label='val_acc')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
    "    plt.title('Training curve'); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(fname, dpi=150); plt.close()\n",
    "\n",
    "def print_report(y_true, y_pred, classes):\n",
    "    print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
    "\n",
    "# =========================\n",
    "# FINE-TUNING PROGRESIVO por sujeto (con validaci√≥n interna)\n",
    "# =========================\n",
    "def _param_groups(model, mode):\n",
    "    if mode == 'out':\n",
    "        train = list(model.out.parameters())\n",
    "    elif mode == 'head':\n",
    "        train = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "    elif mode == 'spatial+head':\n",
    "        train = (list(model.conv_depthwise.parameters()) +\n",
    "                 list(model.bn2.parameters()) +\n",
    "                 list(model.conv_sep_depth.parameters()) +\n",
    "                 list(model.conv_sep_point.parameters()) +\n",
    "                 list(model.bn3.parameters()) +\n",
    "                 list(model.fc.parameters()) +\n",
    "                 list(model.out.parameters()))\n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "    return train\n",
    "\n",
    "def _freeze_for_mode(model, mode):\n",
    "    for p in model.parameters(): p.requires_grad = False\n",
    "    if mode == 'out':\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'head':\n",
    "        for p in model.fc.parameters():  p.requires_grad = True\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'spatial+head':\n",
    "        for p in model.conv_depthwise.parameters(): p.requires_grad = True\n",
    "        for p in model.bn2.parameters():           p.requires_grad = True\n",
    "        for p in model.conv_sep_depth.parameters():p.requires_grad = True\n",
    "        for p in model.conv_sep_point.parameters():p.requires_grad = True\n",
    "        for p in model.bn3.parameters():           p.requires_grad = True\n",
    "        for p in model.fc.parameters():            p.requires_grad = True\n",
    "        for p in model.out.parameters():           p.requires_grad = True\n",
    "\n",
    "def _class_weights(y_np, n_classes):\n",
    "    counts = np.bincount(y_np, minlength=n_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    weights = counts.sum() / counts\n",
    "    weights = weights / weights.mean()\n",
    "    return torch.tensor(weights, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "def _train_one_mode(model, X_cal, y_cal, n_classes, mode,\n",
    "                    epochs=FT_EPOCHS, batch_size=32,\n",
    "                    head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                    l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=RANDOM_STATE)\n",
    "    (tr_idx, va_idx), = sss.split(X_cal, y_cal)\n",
    "    Xtr, ytr = X_cal[tr_idx], y_cal[tr_idx]\n",
    "    Xva, yva = X_cal[va_idx], y_cal[va_idx]\n",
    "\n",
    "    ds_tr = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xtr).float().unsqueeze(1),\n",
    "        torch.from_numpy(ytr).long()\n",
    "    )\n",
    "    ds_va = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xva).float().unsqueeze(1),\n",
    "        torch.from_numpy(yva).long()\n",
    "    )\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    _freeze_for_mode(model, mode)\n",
    "\n",
    "    if mode == 'spatial+head':\n",
    "        base_params = (list(model.conv_depthwise.parameters()) +\n",
    "                       list(model.bn2.parameters()) +\n",
    "                       list(model.conv_sep_depth.parameters()) +\n",
    "                       list(model.conv_sep_point.parameters()) +\n",
    "                       list(model.bn3.parameters()))\n",
    "        head_params = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "        train_params = base_params + head_params\n",
    "        opt = optim.Adam([\n",
    "            {\"params\": base_params, \"lr\": base_lr},\n",
    "            {\"params\": head_params, \"lr\": head_lr},\n",
    "        ])\n",
    "    else:\n",
    "        train_params = _param_groups(model, mode)\n",
    "        opt = optim.Adam(train_params, lr=head_lr)\n",
    "\n",
    "    ref = [p.detach().clone().to(p.device) for p in train_params]\n",
    "    class_w = _class_weights(ytr, n_classes)\n",
    "    crit = nn.CrossEntropyLoss(weight=class_w)\n",
    "\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_val = float('inf'); bad = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            reg = 0.0\n",
    "            for p_cur, p_ref in zip(train_params, ref):\n",
    "                reg = reg + torch.sum((p_cur - p_ref)**2)\n",
    "            loss = loss + l2sp_lambda * reg\n",
    "            loss.backward(); opt.step()\n",
    "            apply_max_norm(model, max_value=2.0, p=2.0)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0; nval = 0\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                loss = crit(logits, yb)\n",
    "                val_loss += loss.item() * xb.size(0); nval += xb.size(0)\n",
    "            val_loss /= max(1, nval)\n",
    "\n",
    "        if val_loss + 1e-7 < best_val:\n",
    "            best_val = val_loss; bad = 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience: break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_numpy(model, X_np, device):\n",
    "    model.eval()\n",
    "    xb = torch.from_numpy(X_np).float().unsqueeze(1).to(device)\n",
    "    logits = model(xb)\n",
    "    return logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "def subject_cv_finetune_predict_progressive(model_global, Xs, ys, device,\n",
    "                                            n_splits=CALIB_CV_FOLDS, n_classes=4):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    y_true_full = np.empty_like(ys); y_pred_full = np.empty_like(ys)\n",
    "\n",
    "    for tr_idx, te_idx in skf.split(Xs, ys):\n",
    "        Xcal, ycal = Xs[tr_idx], ys[tr_idx]\n",
    "        Xho,  yho  = Xs[te_idx], ys[te_idx]\n",
    "\n",
    "        m_out = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_out, Xcal, ycal, n_classes, mode='out',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_out = predict_numpy(m_out, Xho, device); acc_out = (yhat_out == yho).mean()\n",
    "\n",
    "        m_head = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_head, Xcal, ycal, n_classes, mode='head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_head = predict_numpy(m_head, Xho, device); acc_head = (yhat_head == yho).mean()\n",
    "\n",
    "        m_sp = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_sp, Xcal, ycal, n_classes, mode='spatial+head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                        l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_sp = predict_numpy(m_sp, Xho, device); acc_sp = (yhat_sp == yho).mean()\n",
    "\n",
    "        best_idx = np.argmax([acc_out, acc_head, acc_sp])\n",
    "        yhat_best = [yhat_out, yhat_head, yhat_sp][best_idx]\n",
    "\n",
    "        y_true_full[te_idx] = yho; y_pred_full[te_idx] = yhat_best\n",
    "\n",
    "    return y_true_full, y_pred_full\n",
    "\n",
    "# =========================\n",
    "# FOLDS JSON helpers\n",
    "# =========================\n",
    "def save_group_folds_json_with_indices(subject_ids_str, groups_array, n_splits, out_json_path,\n",
    "                                       created_by=\"dose_experiment\", description=None):\n",
    "    out_json_path = Path(out_json_path)\n",
    "    unique_subjects_int = sorted(np.unique(groups_array).tolist())\n",
    "    subject_ids = [f\"S{sid:03d}\" for sid in unique_subjects_int]\n",
    "\n",
    "    if len(subject_ids) < n_splits:\n",
    "        raise ValueError(f\"n_splits={n_splits} mayor que n√∫mero de sujetos={len(subject_ids)}\")\n",
    "\n",
    "    groups = np.arange(len(subject_ids))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    folds = []\n",
    "    fold_i = 0\n",
    "    for train_idx_grp, test_idx_grp in gkf.split(groups, groups, groups):\n",
    "        fold_i += 1\n",
    "        train_sids = [subject_ids[int(i)] for i in train_idx_grp]\n",
    "        test_sids  = [subject_ids[int(i)] for i in test_idx_grp]\n",
    "\n",
    "        train_sids_int = [int(s[1:]) for s in train_sids]\n",
    "        test_sids_int  = [int(s[1:]) for s in test_sids]\n",
    "\n",
    "        tr_idx = np.where(np.isin(groups_array, train_sids_int))[0].tolist()\n",
    "        te_idx = np.where(np.isin(groups_array, test_sids_int))[0].tolist()\n",
    "\n",
    "        folds.append({\n",
    "            \"fold\": int(fold_i),\n",
    "            \"train\": train_sids,\n",
    "            \"test\": test_sids,\n",
    "            \"tr_idx\": tr_idx,\n",
    "            \"te_idx\": te_idx\n",
    "        })\n",
    "\n",
    "    payload = {\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"created_by\": created_by,\n",
    "        \"description\": description if description is not None else \"\",\n",
    "        \"n_splits\": int(n_splits),\n",
    "        \"n_subjects\": len(subject_ids),\n",
    "        \"subject_ids\": subject_ids,\n",
    "        \"folds\": folds\n",
    "    }\n",
    "\n",
    "    out_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Folds JSON con √≠ndices guardado ‚Üí {out_json_path}\")\n",
    "    return out_json_path\n",
    "\n",
    "def load_group_folds_json(path_json, expected_subject_ids=None, strict_check=True):\n",
    "    path_json = Path(path_json)\n",
    "    if not path_json.exists():\n",
    "        raise FileNotFoundError(f\"No existe {path_json}\")\n",
    "    with open(path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        payload = json.load(f)\n",
    "\n",
    "    subj_json = payload.get(\"subject_ids\", [])\n",
    "    if expected_subject_ids is not None:\n",
    "        expected = sorted(list(expected_subject_ids))\n",
    "        if subj_json != expected:\n",
    "            msg = (\"Los subject_ids del JSON no coinciden con expected_subject_ids.\\n\"\n",
    "                   f\"JSON has {len(subj_json)} subjects, expected {len(expected)}.\\n\"\n",
    "                   f\"First 10 JSON: {subj_json[:10]}\\nFirst 10 expected: {expected[:10]}\")\n",
    "            if strict_check:\n",
    "                raise ValueError(msg)\n",
    "            else:\n",
    "                print(\"WARNING: \" + msg)\n",
    "    return payload\n",
    "\n",
    "# =========================\n",
    "# EXPERIMENTO\n",
    "# =========================\n",
    "def run_experiment(save_folds_json=True, folds_json_path=FOLDS_DIR, folds_json_description=\"GroupKFold folds for comparison\"):\n",
    "    mne.set_log_level('WARNING')\n",
    "\n",
    "    subs = subjects_available()\n",
    "    print(f\"Sujetos elegibles: {len(subs)} ‚Üí {subs[:10]}{'...' if len(subs)>10 else ''}\")\n",
    "\n",
    "    X, y, groups, chs = build_dataset_all(subs, scenario=CLASS_SCENARIO, window_mode=WINDOW_MODE)\n",
    "    N, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Listo para entrenar: N={N} | T={T} | C={C} | clases={n_classes} | sujetos={len(np.unique(groups))}\")\n",
    "\n",
    "    ds = EEGTrials(X, y, groups)\n",
    "\n",
    "    # preparar JSON folds\n",
    "    if folds_json_path is None:\n",
    "        folds_json_path = Path(\"folds\") / f\"group_folds_{N_FOLDS}splits.json\"\n",
    "    else:\n",
    "        folds_json_path = Path(folds_json_path)\n",
    "    folds_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    unique_subs = sorted(np.unique(groups).tolist())\n",
    "    subject_ids_str = [f\"S{s:03d}\" for s in unique_subs]\n",
    "\n",
    "    if not folds_json_path.exists():\n",
    "        if not save_folds_json:\n",
    "            raise FileNotFoundError(f\"Folds JSON no encontrado en {folds_json_path} y save_folds_json=False.\")\n",
    "        save_group_folds_json_with_indices(subject_ids_str, groups, n_splits=N_FOLDS,\n",
    "                                           out_json_path=folds_json_path,\n",
    "                                           created_by=\"Joel_Clasificador\",\n",
    "                                           description=folds_json_description)\n",
    "\n",
    "    payload = load_group_folds_json(folds_json_path, expected_subject_ids=subject_ids_str, strict_check=False)\n",
    "    folds = payload[\"folds\"]\n",
    "\n",
    "    # bucle por folds\n",
    "    global_folds = []\n",
    "    ft_prog_folds = []\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "\n",
    "    for f in folds:\n",
    "        fold = f[\"fold\"]\n",
    "        tr_idx = np.asarray(f.get(\"tr_idx\", []), dtype=int)\n",
    "        te_idx = np.asarray(f.get(\"te_idx\", []), dtype=int)\n",
    "\n",
    "        if tr_idx.size == 0 or te_idx.size == 0:\n",
    "            print(f\"Advertencia: fold {fold} sin √≠ndices tr/te v√°lidos. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        # ===== Split de validaci√≥n por sujetos =====\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=GLOBAL_VAL_SPLIT, random_state=RANDOM_STATE)\n",
    "        tr_subj_idx, va_subj_idx = next(gss.split(tr_idx, groups[tr_idx], groups[tr_idx]))\n",
    "        tr_sub_idx = tr_idx[tr_subj_idx]\n",
    "        va_idx     = tr_idx[va_subj_idx]\n",
    "\n",
    "        # Sampler balanceado para train\n",
    "        sampler = build_weighted_sampler(y[tr_sub_idx], groups[tr_sub_idx])\n",
    "\n",
    "        tr_loader = DataLoader(Subset(ds, tr_sub_idx), batch_size=BATCH_SIZE, sampler=sampler, drop_last=False)\n",
    "        va_loader = DataLoader(Subset(ds, va_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "        te_loader = DataLoader(Subset(ds, te_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "        # ===== EEGNet =====\n",
    "        model = EEGNet(n_ch=C, n_classes=n_classes,\n",
    "                       F1=24, D=2, kernel_t=80, k_sep=16,\n",
    "                       pool1_t=4, pool2_t=5, drop1_p=0.40, drop2_p=0.60,\n",
    "                       chdrop_p=0.15).to(DEVICE)\n",
    "\n",
    "        # Opt y scheduler SGDR\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_INIT)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=SGDR_T0, T_mult=SGDR_Tmult)\n",
    "\n",
    "        # M√©trica r√°pida\n",
    "        def _acc(loader):\n",
    "            return evaluate_with_preds(model, loader, use_tta=True, tta_n=5)[2]\n",
    "\n",
    "        # Historia para curvas\n",
    "        history = {'train_acc': [], 'val_acc': []}\n",
    "\n",
    "        # ---- Criterio suave con ponderaci√≥n por clase (+20% BFISTS) ----\n",
    "        class_weights = make_class_weight_tensor(y[tr_sub_idx], n_classes, boost_bfists=1.20)\n",
    "        criterion_soft = WeightedSoftCrossEntropy(class_weights, label_smoothing=0.05)\n",
    "\n",
    "        # ===== Entrenamiento global =====\n",
    "        print(f\"\\n[Fold {fold}/{N_FOLDS}] Entrenando modelo global...\"\n",
    "              f\" (n_train={len(tr_sub_idx)} | n_val={len(va_idx)} | n_test={len(te_idx)})\")\n",
    "\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        best_val = -1.0\n",
    "        bad = 0\n",
    "\n",
    "        for epoch in range(1, EPOCHS_GLOBAL + 1):\n",
    "            train_epoch(model, tr_loader, opt, criterion_soft, n_classes=n_classes,\n",
    "                        do_aug=True, fs=FS, maxnorm=2.0)\n",
    "\n",
    "            # ---- Warmup + SGDR\n",
    "            cur_epoch = epoch - 1 + 1e-8\n",
    "            if epoch <= WARMUP_EPOCHS:\n",
    "                # warmup lineal de 0 ‚Üí LR_INIT\n",
    "                for pg in opt.param_groups:\n",
    "                    pg['lr'] = LR_INIT * epoch / float(WARMUP_EPOCHS)\n",
    "            else:\n",
    "                scheduler.step(cur_epoch)\n",
    "\n",
    "            # eval\n",
    "            tr_acc = _acc(tr_loader)\n",
    "            va_acc = _acc(va_loader)\n",
    "            history['train_acc'].append(tr_acc)\n",
    "            history['val_acc'].append(va_acc)\n",
    "\n",
    "            if (epoch % LOG_EVERY == 0) or epoch in (1, 10, 20, 50, 100):\n",
    "                cur_lr = opt.param_groups[0]['lr']\n",
    "                print(f\"  √âpoca {epoch:3d} | train_acc={tr_acc:.4f} | val_acc={va_acc:.4f} | LR={cur_lr:.5f}\")\n",
    "\n",
    "            if va_acc > best_val + 1e-4:\n",
    "                best_val = va_acc\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "                bad = 0\n",
    "            else:\n",
    "                bad += 1\n",
    "                if bad >= GLOBAL_PATIENCE:\n",
    "                    print(f\"  Early stopping en √©poca {epoch} (mejor val_acc={best_val:.4f})\")\n",
    "                    break\n",
    "\n",
    "        # guardar curva de entrenamiento\n",
    "        curve_path = f\"training_curve_fold{fold}.png\"\n",
    "        plot_training_curves(history, curve_path)\n",
    "        print(f\"‚Ü≥ Curva de entrenamiento guardada: {curve_path}\")\n",
    "\n",
    "        # cargar mejor estado antes de evaluar en test\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "        # ===== Evaluaci√≥n global (inter-sujeto puro) =====\n",
    "        y_true, y_pred, acc_global = evaluate_with_preds(model, te_loader, use_tta=True, tta_n=5)\n",
    "        global_folds.append(acc_global)\n",
    "        all_true.append(y_true); all_pred.append(y_pred)\n",
    "\n",
    "        print(f\"[Fold {fold}/{N_FOLDS}] Global acc={acc_global:.4f}\")\n",
    "        print_report(y_true, y_pred, CLASS_NAMES_4C)\n",
    "\n",
    "        # ---------- Fine-tuning PROGRESIVO por sujeto con 4-fold CV ----------\n",
    "        X_te, y_te, g_te = X[te_idx], y[te_idx], groups[te_idx]\n",
    "\n",
    "        y_true_ft_all, y_pred_ft_all = [], []\n",
    "        used_subjects = 0\n",
    "        for sid in np.unique(g_te):\n",
    "            idx = np.where(g_te == sid)[0]\n",
    "            Xs, ys = X_te[idx], y_te[idx]\n",
    "\n",
    "            if len(ys) < CALIB_CV_FOLDS or len(np.unique(ys)) < 2:\n",
    "                continue\n",
    "\n",
    "            y_true_subj, y_pred_subj = subject_cv_finetune_predict_progressive(\n",
    "                model, Xs, ys, DEVICE, n_splits=CALIB_CV_FOLDS, n_classes=n_classes\n",
    "            )\n",
    "            y_true_ft_all.append(y_true_subj)\n",
    "            y_pred_ft_all.append(y_pred_subj)\n",
    "            used_subjects += 1\n",
    "\n",
    "        if len(y_true_ft_all) > 0:\n",
    "            y_true_ft_all = np.concatenate(y_true_ft_all)\n",
    "            y_pred_ft_all = np.concatenate(y_pred_ft_all)\n",
    "            acc_ft = (y_true_ft_all == y_pred_ft_all).mean()\n",
    "            print(f\"  Fine-tuning PROGRESIVO (por sujeto, {CALIB_CV_FOLDS}-fold CV) acc={acc_ft:.4f} | sujetos={used_subjects}\")\n",
    "            print(f\"  Œî(FT-Global) = {acc_ft - acc_global:+.4f}\")\n",
    "        else:\n",
    "            acc_ft = np.nan\n",
    "            print(\"  Fine-tuning PROGRESIVO no ejecutado (sujeto(s) con muestras insuficientes).\")\n",
    "\n",
    "        ft_prog_folds.append(acc_ft)\n",
    "\n",
    "    # ---------- resultados finales ----------\n",
    "    if len(all_true) > 0:\n",
    "        all_true = np.concatenate(all_true)\n",
    "        all_pred = np.concatenate(all_pred)\n",
    "    else:\n",
    "        all_true = np.array([], dtype=int)\n",
    "        all_pred = np.array([], dtype=int)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTADOS FINALES\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Global folds:\", [f\"{a:.4f}\" for a in global_folds])\n",
    "    if len(global_folds) > 0:\n",
    "        print(f\"Global mean: {np.mean(global_folds):.4f}\")\n",
    "\n",
    "    print(\"Fine-tune PROGRESIVO folds:\", [(\"nan\" if (a is None or np.isnan(a)) else f\"{a:.4f}\") for a in ft_prog_folds])\n",
    "    if len(ft_prog_folds) > 0:\n",
    "        print(f\"Fine-tune PROGRESIVO mean: {np.nanmean(ft_prog_folds):.4f}\")\n",
    "        print(f\"Œî(FT-Global) mean: {np.nanmean(ft_prog_folds) - np.mean(global_folds):+.4f}\")\n",
    "\n",
    "    if all_true.size > 0:\n",
    "        plot_confusion(all_true, all_pred, CLASS_NAMES_4C,\n",
    "                       title=\"Confusion Matrix - Global Model (All Folds)\",\n",
    "                       fname=\"confusion_global_allfolds.png\")\n",
    "        print(\"\\n‚Ü≥ Matriz de confusi√≥n guardada: confusion_global_allfolds.png\")\n",
    "\n",
    "    return {\n",
    "        \"global_folds\": global_folds,\n",
    "        \"ft_prog_folds\": ft_prog_folds,\n",
    "        \"all_true\": all_true,\n",
    "        \"all_pred\": all_pred,\n",
    "        \"folds_json_path\": str(folds_json_path)\n",
    "    }\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üß† INICIANDO EXPERIMENTO CON EEGNet + FINE-TUNING PROGRESIVO (Variant-B)\")\n",
    "    print(f\"üîß Configuraci√≥n: {CLASS_SCENARIO}, {len(EXPECTED_8)} canales, {WINDOW_MODE}\")\n",
    "    print(f\"‚öôÔ∏è  FT: epochs={FT_EPOCHS}, base_lr={FT_BASE_LR}, head_lr={FT_HEAD_LR}, L2SP={FT_L2SP}, patience={FT_PATIENCE}, CV={CALIB_CV_FOLDS}\")\n",
    "    run_experiment()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
