{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e14fe640",
   "metadata": {},
   "source": [
    "# SHALLOWCNN + FINETUNING INTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c8e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Replicaci√≥n fiel del paper \"A Deep Learning MI-EEG Classification Model for BCIs\"\n",
    "# Dose et al., EUSIPCO 2018 ‚Äî Shallow CNN sobre RAW EEG (PhysioNet BCI2000)\n",
    "# Versi√≥n con FINE-TUNING progresivo por sujeto (CV 4-fold, LRs discriminativos, L2-SP, early stopping con validaci√≥n)\n",
    "\n",
    "import os, re, math, random, json, itertools, copy\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# CONFIGURACI√ìN GENERAL\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "FOLDS_DIR = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dispositivo y semilla\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "# Escenario de clases\n",
    "CLASS_SCENARIO = '4c'\n",
    "WINDOW_MODE = '3s'\n",
    "FS = 160.0\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Entrenamiento global\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS_GLOBAL = 100\n",
    "LR = 1e-3\n",
    "\n",
    "# Fine-tuning por sujeto (protocolo robusto)\n",
    "CALIB_CV_FOLDS = 4            # 4-fold CV por sujeto ~ 75/25\n",
    "FT_EPOCHS = 30                 # m√°s alto, pero con ES por validaci√≥n\n",
    "FT_BASE_LR = 5e-5              # convs (temporal+spatial) ‚Äî m√°s bajo\n",
    "FT_HEAD_LR = 1e-3              # fc+out ‚Äî m√°s alto\n",
    "FT_L2SP = 1e-4                 # regularizaci√≥n m√°s suave\n",
    "FT_PATIENCE = 5                # early stopping con validaci√≥n\n",
    "FT_VAL_RATIO = 0.2             # validaci√≥n dentro del set de calibraci√≥n\n",
    "\n",
    "# Sujetos excluidos\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "\n",
    "# Runs\n",
    "MI_RUNS_LR = [4, 8, 12]\n",
    "MI_RUNS_OF = [6, 10, 14]\n",
    "BASELINE_RUNS_EO = [1]\n",
    "\n",
    "# Canales (8 en lugar de 64)\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','CPz']\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES DE CANALES\n",
    "# =========================\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = re.sub(r'([A-Za-z])0([0-9])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])Z$', r'\\1z', s)\n",
    "    s = s.replace('fp', 'Fp').replace('FP', 'Fp')\n",
    "    s = ''.join(ch.upper() if ch != 'z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {}\n",
    "    for ch in raw.ch_names:\n",
    "        lab = normalize_label(ch)\n",
    "        lab = lab[:-1] + 'z' if lab.endswith('Z') else lab\n",
    "        lab = re.sub(r'([A-Z])Z$', r'\\1z', lab)\n",
    "        mapping[ch] = lab\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired_channels=EXPECTED_8):\n",
    "    have = [ch for ch in desired_channels if ch in raw.ch_names]\n",
    "    missing = [ch for ch in desired_channels if ch not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"Warning: faltan canales {missing} en archivo {getattr(raw,'filenames', [''])[0]}\")\n",
    "        return None\n",
    "    raw.reorder_channels([ch for ch in raw.ch_names if ch in desired_channels] +\n",
    "                         [ch for ch in raw.ch_names if ch not in desired_channels])\n",
    "    raw.pick_channels(desired_channels, ordered=True)\n",
    "    return raw\n",
    "\n",
    "# =========================\n",
    "# LECTURA DE EDF y EVENTOS\n",
    "# =========================\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(run_id:int):\n",
    "    if run_id in MI_RUNS_LR: return 'LR'\n",
    "    if run_id in MI_RUNS_OF: return 'OF'\n",
    "    if run_id in BASELINE_RUNS_EO: return 'EO'\n",
    "    return None\n",
    "\n",
    "def read_raw_edf(path: Path):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        mont = mne.channels.make_standard_montage('standard_1020')\n",
    "        raw.set_montage(mont, on_missing='ignore')\n",
    "    except Exception:\n",
    "        pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad=\"auto\")\n",
    "    raw = ensure_channels_order(raw, EXPECTED_8)\n",
    "    if raw is None:\n",
    "        return None\n",
    "    # Band-pass opcional (desactivado)\n",
    "    # raw.filter(l_freq=8., h_freq=30., picks='eeg', method='iir', verbose=False)\n",
    "    return raw\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    if raw.annotations is None or len(raw.annotations) == 0:\n",
    "        return []\n",
    "    def _norm(s): return str(s).strip().upper().replace(' ', '')\n",
    "    res = []\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = _norm(desc)\n",
    "        if tag in ('T1','T2'):\n",
    "            res.append((float(onset), tag))\n",
    "    res.sort()\n",
    "    dedup = []\n",
    "    last_t1 = last_t2 = -1e9\n",
    "    for t, tag in res:\n",
    "        if tag == 'T1':\n",
    "            if (t - last_t1) >= 0.5: dedup.append((t, tag)); last_t1 = t\n",
    "        else:\n",
    "            if (t - last_t2) >= 0.5: dedup.append((t, tag)); last_t2 = t\n",
    "    return dedup\n",
    "\n",
    "# =========================\n",
    "# CONSTRUCCI√ìN DE DATASETS\n",
    "# =========================\n",
    "def subjects_available():\n",
    "    subs = []\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try:\n",
    "            sid = int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir / f\"S{sid:03d}R{r:02d}.edf\").exists() for r in (MI_RUNS_LR + MI_RUNS_OF))\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "def extract_trials_from_run(edf_path: Path, scenario: str, window_mode: str):\n",
    "    subj, run = parse_subject_run(edf_path)\n",
    "    kind = run_kind(run)\n",
    "    if kind not in ('LR','OF','EO'):\n",
    "        return ([], [])\n",
    "\n",
    "    raw = read_raw_edf(edf_path)\n",
    "    if raw is None:\n",
    "        return ([], [])\n",
    "\n",
    "    data = raw.get_data()\n",
    "    fs = raw.info['sfreq']\n",
    "    assert abs(fs - FS) < 1e-6\n",
    "\n",
    "    out = []\n",
    "\n",
    "    if kind in ('LR','OF'):\n",
    "        events = collect_events_T1T2(raw)\n",
    "        if window_mode == '3s':\n",
    "            rel_start, rel_end = 0.0, 3.0\n",
    "        else:\n",
    "            rel_start, rel_end = -1.0, 5.0\n",
    "\n",
    "        for onset_sec, tag in events:\n",
    "            if kind == 'LR':\n",
    "                if tag == 'T1': label = 'L'\n",
    "                elif tag == 'T2': label = 'R'\n",
    "                else: continue\n",
    "            else:\n",
    "                if tag == 'T1': label = 'BFISTS'\n",
    "                elif tag == 'T2': label = 'BFEET'\n",
    "                else: continue\n",
    "\n",
    "            if scenario == '2c' and label not in ('L','R'):\n",
    "                continue\n",
    "            if scenario == '3c' and label not in ('L','R','BFISTS'):\n",
    "                continue\n",
    "            if scenario == '4c' and label not in ('L','R','BFISTS','BFEET'):\n",
    "                continue\n",
    "\n",
    "            s = int(round((raw.first_time + onset_sec + rel_start) * fs))\n",
    "            e = int(round((raw.first_time + onset_sec + rel_end) * fs))\n",
    "            if s < 0 or e > data.shape[1]:\n",
    "                continue\n",
    "\n",
    "            seg = data[:, s:e].T.astype(np.float32)\n",
    "            # Normalizaci√≥n por √©poca canal-a-canal (z-score)\n",
    "            seg = (seg - seg.mean(axis=0, keepdims=True)) / (seg.std(axis=0, keepdims=True) + 1e-6)\n",
    "\n",
    "            if label == 'L':       y = 0\n",
    "            elif label == 'R':     y = 1\n",
    "            elif label == 'BFISTS':y = 2\n",
    "            elif label == 'BFEET': y = 3\n",
    "            else: continue\n",
    "\n",
    "            out.append((seg, y, subj))\n",
    "\n",
    "    elif kind == 'EO':\n",
    "        return ([], raw.ch_names)\n",
    "\n",
    "    return out, raw.ch_names\n",
    "\n",
    "def build_dataset_all(subjects, scenario='4c', window_mode='3s'):\n",
    "    X, y, groups = [], [], []\n",
    "    ch_template = None\n",
    "\n",
    "    for s in tqdm(subjects, desc=\"Construyendo dataset (RAW)\"):\n",
    "        sdir = DATA_RAW / f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "\n",
    "        trials_L, trials_R, trials_FISTS, trials_FEET = [], [], [], []\n",
    "\n",
    "        for r in MI_RUNS_LR:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 0: trials_L.append(seg)\n",
    "                elif lab == 1: trials_R.append(seg)\n",
    "\n",
    "        for r in MI_RUNS_OF:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 2: trials_FISTS.append(seg)\n",
    "                elif lab == 3: trials_FEET.append(seg)\n",
    "\n",
    "        need_per_class = 21\n",
    "        def pick(trials, n, rng):\n",
    "            if len(trials) < n:\n",
    "                idx = rng.choice(len(trials), size=n, replace=True)\n",
    "                return [trials[i] for i in idx]\n",
    "            rng.shuffle(trials)\n",
    "            return trials[:n]\n",
    "\n",
    "        rng = check_random_state(RANDOM_STATE + s)\n",
    "        if len(trials_L)==0 or len(trials_R)==0 or len(trials_FISTS)==0 or len(trials_FEET)==0:\n",
    "            continue\n",
    "\n",
    "        Lp  = pick(trials_L,     need_per_class, rng)\n",
    "        Rp  = pick(trials_R,     need_per_class, rng)\n",
    "        FIp = pick(trials_FISTS, need_per_class, rng)\n",
    "        FEp = pick(trials_FEET,  need_per_class, rng)\n",
    "\n",
    "        pack = [(Lp, 0), (Rp, 1), (FIp, 2), (FEp, 3)]\n",
    "        for segs, lab in pack:\n",
    "            for seg in segs:\n",
    "                X.append(seg)\n",
    "                y.append(lab)\n",
    "                groups.append(s)\n",
    "\n",
    "    X = np.stack(X, axis=0)\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "    groups = np.asarray(groups, dtype=np.int64)\n",
    "\n",
    "    n, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Dataset construido: N={n} | T={T} | C={C} | clases={n_classes} | sujetos √∫nicos={len(np.unique(groups))}\")\n",
    "    return X, y, groups, ch_template\n",
    "\n",
    "# =========================\n",
    "# SHALLOW CNN\n",
    "# =========================\n",
    "class ShallowDose2018(nn.Module):\n",
    "    def __init__(self, n_ch: int, n_classes: int, kernel_t: int = 30, n_feat: int = 40, pool_t: int = 15):\n",
    "        super().__init__()\n",
    "        self.n_ch = n_ch\n",
    "        self.n_classes = n_classes\n",
    "        self.kernel_t = kernel_t\n",
    "        self.n_feat = n_feat\n",
    "        self.pool_t = pool_t\n",
    "\n",
    "        self.temporal = nn.Conv2d(1, n_feat, kernel_size=(kernel_t, 1),\n",
    "                                  padding=(kernel_t // 2, 0), bias=True)\n",
    "        self.spatial  = nn.Conv2d(n_feat, n_feat, kernel_size=(1, n_ch),\n",
    "                                  padding=(0, 0), bias=True)\n",
    "        self.avgpool  = nn.AvgPool2d(kernel_size=(pool_t, 1), stride=(pool_t, 1))\n",
    "        self.act      = nn.ELU()\n",
    "        self.flatten  = nn.Flatten()\n",
    "\n",
    "        self.fc  = None\n",
    "        self.out = None\n",
    "        self._T_in = None\n",
    "\n",
    "    def _build_head(self, T_in: int, device: torch.device):\n",
    "        T_pool = T_in // self.pool_t\n",
    "        feat_dim = self.n_feat * T_pool\n",
    "        self.fc  = nn.Linear(feat_dim, 80, bias=True).to(device)\n",
    "        self.out = nn.Linear(80, self.n_classes, bias=True).to(device)\n",
    "        self._T_in = T_in\n",
    "\n",
    "    def ensure_head(self, T_in: int, device: torch.device):\n",
    "        if (self.fc is None) or (self.out is None) or (self._T_in != T_in):\n",
    "            self._build_head(T_in, device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, _, T, C = x.shape\n",
    "        self.ensure_head(T, x.device)\n",
    "\n",
    "        z = self.temporal(x); z = self.act(z)\n",
    "        z = self.spatial(z);  z = self.act(z)\n",
    "        z = self.avgpool(z)\n",
    "        z = self.flatten(z)\n",
    "        z = self.fc(z); z = self.act(z)\n",
    "        z = self.out(z)\n",
    "        return z\n",
    "\n",
    "# =========================\n",
    "# TORCH DATASET\n",
    "# =========================\n",
    "class EEGTrials(Dataset):\n",
    "    def __init__(self, X, y, groups):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.g = groups.astype(np.int64)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        x = np.expand_dims(x, 0)                 # (1, T, C)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx])\n",
    "\n",
    "CLASS_NAMES_4C = ['Left', 'Right', 'Both Fists', 'Both Feet']\n",
    "\n",
    "# =========================\n",
    "# ENTRENAMIENTO / EVALUACI√ìN\n",
    "# =========================\n",
    "def train_epoch(model, loader, opt, criterion):\n",
    "    model.train()\n",
    "    for xb, yb, _ in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_with_preds(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for xb, yb, _ in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        pred = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "        y_pred.extend(pred)\n",
    "        y_true.extend(yb.numpy().tolist())\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    return y_true, y_pred, float(acc)\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes, title, fname):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.nan_to_num(cm_norm)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm_norm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "        plt.text(j, i, format(cm_norm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm_norm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def print_report(y_true, y_pred, classes):\n",
    "    print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
    "\n",
    "# =========================\n",
    "# FINE-TUNING PROGRESIVO por sujeto (con validaci√≥n interna)\n",
    "# =========================\n",
    "def _param_groups(model, mode):\n",
    "    \"\"\"\n",
    "    Devuelve los par√°metros a entrenar seg√∫n 'mode':\n",
    "      - 'out'            : solo capa final\n",
    "      - 'head'           : fc + out\n",
    "      - 'spatial+head'   : spatial + fc + out  (temporal queda congelada)\n",
    "    \"\"\"\n",
    "    if mode == 'out':\n",
    "        train = list(model.out.parameters())\n",
    "    elif mode == 'head':\n",
    "        train = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "    elif mode == 'spatial+head':\n",
    "        train = (list(model.spatial.parameters())\n",
    "                 + list(model.fc.parameters())\n",
    "                 + list(model.out.parameters()))\n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "    return train\n",
    "\n",
    "def _freeze_for_mode(model, mode):\n",
    "    # Primero congelamos todo\n",
    "    for p in model.parameters(): p.requires_grad = False\n",
    "    # Siempre dejamos la base temporal congelada en este protocolo\n",
    "    # Descongelamos seg√∫n 'mode'\n",
    "    if mode == 'out':\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'head':\n",
    "        for p in model.fc.parameters():  p.requires_grad = True\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'spatial+head':\n",
    "        for p in model.spatial.parameters(): p.requires_grad = True\n",
    "        for p in model.fc.parameters():      p.requires_grad = True\n",
    "        for p in model.out.parameters():     p.requires_grad = True\n",
    "\n",
    "def _class_weights(y_np, n_classes):\n",
    "    # Pesos inversos a la frecuencia por clase en el set de calibraci√≥n\n",
    "    counts = np.bincount(y_np, minlength=n_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    weights = counts.sum() / counts\n",
    "    weights = weights / weights.mean()\n",
    "    return torch.tensor(weights, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "def _make_optimizer(train_params, mode):\n",
    "    # LR m√°s alto para la cabeza, bajo para spatial cuando aplique\n",
    "    if mode == 'spatial+head':\n",
    "        # separar spatial vs head para LRs distintos\n",
    "        spatial, head = [], []\n",
    "        for p in train_params:\n",
    "            # heur√≠stica: par√°metros que pertenecen a spatial tendr√°n .shape acorde\n",
    "            # mejor: detectarlos por referencia al m√≥dulo\n",
    "            pass\n",
    "        # Como no tenemos tags aqu√≠, armamos dos grupos manualmente en la llamada principal.\n",
    "        # Devolvemos None y lo construimos fuera.\n",
    "        return None\n",
    "    else:\n",
    "        return optim.Adam(train_params, lr=FT_HEAD_LR)\n",
    "\n",
    "def _train_one_mode(model, X_cal, y_cal, n_classes, mode,\n",
    "                    epochs=FT_EPOCHS, batch_size=16,\n",
    "                    head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                    l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO):\n",
    "    \"\"\"\n",
    "    Entrena en 'mode' con early stopping sobre un conjunto de validaci√≥n interno.\n",
    "    Devuelve el modelo con los mejores pesos (por val loss).\n",
    "    \"\"\"\n",
    "    # Split Cal -> (train_cal, val_cal)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=RANDOM_STATE)\n",
    "    (tr_idx, va_idx), = sss.split(X_cal, y_cal)\n",
    "    Xtr, ytr = X_cal[tr_idx], y_cal[tr_idx]\n",
    "    Xva, yva = X_cal[va_idx], y_cal[va_idx]\n",
    "\n",
    "    # Datasets\n",
    "    ds_tr = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xtr).float().unsqueeze(1),\n",
    "        torch.from_numpy(ytr).long()\n",
    "    )\n",
    "    ds_va = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xva).float().unsqueeze(1),\n",
    "        torch.from_numpy(yva).long()\n",
    "    )\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    # Congelar / descongelar seg√∫n modo\n",
    "    _freeze_for_mode(model, mode)\n",
    "    # Par√°metros a entrenar y referencia L2-SP\n",
    "    if mode == 'spatial+head':\n",
    "        base_params = list(model.spatial.parameters())\n",
    "        head_params = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "        train_params = base_params + head_params\n",
    "        opt = optim.Adam([\n",
    "            {\"params\": base_params, \"lr\": base_lr},\n",
    "            {\"params\": head_params, \"lr\": head_lr},\n",
    "        ])\n",
    "    else:\n",
    "        train_params = _param_groups(model, mode)\n",
    "        opt = optim.Adam(train_params, lr=head_lr)\n",
    "\n",
    "    ref = [p.detach().clone().to(p.device) for p in train_params]\n",
    "    class_w = _class_weights(ytr, n_classes)\n",
    "    crit = nn.CrossEntropyLoss(weight=class_w)\n",
    "\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_val = float('inf')\n",
    "    bad = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # --- train ---\n",
    "        model.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            # L2-SP hacia referencia de los par√°metros que estamos entrenando\n",
    "            reg = 0.0\n",
    "            for p_cur, p_ref in zip(train_params, ref):\n",
    "                reg = reg + torch.sum((p_cur - p_ref)**2)\n",
    "            loss = loss + l2sp_lambda * reg\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # --- val ---\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            nval = 0\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                loss = crit(logits, yb)  # misma p√©rdida con pesos\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "                nval += xb.size(0)\n",
    "            val_loss /= max(1, nval)\n",
    "\n",
    "        if val_loss + 1e-7 < best_val:\n",
    "            best_val = val_loss\n",
    "            bad = 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_numpy(model, X_np, device):\n",
    "    model.eval()\n",
    "    xb = torch.from_numpy(X_np).float().unsqueeze(1).to(device)  # (N,1,T,C)\n",
    "    logits = model(xb)\n",
    "    return logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "def subject_cv_finetune_predict_progressive(model_global, Xs, ys, device,\n",
    "                                            n_splits=CALIB_CV_FOLDS, n_classes=4):\n",
    "    \"\"\"\n",
    "    Para un sujeto: 4-fold StratifiedKFold.\n",
    "      - En cada fold: se entrena con 3 modos progresivos:\n",
    "          1) 'out'            (solo capa final)\n",
    "          2) 'head'           (fc + out)\n",
    "          3) 'spatial+head'   (spatial + fc + out, temporal congelada)\n",
    "        Se eval√∫a en el test_fold y se elige el mejor.\n",
    "    Devuelve y_true_subj, y_pred_subj (OOF por fold).\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    y_true_full = np.empty_like(ys)\n",
    "    y_pred_full = np.empty_like(ys)\n",
    "\n",
    "    for tr_idx, te_idx in skf.split(Xs, ys):\n",
    "        Xcal, ycal = Xs[tr_idx], ys[tr_idx]\n",
    "        Xho,  yho  = Xs[te_idx], ys[te_idx]\n",
    "\n",
    "        # Stage A: 'out'\n",
    "        m_out = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_out, Xcal, ycal, n_classes, mode='out',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_out = predict_numpy(m_out, Xho, device)\n",
    "        acc_out = (yhat_out == yho).mean()\n",
    "\n",
    "        # Stage B: 'head'\n",
    "        m_head = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_head, Xcal, ycal, n_classes, mode='head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_head = predict_numpy(m_head, Xho, device)\n",
    "        acc_head = (yhat_head == yho).mean()\n",
    "\n",
    "        # Stage C: 'spatial+head'\n",
    "        m_sp = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_sp, Xcal, ycal, n_classes, mode='spatial+head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                        l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_sp = predict_numpy(m_sp, Xho, device)\n",
    "        acc_sp = (yhat_sp == yho).mean()\n",
    "\n",
    "        # Elegir mejor\n",
    "        best_idx = np.argmax([acc_out, acc_head, acc_sp])\n",
    "        yhat_best = [yhat_out, yhat_head, yhat_sp][best_idx]\n",
    "\n",
    "        y_true_full[te_idx] = yho\n",
    "        y_pred_full[te_idx] = yhat_best\n",
    "\n",
    "    return y_true_full, y_pred_full\n",
    "\n",
    "# =========================\n",
    "# FOLDS JSON helpers\n",
    "# =========================\n",
    "def save_group_folds_json_with_indices(subject_ids_str, groups_array, n_splits, out_json_path,\n",
    "                                       created_by=\"dose_experiment\", description=None):\n",
    "    out_json_path = Path(out_json_path)\n",
    "    unique_subjects_int = sorted(np.unique(groups_array).tolist())\n",
    "    subject_ids = [f\"S{sid:03d}\" for sid in unique_subjects_int]\n",
    "\n",
    "    if len(subject_ids) < n_splits:\n",
    "        raise ValueError(f\"n_splits={n_splits} mayor que n√∫mero de sujetos={len(subject_ids)}\")\n",
    "\n",
    "    groups = np.arange(len(subject_ids))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    folds = []\n",
    "    fold_i = 0\n",
    "    for train_idx_grp, test_idx_grp in gkf.split(groups, groups, groups):\n",
    "        fold_i += 1\n",
    "        train_sids = [subject_ids[int(i)] for i in train_idx_grp]\n",
    "        test_sids  = [subject_ids[int(i)] for i in test_idx_grp]\n",
    "\n",
    "        train_sids_int = [int(s[1:]) for s in train_sids]\n",
    "        test_sids_int  = [int(s[1:]) for s in test_sids]\n",
    "\n",
    "        tr_idx = np.where(np.isin(groups_array, train_sids_int))[0].tolist()\n",
    "        te_idx = np.where(np.isin(groups_array, test_sids_int))[0].tolist()\n",
    "\n",
    "        folds.append({\n",
    "            \"fold\": int(fold_i),\n",
    "            \"train\": train_sids,\n",
    "            \"test\": test_sids,\n",
    "            \"tr_idx\": tr_idx,\n",
    "            \"te_idx\": te_idx\n",
    "        })\n",
    "\n",
    "    payload = {\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"created_by\": created_by,\n",
    "        \"description\": description if description is not None else \"\",\n",
    "        \"n_splits\": int(n_splits),\n",
    "        \"n_subjects\": len(subject_ids),\n",
    "        \"subject_ids\": subject_ids,\n",
    "        \"folds\": folds\n",
    "    }\n",
    "\n",
    "    out_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Folds JSON con √≠ndices guardado ‚Üí {out_json_path}\")\n",
    "    return out_json_path\n",
    "\n",
    "def load_group_folds_json(path_json, expected_subject_ids=None, strict_check=True):\n",
    "    path_json = Path(path_json)\n",
    "    if not path_json.exists():\n",
    "        raise FileNotFoundError(f\"No existe {path_json}\")\n",
    "    with open(path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        payload = json.load(f)\n",
    "\n",
    "    subj_json = payload.get(\"subject_ids\", [])\n",
    "    if expected_subject_ids is not None:\n",
    "        expected = sorted(list(expected_subject_ids))\n",
    "        if subj_json != expected:\n",
    "            msg = (\"Los subject_ids del JSON no coinciden con expected_subject_ids.\\n\"\n",
    "                   f\"JSON has {len(subj_json)} subjects, expected {len(expected)}.\\n\"\n",
    "                   f\"First 10 JSON: {subj_json[:10]}\\nFirst 10 expected: {expected[:10]}\")\n",
    "            if strict_check:\n",
    "                raise ValueError(msg)\n",
    "            else:\n",
    "                print(\"WARNING: \" + msg)\n",
    "    return payload\n",
    "\n",
    "# =========================\n",
    "# EXPERIMENTO\n",
    "# =========================\n",
    "def run_experiment(save_folds_json=True, folds_json_path=FOLDS_DIR, folds_json_description=\"GroupKFold folds for comparison\"):\n",
    "    \"\"\"\n",
    "    - Crea/lee JSON con folds por sujeto (incluye tr_idx/te_idx)\n",
    "    - Entrena modelo global por fold (inter-sujeto puro)\n",
    "    - Eval√∫a Global acc en test\n",
    "    - Realiza Fine-Tuning PROGRESIVO por sujeto (4-fold CV) y reporta acc\n",
    "    \"\"\"\n",
    "    mne.set_log_level('WARNING')\n",
    "\n",
    "    # sujetos y dataset\n",
    "    subs = subjects_available()\n",
    "    print(f\"Sujetos elegibles: {len(subs)} ‚Üí {subs[:10]}{'...' if len(subs)>10 else ''}\")\n",
    "\n",
    "    X, y, groups, chs = build_dataset_all(subs, scenario=CLASS_SCENARIO, window_mode=WINDOW_MODE)\n",
    "    N, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Listo para entrenar: N={N} | T={T} | C={C} | clases={n_classes} | sujetos={len(np.unique(groups))}\")\n",
    "\n",
    "    ds = EEGTrials(X, y, groups)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # preparar JSON folds\n",
    "    if folds_json_path is None:\n",
    "        folds_json_path = Path(\"folds\") / f\"group_folds_{N_FOLDS}splits.json\"\n",
    "    else:\n",
    "        folds_json_path = Path(folds_json_path)\n",
    "    folds_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    unique_subs = sorted(np.unique(groups).tolist())\n",
    "    subject_ids_str = [f\"S{s:03d}\" for s in unique_subs]\n",
    "\n",
    "    if not folds_json_path.exists():\n",
    "        if not save_folds_json:\n",
    "            raise FileNotFoundError(f\"Folds JSON no encontrado en {folds_json_path} y save_folds_json=False.\")\n",
    "        save_group_folds_json_with_indices(subject_ids_str, groups, n_splits=N_FOLDS,\n",
    "                                           out_json_path=folds_json_path,\n",
    "                                           created_by=\"Joel_Clasificador\",\n",
    "                                           description=folds_json_description)\n",
    "\n",
    "    payload = load_group_folds_json(folds_json_path, expected_subject_ids=subject_ids_str, strict_check=False)\n",
    "    folds = payload[\"folds\"]\n",
    "\n",
    "    # bucle por folds\n",
    "    global_folds = []\n",
    "    ft_prog_folds = []\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "\n",
    "    for f in folds:\n",
    "        fold = f[\"fold\"]\n",
    "        tr_idx = np.asarray(f.get(\"tr_idx\", []), dtype=int)\n",
    "        te_idx = np.asarray(f.get(\"te_idx\", []), dtype=int)\n",
    "\n",
    "        if tr_idx.size == 0 or te_idx.size == 0:\n",
    "            print(f\"Advertencia: fold {fold} sin √≠ndices tr/te v√°lidos. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        tr_loader = DataLoader(Subset(ds, tr_idx), batch_size=BATCH_SIZE, shuffle=True,  drop_last=False)\n",
    "        te_loader = DataLoader(Subset(ds, te_idx), batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "        model = ShallowDose2018(n_ch=C, n_classes=n_classes).to(DEVICE)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "        print(f\"\\n[Fold {fold}/{N_FOLDS}] Entrenando modelo global... (n_train={len(tr_idx)} | n_test={len(te_idx)})\")\n",
    "        for epoch in range(1, EPOCHS_GLOBAL + 1):\n",
    "            train_epoch(model, tr_loader, opt, criterion)\n",
    "            if epoch % 20 == 0:\n",
    "                print(f\"  √âpoca {epoch}/{EPOCHS_GLOBAL}\")\n",
    "\n",
    "        # Evaluaci√≥n global (inter-sujeto puro)\n",
    "        y_true, y_pred, acc_global = evaluate_with_preds(model, te_loader)\n",
    "        global_folds.append(acc_global)\n",
    "        all_true.append(y_true); all_pred.append(y_pred)\n",
    "\n",
    "        print(f\"[Fold {fold}/{N_FOLDS}] Global acc={acc_global:.4f}\")\n",
    "        print_report(y_true, y_pred, CLASS_NAMES_4C)\n",
    "\n",
    "        # ---------- Fine-tuning PROGRESIVO por sujeto con 4-fold CV ----------\n",
    "        X_te, y_te, g_te = X[te_idx], y[te_idx], groups[te_idx]\n",
    "\n",
    "        y_true_ft_all, y_pred_ft_all = [], []\n",
    "        used_subjects = 0\n",
    "        for sid in np.unique(g_te):\n",
    "            idx = np.where(g_te == sid)[0]\n",
    "            Xs, ys = X_te[idx], y_te[idx]\n",
    "\n",
    "            # Seguridad: requiere al menos n_splits muestras (estratificado).\n",
    "            if len(ys) < CALIB_CV_FOLDS or len(np.unique(ys)) < 2:\n",
    "                continue\n",
    "\n",
    "            y_true_subj, y_pred_subj = subject_cv_finetune_predict_progressive(\n",
    "                model, Xs, ys, DEVICE, n_splits=CALIB_CV_FOLDS, n_classes=n_classes\n",
    "            )\n",
    "            y_true_ft_all.append(y_true_subj)\n",
    "            y_pred_ft_all.append(y_pred_subj)\n",
    "            used_subjects += 1\n",
    "\n",
    "        if len(y_true_ft_all) > 0:\n",
    "            y_true_ft_all = np.concatenate(y_true_ft_all)\n",
    "            y_pred_ft_all = np.concatenate(y_pred_ft_all)\n",
    "            acc_ft = (y_true_ft_all == y_pred_ft_all).mean()\n",
    "            print(f\"  Fine-tuning PROGRESIVO (por sujeto, {CALIB_CV_FOLDS}-fold CV) acc={acc_ft:.4f} | sujetos={used_subjects}\")\n",
    "            print(f\"  Œî(FT-Global) = {acc_ft - acc_global:+.4f}\")\n",
    "        else:\n",
    "            acc_ft = np.nan\n",
    "            print(\"  Fine-tuning PROGRESIVO no ejecutado (sujeto(s) con muestras insuficientes).\")\n",
    "\n",
    "        ft_prog_folds.append(acc_ft)\n",
    "\n",
    "    # ---------- resultados finales ----------\n",
    "    if len(all_true) > 0:\n",
    "        all_true = np.concatenate(all_true)\n",
    "        all_pred = np.concatenate(all_pred)\n",
    "    else:\n",
    "        all_true = np.array([], dtype=int)\n",
    "        all_pred = np.array([], dtype=int)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTADOS FINALES\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Global folds:\", [f\"{a:.4f}\" for a in global_folds])\n",
    "    if len(global_folds) > 0:\n",
    "        print(f\"Global mean: {np.mean(global_folds):.4f}\")\n",
    "\n",
    "    print(\"Fine-tune PROGRESIVO folds:\", [(\"nan\" if (a is None or np.isnan(a)) else f\"{a:.4f}\") for a in ft_prog_folds])\n",
    "    if len(ft_prog_folds) > 0:\n",
    "        print(f\"Fine-tune PROGRESIVO mean: {np.nanmean(ft_prog_folds):.4f}\")\n",
    "        print(f\"Œî(FT-Global) mean: {np.nanmean(ft_prog_folds) - np.mean(global_folds):+.4f}\")\n",
    "\n",
    "    # Matriz de confusi√≥n global (sobre todos los folds)\n",
    "    if all_true.size > 0:\n",
    "        plot_confusion(all_true, all_pred, CLASS_NAMES_4C,\n",
    "                       title=\"Confusion Matrix - Global Model (All Folds)\",\n",
    "                       fname=\"confusion_global_allfolds.png\")\n",
    "        print(\"\\n‚Ü≥ Matriz de confusi√≥n guardada: confusion_global_allfolds.png\")\n",
    "\n",
    "    return {\n",
    "        \"global_folds\": global_folds,\n",
    "        \"ft_prog_folds\": ft_prog_folds,\n",
    "        \"all_true\": all_true,\n",
    "        \"all_pred\": all_pred,\n",
    "        \"folds_json_path\": str(folds_json_path)\n",
    "    }\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üß† INICIANDO EXPERIMENTO CON FINE-TUNING PROGRESIVO (por sujeto, 4-fold CV)\")\n",
    "    print(f\"üîß Configuraci√≥n: {CLASS_SCENARIO}, {len(EXPECTED_8)} canales, {WINDOW_MODE}\")\n",
    "    print(f\"‚öôÔ∏è  FT: epochs={FT_EPOCHS}, base_lr={FT_BASE_LR}, head_lr={FT_HEAD_LR}, L2SP={FT_L2SP}, patience={FT_PATIENCE}, CV={CALIB_CV_FOLDS}\")\n",
    "    run_experiment()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b7cb93",
   "metadata": {},
   "source": [
    "### Su intra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bf0061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Experimento INTRA-subject CV completo (5 folds) con Shallow CNN + Feature Extraction\n",
    "# Adaptado del modelo ‚Äúinter‚Äù que compartiste\n",
    "\n",
    "import os, re, math, random, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# CONFIGURACI√ìN GENERAL\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "CLASS_SCENARIO = '4c'\n",
    "WINDOW_MODE = '3s'\n",
    "FS = 160.0\n",
    "N_FOLDS = 5\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS_GLOBAL = 50\n",
    "LR = 1e-3\n",
    "\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "MI_RUNS_LR = [4, 8, 12]\n",
    "MI_RUNS_OF = [6, 10, 14]\n",
    "EXPECTED_64 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','CPz']\n",
    "\n",
    "CLASSIFIER_TYPE = 'svm'  # 'svm' o 'logistic'\n",
    "CLASS_NAMES_4C = ['Left','Right','Both Fists','Both Feet']\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES DE CANALES\n",
    "# =========================\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = re.sub(r'([A-Za-z])0([0-9])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])Z$', r'\\1z', s)\n",
    "    s = s.replace('fp', 'Fp').replace('FP', 'Fp')\n",
    "    s = ''.join(ch.upper() if ch != 'z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {ch: normalize_label(ch) for ch in raw.ch_names}\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired_channels=EXPECTED_64) -> mne.io.BaseRaw:\n",
    "    have = [ch for ch in desired_channels if ch in raw.ch_names]\n",
    "    missing = [ch for ch in desired_channels if ch not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"Warning: faltan canales {missing} en archivo {getattr(raw,'filenames',[''])[0]}\")\n",
    "        return None\n",
    "    return raw.reorder_channels(desired_channels)\n",
    "\n",
    "# =========================\n",
    "# LECTURA DE EDF y EVENTOS\n",
    "# =========================\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(run_id:int):\n",
    "    if run_id in MI_RUNS_LR: return 'LR'\n",
    "    if run_id in MI_RUNS_OF: return 'OF'\n",
    "    return None\n",
    "\n",
    "def read_raw_edf(path: Path) -> mne.io.BaseRaw:\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        raw.set_montage(mne.channels.make_standard_montage('standard_1020'), on_missing='ignore')\n",
    "    except: pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad='auto')\n",
    "    return ensure_channels_order(raw)\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    if raw.annotations is None or len(raw.annotations) == 0: return []\n",
    "    res = []\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = str(desc).strip().upper().replace(' ','')\n",
    "        if tag in ('T1','T2'): res.append((float(onset), tag))\n",
    "    res.sort()\n",
    "    dedup = []\n",
    "    last_t1 = last_t2 = -1e9\n",
    "    for t, tag in res:\n",
    "        if tag=='T1' and (t-last_t1)>=0.5: dedup.append((t,tag)); last_t1=t\n",
    "        if tag=='T2' and (t-last_t2)>=0.5: dedup.append((t,tag)); last_t2=t\n",
    "    return dedup\n",
    "\n",
    "# =========================\n",
    "# CONSTRUCCI√ìN DE DATASET\n",
    "# =========================\n",
    "def subjects_available():\n",
    "    subs=[]\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try: sid=int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir/f\"S{sid:03d}R{r:02d}.edf\").exists() for r in MI_RUNS_LR+MI_RUNS_OF)\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "def extract_trials_from_run(edf_path: Path):\n",
    "    subj, run = parse_subject_run(edf_path)\n",
    "    kind = run_kind(run)\n",
    "    if kind not in ('LR','OF'): return []\n",
    "\n",
    "    raw = read_raw_edf(edf_path)\n",
    "    data = raw.get_data()\n",
    "    fs = raw.info['sfreq']\n",
    "    events = collect_events_T1T2(raw)\n",
    "\n",
    "    out=[]\n",
    "    for onset, tag in events:\n",
    "        if kind=='LR':\n",
    "            label=0 if tag=='T1' else 1\n",
    "        else:\n",
    "            label=2 if tag=='T1' else 3\n",
    "        s = int(round(onset*fs))\n",
    "        e = int(round((onset+3.0)*fs))\n",
    "        if s<0 or e>data.shape[1]: continue\n",
    "        seg=data[:,s:e].T.astype(np.float32)\n",
    "        out.append((seg,label,subj))\n",
    "    return out\n",
    "\n",
    "def build_dataset_all(subjects):\n",
    "    X,y,groups=[],[],[]\n",
    "    for s in tqdm(subjects,\"Construyendo dataset\"):\n",
    "        sdir = DATA_RAW/f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "        for r in MI_RUNS_LR+MI_RUNS_OF:\n",
    "            path = sdir/f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not path.exists(): continue\n",
    "            trials=extract_trials_from_run(path)\n",
    "            for seg,label,subj in trials:\n",
    "                X.append(seg); y.append(label); groups.append(subj)\n",
    "    X=np.stack(X,0); y=np.array(y,dtype=int); groups=np.array(groups,dtype=int)\n",
    "    print(f\"Dataset construido: N={X.shape[0]} | T={X.shape[1]} | C={X.shape[2]} | sujetos={len(np.unique(groups))}\")\n",
    "    return X,y,groups\n",
    "\n",
    "# =========================\n",
    "# TORCH DATASET\n",
    "# =========================\n",
    "class EEGTrials(Dataset):\n",
    "    def __init__(self,X,y,groups):\n",
    "        self.X=X.astype(np.float32)\n",
    "        self.y=y.astype(np.int64)\n",
    "        self.g=groups.astype(np.int64)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self,idx):\n",
    "        x=self.X[idx]; x=np.expand_dims(x,0)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx])\n",
    "\n",
    "# =========================\n",
    "# SHALLOW CNN\n",
    "# =========================\n",
    "class ShallowDose2018(nn.Module):\n",
    "    def __init__(self,n_ch,n_classes,kernel_t=30,n_feat=40,pool_t=15):\n",
    "        super().__init__()\n",
    "        self.temporal=nn.Conv2d(1,n_feat,(kernel_t,1),padding=(kernel_t//2,0))\n",
    "        self.spatial=nn.Conv2d(n_feat,n_feat,(1,n_ch))\n",
    "        self.avgpool=nn.AvgPool2d((pool_t,1),stride=(pool_t,1))\n",
    "        self.act=nn.ELU()\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.fc=None; self.out=None; self._T_in=None\n",
    "        self.n_classes=n_classes\n",
    "    def _build_head(self,T_in,device):\n",
    "        T_pool=T_in//15\n",
    "        feat_dim=40*T_pool\n",
    "        self.fc=nn.Linear(feat_dim,80).to(device)\n",
    "        self.out=nn.Linear(80,self.n_classes).to(device)\n",
    "        self._T_in=T_in\n",
    "    def ensure_head(self,T_in,device):\n",
    "        if self.fc is None or self.out is None or self._T_in!=T_in:\n",
    "            self._build_head(T_in,device)\n",
    "    def forward(self,x):\n",
    "        B,_,T,C=x.shape\n",
    "        self.ensure_head(T,x.device)\n",
    "        z=self.temporal(x); z=self.act(z)\n",
    "        z=self.spatial(z); z=self.act(z)\n",
    "        z=self.avgpool(z); z=self.flatten(z)\n",
    "        z=self.fc(z); z=self.act(z)\n",
    "        return self.out(z)\n",
    "    def extract_features(self,x):\n",
    "        B,_,T,C=x.shape\n",
    "        self.ensure_head(T,x.device)\n",
    "        z=self.temporal(x); z=self.act(z)\n",
    "        z=self.spatial(z); z=self.act(z)\n",
    "        z=self.avgpool(z); z=self.flatten(z)\n",
    "        z=self.fc(z); z=self.act(z)\n",
    "        return z\n",
    "\n",
    "# =========================\n",
    "# TRAIN + FEATURE EXTRACTION\n",
    "# =========================\n",
    "def train_epoch(model,loader,opt,criterion):\n",
    "    model.train()\n",
    "    for xb,yb,_ in loader:\n",
    "        xb,yb=xb.to(DEVICE),yb.to(DEVICE)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss=criterion(model(xb),yb)\n",
    "        loss.backward(); opt.step()\n",
    "\n",
    "def extract_features_from_model(model,X,batch_size=32):\n",
    "    model.eval()\n",
    "    feats=[]\n",
    "    with torch.no_grad():\n",
    "        for i in range(0,len(X),batch_size):\n",
    "            x_batch=torch.from_numpy(X[i:i+batch_size]).float().unsqueeze(1).to(DEVICE)\n",
    "            feats.append(model.extract_features(x_batch).cpu().numpy())\n",
    "    return np.concatenate(feats,0)\n",
    "\n",
    "# =========================\n",
    "# GLOBAL MODEL + INTRA-SUBJECT SVM\n",
    "# =========================\n",
    "def train_global_model(X, y, model_class=ShallowDose2018, epochs=EPOCHS_GLOBAL):\n",
    "    model = model_class(n_ch=X.shape[2], n_classes=len(np.unique(y))).to(DEVICE)\n",
    "    ds = EEGTrials(X, y, np.zeros(len(y)))\n",
    "    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    opt = optim.Adam(model.parameters(), lr=LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        train_epoch(model, loader, opt, criterion)\n",
    "    return model\n",
    "\n",
    "def intra_subject_svm(features, y, groups, n_splits=5, classifier_type='svm', class_names=None):\n",
    "    subjects = np.unique(groups)\n",
    "    all_accs = []\n",
    "    for s in subjects:\n",
    "        idx = np.where(groups == s)[0]\n",
    "        X_s, y_s = features[idx], y[idx]\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "        subj_accs = []\n",
    "        y_true_all, y_pred_all = [], []\n",
    "        for tr_idx, te_idx in skf.split(X_s, y_s):\n",
    "            X_tr, X_te = X_s[tr_idx], X_s[te_idx]\n",
    "            y_tr, y_te = y_s[tr_idx], y_s[te_idx]\n",
    "\n",
    "            if classifier_type == 'svm':\n",
    "                clf = SVC(kernel='linear', random_state=RANDOM_STATE)\n",
    "            else:\n",
    "                clf = LogisticRegression(max_iter=500, random_state=RANDOM_STATE)\n",
    "            clf.fit(X_tr, y_tr)\n",
    "            y_pred = clf.predict(X_te)\n",
    "            subj_accs.append((y_pred == y_te).mean())\n",
    "            y_true_all.extend(y_te)\n",
    "            y_pred_all.extend(y_pred)\n",
    "\n",
    "        mean_acc = np.mean(subj_accs)\n",
    "        all_accs.append(mean_acc)\n",
    "        print(f\"Sujeto {s} intra-subject acc={mean_acc:.4f}\")\n",
    "\n",
    "        # Matriz de confusi√≥n y classification report\n",
    "        if class_names is not None:\n",
    "            cm = confusion_matrix(y_true_all, y_pred_all)\n",
    "            print(f\"Confusion matrix sujeto {s}:\\n{cm}\")\n",
    "            print(f\"Classification report sujeto {s}:\\n{classification_report(y_true_all, y_pred_all, target_names=class_names)}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Promedio intra-subject accuracy: {np.mean(all_accs):.4f}\")\n",
    "    return all_accs\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "if __name__==\"__main__\":\n",
    "    subs = subjects_available()\n",
    "    X, y, groups = build_dataset_all(subs)\n",
    "\n",
    "    print(\"üîπ Entrenando modelo CNN global...\")\n",
    "    global_model = train_global_model(X, y)\n",
    "\n",
    "    print(\"üîπ Extrayendo features con modelo global...\")\n",
    "    features = extract_features_from_model(global_model, X)\n",
    "\n",
    "    print(\"üîπ Evaluando intra-subject CV usando SVM sobre features...\")\n",
    "    intra_subject_svm(features, y, groups, n_splits=5, classifier_type=CLASSIFIER_TYPE, class_names=CLASS_NAMES_4C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5365ee8",
   "metadata": {},
   "source": [
    "# EGGNET + FINETUNING INTER MEJOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ced05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Replicaci√≥n fiel del paper \"A Deep Learning MI-EEG Classification Model for BCIs\"\n",
    "# Dose et al., EUSIPCO 2018 ‚Äî ahora con EEGNet (Lawhern et al., 2018) en vez de ShallowConvNet\n",
    "# Protocolo: entrenamiento global inter-sujeto + FINE-TUNING PROGRESIVO por sujeto\n",
    "# (CV 4-fold, LRs discriminativos, L2-SP, early stopping con validaci√≥n)\n",
    "\n",
    "import os, re, math, random, json, itertools, copy\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold, StratifiedShuffleSplit, GroupShuffleSplit\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# CONFIGURACI√ìN GENERAL\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "FOLDS_DIR = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dispositivo y semilla\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "# Escenario de clases\n",
    "CLASS_SCENARIO = '4c'\n",
    "WINDOW_MODE = '3s'\n",
    "FS = 160.0\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Entrenamiento global\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS_GLOBAL = 100\n",
    "LR = 1e-3\n",
    "# >>> A√±adidos para diagn√≥stico/ES global <<<\n",
    "GLOBAL_VAL_SPLIT = 0.15   # fracci√≥n de sujetos (dentro del train) para validaci√≥n\n",
    "GLOBAL_PATIENCE  = 10     # √©pocas sin mejora en val_acc\n",
    "LOG_EVERY        = 5      # log cada N √©pocas\n",
    "\n",
    "# Fine-tuning por sujeto (protocolo robusto)\n",
    "CALIB_CV_FOLDS = 4            # 4-fold CV por sujeto ~ 75/25\n",
    "FT_EPOCHS = 30                 # ES con validaci√≥n\n",
    "FT_BASE_LR = 5e-5              # convs \"base\" (temporal/depthwise) ‚Äî m√°s bajo\n",
    "FT_HEAD_LR = 1e-3              # fc+out ‚Äî m√°s alto\n",
    "FT_L2SP = 1e-4                 # regularizaci√≥n suave\n",
    "FT_PATIENCE = 5                # early stopping con validaci√≥n\n",
    "FT_VAL_RATIO = 0.2             # validaci√≥n dentro del set de calibraci√≥n\n",
    "\n",
    "# Sujetos excluidos\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "\n",
    "# Runs\n",
    "MI_RUNS_LR = [4, 8, 12]\n",
    "MI_RUNS_OF = [6, 10, 14]\n",
    "BASELINE_RUNS_EO = [1]\n",
    "\n",
    "# Canales (8 en lugar de 64)\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','CPz']\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES DE CANALES\n",
    "# =========================\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = re.sub(r'([A-Za-z])0([0-9])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])Z$', r'\\1z', s)\n",
    "    s = s.replace('fp', 'Fp').replace('FP', 'Fp')\n",
    "    s = ''.join(ch.upper() if ch != 'z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {}\n",
    "    for ch in raw.ch_names:\n",
    "        lab = normalize_label(ch)\n",
    "        lab = lab[:-1] + 'z' if lab.endswith('Z') else lab\n",
    "        lab = re.sub(r'([A-Z])Z$', r'\\1z', lab)\n",
    "        mapping[ch] = lab\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired_channels=EXPECTED_8):\n",
    "    have = [ch for ch in desired_channels if ch in raw.ch_names]\n",
    "    missing = [ch for ch in desired_channels if ch not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"Warning: faltan canales {missing} en archivo {getattr(raw,'filenames', [''])[0]}\")\n",
    "        return None\n",
    "    # Reordenar y quedarse SOLO con los deseados (8)\n",
    "    raw.reorder_channels([ch for ch in raw.ch_names if ch in desired_channels] +\n",
    "                         [ch for ch in raw.ch_names if ch not in desired_channels])\n",
    "    raw.pick_channels(desired_channels, ordered=True)\n",
    "    return raw\n",
    "\n",
    "# =========================\n",
    "# LECTURA DE EDF y EVENTOS\n",
    "# =========================\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(run_id:int):\n",
    "    if run_id in MI_RUNS_LR: return 'LR'\n",
    "    if run_id in MI_RUNS_OF: return 'OF'\n",
    "    if run_id in BASELINE_RUNS_EO: return 'EO'\n",
    "    return None\n",
    "\n",
    "def read_raw_edf(path: Path):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        mont = mne.channels.make_standard_montage('standard_1020')\n",
    "        raw.set_montage(mont, on_missing='ignore')\n",
    "    except Exception:\n",
    "        pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad=\"auto\")\n",
    "    raw = ensure_channels_order(raw, EXPECTED_8)\n",
    "    if raw is None:\n",
    "        return None\n",
    "    # Band-pass opcional (desactivado)\n",
    "    #raw.filter(l_freq=8., h_freq=30., picks='eeg', method='iir', verbose=False)\n",
    "    return raw\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    if raw.annotations is None or len(raw.annotations) == 0:\n",
    "        return []\n",
    "    def _norm(s): return str(s).strip().upper().replace(' ', '')\n",
    "    res = []\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = _norm(desc)\n",
    "        if tag in ('T1','T2'):\n",
    "            res.append((float(onset), tag))\n",
    "    res.sort()\n",
    "    dedup = []\n",
    "    last_t1 = last_t2 = -1e9\n",
    "    for t, tag in res:\n",
    "        if tag == 'T1':\n",
    "            if (t - last_t1) >= 0.5: dedup.append((t, tag)); last_t1 = t\n",
    "        else:\n",
    "            if (t - last_t2) >= 0.5: dedup.append((t, tag)); last_t2 = t\n",
    "    return dedup\n",
    "\n",
    "# =========================\n",
    "# CONSTRUCCI√ìN DE DATASETS\n",
    "# =========================\n",
    "def subjects_available():\n",
    "    subs = []\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try:\n",
    "            sid = int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir / f\"S{sid:03d}R{r:02d}.edf\").exists() for r in (MI_RUNS_LR + MI_RUNS_OF))\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "def extract_trials_from_run(edf_path: Path, scenario: str, window_mode: str):\n",
    "    subj, run = parse_subject_run(edf_path)\n",
    "    kind = run_kind(run)\n",
    "    if kind not in ('LR','OF','EO'):\n",
    "        return ([], [])\n",
    "\n",
    "    raw = read_raw_edf(edf_path)\n",
    "    if raw is None:\n",
    "        return ([], [])\n",
    "\n",
    "    data = raw.get_data()\n",
    "    fs = raw.info['sfreq']\n",
    "    assert abs(fs - FS) < 1e-6\n",
    "\n",
    "    out = []\n",
    "\n",
    "    if kind in ('LR','OF'):\n",
    "        events = collect_events_T1T2(raw)\n",
    "        if window_mode == '3s':\n",
    "            rel_start, rel_end = 0.0, 3.0\n",
    "        else:\n",
    "            rel_start, rel_end = -1.0, 5.0\n",
    "\n",
    "        for onset_sec, tag in events:\n",
    "            if kind == 'LR':\n",
    "                if tag == 'T1': label = 'L'\n",
    "                elif tag == 'T2': label = 'R'\n",
    "                else: continue\n",
    "            else:\n",
    "                if tag == 'T1': label = 'BFISTS'\n",
    "                elif tag == 'T2': label = 'BFEET'\n",
    "                else: continue\n",
    "\n",
    "            if scenario == '2c' and label not in ('L','R'):\n",
    "                continue\n",
    "            if scenario == '3c' and label not in ('L','R','BFISTS'):\n",
    "                continue\n",
    "            if scenario == '4c' and label not in ('L','R','BFISTS','BFEET'):\n",
    "                continue\n",
    "\n",
    "            s = int(round((raw.first_time + onset_sec + rel_start) * fs))\n",
    "            e = int(round((raw.first_time + onset_sec + rel_end) * fs))\n",
    "            if s < 0 or e > data.shape[1]:\n",
    "                continue\n",
    "\n",
    "            seg = data[:, s:e].T.astype(np.float32)\n",
    "            # Normalizaci√≥n por √©poca canal-a-canal (z-score)\n",
    "            seg = (seg - seg.mean(axis=0, keepdims=True)) / (seg.std(axis=0, keepdims=True) + 1e-6)\n",
    "\n",
    "            if label == 'L':       y = 0\n",
    "            elif label == 'R':     y = 1\n",
    "            elif label == 'BFISTS':y = 2\n",
    "            elif label == 'BFEET': y = 3\n",
    "            else: continue\n",
    "\n",
    "            out.append((seg, y, subj))\n",
    "\n",
    "    elif kind == 'EO':\n",
    "        return ([], raw.ch_names)\n",
    "\n",
    "    return out, raw.ch_names\n",
    "\n",
    "def build_dataset_all(subjects, scenario='4c', window_mode='3s'):\n",
    "    X, y, groups = [], [], []\n",
    "    ch_template = None\n",
    "\n",
    "    for s in tqdm(subjects, desc=\"Construyendo dataset (RAW)\"):\n",
    "        sdir = DATA_RAW / f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "\n",
    "        trials_L, trials_R, trials_FISTS, trials_FEET = [], [], [], []\n",
    "\n",
    "        for r in MI_RUNS_LR:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 0: trials_L.append(seg)\n",
    "                elif lab == 1: trials_R.append(seg)\n",
    "\n",
    "        for r in MI_RUNS_OF:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 2: trials_FISTS.append(seg)\n",
    "                elif lab == 3: trials_FEET.append(seg)\n",
    "\n",
    "        need_per_class = 21\n",
    "        def pick(trials, n, rng):\n",
    "            if len(trials) < n:\n",
    "                idx = rng.choice(len(trials), size=n, replace=True)\n",
    "                return [trials[i] for i in idx]\n",
    "            rng.shuffle(trials)\n",
    "            return trials[:n]\n",
    "\n",
    "        rng = check_random_state(RANDOM_STATE + s)\n",
    "        if len(trials_L)==0 or len(trials_R)==0 or len(trials_FISTS)==0 or len(trials_FEET)==0:\n",
    "            continue\n",
    "\n",
    "        Lp  = pick(trials_L,     need_per_class, rng)\n",
    "        Rp  = pick(trials_R,     need_per_class, rng)\n",
    "        FIp = pick(trials_FISTS, need_per_class, rng)\n",
    "        FEp = pick(trials_FEET,  need_per_class, rng)\n",
    "\n",
    "        pack = [(Lp, 0), (Rp, 1), (FIp, 2), (FEp, 3)]\n",
    "        for segs, lab in pack:\n",
    "            for seg in segs:\n",
    "                X.append(seg)\n",
    "                y.append(lab)\n",
    "                groups.append(s)\n",
    "\n",
    "    X = np.stack(X, axis=0)\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "    groups = np.asarray(groups, dtype=np.int64)\n",
    "\n",
    "    n, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Dataset construido: N={n} | T={T} | C={C} | clases={n_classes} | sujetos √∫nicos={len(np.unique(groups))}\")\n",
    "    return X, y, groups, ch_template\n",
    "\n",
    "# =========================\n",
    "# EEGNet (Lawhern et al., 2018) adaptado a (B,1,T,C)\n",
    "# =========================\n",
    "class EEGNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Entrada: x de forma (B, 1, T, C)  [T=tiempo, C=canales]\n",
    "    Bloques:\n",
    "      1) Temporal conv     : Conv2d(1 -> F1, (kernel_t,1), padding 'same'), BN, ELU\n",
    "      2) Depthwise (espacial): Conv2d(F1 -> F1*D, (1,C), groups=F1, BN, ELU, AvgPool(4,1), Dropout\n",
    "      3) Separable temporal: Depthwise temporal (k_sep,1) groups=F1*D + Pointwise 1x1 a F2, BN, ELU, AvgPool(8,1), Dropout\n",
    "      4) FC -> OUT\n",
    "    \"\"\"\n",
    "    def __init__(self, n_ch: int, n_classes: int,\n",
    "                 F1: int = 8, D: int = 2, kernel_t: int = 64, k_sep: int = 16,\n",
    "                 pool1_t: int = 4, pool2_t: int = 8, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.n_ch = n_ch\n",
    "        self.n_classes = n_classes\n",
    "        self.F1 = F1\n",
    "        self.D = D\n",
    "        self.F2 = F1 * D\n",
    "        self.kernel_t = kernel_t\n",
    "        self.k_sep = k_sep\n",
    "        self.pool1_t = pool1_t\n",
    "        self.pool2_t = pool2_t\n",
    "\n",
    "        # Bloque 1: temporal\n",
    "        self.conv_temporal = nn.Conv2d(1, F1, kernel_size=(kernel_t, 1),\n",
    "                                       padding=(kernel_t // 2, 0), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(F1)\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "        # Bloque 2: depthwise (espacial)\n",
    "        self.conv_depthwise = nn.Conv2d(F1, self.F2, kernel_size=(1, n_ch),\n",
    "                                        groups=F1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.F2)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=(pool1_t, 1), stride=(pool1_t, 1))\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "\n",
    "        # Bloque 3: separable temporal (depthwise temporal + pointwise)\n",
    "        self.conv_sep_depth = nn.Conv2d(self.F2, self.F2, kernel_size=(k_sep, 1),\n",
    "                                        groups=self.F2, padding=(k_sep // 2, 0), bias=False)\n",
    "        self.conv_sep_point = nn.Conv2d(self.F2, self.F2, kernel_size=(1, 1), bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.F2)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(pool2_t, 1), stride=(pool2_t, 1))\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Cabeza din√°mica\n",
    "        self.fc = None\n",
    "        self.out = None\n",
    "        self._T_in = None\n",
    "\n",
    "    def _build_head(self, T_in: int, device: torch.device):\n",
    "        # Con padding 'same' en temporal y separable temporal,\n",
    "        # el tama√±o temporal se reduce por los pools:\n",
    "        T1 = T_in // self.pool1_t\n",
    "        T2 = T1 // self.pool2_t\n",
    "        feat_dim = self.F2 * T2 * 1  # ancho=1 tras conv_depthwise (kernel (1,C))\n",
    "        self.fc = nn.Linear(feat_dim, 80, bias=True).to(device)\n",
    "        self.out = nn.Linear(80, self.n_classes, bias=True).to(device)\n",
    "        self._T_in = T_in\n",
    "\n",
    "    def ensure_head(self, T_in: int, device: torch.device):\n",
    "        if (self.fc is None) or (self.out is None) or (self._T_in != T_in):\n",
    "            self._build_head(T_in, device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B,1,T,C)\n",
    "        B, _, T, C = x.shape\n",
    "        self.ensure_head(T, x.device)\n",
    "\n",
    "        z = self.conv_temporal(x)\n",
    "        z = self.bn1(z); z = self.act(z)\n",
    "\n",
    "        z = self.conv_depthwise(z)   # (B, F2, T, 1)\n",
    "        z = self.bn2(z); z = self.act(z)\n",
    "        z = self.pool1(z)\n",
    "        z = self.drop1(z)\n",
    "\n",
    "        z = self.conv_sep_depth(z)\n",
    "        z = self.conv_sep_point(z)\n",
    "        z = self.bn3(z); z = self.act(z)\n",
    "        z = self.pool2(z)\n",
    "        z = self.drop2(z)\n",
    "\n",
    "        z = self.flatten(z)\n",
    "        z = self.fc(z); z = self.act(z)\n",
    "        z = self.out(z)\n",
    "        return z\n",
    "\n",
    "# =========================\n",
    "# TORCH DATASET\n",
    "# =========================\n",
    "class EEGTrials(Dataset):\n",
    "    def __init__(self, X, y, groups):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.g = groups.astype(np.int64)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        x = np.expand_dims(x, 0)                 # (1, T, C)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx])\n",
    "\n",
    "CLASS_NAMES_4C = ['Left', 'Right', 'Both Fists', 'Both Feet']\n",
    "\n",
    "# =========================\n",
    "# ENTRENAMIENTO / EVALUACI√ìN\n",
    "# =========================\n",
    "def train_epoch(model, loader, opt, criterion):\n",
    "    model.train()\n",
    "    for xb, yb, _ in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_with_preds(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for xb, yb, _ in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        pred = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "        y_pred.extend(pred)\n",
    "        y_true.extend(yb.numpy().tolist())\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    return y_true, y_pred, float(acc)\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes, title, fname):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.nan_to_num(cm_norm)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm_norm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "        plt.text(j, i, format(cm_norm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm_norm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def print_report(y_true, y_pred, classes):\n",
    "    print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
    "\n",
    "# =========================\n",
    "# FINE-TUNING PROGRESIVO por sujeto (con validaci√≥n interna)\n",
    "# =========================\n",
    "def _param_groups(model, mode):\n",
    "    \"\"\"\n",
    "    En EEGNet:\n",
    "      - 'out'            : solo capa final (model.out)\n",
    "      - 'head'           : fc + out\n",
    "      - 'spatial+head'   : depthwise + separable + fc + out  (temporal queda congelada)\n",
    "    \"\"\"\n",
    "    if mode == 'out':\n",
    "        train = list(model.out.parameters())\n",
    "    elif mode == 'head':\n",
    "        train = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "    elif mode == 'spatial+head':\n",
    "        train = (list(model.conv_depthwise.parameters()) +\n",
    "                 list(model.bn2.parameters()) +\n",
    "                 list(model.conv_sep_depth.parameters()) +\n",
    "                 list(model.conv_sep_point.parameters()) +\n",
    "                 list(model.bn3.parameters()) +\n",
    "                 list(model.fc.parameters()) +\n",
    "                 list(model.out.parameters()))\n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "    return train\n",
    "\n",
    "def _freeze_for_mode(model, mode):\n",
    "    # Congelamos todo\n",
    "    for p in model.parameters(): p.requires_grad = False\n",
    "    # Siempre mantenemos CONGELADO el bloque temporal en este protocolo\n",
    "    # (conv_temporal + bn1)\n",
    "    if mode == 'out':\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'head':\n",
    "        for p in model.fc.parameters():  p.requires_grad = True\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'spatial+head':\n",
    "        for p in model.conv_depthwise.parameters(): p.requires_grad = True\n",
    "        for p in model.bn2.parameters():           p.requires_grad = True\n",
    "        for p in model.conv_sep_depth.parameters():p.requires_grad = True\n",
    "        for p in model.conv_sep_point.parameters():p.requires_grad = True\n",
    "        for p in model.bn3.parameters():           p.requires_grad = True\n",
    "        for p in model.fc.parameters():            p.requires_grad = True\n",
    "        for p in model.out.parameters():           p.requires_grad = True\n",
    "\n",
    "def _class_weights(y_np, n_classes):\n",
    "    counts = np.bincount(y_np, minlength=n_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    weights = counts.sum() / counts\n",
    "    weights = weights / weights.mean()\n",
    "    return torch.tensor(weights, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "def _train_one_mode(model, X_cal, y_cal, n_classes, mode,\n",
    "                    epochs=FT_EPOCHS, batch_size=16,\n",
    "                    head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                    l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO):\n",
    "    \"\"\"\n",
    "    Entrena en 'mode' con early stopping sobre un conjunto de validaci√≥n interno.\n",
    "    Devuelve el modelo con los mejores pesos (por val loss).\n",
    "    \"\"\"\n",
    "    # Split Cal -> (train_cal, val_cal)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=RANDOM_STATE)\n",
    "    (tr_idx, va_idx), = sss.split(X_cal, y_cal)\n",
    "    Xtr, ytr = X_cal[tr_idx], y_cal[tr_idx]\n",
    "    Xva, yva = X_cal[va_idx], y_cal[va_idx]\n",
    "\n",
    "    ds_tr = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xtr).float().unsqueeze(1),\n",
    "        torch.from_numpy(ytr).long()\n",
    "    )\n",
    "    ds_va = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xva).float().unsqueeze(1),\n",
    "        torch.from_numpy(yva).long()\n",
    "    )\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    _freeze_for_mode(model, mode)\n",
    "\n",
    "    if mode == 'spatial+head':\n",
    "        # grupos con LR discriminativos\n",
    "        base_params = (list(model.conv_depthwise.parameters()) +\n",
    "                       list(model.bn2.parameters()) +\n",
    "                       list(model.conv_sep_depth.parameters()) +\n",
    "                       list(model.conv_sep_point.parameters()) +\n",
    "                       list(model.bn3.parameters()))\n",
    "        head_params = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "        train_params = base_params + head_params\n",
    "        opt = optim.Adam([\n",
    "            {\"params\": base_params, \"lr\": base_lr},\n",
    "            {\"params\": head_params, \"lr\": head_lr},\n",
    "        ])\n",
    "    else:\n",
    "        train_params = _param_groups(model, mode)\n",
    "        opt = optim.Adam(train_params, lr=head_lr)\n",
    "\n",
    "    ref = [p.detach().clone().to(p.device) for p in train_params]\n",
    "    class_w = _class_weights(ytr, n_classes)\n",
    "    crit = nn.CrossEntropyLoss(weight=class_w)\n",
    "\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_val = float('inf')\n",
    "    bad = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # --- train ---\n",
    "        model.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            # L2-SP hacia referencia de los par√°metros que estamos entrenando\n",
    "            reg = 0.0\n",
    "            for p_cur, p_ref in zip(train_params, ref):\n",
    "                reg = reg + torch.sum((p_cur - p_ref)**2)\n",
    "            loss = loss + l2sp_lambda * reg\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # --- val ---\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            nval = 0\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                loss = crit(logits, yb)\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "                nval += xb.size(0)\n",
    "            val_loss /= max(1, nval)\n",
    "\n",
    "        if val_loss + 1e-7 < best_val:\n",
    "            best_val = val_loss\n",
    "            bad = 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_numpy(model, X_np, device):\n",
    "    model.eval()\n",
    "    xb = torch.from_numpy(X_np).float().unsqueeze(1).to(device)  # (N,1,T,C)\n",
    "    logits = model(xb)\n",
    "    return logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "def subject_cv_finetune_predict_progressive(model_global, Xs, ys, device,\n",
    "                                            n_splits=CALIB_CV_FOLDS, n_classes=4):\n",
    "    \"\"\"\n",
    "    Para un sujeto: 4-fold StratifiedKFold.\n",
    "      - En cada fold: 'out' ‚Üí 'head' ‚Üí 'spatial+head' (temporal congelado)\n",
    "      - Se elige el mejor en el split de holdout del sujeto.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    y_true_full = np.empty_like(ys)\n",
    "    y_pred_full = np.empty_like(ys)\n",
    "\n",
    "    for tr_idx, te_idx in skf.split(Xs, ys):\n",
    "        Xcal, ycal = Xs[tr_idx], ys[tr_idx]\n",
    "        Xho,  yho  = Xs[te_idx], ys[te_idx]\n",
    "\n",
    "        # Stage A: 'out'\n",
    "        m_out = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_out, Xcal, ycal, n_classes, mode='out',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_out = predict_numpy(m_out, Xho, device)\n",
    "        acc_out = (yhat_out == yho).mean()\n",
    "\n",
    "        # Stage B: 'head'\n",
    "        m_head = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_head, Xcal, ycal, n_classes, mode='head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_head = predict_numpy(m_head, Xho, device)\n",
    "        acc_head = (yhat_head == yho).mean()\n",
    "\n",
    "        # Stage C: 'spatial+head'\n",
    "        m_sp = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_sp, Xcal, ycal, n_classes, mode='spatial+head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                        l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_sp = predict_numpy(m_sp, Xho, device)\n",
    "        acc_sp = (yhat_sp == yho).mean()\n",
    "\n",
    "        best_idx = np.argmax([acc_out, acc_head, acc_sp])\n",
    "        yhat_best = [yhat_out, yhat_head, yhat_sp][best_idx]\n",
    "\n",
    "        y_true_full[te_idx] = yho\n",
    "        y_pred_full[te_idx] = yhat_best\n",
    "\n",
    "    return y_true_full, y_pred_full\n",
    "\n",
    "# =========================\n",
    "# FOLDS JSON helpers\n",
    "# =========================\n",
    "def save_group_folds_json_with_indices(subject_ids_str, groups_array, n_splits, out_json_path,\n",
    "                                       created_by=\"dose_experiment\", description=None):\n",
    "    out_json_path = Path(out_json_path)\n",
    "    unique_subjects_int = sorted(np.unique(groups_array).tolist())\n",
    "    subject_ids = [f\"S{sid:03d}\" for sid in unique_subjects_int]\n",
    "\n",
    "    if len(subject_ids) < n_splits:\n",
    "        raise ValueError(f\"n_splits={n_splits} mayor que n√∫mero de sujetos={len(subject_ids)}\")\n",
    "\n",
    "    groups = np.arange(len(subject_ids))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    folds = []\n",
    "    fold_i = 0\n",
    "    for train_idx_grp, test_idx_grp in gkf.split(groups, groups, groups):\n",
    "        fold_i += 1\n",
    "        train_sids = [subject_ids[int(i)] for i in train_idx_grp]\n",
    "        test_sids  = [subject_ids[int(i)] for i in test_idx_grp]\n",
    "\n",
    "        train_sids_int = [int(s[1:]) for s in train_sids]\n",
    "        test_sids_int  = [int(s[1:]) for s in test_sids]\n",
    "\n",
    "        tr_idx = np.where(np.isin(groups_array, train_sids_int))[0].tolist()\n",
    "        te_idx = np.where(np.isin(groups_array, test_sids_int))[0].tolist()\n",
    "\n",
    "        folds.append({\n",
    "            \"fold\": int(fold_i),\n",
    "            \"train\": train_sids,\n",
    "            \"test\": test_sids,\n",
    "            \"tr_idx\": tr_idx,\n",
    "            \"te_idx\": te_idx\n",
    "        })\n",
    "\n",
    "    payload = {\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"created_by\": created_by,\n",
    "        \"description\": description if description is not None else \"\",\n",
    "        \"n_splits\": int(n_splits),\n",
    "        \"n_subjects\": len(subject_ids),\n",
    "        \"subject_ids\": subject_ids,\n",
    "        \"folds\": folds\n",
    "    }\n",
    "\n",
    "    out_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Folds JSON con √≠ndices guardado ‚Üí {out_json_path}\")\n",
    "    return out_json_path\n",
    "\n",
    "def load_group_folds_json(path_json, expected_subject_ids=None, strict_check=True):\n",
    "    path_json = Path(path_json)\n",
    "    if not path_json.exists():\n",
    "        raise FileNotFoundError(f\"No existe {path_json}\")\n",
    "    with open(path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        payload = json.load(f)\n",
    "\n",
    "    subj_json = payload.get(\"subject_ids\", [])\n",
    "    if expected_subject_ids is not None:\n",
    "        expected = sorted(list(expected_subject_ids))\n",
    "        if subj_json != expected:\n",
    "            msg = (\"Los subject_ids del JSON no coinciden con expected_subject_ids.\\n\"\n",
    "                   f\"JSON has {len(subj_json)} subjects, expected {len(expected)}.\\n\"\n",
    "                   f\"First 10 JSON: {subj_json[:10]}\\nFirst 10 expected: {expected[:10]}\")\n",
    "            if strict_check:\n",
    "                raise ValueError(msg)\n",
    "            else:\n",
    "                print(\"WARNING: \" + msg)\n",
    "    return payload\n",
    "\n",
    "# =========================\n",
    "# EXPERIMENTO\n",
    "# =========================\n",
    "def run_experiment(save_folds_json=True, folds_json_path=FOLDS_DIR, folds_json_description=\"GroupKFold folds for comparison\"):\n",
    "    \"\"\"\n",
    "    - Crea/lee JSON con folds por sujeto (incluye tr_idx/te_idx)\n",
    "    - Entrena modelo global por fold (inter-sujeto puro) con validaci√≥n interna por sujetos + ES\n",
    "    - Eval√∫a Global acc en test\n",
    "    - Realiza Fine-Tuning PROGRESIVO por sujeto (4-fold CV) y reporta acc\n",
    "    \"\"\"\n",
    "    mne.set_log_level('WARNING')\n",
    "\n",
    "    # sujetos y dataset\n",
    "    subs = subjects_available()\n",
    "    print(f\"Sujetos elegibles: {len(subs)} ‚Üí {subs[:10]}{'...' if len(subs)>10 else ''}\")\n",
    "\n",
    "    X, y, groups, chs = build_dataset_all(subs, scenario=CLASS_SCENARIO, window_mode=WINDOW_MODE)\n",
    "    N, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Listo para entrenar: N={N} | T={T} | C={C} | clases={n_classes} | sujetos={len(np.unique(groups))}\")\n",
    "\n",
    "    ds = EEGTrials(X, y, groups)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # preparar JSON folds\n",
    "    if folds_json_path is None:\n",
    "        folds_json_path = Path(\"folds\") / f\"group_folds_{N_FOLDS}splits.json\"\n",
    "    else:\n",
    "        folds_json_path = Path(folds_json_path)\n",
    "    folds_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    unique_subs = sorted(np.unique(groups).tolist())\n",
    "    subject_ids_str = [f\"S{s:03d}\" for s in unique_subs]\n",
    "\n",
    "    if not folds_json_path.exists():\n",
    "        if not save_folds_json:\n",
    "            raise FileNotFoundError(f\"Folds JSON no encontrado en {folds_json_path} y save_folds_json=False.\")\n",
    "        save_group_folds_json_with_indices(subject_ids_str, groups, n_splits=N_FOLDS,\n",
    "                                           out_json_path=folds_json_path,\n",
    "                                           created_by=\"Joel_Clasificador\",\n",
    "                                           description=folds_json_description)\n",
    "\n",
    "    payload = load_group_folds_json(folds_json_path, expected_subject_ids=subject_ids_str, strict_check=False)\n",
    "    folds = payload[\"folds\"]\n",
    "\n",
    "    # bucle por folds\n",
    "    global_folds = []\n",
    "    ft_prog_folds = []\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "\n",
    "    for f in folds:\n",
    "        fold = f[\"fold\"]\n",
    "        tr_idx = np.asarray(f.get(\"tr_idx\", []), dtype=int)\n",
    "        te_idx = np.asarray(f.get(\"te_idx\", []), dtype=int)\n",
    "\n",
    "        if tr_idx.size == 0 or te_idx.size == 0:\n",
    "            print(f\"Advertencia: fold {fold} sin √≠ndices tr/te v√°lidos. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        # ===== Split de validaci√≥n por SUJETOS dentro del set de entrenamiento =====\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=GLOBAL_VAL_SPLIT, random_state=RANDOM_STATE)\n",
    "        tr_subj_idx, va_subj_idx = next(gss.split(tr_idx, groups[tr_idx], groups[tr_idx]))\n",
    "        tr_sub_idx = tr_idx[tr_subj_idx]\n",
    "        va_idx     = tr_idx[va_subj_idx]\n",
    "\n",
    "        tr_loader = DataLoader(Subset(ds, tr_sub_idx), batch_size=BATCH_SIZE, shuffle=True,  drop_last=False)\n",
    "        va_loader = DataLoader(Subset(ds, va_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "        te_loader = DataLoader(Subset(ds, te_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "        # ===== EEGNet =====\n",
    "        model = EEGNet(n_ch=C, n_classes=n_classes, F1=8, D=2, kernel_t=64, k_sep=16,\n",
    "                       pool1_t=4, pool2_t=8, dropout=0.5).to(DEVICE)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "        def _acc(loader):\n",
    "            return evaluate_with_preds(model, loader)[2]\n",
    "\n",
    "        # ===== Entrenamiento con logging + EARLY STOPPING por val_acc =====\n",
    "        print(f\"\\n[Fold {fold}/{N_FOLDS}] Entrenando modelo global con validaci√≥n interna por sujetos...\"\n",
    "              f\" (n_train={len(tr_sub_idx)} | n_val={len(va_idx)} | n_test={len(te_idx)})\")\n",
    "\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        best_val = -1.0\n",
    "        bad = 0\n",
    "\n",
    "        for epoch in range(1, EPOCHS_GLOBAL + 1):\n",
    "            train_epoch(model, tr_loader, opt, criterion)\n",
    "\n",
    "            if epoch % LOG_EVERY == 0:\n",
    "                tr_acc = _acc(tr_loader)\n",
    "                va_acc = _acc(va_loader)\n",
    "                print(f\"  √âpoca {epoch:3d} | train_acc={tr_acc:.4f} | val_acc={va_acc:.4f}\")\n",
    "\n",
    "                if va_acc > best_val + 1e-4:\n",
    "                    best_val = va_acc\n",
    "                    best_state = copy.deepcopy(model.state_dict())\n",
    "                    bad = 0\n",
    "                else:\n",
    "                    bad += 1\n",
    "                    if bad >= GLOBAL_PATIENCE:\n",
    "                        print(f\"  Early stopping en √©poca {epoch} (mejor val_acc={best_val:.4f})\")\n",
    "                        break\n",
    "\n",
    "        # cargar mejor estado antes de evaluar en test\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "        # ===== Evaluaci√≥n global (inter-sujeto puro) =====\n",
    "        y_true, y_pred, acc_global = evaluate_with_preds(model, te_loader)\n",
    "        global_folds.append(acc_global)\n",
    "        all_true.append(y_true); all_pred.append(y_pred)\n",
    "\n",
    "        print(f\"[Fold {fold}/{N_FOLDS}] Global acc={acc_global:.4f}\")\n",
    "        print_report(y_true, y_pred, CLASS_NAMES_4C)\n",
    "\n",
    "        # ---------- Fine-tuning PROGRESIVO por sujeto con 4-fold CV ----------\n",
    "        X_te, y_te, g_te = X[te_idx], y[te_idx], groups[te_idx]\n",
    "\n",
    "        y_true_ft_all, y_pred_ft_all = [], []\n",
    "        used_subjects = 0\n",
    "        for sid in np.unique(g_te):\n",
    "            idx = np.where(g_te == sid)[0]\n",
    "            Xs, ys = X_te[idx], y_te[idx]\n",
    "\n",
    "            # Seguridad\n",
    "            if len(ys) < CALIB_CV_FOLDS or len(np.unique(ys)) < 2:\n",
    "                continue\n",
    "\n",
    "            y_true_subj, y_pred_subj = subject_cv_finetune_predict_progressive(\n",
    "                model, Xs, ys, DEVICE, n_splits=CALIB_CV_FOLDS, n_classes=n_classes\n",
    "            )\n",
    "            y_true_ft_all.append(y_true_subj)\n",
    "            y_pred_ft_all.append(y_pred_subj)\n",
    "            used_subjects += 1\n",
    "\n",
    "        if len(y_true_ft_all) > 0:\n",
    "            y_true_ft_all = np.concatenate(y_true_ft_all)\n",
    "            y_pred_ft_all = np.concatenate(y_pred_ft_all)\n",
    "            acc_ft = (y_true_ft_all == y_pred_ft_all).mean()\n",
    "            print(f\"  Fine-tuning PROGRESIVO (por sujeto, {CALIB_CV_FOLDS}-fold CV) acc={acc_ft:.4f} | sujetos={used_subjects}\")\n",
    "            print(f\"  Œî(FT-Global) = {acc_ft - acc_global:+.4f}\")\n",
    "        else:\n",
    "            acc_ft = np.nan\n",
    "            print(\"  Fine-tuning PROGRESIVO no ejecutado (sujeto(s) con muestras insuficientes).\")\n",
    "\n",
    "        ft_prog_folds.append(acc_ft)\n",
    "\n",
    "    # ---------- resultados finales ----------\n",
    "    if len(all_true) > 0:\n",
    "        all_true = np.concatenate(all_true)\n",
    "        all_pred = np.concatenate(all_pred)\n",
    "    else:\n",
    "        all_true = np.array([], dtype=int)\n",
    "        all_pred = np.array([], dtype=int)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTADOS FINALES\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Global folds:\", [f\"{a:.4f}\" for a in global_folds])\n",
    "    if len(global_folds) > 0:\n",
    "        print(f\"Global mean: {np.mean(global_folds):.4f}\")\n",
    "\n",
    "    print(\"Fine-tune PROGRESIVO folds:\", [(\"nan\" if (a is None or np.isnan(a)) else f\"{a:.4f}\") for a in ft_prog_folds])\n",
    "    if len(ft_prog_folds) > 0:\n",
    "        print(f\"Fine-tune PROGRESIVO mean: {np.nanmean(ft_prog_folds):.4f}\")\n",
    "        print(f\"Œî(FT-Global) mean: {np.nanmean(ft_prog_folds) - np.mean(global_folds):+.4f}\")\n",
    "\n",
    "    # Matriz de confusi√≥n global (sobre todos los folds)\n",
    "    if all_true.size > 0:\n",
    "        plot_confusion(all_true, all_pred, CLASS_NAMES_4C,\n",
    "                       title=\"Confusion Matrix - Global Model (All Folds)\",\n",
    "                       fname=\"confusion_global_allfolds.png\")\n",
    "        print(\"\\n‚Ü≥ Matriz de confusi√≥n guardada: confusion_global_allfolds.png\")\n",
    "\n",
    "    return {\n",
    "        \"global_folds\": global_folds,\n",
    "        \"ft_prog_folds\": ft_prog_folds,\n",
    "        \"all_true\": all_true,\n",
    "        \"all_pred\": all_pred,\n",
    "        \"folds_json_path\": str(folds_json_path)\n",
    "    }\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üß† INICIANDO EXPERIMENTO CON EEGNet + FINE-TUNING PROGRESIVO (por sujeto, 4-fold CV)\")\n",
    "    print(f\"üîß Configuraci√≥n: {CLASS_SCENARIO}, {len(EXPECTED_8)} canales, {WINDOW_MODE}\")\n",
    "    print(f\"‚öôÔ∏è  FT: epochs={FT_EPOCHS}, base_lr={FT_BASE_LR}, head_lr={FT_HEAD_LR}, L2SP={FT_L2SP}, patience={FT_PATIENCE}, CV={CALIB_CV_FOLDS}\")\n",
    "    run_experiment()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ff4bc3",
   "metadata": {},
   "source": [
    "# EEGNET + PREENTRENAMIENTO CONTRASTIVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1074d128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Usando dispositivo: cuda\n",
      "üß† INICIANDO EXPERIMENTO CON EEGNet + FINE-TUNING PROGRESIVO (por sujeto, 4-fold CV)\n",
      "üîß Configuraci√≥n: 4c, 8 canales, 3s\n",
      "‚öôÔ∏è  FT: epochs=30, base_lr=5e-05, head_lr=0.001, L2SP=0.0001, patience=5, CV=4\n",
      "üß≤ SupCon: enabled=True, epochs=40, batch=64, temp=0.07\n",
      "Sujetos elegibles: 103 ‚Üí [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Construyendo dataset (RAW):   0%|          | 0/103 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Construyendo dataset (RAW): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:16<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset construido: N=8652 | T=480 | C=8 | clases=4 | sujetos √∫nicos=103\n",
      "Listo para entrenar: N=8652 | T=480 | C=8 | clases=4 | sujetos=103\n",
      "[Fold 1/5] SupCon pretrain on 5796 trials...\n",
      "[SupCon] Epoch 005/40 | loss=5.2721\n",
      "[SupCon] Epoch 010/40 | loss=5.2681\n",
      "[SupCon] Epoch 015/40 | loss=5.2670\n",
      "[SupCon] Epoch 020/40 | loss=5.2654\n",
      "[SupCon] Epoch 025/40 | loss=5.2655\n",
      "[SupCon] Epoch 030/40 | loss=5.2654\n",
      "[SupCon] Epoch 035/40 | loss=5.2648\n",
      "[SupCon] Epoch 040/40 | loss=5.2650\n",
      "[SupCon] Preentrenamiento contrastivo completado (backbone inicializado).\n",
      "\n",
      "[Fold 1/5] Entrenando modelo global con validaci√≥n interna por sujetos... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  √âpoca   5 | train_acc=0.5012 | val_acc=0.4267\n",
      "  √âpoca  10 | train_acc=0.5173 | val_acc=0.4423\n",
      "  √âpoca  15 | train_acc=0.5317 | val_acc=0.4551\n",
      "  √âpoca  20 | train_acc=0.5404 | val_acc=0.4524\n",
      "  √âpoca  25 | train_acc=0.5540 | val_acc=0.4505\n",
      "  √âpoca  30 | train_acc=0.5659 | val_acc=0.4533\n",
      "  √âpoca  35 | train_acc=0.5773 | val_acc=0.4505\n",
      "  √âpoca  40 | train_acc=0.6059 | val_acc=0.4359\n",
      "  √âpoca  45 | train_acc=0.6168 | val_acc=0.4414\n",
      "  √âpoca  50 | train_acc=0.6292 | val_acc=0.4432\n",
      "  √âpoca  55 | train_acc=0.6366 | val_acc=0.4341\n",
      "  √âpoca  60 | train_acc=0.6511 | val_acc=0.4423\n",
      "  √âpoca  65 | train_acc=0.6767 | val_acc=0.4322\n",
      "  Early stopping en √©poca 65 (mejor val_acc=0.4551)\n",
      "[Fold 1/5] Global acc=0.4535\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5021    0.5306    0.5160       441\n",
      "       Right     0.4641    0.5283    0.4942       441\n",
      "  Both Fists     0.3584    0.2812    0.3151       441\n",
      "   Both Feet     0.4644    0.4739    0.4691       441\n",
      "\n",
      "    accuracy                         0.4535      1764\n",
      "   macro avg     0.4473    0.4535    0.4486      1764\n",
      "weighted avg     0.4473    0.4535    0.4486      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5074 | sujetos=21\n",
      "  Œî(FT-Global) = +0.0539\n",
      "[Fold 2/5] SupCon pretrain on 5796 trials...\n",
      "[SupCon] Epoch 005/40 | loss=5.2726\n",
      "[SupCon] Epoch 010/40 | loss=5.2683\n",
      "[SupCon] Epoch 015/40 | loss=5.2668\n",
      "[SupCon] Epoch 020/40 | loss=5.2655\n",
      "[SupCon] Epoch 025/40 | loss=5.2649\n",
      "[SupCon] Epoch 030/40 | loss=5.2655\n",
      "[SupCon] Epoch 035/40 | loss=5.2651\n",
      "[SupCon] Epoch 040/40 | loss=5.2641\n",
      "[SupCon] Preentrenamiento contrastivo completado (backbone inicializado).\n",
      "\n",
      "[Fold 2/5] Entrenando modelo global con validaci√≥n interna por sujetos... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  √âpoca   5 | train_acc=0.4741 | val_acc=0.4386\n",
      "  √âpoca  10 | train_acc=0.4938 | val_acc=0.4725\n",
      "  √âpoca  15 | train_acc=0.5140 | val_acc=0.4579\n",
      "  √âpoca  20 | train_acc=0.5141 | val_acc=0.4551\n",
      "  √âpoca  25 | train_acc=0.5316 | val_acc=0.4496\n",
      "  √âpoca  30 | train_acc=0.5547 | val_acc=0.4643\n",
      "  √âpoca  35 | train_acc=0.5685 | val_acc=0.4634\n",
      "  √âpoca  40 | train_acc=0.5982 | val_acc=0.4606\n",
      "  √âpoca  45 | train_acc=0.6061 | val_acc=0.4679\n",
      "  √âpoca  50 | train_acc=0.6285 | val_acc=0.4762\n",
      "  √âpoca  55 | train_acc=0.6518 | val_acc=0.4615\n",
      "  √âpoca  60 | train_acc=0.6610 | val_acc=0.4625\n",
      "  √âpoca  65 | train_acc=0.6843 | val_acc=0.4542\n",
      "  √âpoca  70 | train_acc=0.7013 | val_acc=0.4487\n",
      "  √âpoca  75 | train_acc=0.7067 | val_acc=0.4515\n",
      "  √âpoca  80 | train_acc=0.7276 | val_acc=0.4423\n",
      "  √âpoca  85 | train_acc=0.7348 | val_acc=0.4652\n",
      "  √âpoca  90 | train_acc=0.7459 | val_acc=0.4460\n",
      "  √âpoca  95 | train_acc=0.7384 | val_acc=0.4386\n",
      "  √âpoca 100 | train_acc=0.7465 | val_acc=0.4551\n",
      "  Early stopping en √©poca 100 (mejor val_acc=0.4762)\n",
      "[Fold 2/5] Global acc=0.5091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5692    0.5782    0.5737       441\n",
      "       Right     0.5644    0.5760    0.5701       441\n",
      "  Both Fists     0.4226    0.5261    0.4687       441\n",
      "   Both Feet     0.4953    0.3560    0.4142       441\n",
      "\n",
      "    accuracy                         0.5091      1764\n",
      "   macro avg     0.5129    0.5091    0.5067      1764\n",
      "weighted avg     0.5129    0.5091    0.5067      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5788 | sujetos=21\n",
      "  Œî(FT-Global) = +0.0697\n",
      "[Fold 3/5] SupCon pretrain on 5796 trials...\n",
      "[SupCon] Epoch 005/40 | loss=5.2713\n",
      "[SupCon] Epoch 010/40 | loss=5.2679\n",
      "[SupCon] Epoch 015/40 | loss=5.2666\n",
      "[SupCon] Epoch 020/40 | loss=5.2662\n",
      "[SupCon] Epoch 025/40 | loss=5.2656\n",
      "[SupCon] Epoch 030/40 | loss=5.2653\n",
      "[SupCon] Epoch 035/40 | loss=5.2654\n",
      "[SupCon] Epoch 040/40 | loss=5.2642\n",
      "[SupCon] Preentrenamiento contrastivo completado (backbone inicializado).\n",
      "\n",
      "[Fold 3/5] Entrenando modelo global con validaci√≥n interna por sujetos... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  √âpoca   5 | train_acc=0.5060 | val_acc=0.4734\n",
      "  √âpoca  10 | train_acc=0.5278 | val_acc=0.4780\n",
      "  √âpoca  15 | train_acc=0.5309 | val_acc=0.4890\n",
      "  √âpoca  20 | train_acc=0.5457 | val_acc=0.5000\n",
      "  √âpoca  25 | train_acc=0.5609 | val_acc=0.5027\n",
      "  √âpoca  30 | train_acc=0.5754 | val_acc=0.4780\n",
      "  √âpoca  35 | train_acc=0.5925 | val_acc=0.4936\n",
      "  √âpoca  40 | train_acc=0.6102 | val_acc=0.4881\n",
      "  √âpoca  45 | train_acc=0.6241 | val_acc=0.4936\n",
      "  √âpoca  50 | train_acc=0.6349 | val_acc=0.4881\n",
      "  √âpoca  55 | train_acc=0.6561 | val_acc=0.4945\n",
      "  √âpoca  60 | train_acc=0.6511 | val_acc=0.4945\n",
      "  √âpoca  65 | train_acc=0.6674 | val_acc=0.4844\n",
      "  √âpoca  70 | train_acc=0.6900 | val_acc=0.4716\n",
      "  √âpoca  75 | train_acc=0.6844 | val_acc=0.4808\n",
      "  Early stopping en √©poca 75 (mejor val_acc=0.5027)\n",
      "[Fold 3/5] Global acc=0.4376\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.4729    0.5147    0.4929       441\n",
      "       Right     0.5154    0.4558    0.4838       441\n",
      "  Both Fists     0.3855    0.3855    0.3855       441\n",
      "   Both Feet     0.3841    0.3946    0.3893       441\n",
      "\n",
      "    accuracy                         0.4376      1764\n",
      "   macro avg     0.4395    0.4376    0.4379      1764\n",
      "weighted avg     0.4395    0.4376    0.4379      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.4779 | sujetos=21\n",
      "  Œî(FT-Global) = +0.0402\n",
      "[Fold 4/5] SupCon pretrain on 5880 trials...\n",
      "[SupCon] Epoch 005/40 | loss=5.2720\n",
      "[SupCon] Epoch 010/40 | loss=5.2680\n",
      "[SupCon] Epoch 015/40 | loss=5.2665\n",
      "[SupCon] Epoch 020/40 | loss=5.2659\n",
      "[SupCon] Epoch 025/40 | loss=5.2665\n",
      "[SupCon] Epoch 030/40 | loss=5.2653\n",
      "[SupCon] Epoch 035/40 | loss=5.2655\n",
      "[SupCon] Epoch 040/40 | loss=5.2649\n",
      "[SupCon] Preentrenamiento contrastivo completado (backbone inicializado).\n",
      "\n",
      "[Fold 4/5] Entrenando modelo global con validaci√≥n interna por sujetos... (n_train=5880 | n_val=1092 | n_test=1680)\n",
      "  √âpoca   5 | train_acc=0.4990 | val_acc=0.4405\n",
      "  √âpoca  10 | train_acc=0.5158 | val_acc=0.4505\n",
      "  √âpoca  15 | train_acc=0.5376 | val_acc=0.4762\n",
      "  √âpoca  20 | train_acc=0.5541 | val_acc=0.4652\n",
      "  √âpoca  25 | train_acc=0.5616 | val_acc=0.4835\n",
      "  √âpoca  30 | train_acc=0.5740 | val_acc=0.4789\n",
      "  √âpoca  35 | train_acc=0.6061 | val_acc=0.4753\n",
      "  √âpoca  40 | train_acc=0.6075 | val_acc=0.4634\n",
      "  √âpoca  45 | train_acc=0.6342 | val_acc=0.4872\n",
      "  √âpoca  50 | train_acc=0.6417 | val_acc=0.4652\n",
      "  √âpoca  55 | train_acc=0.6553 | val_acc=0.4634\n",
      "  √âpoca  60 | train_acc=0.6733 | val_acc=0.4670\n",
      "  √âpoca  65 | train_acc=0.6881 | val_acc=0.4652\n",
      "  √âpoca  70 | train_acc=0.6908 | val_acc=0.4707\n",
      "  √âpoca  75 | train_acc=0.7048 | val_acc=0.4524\n",
      "  √âpoca  80 | train_acc=0.7026 | val_acc=0.4689\n",
      "  √âpoca  85 | train_acc=0.7230 | val_acc=0.4707\n",
      "  √âpoca  90 | train_acc=0.7400 | val_acc=0.4469\n",
      "  √âpoca  95 | train_acc=0.7378 | val_acc=0.4661\n",
      "  Early stopping en √©poca 95 (mejor val_acc=0.4872)\n",
      "[Fold 4/5] Global acc=0.4899\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5252    0.5452    0.5350       420\n",
      "       Right     0.5220    0.5643    0.5423       420\n",
      "  Both Fists     0.4033    0.4667    0.4327       420\n",
      "   Both Feet     0.5296    0.3833    0.4448       420\n",
      "\n",
      "    accuracy                         0.4899      1680\n",
      "   macro avg     0.4950    0.4899    0.4887      1680\n",
      "weighted avg     0.4950    0.4899    0.4887      1680\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c7e7930d2770>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"‚öôÔ∏è  FT: epochs={FT_EPOCHS}, base_lr={FT_BASE_LR}, head_lr={FT_HEAD_LR}, L2SP={FT_L2SP}, patience={FT_PATIENCE}, CV={CALIB_CV_FOLDS}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"üß≤ SupCon: enabled={USE_SUPCON_PRETRAIN}, epochs={SUPCON_EPOCHS}, batch={SUPCON_BATCH}, temp={SUPCON_TEMP}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m     \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-c7e7930d2770>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(save_folds_json, folds_json_path, folds_json_description)\u001b[0m\n\u001b[1;32m   1085\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m             y_true_subj, y_pred_subj = subject_cv_finetune_predict_progressive(\n\u001b[0m\u001b[1;32m   1088\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCALIB_CV_FOLDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m             )\n",
      "\u001b[0;32m<ipython-input-2-c7e7930d2770>\u001b[0m in \u001b[0;36msubject_cv_finetune_predict_progressive\u001b[0;34m(model_global, Xs, ys, device, n_splits, n_classes)\u001b[0m\n\u001b[1;32m    850\u001b[0m         \u001b[0;31m# Stage B: 'head'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0mm_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_global\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m         _train_one_mode(m_head, Xcal, ycal, n_classes, mode='head',\n\u001b[0m\u001b[1;32m    853\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFT_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFT_HEAD_LR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2sp_lambda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFT_L2SP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m                         patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
      "\u001b[0;32m<ipython-input-2-c7e7930d2770>\u001b[0m in \u001b[0;36m_train_one_mode\u001b[0;34m(model, X_cal, y_cal, n_classes, mode, epochs, batch_size, head_lr, base_lr, l2sp_lambda, patience, val_ratio)\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml2sp_lambda\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;31m# --- val ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m                             )\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    262\u001b[0m                 \u001b[0mfused\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fused\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                 \u001b[0mgrad_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"grad_scale\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m                 \u001b[0mfound_inf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"found_inf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             )\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Replicaci√≥n fiel del paper \"A Deep Learning MI-EEG Classification Model for BCIs\"\n",
    "# Dose et al., EUSIPCO 2018 ‚Äî ahora con EEGNet (Lawhern et al., 2018) en vez de ShallowConvNet\n",
    "# Protocolo: entrenamiento global inter-sujeto + FINE-TUNING PROGRESIVO por sujeto\n",
    "# (CV 4-fold, LRs discriminativos, L2-SP, early stopping con validaci√≥n)\n",
    "# + SupCon preentrenamiento contrastivo supervisado (opcional)\n",
    "\n",
    "import os, re, math, random, json, itertools, copy\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold, StratifiedShuffleSplit, GroupShuffleSplit\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# CONFIGURACI√ìN GENERAL\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "FOLDS_DIR = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dispositivo y semilla\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "# Escenario de clases\n",
    "CLASS_SCENARIO = '4c'\n",
    "WINDOW_MODE = '3s'\n",
    "FS = 160.0\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Entrenamiento global\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS_GLOBAL = 100\n",
    "LR = 1e-3\n",
    "# >>> A√±adidos para diagn√≥stico/ES global <<<\n",
    "GLOBAL_VAL_SPLIT = 0.15   # fracci√≥n de sujetos (dentro del train) para validaci√≥n\n",
    "GLOBAL_PATIENCE  = 10     # √©pocas sin mejora en val_acc\n",
    "LOG_EVERY        = 5      # log cada N √©pocas\n",
    "\n",
    "# Fine-tuning por sujeto (protocolo robusto)\n",
    "CALIB_CV_FOLDS = 4            # 4-fold CV por sujeto ~ 75/25\n",
    "FT_EPOCHS = 30                 # ES con validaci√≥n\n",
    "FT_BASE_LR = 5e-5              # convs \"base\" (temporal/depthwise) ‚Äî m√°s bajo\n",
    "FT_HEAD_LR = 1e-3              # fc+out ‚Äî m√°s alto\n",
    "FT_L2SP = 1e-4                 # regularizaci√≥n suave\n",
    "FT_PATIENCE = 5                # early stopping con validaci√≥n\n",
    "FT_VAL_RATIO = 0.2             # validaci√≥n dentro del set de calibraci√≥n\n",
    "\n",
    "# Sujetos excluidos\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "\n",
    "# Runs\n",
    "MI_RUNS_LR = [4, 8, 12]\n",
    "MI_RUNS_OF = [6, 10, 14]\n",
    "BASELINE_RUNS_EO = [1]\n",
    "\n",
    "# Canales (8 en lugar de 64)\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','CPz']\n",
    "\n",
    "# ====== SupCon (preentrenamiento contrastivo supervisado) ======\n",
    "USE_SUPCON_PRETRAIN = True      # <- pon False para desactivarlo\n",
    "SUPCON_EPOCHS = 40\n",
    "SUPCON_BATCH = 64               # intenta que sea >=64 si cabe en GPU\n",
    "SUPCON_LR = 1e-3\n",
    "SUPCON_TEMP = 0.07\n",
    "SUPCON_PROJ_DIM = 128\n",
    "SUPCON_LOG_EVERY = 5\n",
    "\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES DE CANALES\n",
    "# =========================\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = re.sub(r'([A-Za-z])0([0-9])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])Z$', r'\\1z', s)\n",
    "    s = s.replace('fp', 'Fp').replace('FP', 'Fp')\n",
    "    s = ''.join(ch.upper() if ch != 'z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {}\n",
    "    for ch in raw.ch_names:\n",
    "        lab = normalize_label(ch)\n",
    "        lab = lab[:-1] + 'z' if lab.endswith('Z') else lab\n",
    "        lab = re.sub(r'([A-Z])Z$', r'\\1z', lab)\n",
    "        mapping[ch] = lab\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired_channels=EXPECTED_8):\n",
    "    have = [ch for ch in desired_channels if ch in raw.ch_names]\n",
    "    missing = [ch for ch in desired_channels if ch not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"Warning: faltan canales {missing} en archivo {getattr(raw,'filenames', [''])[0]}\")\n",
    "        return None\n",
    "    # Reordenar y quedarse SOLO con los deseados (8)\n",
    "    raw.reorder_channels([ch for ch in raw.ch_names if ch in desired_channels] +\n",
    "                         [ch for ch in raw.ch_names if ch not in desired_channels])\n",
    "    raw.pick_channels(desired_channels, ordered=True)\n",
    "    return raw\n",
    "\n",
    "# =========================\n",
    "# LECTURA DE EDF y EVENTOS\n",
    "# =========================\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(run_id:int):\n",
    "    if run_id in MI_RUNS_LR: return 'LR'\n",
    "    if run_id in MI_RUNS_OF: return 'OF'\n",
    "    if run_id in BASELINE_RUNS_EO: return 'EO'\n",
    "    return None\n",
    "\n",
    "def read_raw_edf(path: Path):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        mont = mne.channels.make_standard_montage('standard_1020')\n",
    "        raw.set_montage(mont, on_missing='ignore')\n",
    "    except Exception:\n",
    "        pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad=\"auto\")\n",
    "    raw = ensure_channels_order(raw, EXPECTED_8)\n",
    "    if raw is None:\n",
    "        return None\n",
    "    # Band-pass opcional (desactivado)\n",
    "    #raw.filter(l_freq=8., h_freq=30., picks='eeg', method='iir', verbose=False)\n",
    "    return raw\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    if raw.annotations is None or len(raw.annotations) == 0:\n",
    "        return []\n",
    "    def _norm(s): return str(s).strip().upper().replace(' ', '')\n",
    "    res = []\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = _norm(desc)\n",
    "        if tag in ('T1','T2'):\n",
    "            res.append((float(onset), tag))\n",
    "    res.sort()\n",
    "    dedup = []\n",
    "    last_t1 = last_t2 = -1e9\n",
    "    for t, tag in res:\n",
    "        if tag == 'T1':\n",
    "            if (t - last_t1) >= 0.5: dedup.append((t, tag)); last_t1 = t\n",
    "        else:\n",
    "            if (t - last_t2) >= 0.5: dedup.append((t, tag)); last_t2 = t\n",
    "    return dedup\n",
    "\n",
    "# =========================\n",
    "# CONSTRUCCI√ìN DE DATASETS\n",
    "# =========================\n",
    "def subjects_available():\n",
    "    subs = []\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try:\n",
    "            sid = int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir / f\"S{sid:03d}R{r:02d}.edf\").exists() for r in (MI_RUNS_LR + MI_RUNS_OF))\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "def extract_trials_from_run(edf_path: Path, scenario: str, window_mode: str):\n",
    "    subj, run = parse_subject_run(edf_path)\n",
    "    kind = run_kind(run)\n",
    "    if kind not in ('LR','OF','EO'):\n",
    "        return ([], [])\n",
    "\n",
    "    raw = read_raw_edf(edf_path)\n",
    "    if raw is None:\n",
    "        return ([], [])\n",
    "\n",
    "    data = raw.get_data()\n",
    "    fs = raw.info['sfreq']\n",
    "    assert abs(fs - FS) < 1e-6\n",
    "\n",
    "    out = []\n",
    "\n",
    "    if kind in ('LR','OF'):\n",
    "        events = collect_events_T1T2(raw)\n",
    "        if window_mode == '3s':\n",
    "            rel_start, rel_end = 0.0, 3.0\n",
    "        else:\n",
    "            rel_start, rel_end = -1.0, 5.0\n",
    "\n",
    "        for onset_sec, tag in events:\n",
    "            if kind == 'LR':\n",
    "                if tag == 'T1': label = 'L'\n",
    "                elif tag == 'T2': label = 'R'\n",
    "                else: continue\n",
    "            else:\n",
    "                if tag == 'T1': label = 'BFISTS'\n",
    "                elif tag == 'T2': label = 'BFEET'\n",
    "                else: continue\n",
    "\n",
    "            if scenario == '2c' and label not in ('L','R'):\n",
    "                continue\n",
    "            if scenario == '3c' and label not in ('L','R','BFISTS'):\n",
    "                continue\n",
    "            if scenario == '4c' and label not in ('L','R','BFISTS','BFEET'):\n",
    "                continue\n",
    "\n",
    "            s = int(round((raw.first_time + onset_sec + rel_start) * fs))\n",
    "            e = int(round((raw.first_time + onset_sec + rel_end) * fs))\n",
    "            if s < 0 or e > data.shape[1]:\n",
    "                continue\n",
    "\n",
    "            seg = data[:, s:e].T.astype(np.float32)\n",
    "            # Normalizaci√≥n por √©poca canal-a-canal (z-score)\n",
    "            seg = (seg - seg.mean(axis=0, keepdims=True)) / (seg.std(axis=0, keepdims=True) + 1e-6)\n",
    "\n",
    "            if label == 'L':       y = 0\n",
    "            elif label == 'R':     y = 1\n",
    "            elif label == 'BFISTS':y = 2\n",
    "            elif label == 'BFEET': y = 3\n",
    "            else: continue\n",
    "\n",
    "            out.append((seg, y, subj))\n",
    "\n",
    "    elif kind == 'EO':\n",
    "        return ([], raw.ch_names)\n",
    "\n",
    "    return out, raw.ch_names\n",
    "\n",
    "def build_dataset_all(subjects, scenario='4c', window_mode='3s'):\n",
    "    X, y, groups = [], [], []\n",
    "    ch_template = None\n",
    "\n",
    "    for s in tqdm(subjects, desc=\"Construyendo dataset (RAW)\"):\n",
    "        sdir = DATA_RAW / f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "\n",
    "        trials_L, trials_R, trials_FISTS, trials_FEET = [], [], [], []\n",
    "\n",
    "        for r in MI_RUNS_LR:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 0: trials_L.append(seg)\n",
    "                elif lab == 1: trials_R.append(seg)\n",
    "\n",
    "        for r in MI_RUNS_OF:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 2: trials_FISTS.append(seg)\n",
    "                elif lab == 3: trials_FEET.append(seg)\n",
    "\n",
    "        need_per_class = 21\n",
    "        def pick(trials, n, rng):\n",
    "            if len(trials) < n:\n",
    "                idx = rng.choice(len(trials), size=n, replace=True)\n",
    "                return [trials[i] for i in idx]\n",
    "            rng.shuffle(trials)\n",
    "            return trials[:n]\n",
    "\n",
    "        rng = check_random_state(RANDOM_STATE + s)\n",
    "        if len(trials_L)==0 or len(trials_R)==0 or len(trials_FISTS)==0 or len(trials_FEET)==0:\n",
    "            continue\n",
    "\n",
    "        Lp  = pick(trials_L,     need_per_class, rng)\n",
    "        Rp  = pick(trials_R,     need_per_class, rng)\n",
    "        FIp = pick(trials_FISTS, need_per_class, rng)\n",
    "        FEp = pick(trials_FEET,  need_per_class, rng)\n",
    "\n",
    "        pack = [(Lp, 0), (Rp, 1), (FIp, 2), (FEp, 3)]\n",
    "        for segs, lab in pack:\n",
    "            for seg in segs:\n",
    "                X.append(seg)\n",
    "                y.append(lab)\n",
    "                groups.append(s)\n",
    "\n",
    "    X = np.stack(X, axis=0)\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "    groups = np.asarray(groups, dtype=np.int64)\n",
    "\n",
    "    n, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Dataset construido: N={n} | T={T} | C={C} | clases={n_classes} | sujetos √∫nicos={len(np.unique(groups))}\")\n",
    "    return X, y, groups, ch_template\n",
    "\n",
    "# =========================\n",
    "# EEGNet (Lawhern et al., 2018) adaptado a (B,1,T,C)\n",
    "# =========================\n",
    "class EEGNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Entrada: x de forma (B, 1, T, C)  [T=tiempo, C=canales]\n",
    "    Bloques:\n",
    "      1) Temporal conv     : Conv2d(1 -> F1, (kernel_t,1), padding 'same'), BN, ELU\n",
    "      2) Depthwise (espacial): Conv2d(F1 -> F1*D, (1,C), groups=F1, BN, ELU, AvgPool(4,1), Dropout\n",
    "      3) Separable temporal: Depthwise temporal (k_sep,1) groups=F1*D + Pointwise 1x1 a F2, BN, ELU, AvgPool(8,1), Dropout\n",
    "      4) FC -> OUT\n",
    "    \"\"\"\n",
    "    def __init__(self, n_ch: int, n_classes: int,\n",
    "                 F1: int = 8, D: int = 2, kernel_t: int = 64, k_sep: int = 16,\n",
    "                 pool1_t: int = 4, pool2_t: int = 8, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.n_ch = n_ch\n",
    "        self.n_classes = n_classes\n",
    "        self.F1 = F1\n",
    "        self.D = D\n",
    "        self.F2 = F1 * D\n",
    "        self.kernel_t = kernel_t\n",
    "        self.k_sep = k_sep\n",
    "        self.pool1_t = pool1_t\n",
    "        self.pool2_t = pool2_t\n",
    "\n",
    "        # Bloque 1: temporal\n",
    "        self.conv_temporal = nn.Conv2d(1, F1, kernel_size=(kernel_t, 1),\n",
    "                                       padding=(kernel_t // 2, 0), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(F1)\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "        # Bloque 2: depthwise (espacial)\n",
    "        self.conv_depthwise = nn.Conv2d(F1, self.F2, kernel_size=(1, n_ch),\n",
    "                                        groups=F1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.F2)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=(pool1_t, 1), stride=(pool1_t, 1))\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "\n",
    "        # Bloque 3: separable temporal (depthwise temporal + pointwise)\n",
    "        self.conv_sep_depth = nn.Conv2d(self.F2, self.F2, kernel_size=(k_sep, 1),\n",
    "                                        groups=self.F2, padding=(k_sep // 2, 0), bias=False)\n",
    "        self.conv_sep_point = nn.Conv2d(self.F2, self.F2, kernel_size=(1, 1), bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.F2)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(pool2_t, 1), stride=(pool2_t, 1))\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Cabeza din√°mica\n",
    "        self.fc = None\n",
    "        self.out = None\n",
    "        self._T_in = None\n",
    "\n",
    "    def _build_head(self, T_in: int, device: torch.device):\n",
    "        # Con padding 'same' en temporal y separable temporal,\n",
    "        # el tama√±o temporal se reduce por los pools:\n",
    "        T1 = T_in // self.pool1_t\n",
    "        T2 = T1 // self.pool2_t\n",
    "        feat_dim = self.F2 * T2 * 1  # ancho=1 tras conv_depthwise (kernel (1,C))\n",
    "        self.fc = nn.Linear(feat_dim, 80, bias=True).to(device)\n",
    "        self.out = nn.Linear(80, self.n_classes, bias=True).to(device)\n",
    "        self._T_in = T_in\n",
    "\n",
    "    def ensure_head(self, T_in: int, device: torch.device):\n",
    "        if (self.fc is None) or (self.out is None) or (self._T_in != T_in):\n",
    "            self._build_head(T_in, device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B,1,T,C)\n",
    "        B, _, T, C = x.shape\n",
    "        self.ensure_head(T, x.device)\n",
    "\n",
    "        z = self.conv_temporal(x)\n",
    "        z = self.bn1(z); z = self.act(z)\n",
    "\n",
    "        z = self.conv_depthwise(z)   # (B, F2, T, 1)\n",
    "        z = self.bn2(z); z = self.act(z)\n",
    "        z = self.pool1(z)\n",
    "        z = self.drop1(z)\n",
    "\n",
    "        z = self.conv_sep_depth(z)\n",
    "        z = self.conv_sep_point(z)\n",
    "        z = self.bn3(z); z = self.act(z)\n",
    "        z = self.pool2(z)\n",
    "        z = self.drop2(z)\n",
    "\n",
    "        z = self.flatten(z)\n",
    "        z = self.fc(z); z = self.act(z)\n",
    "        z = self.out(z)\n",
    "        return z\n",
    "\n",
    "    def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Devuelve el embedding pre-logits tras fc+ELU (dim=80).\n",
    "        x: (B,1,T,C)\n",
    "        \"\"\"\n",
    "        B, _, T, C = x.shape\n",
    "        self.ensure_head(T, x.device)\n",
    "\n",
    "        z = self.conv_temporal(x); z = self.bn1(z); z = self.act(z)\n",
    "        z = self.conv_depthwise(z); z = self.bn2(z); z = self.act(z)\n",
    "        z = self.pool1(z); z = self.drop1(z)\n",
    "        z = self.conv_sep_depth(z); z = self.conv_sep_point(z)\n",
    "        z = self.bn3(z); z = self.act(z)\n",
    "        z = self.pool2(z); z = self.drop2(z)\n",
    "        z = self.flatten(z)\n",
    "        z = self.fc(z); z = self.act(z)   # embedding 80-D\n",
    "        return z\n",
    "\n",
    "# =========================\n",
    "# TORCH DATASET\n",
    "# =========================\n",
    "class EEGTrials(Dataset):\n",
    "    def __init__(self, X, y, groups):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.g = groups.astype(np.int64)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        x = np.expand_dims(x, 0)                 # (1, T, C)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx])\n",
    "\n",
    "CLASS_NAMES_4C = ['Left', 'Right', 'Both Fists', 'Both Feet']\n",
    "\n",
    "# ====== Augmentations EEG para SupCon ======\n",
    "class EEGAugment(torch.nn.Module):\n",
    "    def __init__(self, p_jitter=0.5, max_jitter=16,  # ~100 ms a 160 Hz\n",
    "                 p_noise=0.8, noise_std=0.02,\n",
    "                 p_tmask=0.4, tmask_max=32,          # ~200 ms\n",
    "                 p_cdrop=0.2, cdrop_max=1):          # drop 0-1 canales\n",
    "        super().__init__()\n",
    "        self.p_jitter = p_jitter\n",
    "        self.max_jitter = max_jitter\n",
    "        self.p_noise = p_noise\n",
    "        self.noise_std = noise_std\n",
    "        self.p_tmask = p_tmask\n",
    "        self.tmask_max = tmask_max\n",
    "        self.p_cdrop = p_cdrop\n",
    "        self.cdrop_max = cdrop_max\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Acepta:\n",
    "          - (1, T, C)  -> ejemplo √∫nico (3D)\n",
    "          - (B, 1, T, C) -> batch (4D)\n",
    "        Devuelve con la misma dimensionalidad de entrada.\n",
    "        \"\"\"\n",
    "        original_3d = False\n",
    "        if x.dim() == 3:           # (1, T, C)\n",
    "            original_3d = True\n",
    "            x = x.unsqueeze(0)     # -> (B=1, 1, T, C)\n",
    "        elif x.dim() != 4:\n",
    "            raise ValueError(f\"EEGAugment espera 3D o 4D, recibido {x.dim()}D\")\n",
    "\n",
    "        B, _, T, C = x.shape\n",
    "        out = x.clone()\n",
    "\n",
    "        # Jitter temporal\n",
    "        if torch.rand(1).item() < self.p_jitter and T > 2:\n",
    "            shift = int(torch.randint(-self.max_jitter, self.max_jitter + 1, (1,)).item())\n",
    "            if shift > 0:\n",
    "                out[:, :, shift:, :] = out[:, :, :-shift, :].clone()\n",
    "            elif shift < 0:\n",
    "                out[:, :, :shift, :] = out[:, :, -shift:, :].clone()\n",
    "\n",
    "        # Ruido gaussiano leve\n",
    "        if torch.rand(1).item() < self.p_noise:\n",
    "            out = out + torch.randn_like(out) * self.noise_std\n",
    "\n",
    "        # Time masking corto\n",
    "        if torch.rand(1).item() < self.p_tmask and T > 4:\n",
    "            w = int(torch.randint(1, self.tmask_max + 1, (1,)).item())\n",
    "            s = int(torch.randint(0, max(1, T - w), (1,)).item())\n",
    "            out[:, :, s:s + w, :] = 0.0\n",
    "\n",
    "        # Channel dropout\n",
    "        if torch.rand(1).item() < self.p_cdrop and C > 1:\n",
    "            k = int(torch.randint(1, self.cdrop_max + 1, (1,)).item())\n",
    "            ch = torch.randperm(C)[:k]\n",
    "            out[:, :, :, ch] = 0.0\n",
    "\n",
    "        if original_3d:\n",
    "            out = out.squeeze(0)   # vuelve a (1, T, C)\n",
    "        return out\n",
    "\n",
    "class ContrastiveTrials(Dataset):\n",
    "    \"\"\" Devuelve dos vistas aumentadas + etiqueta \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.aug = EEGAugment()\n",
    "\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]               # (T,C)\n",
    "        y = self.y[idx]\n",
    "        x = np.expand_dims(x, 0)      # (1,T,C)\n",
    "        x = torch.from_numpy(x)\n",
    "        v1 = self.aug(x.clone())\n",
    "        v2 = self.aug(x.clone())\n",
    "        return v1, v2, torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# ====== Proyecci√≥n + p√©rdida SupCon ======\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, in_dim=80, proj_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, in_dim, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_dim, proj_dim, bias=True)\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        z = self.net(z)\n",
    "        z = torch.nn.functional.normalize(z, dim=-1)\n",
    "        return z\n",
    "\n",
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Supervised Contrastive Loss (Khosla et al.)\n",
    "    features: (B, n_views, D) normalizadas\n",
    "    labels  : (B,)\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.tau = temperature\n",
    "\n",
    "    def forward(self, features, labels):\n",
    "        device = features.device\n",
    "        B, V, D = features.shape\n",
    "        feat = features.view(B*V, D)\n",
    "        labels = labels.view(B)\n",
    "        labels = labels.contiguous().view(-1, 1)                    # (B,1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(device)        # (B,B)\n",
    "\n",
    "        sim = torch.div(torch.matmul(feat, feat.T), self.tau)       # (BV,BV)\n",
    "        logits_mask = torch.ones_like(sim) - torch.eye(B*V, device=device)\n",
    "        sim = sim * logits_mask\n",
    "\n",
    "        mask = mask.repeat(V, V)                                     # (BV,BV)\n",
    "\n",
    "        exp_sim = torch.exp(sim) * logits_mask\n",
    "        log_prob = sim - torch.log(exp_sim.sum(dim=1, keepdim=True) + 1e-12)\n",
    "\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(dim=1) / (mask.sum(dim=1) + 1e-12)\n",
    "        loss = - mean_log_prob_pos.mean()\n",
    "        return loss\n",
    "\n",
    "def supcon_pretrain(model: EEGNet, X_tr: np.ndarray, y_tr: np.ndarray,\n",
    "                    epochs: int = SUPCON_EPOCHS, batch_size: int = SUPCON_BATCH,\n",
    "                    lr: float = SUPCON_LR, temperature: float = SUPCON_TEMP,\n",
    "                    proj_dim: int = SUPCON_PROJ_DIM, log_every: int = SUPCON_LOG_EVERY):\n",
    "    \"\"\"\n",
    "    Preentrena el backbone de EEGNet con SupCon usando etiquetas (positivos = misma clase).\n",
    "    Usa dos vistas augmentadas por ensayo.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    # asegurar construcci√≥n de cabeza (necesitamos conocer T)\n",
    "    with torch.no_grad():\n",
    "        dummy = torch.from_numpy(X_tr[:2]).float().unsqueeze(1).to(DEVICE)\n",
    "        _ = model(dummy)\n",
    "\n",
    "    proj = ProjectionHead(in_dim=80, proj_dim=proj_dim).to(DEVICE)\n",
    "    crit = SupConLoss(temperature=temperature)\n",
    "    ds = ContrastiveTrials(X_tr, y_tr)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    # Entrenamos backbone + proyector\n",
    "    opt = optim.Adam(list(model.parameters()) + list(proj.parameters()), lr=lr)\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        running = 0.0\n",
    "        n = 0\n",
    "        for v1, v2, yb in dl:\n",
    "            v1 = v1.to(DEVICE)  # (B,1,T,C)\n",
    "            v2 = v2.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            z1 = model.forward_features(v1)         # (B,80)\n",
    "            z2 = model.forward_features(v2)         # (B,80)\n",
    "            p1 = proj(z1)                           # (B,D)\n",
    "            p2 = proj(z2)                           # (B,D)\n",
    "            feats = torch.stack([p1, p2], dim=1)    # (B,2,D)\n",
    "\n",
    "            loss = crit(feats, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            bs = yb.size(0)\n",
    "            running += loss.item() * bs\n",
    "            n += bs\n",
    "\n",
    "        if ep % log_every == 0:\n",
    "            print(f\"[SupCon] Epoch {ep:03d}/{epochs} | loss={running/max(1,n):.4f}\")\n",
    "\n",
    "    print(\"[SupCon] Preentrenamiento contrastivo completado (backbone inicializado).\")\n",
    "\n",
    "# =========================\n",
    "# ENTRENAMIENTO / EVALUACI√ìN\n",
    "# =========================\n",
    "def train_epoch(model, loader, opt, criterion):\n",
    "    model.train()\n",
    "    for xb, yb, _ in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_with_preds(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for xb, yb, _ in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        pred = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "        y_pred.extend(pred)\n",
    "        y_true.extend(yb.numpy().tolist())\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    return y_true, y_pred, float(acc)\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes, title, fname):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.nan_to_num(cm_norm)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm_norm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "        plt.text(j, i, format(cm_norm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm_norm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def print_report(y_true, y_pred, classes):\n",
    "    print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
    "\n",
    "# =========================\n",
    "# FINE-TUNING PROGRESIVO por sujeto (con validaci√≥n interna)\n",
    "# =========================\n",
    "def _param_groups(model, mode):\n",
    "    \"\"\"\n",
    "    En EEGNet:\n",
    "      - 'out'            : solo capa final (model.out)\n",
    "      - 'head'           : fc + out\n",
    "      - 'spatial+head'   : depthwise + separable + fc + out  (temporal queda congelada)\n",
    "    \"\"\"\n",
    "    if mode == 'out':\n",
    "        train = list(model.out.parameters())\n",
    "    elif mode == 'head':\n",
    "        train = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "    elif mode == 'spatial+head':\n",
    "        train = (list(model.conv_depthwise.parameters()) +\n",
    "                 list(model.bn2.parameters()) +\n",
    "                 list(model.conv_sep_depth.parameters()) +\n",
    "                 list(model.conv_sep_point.parameters()) +\n",
    "                 list(model.bn3.parameters()) +\n",
    "                 list(model.fc.parameters()) +\n",
    "                 list(model.out.parameters()))\n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "    return train\n",
    "\n",
    "def _freeze_for_mode(model, mode):\n",
    "    # Congelamos todo\n",
    "    for p in model.parameters(): p.requires_grad = False\n",
    "    # Siempre mantenemos CONGELADO el bloque temporal en este protocolo\n",
    "    # (conv_temporal + bn1)\n",
    "    if mode == 'out':\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'head':\n",
    "        for p in model.fc.parameters():  p.requires_grad = True\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'spatial+head':\n",
    "        for p in model.conv_depthwise.parameters(): p.requires_grad = True\n",
    "        for p in model.bn2.parameters():           p.requires_grad = True\n",
    "        for p in model.conv_sep_depth.parameters():p.requires_grad = True\n",
    "        for p in model.conv_sep_point.parameters():p.requires_grad = True\n",
    "        for p in model.bn3.parameters():           p.requires_grad = True\n",
    "        for p in model.fc.parameters():            p.requires_grad = True\n",
    "        for p in model.out.parameters():           p.requires_grad = True\n",
    "\n",
    "def _class_weights(y_np, n_classes):\n",
    "    counts = np.bincount(y_np, minlength=n_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    weights = counts.sum() / counts\n",
    "    weights = weights / weights.mean()\n",
    "    return torch.tensor(weights, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "def _train_one_mode(model, X_cal, y_cal, n_classes, mode,\n",
    "                    epochs=FT_EPOCHS, batch_size=16,\n",
    "                    head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                    l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO):\n",
    "    \"\"\"\n",
    "    Entrena en 'mode' con early stopping sobre un conjunto de validaci√≥n interno.\n",
    "    Devuelve el modelo con los mejores pesos (por val loss).\n",
    "    \"\"\"\n",
    "    # Split Cal -> (train_cal, val_cal)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=RANDOM_STATE)\n",
    "    (tr_idx, va_idx), = sss.split(X_cal, y_cal)\n",
    "    Xtr, ytr = X_cal[tr_idx], y_cal[tr_idx]\n",
    "    Xva, yva = X_cal[va_idx], y_cal[va_idx]\n",
    "\n",
    "    ds_tr = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xtr).float().unsqueeze(1),\n",
    "        torch.from_numpy(ytr).long()\n",
    "    )\n",
    "    ds_va = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xva).float().unsqueeze(1),\n",
    "        torch.from_numpy(yva).long()\n",
    "    )\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    _freeze_for_mode(model, mode)\n",
    "\n",
    "    if mode == 'spatial+head':\n",
    "        # grupos con LR discriminativos\n",
    "        base_params = (list(model.conv_depthwise.parameters()) +\n",
    "                       list(model.bn2.parameters()) +\n",
    "                       list(model.conv_sep_depth.parameters()) +\n",
    "                       list(model.conv_sep_point.parameters()) +\n",
    "                       list(model.bn3.parameters()))\n",
    "        head_params = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "        train_params = base_params + head_params\n",
    "        opt = optim.Adam([\n",
    "            {\"params\": base_params, \"lr\": base_lr},\n",
    "            {\"params\": head_params, \"lr\": head_lr},\n",
    "        ])\n",
    "    else:\n",
    "        train_params = _param_groups(model, mode)\n",
    "        opt = optim.Adam(train_params, lr=head_lr)\n",
    "\n",
    "    ref = [p.detach().clone().to(p.device) for p in train_params]\n",
    "    class_w = _class_weights(ytr, n_classes)\n",
    "    crit = nn.CrossEntropyLoss(weight=class_w)\n",
    "\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_val = float('inf')\n",
    "    bad = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # --- train ---\n",
    "        model.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            # L2-SP hacia referencia de los par√°metros que estamos entrenando\n",
    "            reg = 0.0\n",
    "            for p_cur, p_ref in zip(train_params, ref):\n",
    "                reg = reg + torch.sum((p_cur - p_ref)**2)\n",
    "            loss = loss + l2sp_lambda * reg\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # --- val ---\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            nval = 0\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                loss = crit(logits, yb)\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "                nval += xb.size(0)\n",
    "            val_loss /= max(1, nval)\n",
    "\n",
    "        if val_loss + 1e-7 < best_val:\n",
    "            best_val = val_loss\n",
    "            bad = 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_numpy(model, X_np, device):\n",
    "    model.eval()\n",
    "    xb = torch.from_numpy(X_np).float().unsqueeze(1).to(device)  # (N,1,T,C)\n",
    "    logits = model(xb)\n",
    "    return logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "def subject_cv_finetune_predict_progressive(model_global, Xs, ys, device,\n",
    "                                            n_splits=CALIB_CV_FOLDS, n_classes=4):\n",
    "    \"\"\"\n",
    "    Para un sujeto: 4-fold StratifiedKFold.\n",
    "      - En cada fold: 'out' ‚Üí 'head' ‚Üí 'spatial+head' (temporal congelado)\n",
    "      - Se elige el mejor en el split de holdout del sujeto.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    y_true_full = np.empty_like(ys)\n",
    "    y_pred_full = np.empty_like(ys)\n",
    "\n",
    "    for tr_idx, te_idx in skf.split(Xs, ys):\n",
    "        Xcal, ycal = Xs[tr_idx], ys[tr_idx]\n",
    "        Xho,  yho  = Xs[te_idx], ys[te_idx]\n",
    "\n",
    "        # Stage A: 'out'\n",
    "        m_out = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_out, Xcal, ycal, n_classes, mode='out',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_out = predict_numpy(m_out, Xho, device)\n",
    "\n",
    "        # Stage B: 'head'\n",
    "        m_head = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_head, Xcal, ycal, n_classes, mode='head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_head = predict_numpy(m_head, Xho, device)\n",
    "\n",
    "        # Stage C: 'spatial+head'\n",
    "        m_sp = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_sp, Xcal, ycal, n_classes, mode='spatial+head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                        l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_sp = predict_numpy(m_sp, Xho, device)\n",
    "\n",
    "        # mejor de las tres\n",
    "        accs = [ (yhat_out == yho).mean(), (yhat_head == yho).mean(), (yhat_sp == yho).mean() ]\n",
    "        best_idx = int(np.argmax(accs))\n",
    "        yhat_best = [yhat_out, yhat_head, yhat_sp][best_idx]\n",
    "\n",
    "        y_true_full[te_idx] = yho\n",
    "        y_pred_full[te_idx] = yhat_best\n",
    "\n",
    "    return y_true_full, y_pred_full\n",
    "\n",
    "# =========================\n",
    "# FOLDS JSON helpers\n",
    "# =========================\n",
    "def save_group_folds_json_with_indices(subject_ids_str, groups_array, n_splits, out_json_path,\n",
    "                                       created_by=\"dose_experiment\", description=None):\n",
    "    out_json_path = Path(out_json_path)\n",
    "    unique_subjects_int = sorted(np.unique(groups_array).tolist())\n",
    "    subject_ids = [f\"S{sid:03d}\" for sid in unique_subjects_int]\n",
    "\n",
    "    if len(subject_ids) < n_splits:\n",
    "        raise ValueError(f\"n_splits={n_splits} mayor que n√∫mero de sujetos={len(subject_ids)}\")\n",
    "\n",
    "    groups = np.arange(len(subject_ids))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    folds = []\n",
    "    fold_i = 0\n",
    "    for train_idx_grp, test_idx_grp in gkf.split(groups, groups, groups):\n",
    "        fold_i += 1\n",
    "        train_sids = [subject_ids[int(i)] for i in train_idx_grp]\n",
    "        test_sids  = [subject_ids[int(i)] for i in test_idx_grp]\n",
    "\n",
    "        train_sids_int = [int(s[1:]) for s in train_sids]\n",
    "        test_sids_int  = [int(s[1:]) for s in test_sids]\n",
    "\n",
    "        tr_idx = np.where(np.isin(groups_array, train_sids_int))[0].tolist()\n",
    "        te_idx = np.where(np.isin(groups_array, test_sids_int))[0].tolist()\n",
    "\n",
    "        folds.append({\n",
    "            \"fold\": int(fold_i),\n",
    "            \"train\": train_sids,\n",
    "            \"test\": test_sids,\n",
    "            \"tr_idx\": tr_idx,\n",
    "            \"te_idx\": te_idx\n",
    "        })\n",
    "\n",
    "    payload = {\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"created_by\": created_by,\n",
    "        \"description\": description if description is not None else \"\",\n",
    "        \"n_splits\": int(n_splits),\n",
    "        \"n_subjects\": len(subject_ids),\n",
    "        \"subject_ids\": subject_ids,\n",
    "        \"folds\": folds\n",
    "    }\n",
    "\n",
    "    out_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Folds JSON con √≠ndices guardado ‚Üí {out_json_path}\")\n",
    "    return out_json_path\n",
    "\n",
    "def load_group_folds_json(path_json, expected_subject_ids=None, strict_check=True):\n",
    "    path_json = Path(path_json)\n",
    "    if not path_json.exists():\n",
    "        raise FileNotFoundError(f\"No existe {path_json}\")\n",
    "    with open(path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        payload = json.load(f)\n",
    "\n",
    "    subj_json = payload.get(\"subject_ids\", [])\n",
    "    if expected_subject_ids is not None:\n",
    "        expected = sorted(list(expected_subject_ids))\n",
    "        if subj_json != expected:\n",
    "            msg = (\"Los subject_ids del JSON no coinciden con expected_subject_ids.\\n\"\n",
    "                   f\"JSON has {len(subj_json)} subjects, expected {len(expected)}.\\n\"\n",
    "                   f\"First 10 JSON: {subj_json[:10]}\\nFirst 10 expected: {expected[:10]}\")\n",
    "            if strict_check:\n",
    "                raise ValueError(msg)\n",
    "            else:\n",
    "                print(\"WARNING: \" + msg)\n",
    "    return payload\n",
    "\n",
    "# =========================\n",
    "# EXPERIMENTO\n",
    "# =========================\n",
    "def run_experiment(save_folds_json=True, folds_json_path=FOLDS_DIR, folds_json_description=\"GroupKFold folds for comparison\"):\n",
    "    \"\"\"\n",
    "    - Crea/lee JSON con folds por sujeto (incluye tr_idx/te_idx)\n",
    "    - Entrena modelo global por fold (inter-sujeto puro) con validaci√≥n interna por sujetos + ES\n",
    "    - Eval√∫a Global acc en test\n",
    "    - Realiza Fine-Tuning PROGRESIVO por sujeto (4-fold CV) y reporta acc\n",
    "    \"\"\"\n",
    "    mne.set_log_level('WARNING')\n",
    "\n",
    "    # sujetos y dataset\n",
    "    subs = subjects_available()\n",
    "    print(f\"Sujetos elegibles: {len(subs)} ‚Üí {subs[:10]}{'...' if len(subs)>10 else ''}\")\n",
    "\n",
    "    X, y, groups, chs = build_dataset_all(subs, scenario=CLASS_SCENARIO, window_mode=WINDOW_MODE)\n",
    "    N, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Listo para entrenar: N={N} | T={T} | C={C} | clases={n_classes} | sujetos={len(np.unique(groups))}\")\n",
    "\n",
    "    ds = EEGTrials(X, y, groups)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # preparar JSON folds\n",
    "    if folds_json_path is None:\n",
    "        folds_json_path = Path(\"folds\") / f\"group_folds_{N_FOLDS}splits.json\"\n",
    "    else:\n",
    "        folds_json_path = Path(folds_json_path)\n",
    "    folds_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    unique_subs = sorted(np.unique(groups).tolist())\n",
    "    subject_ids_str = [f\"S{s:03d}\" for s in unique_subs]\n",
    "\n",
    "    if not folds_json_path.exists():\n",
    "        if not save_folds_json:\n",
    "            raise FileNotFoundError(f\"Folds JSON no encontrado en {folds_json_path} y save_folds_json=False.\")\n",
    "        save_group_folds_json_with_indices(subject_ids_str, groups, n_splits=N_FOLDS,\n",
    "                                           out_json_path=folds_json_path,\n",
    "                                           created_by=\"Joel_Clasificador\",\n",
    "                                           description=folds_json_description)\n",
    "\n",
    "    payload = load_group_folds_json(folds_json_path, expected_subject_ids=subject_ids_str, strict_check=False)\n",
    "    folds = payload[\"folds\"]\n",
    "\n",
    "    # bucle por folds\n",
    "    global_folds = []\n",
    "    ft_prog_folds = []\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "\n",
    "    for f in folds:\n",
    "        fold = f[\"fold\"]\n",
    "        tr_idx = np.asarray(f.get(\"tr_idx\", []), dtype=int)\n",
    "        te_idx = np.asarray(f.get(\"te_idx\", []), dtype=int)\n",
    "\n",
    "        if tr_idx.size == 0 or te_idx.size == 0:\n",
    "            print(f\"Advertencia: fold {fold} sin √≠ndices tr/te v√°lidos. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        # ===== Split de validaci√≥n por SUJETOS dentro del set de entrenamiento =====\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=GLOBAL_VAL_SPLIT, random_state=RANDOM_STATE)\n",
    "        tr_subj_idx, va_subj_idx = next(gss.split(tr_idx, groups[tr_idx], groups[tr_idx]))\n",
    "        tr_sub_idx = tr_idx[tr_subj_idx]\n",
    "        va_idx     = tr_idx[va_subj_idx]\n",
    "\n",
    "        tr_loader = DataLoader(Subset(ds, tr_sub_idx), batch_size=BATCH_SIZE, shuffle=True,  drop_last=False)\n",
    "        va_loader = DataLoader(Subset(ds, va_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "        te_loader = DataLoader(Subset(ds, te_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "        # ===== EEGNet =====\n",
    "        model = EEGNet(n_ch=C, n_classes=n_classes, F1=8, D=2, kernel_t=64, k_sep=16,\n",
    "                       pool1_t=4, pool2_t=8, dropout=0.5).to(DEVICE)\n",
    "\n",
    "        # ===== PREENTRENAMIENTO SUPCON (opcional) =====\n",
    "        if USE_SUPCON_PRETRAIN:\n",
    "            X_tr_sup = X[tr_sub_idx]\n",
    "            y_tr_sup = y[tr_sub_idx]\n",
    "            print(f\"[Fold {fold}/{N_FOLDS}] SupCon pretrain on {len(X_tr_sup)} trials...\")\n",
    "            supcon_pretrain(model, X_tr_sup, y_tr_sup,\n",
    "                            epochs=SUPCON_EPOCHS, batch_size=SUPCON_BATCH,\n",
    "                            lr=SUPCON_LR, temperature=SUPCON_TEMP, proj_dim=SUPCON_PROJ_DIM,\n",
    "                            log_every=SUPCON_LOG_EVERY)\n",
    "\n",
    "        # Optimizador para entrenamiento supervisado\n",
    "        opt = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "        def _acc(loader):\n",
    "            return evaluate_with_preds(model, loader)[2]\n",
    "\n",
    "        # ===== Entrenamiento con logging + EARLY STOPPING por val_acc =====\n",
    "        print(f\"\\n[Fold {fold}/{N_FOLDS}] Entrenando modelo global con validaci√≥n interna por sujetos...\"\n",
    "              f\" (n_train={len(tr_sub_idx)} | n_val={len(va_idx)} | n_test={len(te_idx)})\")\n",
    "\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        best_val = -1.0\n",
    "        bad = 0\n",
    "\n",
    "        for epoch in range(1, EPOCHS_GLOBAL + 1):\n",
    "            train_epoch(model, tr_loader, opt, criterion)\n",
    "\n",
    "            if epoch % LOG_EVERY == 0:\n",
    "                tr_acc = _acc(tr_loader)\n",
    "                va_acc = _acc(va_loader)\n",
    "                print(f\"  √âpoca {epoch:3d} | train_acc={tr_acc:.4f} | val_acc={va_acc:.4f}\")\n",
    "\n",
    "                if va_acc > best_val + 1e-4:\n",
    "                    best_val = va_acc\n",
    "                    best_state = copy.deepcopy(model.state_dict())\n",
    "                    bad = 0\n",
    "                else:\n",
    "                    bad += 1\n",
    "                    if bad >= GLOBAL_PATIENCE:\n",
    "                        print(f\"  Early stopping en √©poca {epoch} (mejor val_acc={best_val:.4f})\")\n",
    "                        break\n",
    "\n",
    "        # cargar mejor estado antes de evaluar en test\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "        # ===== Evaluaci√≥n global (inter-sujeto puro) =====\n",
    "        y_true, y_pred, acc_global = evaluate_with_preds(model, te_loader)\n",
    "        global_folds.append(acc_global)\n",
    "        all_true.append(y_true); all_pred.append(y_pred)\n",
    "\n",
    "        print(f\"[Fold {fold}/{N_FOLDS}] Global acc={acc_global:.4f}\")\n",
    "        print_report(y_true, y_pred, CLASS_NAMES_4C)\n",
    "\n",
    "        # ---------- Fine-tuning PROGRESIVO por sujeto con 4-fold CV ----------\n",
    "        X_te, y_te, g_te = X[te_idx], y[te_idx], groups[te_idx]\n",
    "\n",
    "        y_true_ft_all, y_pred_ft_all = [], []\n",
    "        used_subjects = 0\n",
    "        for sid in np.unique(g_te):\n",
    "            idx = np.where(g_te == sid)[0]\n",
    "            Xs, ys = X_te[idx], y_te[idx]\n",
    "\n",
    "            # Seguridad\n",
    "            if len(ys) < CALIB_CV_FOLDS or len(np.unique(ys)) < 2:\n",
    "                continue\n",
    "\n",
    "            y_true_subj, y_pred_subj = subject_cv_finetune_predict_progressive(\n",
    "                model, Xs, ys, DEVICE, n_splits=CALIB_CV_FOLDS, n_classes=n_classes\n",
    "            )\n",
    "            y_true_ft_all.append(y_true_subj)\n",
    "            y_pred_ft_all.append(y_pred_subj)\n",
    "            used_subjects += 1\n",
    "\n",
    "        if len(y_true_ft_all) > 0:\n",
    "            y_true_ft_all = np.concatenate(y_true_ft_all)\n",
    "            y_pred_ft_all = np.concatenate(y_pred_ft_all)\n",
    "            acc_ft = (y_true_ft_all == y_pred_ft_all).mean()\n",
    "            print(f\"  Fine-tuning PROGRESIVO (por sujeto, {CALIB_CV_FOLDS}-fold CV) acc={acc_ft:.4f} | sujetos={used_subjects}\")\n",
    "            print(f\"  Œî(FT-Global) = {acc_ft - acc_global:+.4f}\")\n",
    "        else:\n",
    "            acc_ft = np.nan\n",
    "            print(\"  Fine-tuning PROGRESIVO no ejecutado (sujeto(s) con muestras insuficientes).\")\n",
    "\n",
    "        ft_prog_folds.append(acc_ft)\n",
    "\n",
    "    # ---------- resultados finales ----------\n",
    "    if len(all_true) > 0:\n",
    "        all_true = np.concatenate(all_true)\n",
    "        all_pred = np.concatenate(all_pred)\n",
    "    else:\n",
    "        all_true = np.array([], dtype=int)\n",
    "        all_pred = np.array([], dtype=int)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTADOS FINALES\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Global folds:\", [f\"{a:.4f}\" for a in global_folds])\n",
    "    if len(global_folds) > 0:\n",
    "        print(f\"Global mean: {np.mean(global_folds):.4f}\")\n",
    "\n",
    "    print(\"Fine-tune PROGRESIVO folds:\", [(\"nan\" if (a is None or np.isnan(a)) else f\"{a:.4f}\") for a in ft_prog_folds])\n",
    "    if len(ft_prog_folds) > 0:\n",
    "        print(f\"Fine-tune PROGRESIVO mean: {np.nanmean(ft_prog_folds):.4f}\")\n",
    "        print(f\"Œî(FT-Global) mean: {np.nanmean(ft_prog_folds) - np.mean(global_folds):+.4f}\")\n",
    "\n",
    "    # Matriz de confusi√≥n global (sobre todos los folds)\n",
    "    if all_true.size > 0:\n",
    "        plot_confusion(all_true, all_pred, CLASS_NAMES_4C,\n",
    "                       title=\"Confusion Matrix - Global Model (All Folds)\",\n",
    "                       fname=\"confusion_global_allfolds.png\")\n",
    "        print(\"\\n‚Ü≥ Matriz de confusi√≥n guardada: confusion_global_allfolds.png\")\n",
    "\n",
    "    return {\n",
    "        \"global_folds\": global_folds,\n",
    "        \"ft_prog_folds\": ft_prog_folds,\n",
    "        \"all_true\": all_true,\n",
    "        \"all_pred\": all_pred,\n",
    "        \"folds_json_path\": str(folds_json_path)\n",
    "    }\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üß† INICIANDO EXPERIMENTO CON EEGNet + FINE-TUNING PROGRESIVO (por sujeto, 4-fold CV)\")\n",
    "    print(f\"üîß Configuraci√≥n: {CLASS_SCENARIO}, {len(EXPECTED_8)} canales, {WINDOW_MODE}\")\n",
    "    print(f\"‚öôÔ∏è  FT: epochs={FT_EPOCHS}, base_lr={FT_BASE_LR}, head_lr={FT_HEAD_LR}, L2SP={FT_L2SP}, patience={FT_PATIENCE}, CV={CALIB_CV_FOLDS}\")\n",
    "    print(f\"üß≤ SupCon: enabled={USE_SUPCON_PRETRAIN}, epochs={SUPCON_EPOCHS}, batch={SUPCON_BATCH}, temp={SUPCON_TEMP}\")\n",
    "    run_experiment()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
