{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "864131d1",
   "metadata": {},
   "source": [
    "### Rutas, deps, utilidades básicas (crear carpetas, seeds, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba5dbe12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# %% [setup]\n",
    "import os, sys, random, time, json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# ---------- Rutas base (ajusta si tu repo difiere) ----------\n",
    "PROJ      = Path('..').resolve().parent   # asumiendo este notebook vive en eegnet/\n",
    "DATA_PROC = PROJ / 'data' / 'processed'\n",
    "\n",
    "OUT_INTRA = PROJ / 'models' / 'eegnet'\n",
    "OUT_LOSO  = PROJ / 'models' / 'eegnet'\n",
    "OUT_INTER = PROJ / 'models' / 'inter_fixed_eegnet'\n",
    "for d in (OUT_INTRA, OUT_LOSO, OUT_INTER):\n",
    "    (d/'figures').mkdir(parents=True, exist_ok=True)\n",
    "    (d/'tables').mkdir(parents=True, exist_ok=True)\n",
    "    (d/'logs').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Utilidades ----------\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class AverageMeter:\n",
    "    def __init__(self): self.reset()\n",
    "    def reset(self):\n",
    "        self.sum = 0.0; self.cnt = 0; self.avg = 0.0\n",
    "    def update(self, val, n=1):\n",
    "        self.sum += float(val) * n; self.cnt += n; self.avg = self.sum / max(1, self.cnt)\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def list_subjects(fif_dir=DATA_PROC, pattern='S???_MI-epo.fif'):\n",
    "    return sorted({Path(p).stem.split('_')[0] for p in Path(fif_dir).glob(pattern)})\n",
    "\n",
    "def epochs_to_arrays(epochs: mne.Epochs):\n",
    "    X = epochs.get_data().astype(np.float32)         # (N, C, T)\n",
    "    y = epochs.events[:, -1].astype(np.int64)        # ints\n",
    "    return X, y\n",
    "\n",
    "def epochs_to_labels(epochs: mne.Epochs):\n",
    "    inv = {v:k for k,v in epochs.event_id.items()}\n",
    "    y = np.array([inv[e[-1]] for e in epochs.events], dtype=object)\n",
    "    return y\n",
    "\n",
    "def ts_now(): return datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9209d9b",
   "metadata": {},
   "source": [
    "### Modelo EEGNet v4 (PyTorch) con hiperparámetros potentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8581e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [modelo]\n",
    "class EEGNet(nn.Module):\n",
    "    \"\"\"\n",
    "    EEGNet (Lawhern et al., 2018) para entrada [B, 1, C, T].\n",
    "    Parámetros típicos: F1=8, D=2, F2=16, kernel_time=64, pool1=4, pool2=8, dropout=0.5\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes, C, T, F1=8, D=2, F2=16, kernel_time=64,\n",
    "                 pool1=4, pool2=8, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # Temporal\n",
    "        self.conv_time = nn.Conv2d(1, F1, (1, kernel_time),\n",
    "                                   padding=(0, kernel_time//2), bias=False)\n",
    "        self.bn_time = nn.BatchNorm2d(F1)\n",
    "\n",
    "        # Spatial (depthwise)\n",
    "        self.depthwise = nn.Conv2d(F1, F1*D, (C, 1), groups=F1, bias=False)\n",
    "        self.bn_depth = nn.BatchNorm2d(F1*D)\n",
    "        self.pool1 = nn.AvgPool2d((1, pool1))\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "\n",
    "        # Separable (depthwise temporal + pointwise)\n",
    "        self.sep_depth = nn.Conv2d(F1*D, F1*D, (1, 16), padding=(0, 16//2),\n",
    "                                   groups=F1*D, bias=False)\n",
    "        self.sep_point = nn.Conv2d(F1*D, F2, (1, 1), bias=False)\n",
    "        self.bn_sep = nn.BatchNorm2d(F2)\n",
    "        self.pool2 = nn.AvgPool2d((1, pool2))\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "        # Clasificador (se define lazy en forward)\n",
    "        self.classifier = None\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.conv_time(x); x = self.bn_time(x); x = F.elu(x)\n",
    "        x = self.depthwise(x); x = self.bn_depth(x); x = F.elu(x)\n",
    "        x = self.pool1(x); x = self.drop1(x)\n",
    "        x = self.sep_depth(x); x = self.sep_point(x); x = self.bn_sep(x); x = F.elu(x)\n",
    "        x = self.pool2(x); x = self.drop2(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        if self.classifier is None:\n",
    "            feat_dim = x.shape[1] * x.shape[2] * x.shape[3]\n",
    "            self.classifier = nn.Linear(feat_dim, self.n_classes).to(x.device)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c70dbe6",
   "metadata": {},
   "source": [
    "### Dataset, normalización por canal (stats de train), augmentations suaves y DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5db82d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [dataloader — FIX: targets 0..K-1 consistente]\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "\n",
    "def _make_windows_indices(n_times, win_size, stride):\n",
    "    if stride <= 0:\n",
    "        s = max(0, (n_times - win_size)//2)\n",
    "        return [(s, min(s + win_size, n_times))]\n",
    "    idx = []\n",
    "    for s in range(0, max(1, n_times - win_size + 1), stride):\n",
    "        e = s + win_size\n",
    "        if e <= n_times: idx.append((s, e))\n",
    "    if not idx:\n",
    "        idx = [(0, n_times)]\n",
    "    return idx\n",
    "\n",
    "def fit_channel_normalizer(X_train):\n",
    "    mean_c = X_train.mean(axis=(0,2))\n",
    "    std_c  = X_train.std(axis=(0,2)) + 1e-7\n",
    "    return mean_c.astype(np.float32), std_c.astype(np.float32)\n",
    "\n",
    "def augment_light(x, p_noise=0.3, p_scale=0.3):\n",
    "    C,T = x.shape\n",
    "    if np.random.rand() < p_noise:\n",
    "        x = x + 0.01*np.random.randn(C,T).astype(np.float32)\n",
    "    if np.random.rand() < p_scale:\n",
    "        scale = (0.9 + 0.2*np.random.rand(C)).astype(np.float32)\n",
    "        x = x * scale[:, None]\n",
    "    return x\n",
    "\n",
    "class EEGWindowDataset(Dataset):\n",
    "    def __init__(self, X, y_enc, win_size, stride=0, mean_=None, std_=None,\n",
    "                 augment_fn=None, add_channel_dim=True):\n",
    "        assert X.ndim == 3\n",
    "        self.X = X; self.y = y_enc\n",
    "        self.n_trials, self.n_ch, self.n_times = X.shape\n",
    "        self.win_size = min(int(win_size), self.n_times)\n",
    "        self.stride = int(stride)\n",
    "        self.augment_fn = augment_fn\n",
    "        self.add_channel_dim = add_channel_dim\n",
    "\n",
    "        self.win_index = []\n",
    "        if self.stride > 0:\n",
    "            widx = _make_windows_indices(self.n_times, self.win_size, self.stride)\n",
    "            for t in range(self.n_trials):\n",
    "                for (s,e) in widx: self.win_index.append((t,s,e))\n",
    "        else:\n",
    "            (s,e) = _make_windows_indices(self.n_times, self.win_size, 0)[0]\n",
    "            for t in range(self.n_trials): self.win_index.append((t,s,e))\n",
    "\n",
    "        if mean_ is None or std_ is None:\n",
    "            self.mean_ = np.zeros((self.n_ch,), dtype=np.float32)\n",
    "            self.std_  = np.ones((self.n_ch,), dtype=np.float32)\n",
    "        else:\n",
    "            self.mean_ = mean_.astype(np.float32)\n",
    "            self.std_  = np.clip(std_.astype(np.float32), 1e-7, None)\n",
    "\n",
    "    def __len__(self): return len(self.win_index)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        t, s, e = self.win_index[i]\n",
    "        x = self.X[t, :, s:e]                           # (C,Tw)\n",
    "        x = (x - self.mean_[:, None]) / self.std_[:, None]\n",
    "        if self.augment_fn is not None: x = self.augment_fn(x)\n",
    "        x = torch.from_numpy(x)                         # (C,Tw)\n",
    "        if self.add_channel_dim: x = x.unsqueeze(0)     # (1,C,Tw)\n",
    "        y = torch.tensor(self.y[t], dtype=torch.long)   # ya 0..K-1\n",
    "        return x, y\n",
    "\n",
    "def _epochs_to_arrays_and_labels(epochs: mne.Epochs):\n",
    "    X = epochs.get_data().astype(np.float32)         # (N,C,T)\n",
    "    inv = {v:k for k,v in epochs.event_id.items()}\n",
    "    y_names = np.array([inv[e[-1]] for e in epochs.events], dtype=object)\n",
    "    return X, y_names\n",
    "\n",
    "def make_eegnet_loaders_from_epochs(\n",
    "    ep_train, ep_val, ep_test, fs,\n",
    "    crop_sec=(0.5,3.5),\n",
    "    train_win_sec=1.5, train_stride_sec=0.25,\n",
    "    eval_win_sec=None, batch_size=64, num_workers=4, use_augment=True\n",
    "):\n",
    "    # Recorte temporal coherente\n",
    "    if crop_sec is not None:\n",
    "        ep_train = ep_train.copy().crop(*crop_sec)\n",
    "        ep_val   = ep_val.copy().crop(*crop_sec)\n",
    "        ep_test  = ep_test.copy().crop(*crop_sec)\n",
    "\n",
    "    # Arrays y etiquetas nominales\n",
    "    Xtr, ytr_names = _epochs_to_arrays_and_labels(ep_train)\n",
    "    Xva, yva_names = _epochs_to_arrays_and_labels(ep_val)\n",
    "    Xte, yte_names = _epochs_to_arrays_and_labels(ep_test)\n",
    "\n",
    "    # Encoder global (train+val+test) para que todo quede 0..K-1\n",
    "    le = LabelEncoder().fit(np.concatenate([ytr_names, yva_names, yte_names]))\n",
    "    ytr = le.transform(ytr_names).astype(np.int64)\n",
    "    yva = le.transform(yva_names).astype(np.int64)\n",
    "    yte = le.transform(yte_names).astype(np.int64)\n",
    "\n",
    "    # Normalización por canal aprendida SOLO en TRAIN\n",
    "    mean_c, std_c = fit_channel_normalizer(Xtr)\n",
    "\n",
    "    # Ventanas\n",
    "    win_tr = int(round((train_win_sec or (crop_sec[1]-crop_sec[0]))*fs))\n",
    "    stride = int(round(train_stride_sec*fs)) if train_stride_sec else 0\n",
    "    win_ev = int(round((eval_win_sec if eval_win_sec is not None else train_win_sec)*fs))\n",
    "\n",
    "    ds_tr = EEGWindowDataset(Xtr,ytr,win_tr,stride,mean_c,std_c,augment_light if use_augment else None,True)\n",
    "    ds_va = EEGWindowDataset(Xva,yva,win_ev,0,mean_c,std_c,None,True)\n",
    "    ds_te = EEGWindowDataset(Xte,yte,win_ev,0,mean_c,std_c,None,True)\n",
    "\n",
    "    dl_tr = DataLoader(ds_tr,batch_size=batch_size,shuffle=True,num_workers=num_workers,\n",
    "                       pin_memory=True, drop_last=True)\n",
    "    dl_va = DataLoader(ds_va,batch_size=batch_size,shuffle=False,num_workers=num_workers,\n",
    "                       pin_memory=True)\n",
    "    dl_te = DataLoader(ds_te,batch_size=batch_size,shuffle=False,num_workers=num_workers,\n",
    "                       pin_memory=True)\n",
    "\n",
    "    # Pesos de clase con etiquetas codificadas 0..K-1\n",
    "    cnt = Counter(ytr.tolist()); n = sum(cnt.values())\n",
    "    idxs = sorted(cnt.keys())\n",
    "    weights = torch.tensor([n/cnt[c] for c in idxs], dtype=torch.float32)\n",
    "\n",
    "    info = dict(mean=mean_c, std=std_c, n_classes=len(idxs), class_weights=weights,\n",
    "                n_train_windows=len(ds_tr), n_val_windows=len(ds_va), n_test_windows=len(ds_te),\n",
    "                C=Xtr.shape[1], T=win_tr, label_encoder_classes=le.classes_)\n",
    "    return dl_tr, dl_va, dl_te, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba844df",
   "metadata": {},
   "source": [
    "### Entrenamiento, evaluación y calibración few-shot (BN + FC, opcional depthwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f27f92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [train+eval con logging cada 10]\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    loss_m = AverageMeter(); correct=0; total=0\n",
    "    for xb,yb in loader:\n",
    "        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_m.update(loss.item(), xb.size(0))\n",
    "        pred = logits.argmax(1); correct += (pred==yb).sum().item(); total += yb.numel()\n",
    "    return loss_m.avg, correct/max(1,total)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, loader, device):\n",
    "    model.eval()\n",
    "    ys=[]; yh=[]\n",
    "    for xb,yb in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        logits = model(xb)\n",
    "        yh.append(logits.argmax(1).cpu().numpy())\n",
    "        ys.append(yb.numpy())\n",
    "    y_true = np.concatenate(ys); y_hat = np.concatenate(yh)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    f1m = f1_score(y_true, y_hat, average='macro')\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    return acc, f1m, cm\n",
    "\n",
    "def fit_model(model, dl_tr, dl_va, max_epochs=150, lr=3e-4, weight_decay=1e-3,\n",
    "              device='cuda', class_weights=None, patience=20, log_file=None):\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device) if class_weights is not None else None)\n",
    "\n",
    "    # logger simple a archivo solo cada 10 épocas\n",
    "    def log10(msg, epoch=None):\n",
    "        if epoch is None or (epoch % 10 == 0):\n",
    "            print(msg)\n",
    "            if log_file is not None:\n",
    "                with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(msg + \"\\n\")\n",
    "\n",
    "    best = {'val_f1': -1, 'state': None, 'epoch': -1}\n",
    "    bad = 0\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        tr_loss, tr_acc = train_epoch(model, dl_tr, optimizer, criterion, device)\n",
    "        va_acc, va_f1, _ = evaluate_model(model, dl_va, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        log10(f\"[Epoch {epoch:03d}] train_loss={tr_loss:.4f} | train_acc={tr_acc:.3f} | val_acc={va_acc:.3f} | val_f1={va_f1:.3f}\",\n",
    "              epoch=epoch)\n",
    "\n",
    "        if va_f1 > best['val_f1']:\n",
    "            best.update(val_f1=va_f1, state={k:v.detach().cpu() for k,v in model.state_dict().items()}, epoch=epoch)\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                log10(f\"[EarlyStop] sin mejora en {patience} épocas (mejor f1={best['val_f1']:.3f} @ {best['epoch']})\",\n",
    "                      epoch=epoch)\n",
    "                break\n",
    "\n",
    "    # resumen final (se imprime y guarda)\n",
    "    final_msg = f\"[TRAIN END] best_val_f1={best['val_f1']:.3f} @ epoch={best['epoch']}\"\n",
    "    print(final_msg)\n",
    "    if log_file is not None:\n",
    "        with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(final_msg + \"\\n\")\n",
    "\n",
    "    if best['state'] is not None:\n",
    "        model.load_state_dict({k:v for k,v in best['state'].items()})\n",
    "    return model, best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b72ad2",
   "metadata": {},
   "source": [
    "### INTRA — k-fold por sujeto (reporta CSV/TXT + confusiones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1513d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [pipelines: INTRA, LOSO, INTER + calibración]\n",
    "def split_calibration(ep_test: mne.Epochs, k_per_class=5, seed=42):\n",
    "    if not k_per_class or k_per_class <= 0:\n",
    "        return None, ep_test\n",
    "    rng = np.random.RandomState(int(seed))\n",
    "    labels = ep_test.events[:, -1]\n",
    "    ep_calib_list, ep_eval_list = [], []\n",
    "    for code in np.unique(labels):\n",
    "        idx = np.where(labels == code)[0]\n",
    "        rng.shuffle(idx)\n",
    "        take = min(k_per_class, len(idx))\n",
    "        if take > 0: ep_calib_list.append(ep_test.copy()[idx[:take]])\n",
    "        if len(idx) > take: ep_eval_list.append(ep_test.copy()[idx[take:]])\n",
    "    ep_calib = mne.concatenate_epochs(ep_calib_list, on_mismatch='ignore') if ep_calib_list else None\n",
    "    ep_eval  = mne.concatenate_epochs(ep_eval_list,  on_mismatch='ignore') if ep_eval_list else ep_test\n",
    "    return ep_calib, ep_eval\n",
    "\n",
    "# -------------------- INTRA (k-fold) --------------------\n",
    "def run_intra_all_eegnet(\n",
    "    fif_dir=DATA_PROC, k=5, crop_window=(0.5,3.5),\n",
    "    max_epochs=120, lr=3e-4, weight_decay=1e-3,\n",
    "    F1=16, D=2, F2=32, kernel_time=64, dropout=0.5, pool1=4, pool2=8,\n",
    "    train_win_sec=1.5, train_stride_sec=0.25, eval_win_sec=None,\n",
    "    batch_size=128, seed=42, fs=250.0, use_augment=True, log_tag=\"intra\"\n",
    "):\n",
    "    set_seed(seed)\n",
    "    ts = ts_now()\n",
    "    rows = []\n",
    "    rows_subj = [] \n",
    "\n",
    "    subs = list_subjects(fif_dir)\n",
    "    print(f\"[INTRA-EEGNet] sujetos: {subs}\")\n",
    "\n",
    "    # asegurar carpeta de logs\n",
    "    (OUT_INTRA/'logs').mkdir(parents=True, exist_ok=True)\n",
    "    log_file = OUT_INTRA/'logs'/f\"{ts}_eegnet_{log_tag}.txt\"\n",
    "\n",
    "    for sid in subs:\n",
    "        ep = mne.read_epochs(str(Path(fif_dir)/f\"{sid}_MI-epo.fif\"), preload=True, verbose=False)\n",
    "        if crop_window is not None:\n",
    "            ep.crop(*crop_window)\n",
    "\n",
    "        # Etiquetas base del sujeto (para estratificar)\n",
    "        y = epochs_to_labels(ep)\n",
    "        le = LabelEncoder().fit(y)\n",
    "        y_enc = le.transform(y)\n",
    "\n",
    "        idx = np.arange(len(ep))\n",
    "        skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "\n",
    "        for fold, (tr_idx, va_idx) in enumerate(skf.split(idx, y_enc), start=1):\n",
    "            ep_tr = ep.copy()[tr_idx]\n",
    "            ep_va = ep.copy()[va_idx]\n",
    "\n",
    "            dl_tr, dl_va, dl_te, info = make_eegnet_loaders_from_epochs(\n",
    "                ep_tr, ep_va, ep_va,\n",
    "                fs=fs, crop_sec=None,\n",
    "                train_win_sec=train_win_sec,\n",
    "                train_stride_sec=train_stride_sec,\n",
    "                eval_win_sec=(eval_win_sec if eval_win_sec is not None else train_win_sec),\n",
    "                batch_size=batch_size, use_augment=use_augment\n",
    "            )\n",
    "\n",
    "            n_classes = info['n_classes']\n",
    "            C = ep.info['nchan']\n",
    "            Tw = int(info['T'])\n",
    "\n",
    "            model = EEGNet(\n",
    "                n_classes, C, Tw,\n",
    "                F1=F1, D=D, F2=F2, kernel_time=kernel_time,\n",
    "                pool1=pool1, pool2=pool2, dropout=dropout\n",
    "            )\n",
    "\n",
    "            model, best = fit_model(\n",
    "                model, dl_tr, dl_va,\n",
    "                max_epochs=max_epochs, lr=lr, weight_decay=weight_decay,\n",
    "                device=device, class_weights=info['class_weights'],\n",
    "                patience=20, log_file=str(log_file)\n",
    "            )\n",
    "\n",
    "            acc, f1m, _ = evaluate_model(model, dl_va, device)\n",
    "            msg = f\"[INTRA] {sid} | fold {fold} | acc={acc:.3f} | f1m={f1m:.3f}\"\n",
    "            print(msg)\n",
    "            with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(msg + \"\\n\")\n",
    "\n",
    "            rows.append(dict(subject=sid, fold=fold, acc=float(acc), f1_macro=float(f1m)))\n",
    "\n",
    "        # --- PROMEDIO POR SUJETO (después de completar los k folds del sujeto) ---\n",
    "        subj_rows = [r for r in rows if r['subject'] == sid]\n",
    "        acc_mean = float(np.mean([r['acc'] for r in subj_rows])) if subj_rows else 0.0\n",
    "        f1_mean  = float(np.mean([r['f1_macro'] for r in subj_rows])) if subj_rows else 0.0\n",
    "        rows_subj.append(dict(subject=sid, acc_mean=acc_mean, f1_mean=f1_mean, n_folds=len(subj_rows)))\n",
    "\n",
    "        msg_subj = f\"[INTRA] {sid} | mean_acc={acc_mean:.3f} | mean_f1={f1_mean:.3f}\"\n",
    "        print(msg_subj)\n",
    "        with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(msg_subj + \"\\n\")\n",
    "\n",
    "    # --- DataFrames finales ---\n",
    "    df = pd.DataFrame(rows).sort_values(['subject','fold'])\n",
    "    df_avg_subject = pd.DataFrame(rows_subj).sort_values('subject')\n",
    "\n",
    "    # Promedio GLOBAL como media de los promedios por sujeto\n",
    "    if len(df_avg_subject):\n",
    "        acc_mu = float(df_avg_subject['acc_mean'].mean())\n",
    "        f1_mu  = float(df_avg_subject['f1_mean'].mean())\n",
    "    else:\n",
    "        acc_mu = 0.0\n",
    "        f1_mu = 0.0\n",
    "\n",
    "    print(f\"[INTRA GLOBAL] mean_acc={acc_mu:.3f} | mean_f1={f1_mu:.3f}\")\n",
    "    with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"[INTRA GLOBAL] mean_acc={acc_mu:.3f} | mean_f1={f1_mu:.3f}\\n\")\n",
    "\n",
    "    # --- Guardar CSVs ---\n",
    "    (OUT_INTRA/'tables').mkdir(parents=True, exist_ok=True)\n",
    "    ts2 = ts_now()\n",
    "    out_csv_all = OUT_INTRA/'tables'/f\"{ts2}_eegnet_intra_all.csv\"\n",
    "    out_csv_avg = OUT_INTRA/'tables'/f\"{ts2}_eegnet_intra_subject_avg.csv\"\n",
    "    df.to_csv(out_csv_all, index=False)\n",
    "    df_avg_subject.to_csv(out_csv_avg, index=False)\n",
    "    print(\"CSV →\", out_csv_all)\n",
    "    print(\"CSV →\", out_csv_avg)\n",
    "\n",
    "    return df, df_avg_subject\n",
    "\n",
    "\n",
    "# -------------------- LOSO --------------------\n",
    "def run_loso_eegnet(\n",
    "    fif_dir=DATA_PROC, crop_window=(0.5,3.5),\n",
    "    max_epochs=150, lr=3e-4, weight_decay=1e-3,\n",
    "    F1=16, D=2, F2=32, kernel_time=64, dropout=0.5, pool1=4, pool2=8,\n",
    "    train_win_sec=1.5, train_stride_sec=0.25, eval_win_sec=None,\n",
    "    batch_size=128, seed=42, fs=250.0, use_augment=True,\n",
    "    calibrate_k_per_class=0, calibrate_mode='head', log_tag=\"loso\"\n",
    "):\n",
    "    set_seed(seed)\n",
    "    ts = ts_now()\n",
    "    rows=[]\n",
    "    subs = list_subjects(fif_dir)\n",
    "    log_file = OUT_LOSO/'logs'/f\"{ts}_eegnet_{log_tag}.txt\"\n",
    "\n",
    "    for s_test in subs:\n",
    "        train_ids = [s for s in subs if s != s_test]\n",
    "        ep_tr = mne.concatenate_epochs([mne.read_epochs(str(Path(fif_dir)/f\"{s}_MI-epo.fif\"),\n",
    "                                                       preload=True, verbose=False) for s in train_ids],\n",
    "                                       on_mismatch='ignore')\n",
    "        ep_te = mne.read_epochs(str(Path(fif_dir)/f\"{s_test}_MI-epo.fif\"), preload=True, verbose=False)\n",
    "        ep_te = ep_te.reorder_channels(ep_tr.ch_names)\n",
    "\n",
    "        if crop_window is not None:\n",
    "            ep_tr.crop(*crop_window); ep_te.crop(*crop_window)\n",
    "\n",
    "        # split calib/eval dentro de TEST\n",
    "        ep_calib, ep_eval = split_calibration(ep_te, k_per_class=calibrate_k_per_class, seed=seed)\n",
    "\n",
    "        # separa VALIDATION del TRAIN (para early stopping)\n",
    "        y_tr = epochs_to_labels(ep_tr); le = LabelEncoder().fit(y_tr)\n",
    "        idx = np.arange(len(ep_tr))\n",
    "        tr_idx, va_idx = train_test_split(idx, test_size=0.15, random_state=seed,\n",
    "                                          shuffle=True, stratify=le.transform(y_tr))\n",
    "        ep_tr_sub = ep_tr.copy()[tr_idx]; ep_va = ep_tr.copy()[va_idx]\n",
    "\n",
    "        dl_tr, dl_va, dl_eval, info = make_eegnet_loaders_from_epochs(\n",
    "            ep_tr_sub, ep_va, ep_eval, fs=fs, crop_sec=None,\n",
    "            train_win_sec=train_win_sec, train_stride_sec=train_stride_sec,\n",
    "            eval_win_sec=(eval_win_sec if eval_win_sec is not None else train_win_sec),\n",
    "            batch_size=batch_size, use_augment=use_augment\n",
    "        )\n",
    "        n_classes = info['n_classes']; C = ep_tr.info['nchan']; Tw = int(info['T'])\n",
    "        model = EEGNet(n_classes, C, Tw, F1=F1, D=D, F2=F2, kernel_time=kernel_time,\n",
    "                       pool1=pool1, pool2=pool2, dropout=dropout)\n",
    "\n",
    "        model, best = fit_model(model, dl_tr, dl_va, max_epochs=max_epochs, lr=lr,\n",
    "                                weight_decay=weight_decay, device=device,\n",
    "                                class_weights=info['class_weights'], patience=20,\n",
    "                                log_file=str(log_file))\n",
    "\n",
    "        # calibración few-shot si aplica\n",
    "        if calibrate_k_per_class and calibrate_k_per_class > 0 and ep_calib is not None:\n",
    "            _, _, dl_calib, _ = make_eegnet_loaders_from_epochs(\n",
    "                ep_tr, ep_va, ep_calib, fs=fs, crop_sec=None,\n",
    "                train_win_sec=train_win_sec, train_stride_sec=train_stride_sec,\n",
    "                eval_win_sec=(eval_win_sec if eval_win_sec is not None else train_win_sec),\n",
    "                batch_size=batch_size, use_augment=False\n",
    "            )\n",
    "            if calibrate_mode.lower() == 'head':\n",
    "                for p in model.parameters(): p.requires_grad_(False)\n",
    "                for p in model.classifier.parameters(): p.requires_grad_(True)\n",
    "                opt = torch.optim.AdamW(model.classifier.parameters(), lr=1e-3, weight_decay=0.0)\n",
    "                crit = torch.nn.CrossEntropyLoss()\n",
    "                model.to(device); model.train()\n",
    "                for xb,yb in dl_calib:\n",
    "                    xb,yb = xb.to(device), yb.to(device)\n",
    "                    opt.zero_grad(set_to_none=True)\n",
    "                    logits = model(xb); loss = crit(logits, yb)\n",
    "                    loss.backward(); opt.step()\n",
    "            else:\n",
    "                opt = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.0)\n",
    "                crit = torch.nn.CrossEntropyLoss()\n",
    "                model.to(device); model.train()\n",
    "                steps = min(300, len(dl_calib))\n",
    "                it = 0\n",
    "                for xb,yb in dl_calib:\n",
    "                    xb,yb = xb.to(device), yb.to(device)\n",
    "                    opt.zero_grad(set_to_none=True)\n",
    "                    logits = model(xb); loss = crit(logits, yb)\n",
    "                    loss.backward(); opt.step()\n",
    "                    it += 1\n",
    "                    if it >= steps: break\n",
    "\n",
    "        acc, f1m, cm = evaluate_model(model, dl_eval, device)\n",
    "        msg = f\"[LOSO] test={s_test} | acc={acc:.3f} | f1m={f1m:.3f}\"\n",
    "        print(msg)\n",
    "        with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(msg + \"\\n\")\n",
    "        rows.append(dict(test_subject=s_test, acc=float(acc), f1_macro=float(f1m), n_test=len(ep_eval)))\n",
    "\n",
    "        # figura por sujeto (opcional)\n",
    "        fig, ax = plt.subplots(figsize=(5,4), dpi=140)\n",
    "        ConfusionMatrixDisplay(cm).plot(ax=ax, cmap='Blues', colorbar=False, values_format='d')\n",
    "        ax.set_title(f\"EEGNet LOSO — {s_test}\")\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(OUT_LOSO/'figures'/f\"cm_loso_{s_test}.png\"); plt.close(fig)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    # fila GLOBAL\n",
    "    g_acc = df.acc.mean(); g_f1 = df.f1_macro.mean()\n",
    "    df.loc[len(df)] = dict(test_subject='GLOBAL', acc=g_acc, f1_macro=g_f1, n_test=int(df.n_test.sum()))\n",
    "    print(f\"[LOSO GLOBAL] acc_avg={g_acc:.3f} | f1m_avg={g_f1:.3f}\")\n",
    "    with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"[LOSO GLOBAL] acc_avg={g_acc:.3f} | f1m_avg={g_f1:.3f}\\n\")\n",
    "\n",
    "    ts2 = ts_now()\n",
    "    out_csv = OUT_LOSO/'tables'/f\"{ts2}_eegnet_loso.csv\"\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(\"CSV →\", out_csv)\n",
    "    return df\n",
    "\n",
    "# -------------------- INTER (test fijo) --------------------\n",
    "FIXED_TEST_SUBJECTS = [\n",
    "    'S007','S025','S029','S031','S032','S034','S035','S042','S043','S049','S056','S058','S062','S072',\n",
    "    'S001','S010','S013','S017','S019','S030',\n",
    "    'S005','S006','S009','S097'\n",
    "]\n",
    "\n",
    "def make_split_with_fixed_test(fif_dir=DATA_PROC, fixed_test=(), val_size=16, seed=42):\n",
    "    all_files = sorted(Path(fif_dir).glob('S???_MI-epo.fif'))\n",
    "    sids = [p.stem.split('_')[0] for p in all_files]\n",
    "    existing = set(sids)\n",
    "    test_subs = [s for s in list(fixed_test) if s in existing]\n",
    "    remaining = [s for s in sids if s not in test_subs]\n",
    "    if len(remaining) <= val_size:\n",
    "        raise ValueError(\"No hay suficientes sujetos para validación.\")\n",
    "    train_subs, val_subs = train_test_split(remaining, test_size=val_size, random_state=seed, shuffle=True)\n",
    "    return train_subs, val_subs, test_subs\n",
    "\n",
    "def run_inter_fixedsplit_eegnet(\n",
    "    fif_dir=DATA_PROC, crop_window=(0.5,3.5),\n",
    "    max_epochs=180, lr=3e-4, weight_decay=1e-3,\n",
    "    F1=16, D=2, F2=32, kernel_time=64, dropout=0.5, pool1=4, pool2=8,\n",
    "    train_win_sec=1.5, train_stride_sec=0.25, eval_win_sec=None,\n",
    "    batch_size=128, seed=42, fs=250.0, use_augment=True,\n",
    "    refit_on_trainval_for_test=True,\n",
    "    calibrate_k_per_class=0, calibrate_mode='head', log_tag=\"inter\"\n",
    "):\n",
    "    set_seed(seed)\n",
    "    ts = ts_now()\n",
    "    log_file = OUT_INTER/'logs'/f\"{ts}_eegnet_{log_tag}.txt\"\n",
    "\n",
    "    train_subs, val_subs, test_subs = make_split_with_fixed_test(fif_dir, FIXED_TEST_SUBJECTS, val_size=16, seed=seed)\n",
    "    print(f\"TRAIN={len(train_subs)} | VAL={len(val_subs)} | TEST(fijo)={len(test_subs)}\")\n",
    "    with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"TRAIN={len(train_subs)} | VAL={len(val_subs)} | TEST={len(test_subs)}\\n\")\n",
    "\n",
    "    ep_tr = mne.concatenate_epochs([mne.read_epochs(str(Path(fif_dir)/f\"{s}_MI-epo.fif\"),\n",
    "                                                   preload=True, verbose=False) for s in train_subs],\n",
    "                                   on_mismatch='ignore')\n",
    "    ep_va = mne.concatenate_epochs([mne.read_epochs(str(Path(fif_dir)/f\"{s}_MI-epo.fif\"),\n",
    "                                                   preload=True, verbose=False) for s in val_subs],\n",
    "                                   on_mismatch='ignore')\n",
    "    ep_te_all = mne.concatenate_epochs([mne.read_epochs(str(Path(fif_dir)/f\"{s}_MI-epo.fif\"),\n",
    "                                                       preload=True, verbose=False) for s in test_subs],\n",
    "                                       on_mismatch='ignore')\n",
    "    ep_va = ep_va.reorder_channels(ep_tr.ch_names)\n",
    "    ep_te_all = ep_te_all.reorder_channels(ep_tr.ch_names)\n",
    "\n",
    "    # Entrena (TRAIN vs VAL)\n",
    "    dl_tr, dl_va, _, info = make_eegnet_loaders_from_epochs(\n",
    "        ep_tr, ep_va, ep_va, fs=fs, crop_sec=crop_window,\n",
    "        train_win_sec=train_win_sec, train_stride_sec=train_stride_sec,\n",
    "        eval_win_sec=(eval_win_sec if eval_win_sec is not None else train_win_sec),\n",
    "        batch_size=batch_size, use_augment=use_augment\n",
    "    )\n",
    "    n_classes = info['n_classes']; C = ep_tr.info['nchan']; Tw = int(info['T'])\n",
    "    model = EEGNet(n_classes, C, Tw, F1=F1, D=D, F2=F2, kernel_time=kernel_time,\n",
    "                   pool1=pool1, pool2=pool2, dropout=dropout)\n",
    "\n",
    "    model, best = fit_model(model, dl_tr, dl_va, max_epochs=max_epochs, lr=lr,\n",
    "                            weight_decay=weight_decay, device=device,\n",
    "                            class_weights=info['class_weights'], patience=20,\n",
    "                            log_file=str(log_file))\n",
    "\n",
    "    # Test: refit base (TRAIN+VAL) opcional para normalización y ventanas\n",
    "    ep_trva = mne.concatenate_epochs([ep_tr, ep_va], on_mismatch='ignore') if refit_on_trainval_for_test else ep_tr\n",
    "    ep_calib, ep_eval = split_calibration(ep_te_all, k_per_class=calibrate_k_per_class, seed=seed)\n",
    "\n",
    "    # Loaders para eval y (si aplica) calib\n",
    "    _, _, dl_eval, _ = make_eegnet_loaders_from_epochs(\n",
    "        ep_trva, ep_va, ep_eval, fs=fs, crop_sec=crop_window,\n",
    "        train_win_sec=train_win_sec, train_stride_sec=train_stride_sec,\n",
    "        eval_win_sec=(eval_win_sec if eval_win_sec is not None else train_win_sec),\n",
    "        batch_size=batch_size, use_augment=False\n",
    "    )\n",
    "\n",
    "    if calibrate_k_per_class and calibrate_k_per_class > 0 and ep_calib is not None:\n",
    "        _, _, dl_calib, _ = make_eegnet_loaders_from_epochs(\n",
    "            ep_trva, ep_va, ep_calib, fs=fs, crop_sec=crop_window,\n",
    "            train_win_sec=train_win_sec, train_stride_sec=train_stride_sec,\n",
    "            eval_win_sec=(eval_win_sec if eval_win_sec is not None else train_win_sec),\n",
    "            batch_size=batch_size, use_augment=False\n",
    "        )\n",
    "        if calibrate_mode.lower() == 'head':\n",
    "            for p in model.parameters(): p.requires_grad_((False))\n",
    "            for p in model.classifier.parameters(): p.requires_grad_(True)\n",
    "            opt = torch.optim.AdamW(model.classifier.parameters(), lr=1e-3, weight_decay=0.0)\n",
    "            crit = torch.nn.CrossEntropyLoss()\n",
    "            model.to(device); model.train()\n",
    "            for xb,yb in dl_calib:\n",
    "                xb,yb = xb.to(device), yb.to(device)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                logits = model(xb); loss = crit(logits, yb)\n",
    "                loss.backward(); opt.step()\n",
    "        else:\n",
    "            opt = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.0)\n",
    "            crit = torch.nn.CrossEntropyLoss()\n",
    "            model.to(device); model.train()\n",
    "            steps = min(300, len(dl_calib))\n",
    "            it = 0\n",
    "            for xb,yb in dl_calib:\n",
    "                xb,yb = xb.to(device), yb.to(device)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                logits = model(xb); loss = crit(logits, yb)\n",
    "                loss.backward(); opt.step()\n",
    "                it += 1\n",
    "                if it >= steps: break\n",
    "\n",
    "    acc_te, f1_te, cm_te = evaluate_model(model, dl_eval, device)\n",
    "    msg = f\"[INTER-EEGNet] ACC_test={acc_te:.3f} | F1m_test={f1_te:.3f}\"\n",
    "    print(msg)\n",
    "    with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(msg + \"\\n\")\n",
    "\n",
    "    # Figura y tablas\n",
    "    fig, ax = plt.subplots(figsize=(5.4, 4.6), dpi=140)\n",
    "    ConfusionMatrixDisplay(cm_te).plot(ax=ax, cmap=\"Blues\", colorbar=True, values_format='d')\n",
    "    ax.set_title(\"EEGNet — TEST fijo\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(OUT_INTER/'figures'/f\"inter_eegnet_confusion_{ts}.png\"); plt.close(fig)\n",
    "\n",
    "    df = pd.DataFrame([dict(\n",
    "        mode=\"inter_fixedsplit_eegnet\",\n",
    "        acc_test=float(acc_te), f1_test=float(f1_te),\n",
    "        n_train=len(train_subs), n_val=len(val_subs), n_test=len(test_subs),\n",
    "        crop=str(crop_window), train_win_sec=float(train_win_sec),\n",
    "        stride_sec=float(train_stride_sec), eval_win_sec=float(eval_win_sec or train_win_sec),\n",
    "        F1=int(F1), D=int(D), F2=int(F2), kernel_time=int(kernel_time),\n",
    "        dropout=float(dropout), pool1=int(pool1), pool2=int(pool2),\n",
    "        refit_on_trainval_for_test=bool(refit_on_trainval_for_test),\n",
    "        calibrate_k_per_class=int(calibrate_k_per_class or 0),\n",
    "        calibrate_mode=str(calibrate_mode)\n",
    "    )])\n",
    "    out_csv = OUT_INTER/'tables'/f\"{ts}_inter_eegnet.csv\"\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(\"CSV →\", out_csv)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822d650e",
   "metadata": {},
   "source": [
    "### Ejemplos de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16b6d4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN=63 | VAL=16 | TEST(fijo)=24\n",
      "Not setting metadata\n",
      "5287 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "1367 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "2043 matching events found\n",
      "No baseline correction applied\n",
      "[Epoch 010] train_loss=1.3678 | train_acc=0.297 | val_acc=0.300 | val_f1=0.296\n",
      "[Epoch 020] train_loss=1.3551 | train_acc=0.316 | val_acc=0.296 | val_f1=0.296\n",
      "[Epoch 030] train_loss=1.3512 | train_acc=0.323 | val_acc=0.296 | val_f1=0.295\n",
      "[TRAIN END] best_val_f1=0.309 @ epoch=13\n",
      "Not setting metadata\n",
      "6654 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "40 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "2003 matching events found\n",
      "No baseline correction applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-8b1a83ddf946>:14: RuntimeWarning: The events passed to the Epochs constructor are not chronologically ordered.\n",
      "  ep_calib = mne.concatenate_epochs(ep_calib_list, on_mismatch='ignore') if ep_calib_list else None\n",
      "<ipython-input-5-8b1a83ddf946>:15: RuntimeWarning: The events passed to the Epochs constructor are not chronologically ordered.\n",
      "  ep_eval  = mne.concatenate_epochs(ep_eval_list,  on_mismatch='ignore') if ep_eval_list else ep_test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INTER-EEGNet] ACC_test=0.377 | F1m_test=0.379\n",
      "CSV → /root/Proyecto/EEG_Clasificador/models/inter_fixed_eegnet/tables/20251006-214917_inter_eegnet.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-6e28ae75-2087-40f0-8fdc-a9631aa9b95c\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mode</th>\n",
       "      <th>acc_test</th>\n",
       "      <th>f1_test</th>\n",
       "      <th>n_train</th>\n",
       "      <th>n_val</th>\n",
       "      <th>n_test</th>\n",
       "      <th>crop</th>\n",
       "      <th>train_win_sec</th>\n",
       "      <th>stride_sec</th>\n",
       "      <th>eval_win_sec</th>\n",
       "      <th>F1</th>\n",
       "      <th>D</th>\n",
       "      <th>F2</th>\n",
       "      <th>kernel_time</th>\n",
       "      <th>dropout</th>\n",
       "      <th>pool1</th>\n",
       "      <th>pool2</th>\n",
       "      <th>refit_on_trainval_for_test</th>\n",
       "      <th>calibrate_k_per_class</th>\n",
       "      <th>calibrate_mode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inter_fixedsplit_eegnet</td>\n",
       "      <td>0.376935</td>\n",
       "      <td>0.378948</td>\n",
       "      <td>63</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>(0.5, 3.5)</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.5</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>head</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6e28ae75-2087-40f0-8fdc-a9631aa9b95c')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-6e28ae75-2087-40f0-8fdc-a9631aa9b95c button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-6e28ae75-2087-40f0-8fdc-a9631aa9b95c');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                      mode  acc_test   f1_test  n_train  n_val  n_test  \\\n",
       "0  inter_fixedsplit_eegnet  0.376935  0.378948       63     16      24   \n",
       "\n",
       "         crop  train_win_sec  stride_sec  eval_win_sec  F1  D  F2  \\\n",
       "0  (0.5, 3.5)            1.5        0.25           1.5  16  2  32   \n",
       "\n",
       "   kernel_time  dropout  pool1  pool2  refit_on_trainval_for_test  \\\n",
       "0           64      0.5      4      8                        True   \n",
       "\n",
       "   calibrate_k_per_class calibrate_mode  \n",
       "0                     10           head  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% [ejemplos de ejecución]\n",
    "# Ajusta hiperparámetros aquí\n",
    "COMMON = dict(\n",
    "    crop_window=(0.5, 3.5),\n",
    "    F1=16, D=2, F2=32, kernel_time=64, dropout=0.5, pool1=4, pool2=8,\n",
    "    train_win_sec=1.5, train_stride_sec=0.25, eval_win_sec=1.5,\n",
    "    batch_size=256, seed=42, fs=160.0, use_augment=True\n",
    ")\n",
    "\n",
    "# -------- INTRA (k-fold) --------\n",
    "# df_intra, df_intra_subject_avg = run_intra_all_eegnet(\n",
    "#     fif_dir=DATA_PROC,\n",
    "#     k=5,\n",
    "#     max_epochs=200, lr=3e-4, weight_decay=1e-3,\n",
    "#     **COMMON, log_tag=\"intra_default\"\n",
    "# )\n",
    "# try:\n",
    "#     display(df_intra.head())\n",
    "#     display(df_intra_subject_avg.head())\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# -------- LOSO --------\n",
    "# df_loso = run_loso_eegnet(\n",
    "#     fif_dir=DATA_PROC,\n",
    "#     max_epochs=150, lr=3e-4, weight_decay=1e-3,\n",
    "#     calibrate_k_per_class=5,      # 0/None => sin calibración\n",
    "#     calibrate_mode='head',        # 'head' recomendado\n",
    "#     **COMMON, log_tag=\"loso_default\"\n",
    "# )\n",
    "# try:\n",
    "#     display(df_loso.head())\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# -------- INTER (test fijo + calibración) --------\n",
    "df_inter = run_inter_fixedsplit_eegnet(\n",
    "    fif_dir=DATA_PROC,\n",
    "    max_epochs=180, lr=3e-4, weight_decay=1e-3,\n",
    "    refit_on_trainval_for_test=True,\n",
    "    calibrate_k_per_class=10,     # few-shot por clase\n",
    "    calibrate_mode='head',\n",
    "    **COMMON, log_tag=\"inter_default\"\n",
    ")\n",
    "try:\n",
    "    display(df_inter)\n",
    "except:\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
