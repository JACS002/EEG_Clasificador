{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64483706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Usando dispositivo: cuda\n",
      "üß† INICIANDO EXPERIMENTO CON EEGNet + FINE-TUNING PROGRESIVO (augments + SGDR + tweaks)\n",
      "üîß Configuraci√≥n: 4c, 8 canales, 6s\n",
      "‚öôÔ∏è  FT: epochs=30, base_lr=5e-05, head_lr=0.001, L2SP=0.0001, patience=5, CV=4\n",
      "Sujetos elegibles: 103 ‚Üí [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Construyendo dataset (RAW): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:41<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset construido: N=8652 | T=960 | C=8 | clases=4 | sujetos √∫nicos=103\n",
      "Listo para entrenar: N=8652 | T=960 | C=8 | clases=4 | sujetos=103\n",
      "\n",
      "[Fold 1/5] Entrenando modelo global... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  √âpoca   1 | train_acc=0.4605 | val_acc=0.4304 | LR=0.01000\n",
      "  √âpoca   5 | train_acc=0.5138 | val_acc=0.4643 | LR=0.00250\n",
      "  √âpoca  10 | train_acc=0.5226 | val_acc=0.4744 | LR=0.00854\n",
      "  √âpoca  15 | train_acc=0.5514 | val_acc=0.4872 | LR=0.00250\n",
      "  √âpoca  20 | train_acc=0.5371 | val_acc=0.4698 | LR=0.00996\n",
      "  √âpoca  25 | train_acc=0.5229 | val_acc=0.4799 | LR=0.00854\n",
      "  Early stopping en √©poca 26 (mejor val_acc=0.4881)\n",
      "‚Ü≥ Curva de entrenamiento guardada: training_curve_fold1.png\n",
      "[Fold 1/5] Global acc=0.4575\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5516    0.4603    0.5019       441\n",
      "       Right     0.5642    0.4785    0.5178       441\n",
      "  Both Fists     0.3540    0.5057    0.4164       441\n",
      "   Both Feet     0.4337    0.3855    0.4082       441\n",
      "\n",
      "    accuracy                         0.4575      1764\n",
      "   macro avg     0.4759    0.4575    0.4611      1764\n",
      "weighted avg     0.4759    0.4575    0.4611      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5130 | sujetos=21\n",
      "  Œî(FT-Global) = +0.0556\n",
      "\n",
      "[Fold 2/5] Entrenando modelo global... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  √âpoca   1 | train_acc=0.4155 | val_acc=0.4029 | LR=0.01000\n",
      "  √âpoca   5 | train_acc=0.4707 | val_acc=0.4359 | LR=0.00250\n",
      "  √âpoca  10 | train_acc=0.5117 | val_acc=0.4560 | LR=0.00854\n",
      "  √âpoca  15 | train_acc=0.5133 | val_acc=0.4588 | LR=0.00250\n",
      "  √âpoca  20 | train_acc=0.5312 | val_acc=0.4734 | LR=0.00996\n",
      "  √âpoca  25 | train_acc=0.5345 | val_acc=0.4762 | LR=0.00854\n",
      "  √âpoca  30 | train_acc=0.5561 | val_acc=0.4808 | LR=0.00565\n",
      "  √âpoca  35 | train_acc=0.5866 | val_acc=0.4881 | LR=0.00250\n",
      "  √âpoca  40 | train_acc=0.5714 | val_acc=0.4597 | LR=0.00038\n",
      "  Early stopping en √©poca 43 (mejor val_acc=0.4908)\n",
      "‚Ü≥ Curva de entrenamiento guardada: training_curve_fold2.png\n",
      "[Fold 2/5] Global acc=0.5351\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.6128    0.6281    0.6204       441\n",
      "       Right     0.5934    0.5692    0.5810       441\n",
      "  Both Fists     0.4307    0.5215    0.4718       441\n",
      "   Both Feet     0.5239    0.4218    0.4673       441\n",
      "\n",
      "    accuracy                         0.5351      1764\n",
      "   macro avg     0.5402    0.5351    0.5351      1764\n",
      "weighted avg     0.5402    0.5351    0.5351      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.6173 | sujetos=21\n",
      "  Œî(FT-Global) = +0.0822\n",
      "\n",
      "[Fold 3/5] Entrenando modelo global... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  √âpoca   1 | train_acc=0.4432 | val_acc=0.4332 | LR=0.01000\n",
      "  √âpoca   5 | train_acc=0.5350 | val_acc=0.5165 | LR=0.00250\n",
      "  √âpoca  10 | train_acc=0.5147 | val_acc=0.5375 | LR=0.00854\n",
      "  √âpoca  15 | train_acc=0.5336 | val_acc=0.5385 | LR=0.00250\n",
      "  √âpoca  20 | train_acc=0.5450 | val_acc=0.5375 | LR=0.00996\n",
      "  √âpoca  25 | train_acc=0.5212 | val_acc=0.5128 | LR=0.00854\n",
      "  Early stopping en √©poca 29 (mejor val_acc=0.5421)\n",
      "‚Ü≥ Curva de entrenamiento guardada: training_curve_fold3.png\n",
      "[Fold 3/5] Global acc=0.4711\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5245    0.5102    0.5172       441\n",
      "       Right     0.5631    0.5261    0.5440       441\n",
      "  Both Fists     0.3818    0.4762    0.4238       441\n",
      "   Both Feet     0.4397    0.3719    0.4029       441\n",
      "\n",
      "    accuracy                         0.4711      1764\n",
      "   macro avg     0.4773    0.4711    0.4720      1764\n",
      "weighted avg     0.4773    0.4711    0.4720      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5323 | sujetos=21\n",
      "  Œî(FT-Global) = +0.0612\n",
      "\n",
      "[Fold 4/5] Entrenando modelo global... (n_train=5880 | n_val=1092 | n_test=1680)\n",
      "  √âpoca   1 | train_acc=0.4437 | val_acc=0.4313 | LR=0.01000\n",
      "  √âpoca   5 | train_acc=0.5162 | val_acc=0.4853 | LR=0.00250\n",
      "  √âpoca  10 | train_acc=0.5167 | val_acc=0.4789 | LR=0.00854\n",
      "  √âpoca  15 | train_acc=0.5304 | val_acc=0.4991 | LR=0.00250\n",
      "  √âpoca  20 | train_acc=0.5204 | val_acc=0.4927 | LR=0.00996\n",
      "  √âpoca  25 | train_acc=0.5369 | val_acc=0.4863 | LR=0.00854\n",
      "  Early stopping en √©poca 26 (mejor val_acc=0.5192)\n",
      "‚Ü≥ Curva de entrenamiento guardada: training_curve_fold4.png\n",
      "[Fold 4/5] Global acc=0.5107\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5940    0.5643    0.5788       420\n",
      "       Right     0.5891    0.4643    0.5193       420\n",
      "  Both Fists     0.4256    0.5786    0.4904       420\n",
      "   Both Feet     0.4828    0.4357    0.4581       420\n",
      "\n",
      "    accuracy                         0.5107      1680\n",
      "   macro avg     0.5229    0.5107    0.5116      1680\n",
      "weighted avg     0.5229    0.5107    0.5116      1680\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5774 | sujetos=20\n",
      "  Œî(FT-Global) = +0.0667\n",
      "\n",
      "[Fold 5/5] Entrenando modelo global... (n_train=5880 | n_val=1092 | n_test=1680)\n",
      "  √âpoca   1 | train_acc=0.4199 | val_acc=0.4167 | LR=0.01000\n",
      "  √âpoca   5 | train_acc=0.4830 | val_acc=0.4643 | LR=0.00250\n",
      "  √âpoca  10 | train_acc=0.5071 | val_acc=0.4588 | LR=0.00854\n",
      "  √âpoca  15 | train_acc=0.5031 | val_acc=0.4835 | LR=0.00250\n",
      "  √âpoca  20 | train_acc=0.5122 | val_acc=0.4643 | LR=0.00996\n",
      "  Early stopping en √©poca 24 (mejor val_acc=0.4954)\n",
      "‚Ü≥ Curva de entrenamiento guardada: training_curve_fold5.png\n",
      "[Fold 5/5] Global acc=0.5190\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.6141    0.5190    0.5626       420\n",
      "       Right     0.5718    0.4643    0.5125       420\n",
      "  Both Fists     0.4377    0.5524    0.4884       420\n",
      "   Both Feet     0.5000    0.5405    0.5195       420\n",
      "\n",
      "    accuracy                         0.5190      1680\n",
      "   macro avg     0.5309    0.5190    0.5207      1680\n",
      "weighted avg     0.5309    0.5190    0.5207      1680\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5863 | sujetos=20\n",
      "  Œî(FT-Global) = +0.0673\n",
      "\n",
      "============================================================\n",
      "RESULTADOS FINALES\n",
      "============================================================\n",
      "Global folds: ['0.4575', '0.5351', '0.4711', '0.5107', '0.5190']\n",
      "Global mean: 0.4987\n",
      "Fine-tune PROGRESIVO folds: ['0.5130', '0.6173', '0.5323', '0.5774', '0.5863']\n",
      "Fine-tune PROGRESIVO mean: 0.5653\n",
      "Œî(FT-Global) mean: +0.0666\n",
      "\n",
      "‚Ü≥ Matriz de confusi√≥n guardada: confusion_global_allfolds.png\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# EEGNet + protocolo global + fine-tuning progresivo por sujeto (augments + SGDR + tweaks)\n",
    "# Cambios clave:\n",
    "# 1) Capacidad ‚Üë: F1=24, fc=128\n",
    "# 2) Loss ponderada por clase (+20% a Both Fists) con soporte para mixup (WeightedSoftCrossEntropy)\n",
    "# 3) Cutout focalizado (30‚Äì60 ms) solo para clases OF (2,3)\n",
    "# 4) SGDR con T0=6, Tmult=2\n",
    "# 5) TTA (n=5, jitter ¬±25 ms) en evaluaci√≥n\n",
    "\n",
    "import os, re, math, random, json, itertools, copy\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold, StratifiedShuffleSplit, GroupShuffleSplit\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# CONFIGURACI√ìN GENERAL\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "FOLDS_DIR = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dispositivo y semilla\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE); np.random.seed(RANDOM_STATE); random.seed(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "# Escenario y ventana\n",
    "CLASS_SCENARIO = '4c'\n",
    "WINDOW_MODE = '6s'   # mantienes 6s\n",
    "FS = 160.0\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Entrenamiento global\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS_GLOBAL = 100\n",
    "LR_INIT = 1e-2\n",
    "SGDR_T0 = 6      # ‚Üì ciclos cortos al inicio\n",
    "SGDR_Tmult = 2\n",
    "\n",
    "# Validaci√≥n/ES global\n",
    "GLOBAL_VAL_SPLIT = 0.15\n",
    "GLOBAL_PATIENCE  = 10\n",
    "LOG_EVERY        = 5\n",
    "\n",
    "# Fine-tuning por sujeto (protocolo robusto)\n",
    "CALIB_CV_FOLDS = 4\n",
    "FT_EPOCHS = 30\n",
    "FT_BASE_LR = 5e-5\n",
    "FT_HEAD_LR = 1e-3\n",
    "FT_L2SP = 1e-4\n",
    "FT_PATIENCE = 5\n",
    "FT_VAL_RATIO = 0.2\n",
    "\n",
    "# Normalizaci√≥n por √©poca canal-a-canal (z-score)\n",
    "NORM_EPOCH_ZSCORE = True\n",
    "\n",
    "# Sujetos excluidos\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "\n",
    "# Runs\n",
    "MI_RUNS_LR = [4, 8, 12]\n",
    "MI_RUNS_OF = [6, 10, 14]\n",
    "BASELINE_RUNS_EO = [1]\n",
    "\n",
    "# Canales (8 con FCz)\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','FCz']\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES DE CANALES\n",
    "# =========================\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = re.sub(r'([A-Za-z])0([0-9])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])Z$', r'\\1z', s)\n",
    "    s = s.replace('fp', 'Fp').replace('FP', 'Fp')\n",
    "    s = ''.join(ch.upper() if ch != 'z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {}\n",
    "    for ch in raw.ch_names:\n",
    "        lab = normalize_label(ch)\n",
    "        lab = lab[:-1] + 'z' if lab.endswith('Z') else lab\n",
    "        lab = re.sub(r'([A-Z])Z$', r'\\1z', lab)\n",
    "        mapping[ch] = lab\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired_channels=EXPECTED_8):\n",
    "    missing = [ch for ch in desired_channels if ch not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"Warning: faltan canales {missing} en archivo {getattr(raw,'filenames', [''])[0]}\")\n",
    "        return None\n",
    "    raw.reorder_channels([ch for ch in raw.ch_names if ch in desired_channels] +\n",
    "                         [ch for ch in raw.ch_names if ch not in desired_channels])\n",
    "    raw.pick_channels(desired_channels, ordered=True)\n",
    "    return raw\n",
    "\n",
    "# =========================\n",
    "# LECTURA DE EDF y EVENTOS\n",
    "# =========================\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(run_id:int):\n",
    "    if run_id in MI_RUNS_LR: return 'LR'\n",
    "    if run_id in MI_RUNS_OF: return 'OF'\n",
    "    if run_id in BASELINE_RUNS_EO: return 'EO'\n",
    "    return None\n",
    "\n",
    "# --- Notch adaptativo: lee CSV si existe (subject, run, snr50_db, snr60_db)\n",
    "_SNR_TABLE = None\n",
    "def _load_snr_table():\n",
    "    global _SNR_TABLE\n",
    "    if _SNR_TABLE is not None:\n",
    "        return _SNR_TABLE\n",
    "    csv_path = PROJ / 'reports' / 'psd_mains' / 'psd_mains_summary.csv'\n",
    "    if csv_path.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            _SNR_TABLE = df\n",
    "        except Exception as e:\n",
    "            print(f\"[SNR] No se pudo leer {csv_path}: {e}\")\n",
    "            _SNR_TABLE = None\n",
    "    return _SNR_TABLE\n",
    "\n",
    "def _decide_notch(subject, run, th_db=10.0):\n",
    "    df = _load_snr_table()\n",
    "    if df is None:  # default\n",
    "        return 60.0\n",
    "    row = df[(df['subject']==subject) & (df['run']==run)]\n",
    "    if row.empty:\n",
    "        return 60.0\n",
    "    snr50 = float(row['snr50_db'].iloc[0]); snr60 = float(row['snr60_db'].iloc[0])\n",
    "    if snr60 >= th_db and snr60 >= snr50: return 60.0\n",
    "    if snr50 >= th_db and snr50 >  snr60: return 50.0\n",
    "    return None  # no notch si no sobresale\n",
    "\n",
    "def read_raw_edf(path: Path):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        mont = mne.channels.make_standard_montage('standard_1020')\n",
    "        raw.set_montage(mont, on_missing='ignore')\n",
    "    except Exception:\n",
    "        pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad=\"auto\")\n",
    "    raw = ensure_channels_order(raw, EXPECTED_8)\n",
    "    if raw is None:\n",
    "        return None\n",
    "\n",
    "    # Notch adaptativo (por SNR); si no hay CSV, por defecto 60 Hz\n",
    "    sid, rid = parse_subject_run(path)\n",
    "    notch = _decide_notch(sid, rid)\n",
    "    if notch is not None:\n",
    "        raw.notch_filter(freqs=[float(notch)], picks='eeg', method='spectrum_fit', phase='zero')\n",
    "\n",
    "    return raw\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    if raw.annotations is None or len(raw.annotations) == 0:\n",
    "        return []\n",
    "    def _norm(s): return str(s).strip().upper().replace(' ', '')\n",
    "    res = []\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = _norm(desc)\n",
    "        if tag in ('T1','T2'):\n",
    "            res.append((float(onset), tag))\n",
    "    res.sort()\n",
    "    dedup = []\n",
    "    last_t1 = last_t2 = -1e9\n",
    "    for t, tag in res:\n",
    "        if tag == 'T1':\n",
    "            if (t - last_t1) >= 0.5: dedup.append((t, tag)); last_t1 = t\n",
    "        else:\n",
    "            if (t - last_t2) >= 0.5: dedup.append((t, tag)); last_t2 = t\n",
    "    return dedup\n",
    "\n",
    "# =========================\n",
    "# CONSTRUCCI√ìN DE DATASETS\n",
    "# =========================\n",
    "def subjects_available():\n",
    "    subs = []\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try: sid = int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir / f\"S{sid:03d}R{r:02d}.edf\").exists() for r in (MI_RUNS_LR + MI_RUNS_OF))\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "def extract_trials_from_run(edf_path: Path, scenario: str, window_mode: str):\n",
    "    subj, run = parse_subject_run(edf_path)\n",
    "    kind = run_kind(run)\n",
    "    if kind not in ('LR','OF','EO'):\n",
    "        return ([], [])\n",
    "\n",
    "    raw = read_raw_edf(edf_path)\n",
    "    if raw is None:\n",
    "        return ([], [])\n",
    "\n",
    "    data = raw.get_data()\n",
    "    fs = raw.info['sfreq']\n",
    "    assert abs(fs - FS) < 1e-6\n",
    "\n",
    "    out = []\n",
    "\n",
    "    if kind in ('LR','OF'):\n",
    "        events = collect_events_T1T2(raw)\n",
    "        if window_mode == '3s':\n",
    "            rel_start, rel_end = 0.0, 3.0\n",
    "        else:\n",
    "            rel_start, rel_end = -1.0, 5.0\n",
    "\n",
    "        for onset_sec, tag in events:\n",
    "            if kind == 'LR':\n",
    "                if tag == 'T1': label = 'L'\n",
    "                elif tag == 'T2': label = 'R'\n",
    "                else: continue\n",
    "            else:\n",
    "                if tag == 'T1': label = 'BFISTS'\n",
    "                elif tag == 'T2': label = 'BFEET'\n",
    "                else: continue\n",
    "\n",
    "            if scenario == '2c' and label not in ('L','R'): continue\n",
    "            if scenario == '3c' and label not in ('L','R','BFISTS'): continue\n",
    "            if scenario == '4c' and label not in ('L','R','BFISTS','BFEET'): continue\n",
    "\n",
    "            s = int(round((raw.first_time + onset_sec + rel_start) * fs))\n",
    "            e = int(round((raw.first_time + onset_sec + rel_end) * fs))\n",
    "            if s < 0 or e > data.shape[1]:\n",
    "                continue\n",
    "\n",
    "            seg = data[:, s:e].T.astype(np.float32)\n",
    "            if NORM_EPOCH_ZSCORE:\n",
    "                seg = (seg - seg.mean(axis=0, keepdims=True)) / (seg.std(axis=0, keepdims=True) + 1e-6)\n",
    "\n",
    "            if label == 'L':       y = 0\n",
    "            elif label == 'R':     y = 1\n",
    "            elif label == 'BFISTS':y = 2\n",
    "            elif label == 'BFEET': y = 3\n",
    "            else: continue\n",
    "\n",
    "            out.append((seg, y, subj))\n",
    "\n",
    "    elif kind == 'EO':\n",
    "        return ([], raw.ch_names)\n",
    "\n",
    "    return out, raw.ch_names\n",
    "\n",
    "def build_dataset_all(subjects, scenario='4c', window_mode='3s'):\n",
    "    X, y, groups = [], [], []\n",
    "    ch_template = None\n",
    "\n",
    "    for s in tqdm(subjects, desc=\"Construyendo dataset (RAW)\"):\n",
    "        sdir = DATA_RAW / f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "\n",
    "        trials_L, trials_R, trials_FISTS, trials_FEET = [], [], [], []\n",
    "\n",
    "        for r in MI_RUNS_LR:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 0: trials_L.append(seg)\n",
    "                elif lab == 1: trials_R.append(seg)\n",
    "\n",
    "        for r in MI_RUNS_OF:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 2: trials_FISTS.append(seg)\n",
    "                elif lab == 3: trials_FEET.append(seg)\n",
    "\n",
    "        need_per_class = 21\n",
    "        def pick(trials, n, rng):\n",
    "            if len(trials) < n:\n",
    "                idx = rng.choice(len(trials), size=n, replace=True)\n",
    "                return [trials[i] for i in idx]\n",
    "            rng.shuffle(trials)\n",
    "            return trials[:n]\n",
    "\n",
    "        rng = check_random_state(RANDOM_STATE + s)\n",
    "        if len(trials_L)==0 or len(trials_R)==0 or len(trials_FISTS)==0 or len(trials_FEET)==0:\n",
    "            continue\n",
    "\n",
    "        Lp  = pick(trials_L,     need_per_class, rng)\n",
    "        Rp  = pick(trials_R,     need_per_class, rng)\n",
    "        FIp = pick(trials_FISTS, need_per_class, rng)\n",
    "        FEp = pick(trials_FEET,  need_per_class, rng)\n",
    "\n",
    "        pack = [(Lp, 0), (Rp, 1), (FIp, 2), (FEp, 3)]\n",
    "        for segs, lab in pack:\n",
    "            for seg in segs:\n",
    "                X.append(seg); y.append(lab); groups.append(s)\n",
    "\n",
    "    X = np.stack(X, axis=0)\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "    groups = np.asarray(groups, dtype=np.int64)\n",
    "\n",
    "    n, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Dataset construido: N={n} | T={T} | C={C} | clases={n_classes} | sujetos √∫nicos={len(np.unique(groups))}\")\n",
    "    return X, y, groups, ch_template\n",
    "\n",
    "# =========================\n",
    "# AUGMENTS (solo train)\n",
    "# =========================\n",
    "def do_time_jitter(x, max_ms=50, fs=160.0):\n",
    "    # x: (B,1,T,C) torch.float\n",
    "    max_shift = int(round(max_ms/1000.0 * fs))\n",
    "    if max_shift <= 0: return x\n",
    "    B,_,T,C = x.shape\n",
    "    shifts = torch.randint(low=-max_shift, high=max_shift+1, size=(B,), device=x.device)\n",
    "    out = torch.empty_like(x)\n",
    "    for i,s in enumerate(shifts):\n",
    "        if s==0: out[i] = x[i]; continue\n",
    "        if s>0:\n",
    "            out[i,:,s:,:] = x[i,:,:T-s,:]\n",
    "            out[i,:,:s,:] = 0\n",
    "        else:\n",
    "            s = -s\n",
    "            out[i,:,:T-s,:] = x[i,:,s:,:]\n",
    "            out[i,:,T-s:,:] = 0\n",
    "    return out\n",
    "\n",
    "def do_gaussian_noise(x, sigma=0.01):\n",
    "    if sigma<=0: return x\n",
    "    return x + sigma*torch.randn_like(x)\n",
    "\n",
    "def do_temporal_cutout(x, min_ms=100, max_ms=150, fs=160.0):\n",
    "    B,_,T,C = x.shape\n",
    "    Lmin = int(round(min_ms/1000.0*fs))\n",
    "    Lmax = int(round(max_ms/1000.0*fs))\n",
    "    if Lmin<=0 or Lmax<=0 or Lmin>Lmax: return x\n",
    "    out = x.clone()\n",
    "    for i in range(B):\n",
    "        L = random.randint(Lmin, Lmax)\n",
    "        if L>=T: continue\n",
    "        s = random.randint(0, T-L)\n",
    "        out[i,:,s:s+L,:] = 0\n",
    "    return out\n",
    "\n",
    "def do_temporal_cutout_masked(x, y, classes_mask={2,3}, min_ms=30, max_ms=60, fs=160.0):\n",
    "    # Aplica cutout solo si y pertenece a classes_mask\n",
    "    B,_,T,C = x.shape\n",
    "    if B == 0: return x\n",
    "    Lmin = int(round(min_ms/1000.0*fs))\n",
    "    Lmax = int(round(max_ms/1000.0*fs))\n",
    "    if Lmin<=0 or Lmax<=0 or Lmin>Lmax: return x\n",
    "    out = x.clone()\n",
    "    y_np = y.detach().cpu().numpy()\n",
    "    for i in range(B):\n",
    "        if int(y_np[i]) not in classes_mask: \n",
    "            continue\n",
    "        L = random.randint(Lmin, Lmax)\n",
    "        if L>=T: continue\n",
    "        s = random.randint(0, T-L)\n",
    "        out[i,:,s:s+L,:] = 0\n",
    "    return out\n",
    "\n",
    "def mixup_batch(x, y, n_classes, alpha=0.2):\n",
    "    if alpha<=0:\n",
    "        y_onehot = torch.nn.functional.one_hot(y, num_classes=n_classes).float()\n",
    "        return x, y_onehot, 1.0\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    perm = torch.randperm(x.size(0), device=x.device)\n",
    "    x_mix = lam*x + (1-lam)*x[perm]\n",
    "    y_a = torch.nn.functional.one_hot(y, num_classes=n_classes).float()\n",
    "    y_b = y_a[perm]\n",
    "    y_mix = lam*y_a + (1-lam)*y_b\n",
    "    return x_mix, y_mix, lam\n",
    "\n",
    "class WeightedSoftCrossEntropy(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-entropy suave (targets en probas, p.ej. mixup) con pesos por clase.\n",
    "    \"\"\"\n",
    "    def __init__(self, class_weights=None, label_smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.register_buffer('w', None if class_weights is None else class_weights.clone().float())\n",
    "        self.ls = float(label_smoothing)\n",
    "\n",
    "    def forward(self, logits, target_probs):\n",
    "        # label smoothing: mezcla target_probs con uniforme\n",
    "        if self.ls > 0:\n",
    "            K = logits.size(1)\n",
    "            target_probs = (1-self.ls)*target_probs + self.ls*(1.0/K)\n",
    "        logp = torch.log_softmax(logits, dim=1)  # (B,C)\n",
    "        loss_per_class = -(target_probs * logp)  # (B,C)\n",
    "        if self.w is not None:\n",
    "            loss_per_class = loss_per_class * self.w.unsqueeze(0)  # pesar por clase\n",
    "        loss = loss_per_class.sum(dim=1).mean()\n",
    "        return loss\n",
    "\n",
    "# =========================\n",
    "# EEGNet (Lawhern et al., 2018) adaptado a (B,1,T,C) + ChannelDropout\n",
    "# =========================\n",
    "class ChannelDropout(nn.Module):\n",
    "    def __init__(self, p=0.1):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    def forward(self, x):\n",
    "        if not self.training or self.p<=0: return x\n",
    "        B,_,T,C = x.shape\n",
    "        mask = (torch.rand(B,1,1,C, device=x.device) > self.p).float()\n",
    "        return x * mask\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Entrada: x de forma (B, 1, T, C)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_ch: int, n_classes: int,\n",
    "                 F1: int = 24, D: int = 2, kernel_t: int = 64, k_sep: int = 16,\n",
    "                 pool1_t: int = 4, pool2_t: int = 6,\n",
    "                 drop1_p: float = 0.35, drop2_p: float = 0.6,\n",
    "                 chdrop_p: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.n_ch = n_ch\n",
    "        self.n_classes = n_classes\n",
    "        self.F1 = F1\n",
    "        self.D = D\n",
    "        self.F2 = F1 * D\n",
    "        self.kernel_t = kernel_t\n",
    "        self.k_sep = k_sep\n",
    "        self.pool1_t = pool1_t\n",
    "        self.pool2_t = pool2_t\n",
    "\n",
    "        self.chdrop = ChannelDropout(p=chdrop_p)\n",
    "\n",
    "        # Bloque 1: temporal\n",
    "        self.conv_temporal = nn.Conv2d(1, F1, kernel_size=(kernel_t, 1),\n",
    "                                       padding=(kernel_t // 2, 0), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(F1, momentum=0.99, eps=1e-3)\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "        # Bloque 2: depthwise (espacial)\n",
    "        self.conv_depthwise = nn.Conv2d(F1, self.F2, kernel_size=(1, n_ch),\n",
    "                                        groups=F1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.F2, momentum=0.99, eps=1e-3)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=(pool1_t, 1), stride=(pool1_t, 1))\n",
    "        self.drop1 = nn.Dropout(drop1_p)\n",
    "\n",
    "        # Bloque 3: separable temporal\n",
    "        self.conv_sep_depth = nn.Conv2d(self.F2, self.F2, kernel_size=(k_sep, 1),\n",
    "                                        groups=self.F2, padding=(k_sep // 2, 0), bias=False)\n",
    "        self.conv_sep_point = nn.Conv2d(self.F2, self.F2, kernel_size=(1, 1), bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.F2, momentum=0.99, eps=1e-3)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(pool2_t, 1), stride=(pool2_t, 1))\n",
    "        self.drop2 = nn.Dropout(drop2_p)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Cabeza din√°mica\n",
    "        self.fc = None\n",
    "        self.out = None\n",
    "        self._T_in = None\n",
    "\n",
    "    def _build_head(self, T_in: int, device: torch.device):\n",
    "        T1 = T_in // self.pool1_t\n",
    "        T2 = T1 // self.pool2_t\n",
    "        feat_dim = self.F2 * T2 * 1\n",
    "        self.fc  = nn.Linear(feat_dim, 128, bias=True).to(device)\n",
    "        self.out = nn.Linear(128, self.n_classes, bias=True).to(device)\n",
    "        self._T_in = T_in\n",
    "\n",
    "    def ensure_head(self, T_in: int, device: torch.device):\n",
    "        if (self.fc is None) or (self.out is None) or (self._T_in != T_in):\n",
    "            self._build_head(T_in, device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, _, T, C = x.shape\n",
    "        self.ensure_head(T, x.device)\n",
    "\n",
    "        x = self.chdrop(x)  # ChannelDropout\n",
    "\n",
    "        z = self.conv_temporal(x)\n",
    "        z = self.bn1(z); z = self.act(z)\n",
    "\n",
    "        z = self.conv_depthwise(z)   # (B, F2, T, 1)\n",
    "        z = self.bn2(z); z = self.act(z)\n",
    "        z = self.pool1(z)\n",
    "        z = self.drop1(z)\n",
    "\n",
    "        z = self.conv_sep_depth(z)\n",
    "        z = self.conv_sep_point(z)\n",
    "        z = self.bn3(z); z = self.act(z)\n",
    "        z = self.pool2(z)\n",
    "        z = self.drop2(z)\n",
    "\n",
    "        z = self.flatten(z)\n",
    "        z = self.fc(z); z = self.act(z)\n",
    "        z = self.out(z)\n",
    "        return z\n",
    "\n",
    "# =========================\n",
    "# TORCH DATASET\n",
    "# =========================\n",
    "class EEGTrials(Dataset):\n",
    "    def __init__(self, X, y, groups):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.g = groups.astype(np.int64)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        x = np.expand_dims(x, 0)                 # (1, T, C)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx])\n",
    "\n",
    "CLASS_NAMES_4C = ['Left', 'Right', 'Both Fists', 'Both Feet']\n",
    "\n",
    "# =========================\n",
    "# UTIL: MAX-NORM\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def apply_max_norm(model, max_value=2.0, p=2.0):\n",
    "    layers = []\n",
    "    if hasattr(model, 'conv_depthwise'): layers.append(model.conv_depthwise)\n",
    "    if hasattr(model, 'conv_sep_point'): layers.append(model.conv_sep_point)\n",
    "    if hasattr(model, 'fc'):             layers.append(model.fc)\n",
    "    if hasattr(model, 'out'):            layers.append(model.out)\n",
    "    for layer in layers:\n",
    "        if hasattr(layer, 'weight') and layer.weight is not None:\n",
    "            w = layer.weight.data\n",
    "            norms = w.view(w.size(0), -1).norm(p=p, dim=1, keepdim=True)\n",
    "            desired = torch.clamp(norms, max=max_value)\n",
    "            w.view(w.size(0), -1).mul_(desired / (1e-8 + norms))\n",
    "\n",
    "# =========================\n",
    "# ENTRENAMIENTO / EVALUACI√ìN\n",
    "# =========================\n",
    "def build_weighted_sampler(y, groups):\n",
    "    # pesos inversos por clase y por sujeto ‚Üí promueve balance en cada batch\n",
    "    y = np.asarray(y); groups = np.asarray(groups)\n",
    "    class_counts = np.bincount(y, minlength=len(np.unique(y))).astype(float)\n",
    "    class_w = 1.0 / class_counts[y]\n",
    "    subj_vals, subj_counts = np.unique(groups, return_counts=True)\n",
    "    subj_map = {s:c for s,c in zip(subj_vals, subj_counts)}\n",
    "    subj_w = np.array([1.0/subj_map[g] for g in groups], dtype=float)\n",
    "    w = class_w * subj_w\n",
    "    w = w / w.mean()\n",
    "    w_t = torch.from_numpy(w).float()\n",
    "    sampler = WeightedRandomSampler(weights=w_t, num_samples=len(w_t), replacement=True)\n",
    "    return sampler\n",
    "\n",
    "def make_class_weight_tensor(y_indices, n_classes, boost_bfists=1.20):\n",
    "    # pesos inversos por clase, normalizados; clase 2 (*Both Fists*) con boost\n",
    "    counts = np.bincount(y_indices, minlength=n_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    w = counts.sum() / counts\n",
    "    w = w / w.mean()\n",
    "    w[2] *= boost_bfists\n",
    "    w = w / w.mean()\n",
    "    return torch.tensor(w, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "def train_epoch(model, loader, opt, criterion_soft, n_classes,\n",
    "                do_aug=True, fs=160.0, maxnorm=None):\n",
    "    model.train()\n",
    "    for xb, yb, _ in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "\n",
    "        # ---- AUGMENTS (orden: jitter ‚Üí noise ‚Üí cutout focalizado ‚Üí mixup) ----\n",
    "        if do_aug:\n",
    "            xb = do_time_jitter(xb, max_ms=50, fs=fs)\n",
    "            xb = do_gaussian_noise(xb, sigma=0.01)\n",
    "            xb = do_temporal_cutout_masked(xb, yb, classes_mask={2,3}, min_ms=30, max_ms=60, fs=fs)\n",
    "            xb, yt, _ = mixup_batch(xb, yb, n_classes=n_classes, alpha=0.2)\n",
    "            logits = model(xb)\n",
    "            loss = criterion_soft(logits, yt)\n",
    "        else:\n",
    "            logits = model(xb)\n",
    "            # yb como √≠ndices ‚Üí convertir a one-hot para criterio suave\n",
    "            yt = torch.nn.functional.one_hot(yb, num_classes=n_classes).float()\n",
    "            loss = criterion_soft(logits, yt)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if maxnorm is not None:\n",
    "            apply_max_norm(model, max_value=maxnorm, p=2.0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _predict_tta(model, xb, n=5, fs=160.0):\n",
    "    outs = []\n",
    "    for _ in range(n):\n",
    "        xj = do_time_jitter(xb, max_ms=25, fs=fs)\n",
    "        outs.append(model(xj))\n",
    "    return torch.stack(outs, dim=0).mean(dim=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_with_preds(model, loader, use_tta=True, tta_n=5):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for xb, yb, _ in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        if use_tta:\n",
    "            logits = _predict_tta(model, xb, n=tta_n, fs=FS)\n",
    "        else:\n",
    "            logits = model(xb)\n",
    "        pred = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "        y_pred.extend(pred)\n",
    "        y_true.extend(yb.numpy().tolist())\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    return y_true, y_pred, float(acc)\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes, title, fname):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.nan_to_num(cm_norm)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm_norm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "        plt.text(j, i, format(cm_norm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm_norm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label'); plt.xlabel('Predicted label')\n",
    "    plt.tight_layout(); plt.savefig(fname, dpi=150, bbox_inches='tight'); plt.close()\n",
    "\n",
    "def plot_training_curves(history, fname):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(history['train_acc'], label='train_acc')\n",
    "    plt.plot(history['val_acc'], label='val_acc')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
    "    plt.title('Training curve'); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(fname, dpi=150); plt.close()\n",
    "\n",
    "def print_report(y_true, y_pred, classes):\n",
    "    print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
    "\n",
    "# =========================\n",
    "# FINE-TUNING PROGRESIVO por sujeto (con validaci√≥n interna)\n",
    "# =========================\n",
    "def _param_groups(model, mode):\n",
    "    if mode == 'out':\n",
    "        train = list(model.out.parameters())\n",
    "    elif mode == 'head':\n",
    "        train = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "    elif mode == 'spatial+head':\n",
    "        train = (list(model.conv_depthwise.parameters()) +\n",
    "                 list(model.bn2.parameters()) +\n",
    "                 list(model.conv_sep_depth.parameters()) +\n",
    "                 list(model.conv_sep_point.parameters()) +\n",
    "                 list(model.bn3.parameters()) +\n",
    "                 list(model.fc.parameters()) +\n",
    "                 list(model.out.parameters()))\n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "    return train\n",
    "\n",
    "def _freeze_for_mode(model, mode):\n",
    "    for p in model.parameters(): p.requires_grad = False\n",
    "    if mode == 'out':\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'head':\n",
    "        for p in model.fc.parameters():  p.requires_grad = True\n",
    "        for p in model.out.parameters(): p.requires_grad = True\n",
    "    elif mode == 'spatial+head':\n",
    "        for p in model.conv_depthwise.parameters(): p.requires_grad = True\n",
    "        for p in model.bn2.parameters():           p.requires_grad = True\n",
    "        for p in model.conv_sep_depth.parameters():p.requires_grad = True\n",
    "        for p in model.conv_sep_point.parameters():p.requires_grad = True\n",
    "        for p in model.bn3.parameters():           p.requires_grad = True\n",
    "        for p in model.fc.parameters():            p.requires_grad = True\n",
    "        for p in model.out.parameters():           p.requires_grad = True\n",
    "\n",
    "def _class_weights(y_np, n_classes):\n",
    "    counts = np.bincount(y_np, minlength=n_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    weights = counts.sum() / counts\n",
    "    weights = weights / weights.mean()\n",
    "    return torch.tensor(weights, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "def _train_one_mode(model, X_cal, y_cal, n_classes, mode,\n",
    "                    epochs=FT_EPOCHS, batch_size=32,\n",
    "                    head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                    l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=RANDOM_STATE)\n",
    "    (tr_idx, va_idx), = sss.split(X_cal, y_cal)\n",
    "    Xtr, ytr = X_cal[tr_idx], y_cal[tr_idx]\n",
    "    Xva, yva = X_cal[va_idx], y_cal[va_idx]\n",
    "\n",
    "    ds_tr = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xtr).float().unsqueeze(1),\n",
    "        torch.from_numpy(ytr).long()\n",
    "    )\n",
    "    ds_va = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xva).float().unsqueeze(1),\n",
    "        torch.from_numpy(yva).long()\n",
    "    )\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    _freeze_for_mode(model, mode)\n",
    "\n",
    "    if mode == 'spatial+head':\n",
    "        base_params = (list(model.conv_depthwise.parameters()) +\n",
    "                       list(model.bn2.parameters()) +\n",
    "                       list(model.conv_sep_depth.parameters()) +\n",
    "                       list(model.conv_sep_point.parameters()) +\n",
    "                       list(model.bn3.parameters()))\n",
    "        head_params = list(model.fc.parameters()) + list(model.out.parameters())\n",
    "        train_params = base_params + head_params\n",
    "        opt = optim.Adam([\n",
    "            {\"params\": base_params, \"lr\": base_lr},\n",
    "            {\"params\": head_params, \"lr\": head_lr},\n",
    "        ])\n",
    "    else:\n",
    "        train_params = _param_groups(model, mode)\n",
    "        opt = optim.Adam(train_params, lr=head_lr)\n",
    "\n",
    "    ref = [p.detach().clone().to(p.device) for p in train_params]\n",
    "    class_w = _class_weights(ytr, n_classes)\n",
    "    crit = nn.CrossEntropyLoss(weight=class_w)\n",
    "\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_val = float('inf'); bad = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            reg = 0.0\n",
    "            for p_cur, p_ref in zip(train_params, ref):\n",
    "                reg = reg + torch.sum((p_cur - p_ref)**2)\n",
    "            loss = loss + l2sp_lambda * reg\n",
    "            loss.backward(); opt.step()\n",
    "            apply_max_norm(model, max_value=2.0, p=2.0)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0; nval = 0\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                loss = crit(logits, yb)\n",
    "                val_loss += loss.item() * xb.size(0); nval += xb.size(0)\n",
    "            val_loss /= max(1, nval)\n",
    "\n",
    "        if val_loss + 1e-7 < best_val:\n",
    "            best_val = val_loss; bad = 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience: break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_numpy(model, X_np, device):\n",
    "    model.eval()\n",
    "    xb = torch.from_numpy(X_np).float().unsqueeze(1).to(device)\n",
    "    logits = model(xb)\n",
    "    return logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "def subject_cv_finetune_predict_progressive(model_global, Xs, ys, device,\n",
    "                                            n_splits=CALIB_CV_FOLDS, n_classes=4):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    y_true_full = np.empty_like(ys); y_pred_full = np.empty_like(ys)\n",
    "\n",
    "    for tr_idx, te_idx in skf.split(Xs, ys):\n",
    "        Xcal, ycal = Xs[tr_idx], ys[tr_idx]\n",
    "        Xho,  yho  = Xs[te_idx], ys[te_idx]\n",
    "\n",
    "        m_out = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_out, Xcal, ycal, n_classes, mode='out',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_out = predict_numpy(m_out, Xho, device); acc_out = (yhat_out == yho).mean()\n",
    "\n",
    "        m_head = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_head, Xcal, ycal, n_classes, mode='head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_head = predict_numpy(m_head, Xho, device); acc_head = (yhat_head == yho).mean()\n",
    "\n",
    "        m_sp = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_sp, Xcal, ycal, n_classes, mode='spatial+head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                        l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_sp = predict_numpy(m_sp, Xho, device); acc_sp = (yhat_sp == yho).mean()\n",
    "\n",
    "        best_idx = np.argmax([acc_out, acc_head, acc_sp])\n",
    "        yhat_best = [yhat_out, yhat_head, yhat_sp][best_idx]\n",
    "\n",
    "        y_true_full[te_idx] = yho; y_pred_full[te_idx] = yhat_best\n",
    "\n",
    "    return y_true_full, y_pred_full\n",
    "\n",
    "# =========================\n",
    "# FOLDS JSON helpers\n",
    "# =========================\n",
    "def save_group_folds_json_with_indices(subject_ids_str, groups_array, n_splits, out_json_path,\n",
    "                                       created_by=\"dose_experiment\", description=None):\n",
    "    out_json_path = Path(out_json_path)\n",
    "    unique_subjects_int = sorted(np.unique(groups_array).tolist())\n",
    "    subject_ids = [f\"S{sid:03d}\" for sid in unique_subjects_int]\n",
    "\n",
    "    if len(subject_ids) < n_splits:\n",
    "        raise ValueError(f\"n_splits={n_splits} mayor que n√∫mero de sujetos={len(subject_ids)}\")\n",
    "\n",
    "    groups = np.arange(len(subject_ids))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    folds = []\n",
    "    fold_i = 0\n",
    "    for train_idx_grp, test_idx_grp in gkf.split(groups, groups, groups):\n",
    "        fold_i += 1\n",
    "        train_sids = [subject_ids[int(i)] for i in train_idx_grp]\n",
    "        test_sids  = [subject_ids[int(i)] for i in test_idx_grp]\n",
    "\n",
    "        train_sids_int = [int(s[1:]) for s in train_sids]\n",
    "        test_sids_int  = [int(s[1:]) for s in test_sids]\n",
    "\n",
    "        tr_idx = np.where(np.isin(groups_array, train_sids_int))[0].tolist()\n",
    "        te_idx = np.where(np.isin(groups_array, test_sids_int))[0].tolist()\n",
    "\n",
    "        folds.append({\n",
    "            \"fold\": int(fold_i),\n",
    "            \"train\": train_sids,\n",
    "            \"test\": test_sids,\n",
    "            \"tr_idx\": tr_idx,\n",
    "            \"te_idx\": te_idx\n",
    "        })\n",
    "\n",
    "    payload = {\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"created_by\": created_by,\n",
    "        \"description\": description if description is not None else \"\",\n",
    "        \"n_splits\": int(n_splits),\n",
    "        \"n_subjects\": len(subject_ids),\n",
    "        \"subject_ids\": subject_ids,\n",
    "        \"folds\": folds\n",
    "    }\n",
    "\n",
    "    out_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Folds JSON con √≠ndices guardado ‚Üí {out_json_path}\")\n",
    "    return out_json_path\n",
    "\n",
    "def load_group_folds_json(path_json, expected_subject_ids=None, strict_check=True):\n",
    "    path_json = Path(path_json)\n",
    "    if not path_json.exists():\n",
    "        raise FileNotFoundError(f\"No existe {path_json}\")\n",
    "    with open(path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        payload = json.load(f)\n",
    "\n",
    "    subj_json = payload.get(\"subject_ids\", [])\n",
    "    if expected_subject_ids is not None:\n",
    "        expected = sorted(list(expected_subject_ids))\n",
    "        if subj_json != expected:\n",
    "            msg = (\"Los subject_ids del JSON no coinciden con expected_subject_ids.\\n\"\n",
    "                   f\"JSON has {len(subj_json)} subjects, expected {len(expected)}.\\n\"\n",
    "                   f\"First 10 JSON: {subj_json[:10]}\\nFirst 10 expected: {expected[:10]}\")\n",
    "            if strict_check:\n",
    "                raise ValueError(msg)\n",
    "            else:\n",
    "                print(\"WARNING: \" + msg)\n",
    "    return payload\n",
    "\n",
    "# =========================\n",
    "# EXPERIMENTO\n",
    "# =========================\n",
    "def run_experiment(save_folds_json=True, folds_json_path=FOLDS_DIR, folds_json_description=\"GroupKFold folds for comparison\"):\n",
    "    mne.set_log_level('WARNING')\n",
    "\n",
    "    subs = subjects_available()\n",
    "    print(f\"Sujetos elegibles: {len(subs)} ‚Üí {subs[:10]}{'...' if len(subs)>10 else ''}\")\n",
    "\n",
    "    X, y, groups, chs = build_dataset_all(subs, scenario=CLASS_SCENARIO, window_mode=WINDOW_MODE)\n",
    "    N, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Listo para entrenar: N={N} | T={T} | C={C} | clases={n_classes} | sujetos={len(np.unique(groups))}\")\n",
    "\n",
    "    ds = EEGTrials(X, y, groups)\n",
    "\n",
    "    # preparar JSON folds\n",
    "    if folds_json_path is None:\n",
    "        folds_json_path = Path(\"folds\") / f\"group_folds_{N_FOLDS}splits.json\"\n",
    "    else:\n",
    "        folds_json_path = Path(folds_json_path)\n",
    "    folds_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    unique_subs = sorted(np.unique(groups).tolist())\n",
    "    subject_ids_str = [f\"S{s:03d}\" for s in unique_subs]\n",
    "\n",
    "    if not folds_json_path.exists():\n",
    "        if not save_folds_json:\n",
    "            raise FileNotFoundError(f\"Folds JSON no encontrado en {folds_json_path} y save_folds_json=False.\")\n",
    "        save_group_folds_json_with_indices(subject_ids_str, groups, n_splits=N_FOLDS,\n",
    "                                           out_json_path=folds_json_path,\n",
    "                                           created_by=\"Joel_Clasificador\",\n",
    "                                           description=folds_json_description)\n",
    "\n",
    "    payload = load_group_folds_json(folds_json_path, expected_subject_ids=subject_ids_str, strict_check=False)\n",
    "    folds = payload[\"folds\"]\n",
    "\n",
    "    # bucle por folds\n",
    "    global_folds = []\n",
    "    ft_prog_folds = []\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "\n",
    "    for f in folds:\n",
    "        fold = f[\"fold\"]\n",
    "        tr_idx = np.asarray(f.get(\"tr_idx\", []), dtype=int)\n",
    "        te_idx = np.asarray(f.get(\"te_idx\", []), dtype=int)\n",
    "\n",
    "        if tr_idx.size == 0 or te_idx.size == 0:\n",
    "            print(f\"Advertencia: fold {fold} sin √≠ndices tr/te v√°lidos. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        # ===== Split de validaci√≥n por sujetos =====\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=GLOBAL_VAL_SPLIT, random_state=RANDOM_STATE)\n",
    "        tr_subj_idx, va_subj_idx = next(gss.split(tr_idx, groups[tr_idx], groups[tr_idx]))\n",
    "        tr_sub_idx = tr_idx[tr_subj_idx]\n",
    "        va_idx     = tr_idx[va_subj_idx]\n",
    "\n",
    "        # Sampler balanceado para train\n",
    "        sampler = build_weighted_sampler(y[tr_sub_idx], groups[tr_sub_idx])\n",
    "\n",
    "        tr_loader = DataLoader(Subset(ds, tr_sub_idx), batch_size=BATCH_SIZE, sampler=sampler, drop_last=False)\n",
    "        va_loader = DataLoader(Subset(ds, va_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "        te_loader = DataLoader(Subset(ds, te_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "        # ===== EEGNet =====\n",
    "        model = EEGNet(n_ch=C, n_classes=n_classes,\n",
    "                       F1=24, D=2, kernel_t=64, k_sep=16,\n",
    "                       pool1_t=4, pool2_t=6, drop1_p=0.35, drop2_p=0.6,\n",
    "                       chdrop_p=0.1).to(DEVICE)\n",
    "\n",
    "        # Opt y scheduler SGDR\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_INIT)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=SGDR_T0, T_mult=SGDR_Tmult)\n",
    "\n",
    "        # M√©trica r√°pida\n",
    "        def _acc(loader):\n",
    "            return evaluate_with_preds(model, loader, use_tta=True, tta_n=5)[2]\n",
    "\n",
    "        # Historia para curvas\n",
    "        history = {'train_acc': [], 'val_acc': []}\n",
    "\n",
    "        # ---- Criterio suave con ponderaci√≥n por clase (+20% BFISTS) ----\n",
    "        class_weights = make_class_weight_tensor(y[tr_sub_idx], n_classes, boost_bfists=1.20)\n",
    "        criterion_soft = WeightedSoftCrossEntropy(class_weights, label_smoothing=0.05)\n",
    "\n",
    "        # ===== Entrenamiento global =====\n",
    "        print(f\"\\n[Fold {fold}/{N_FOLDS}] Entrenando modelo global...\"\n",
    "              f\" (n_train={len(tr_sub_idx)} | n_val={len(va_idx)} | n_test={len(te_idx)})\")\n",
    "\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        best_val = -1.0\n",
    "        bad = 0\n",
    "\n",
    "        for epoch in range(1, EPOCHS_GLOBAL + 1):\n",
    "            train_epoch(model, tr_loader, opt, criterion_soft, n_classes=n_classes,\n",
    "                        do_aug=True, fs=FS, maxnorm=2.0)\n",
    "            scheduler.step(epoch-1 + 1e-8)  # tick suave\n",
    "\n",
    "            # eval\n",
    "            tr_acc = _acc(tr_loader)\n",
    "            va_acc = _acc(va_loader)\n",
    "            history['train_acc'].append(tr_acc)\n",
    "            history['val_acc'].append(va_acc)\n",
    "\n",
    "            if (epoch % LOG_EVERY == 0) or epoch in (1, 10, 20, 50, 100):\n",
    "                cur_lr = opt.param_groups[0]['lr']\n",
    "                print(f\"  √âpoca {epoch:3d} | train_acc={tr_acc:.4f} | val_acc={va_acc:.4f} | LR={cur_lr:.5f}\")\n",
    "\n",
    "            if va_acc > best_val + 1e-4:\n",
    "                best_val = va_acc\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "                bad = 0\n",
    "            else:\n",
    "                bad += 1\n",
    "                if bad >= GLOBAL_PATIENCE:\n",
    "                    print(f\"  Early stopping en √©poca {epoch} (mejor val_acc={best_val:.4f})\")\n",
    "                    break\n",
    "\n",
    "        # guardar curva de entrenamiento\n",
    "        curve_path = f\"training_curve_fold{fold}.png\"\n",
    "        plot_training_curves(history, curve_path)\n",
    "        print(f\"‚Ü≥ Curva de entrenamiento guardada: {curve_path}\")\n",
    "\n",
    "        # cargar mejor estado antes de evaluar en test\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "        # ===== Evaluaci√≥n global (inter-sujeto puro) =====\n",
    "        y_true, y_pred, acc_global = evaluate_with_preds(model, te_loader, use_tta=True, tta_n=5)\n",
    "        global_folds.append(acc_global)\n",
    "        all_true.append(y_true); all_pred.append(y_pred)\n",
    "\n",
    "        print(f\"[Fold {fold}/{N_FOLDS}] Global acc={acc_global:.4f}\")\n",
    "        print_report(y_true, y_pred, CLASS_NAMES_4C)\n",
    "\n",
    "        # ---------- Fine-tuning PROGRESIVO por sujeto con 4-fold CV ----------\n",
    "        X_te, y_te, g_te = X[te_idx], y[te_idx], groups[te_idx]\n",
    "\n",
    "        y_true_ft_all, y_pred_ft_all = [], []\n",
    "        used_subjects = 0\n",
    "        for sid in np.unique(g_te):\n",
    "            idx = np.where(g_te == sid)[0]\n",
    "            Xs, ys = X_te[idx], y_te[idx]\n",
    "\n",
    "            if len(ys) < CALIB_CV_FOLDS or len(np.unique(ys)) < 2:\n",
    "                continue\n",
    "\n",
    "            y_true_subj, y_pred_subj = subject_cv_finetune_predict_progressive(\n",
    "                model, Xs, ys, DEVICE, n_splits=CALIB_CV_FOLDS, n_classes=n_classes\n",
    "            )\n",
    "            y_true_ft_all.append(y_true_subj)\n",
    "            y_pred_ft_all.append(y_pred_subj)\n",
    "            used_subjects += 1\n",
    "\n",
    "        if len(y_true_ft_all) > 0:\n",
    "            y_true_ft_all = np.concatenate(y_true_ft_all)\n",
    "            y_pred_ft_all = np.concatenate(y_pred_ft_all)\n",
    "            acc_ft = (y_true_ft_all == y_pred_ft_all).mean()\n",
    "            print(f\"  Fine-tuning PROGRESIVO (por sujeto, {CALIB_CV_FOLDS}-fold CV) acc={acc_ft:.4f} | sujetos={used_subjects}\")\n",
    "            print(f\"  Œî(FT-Global) = {acc_ft - acc_global:+.4f}\")\n",
    "        else:\n",
    "            acc_ft = np.nan\n",
    "            print(\"  Fine-tuning PROGRESIVO no ejecutado (sujeto(s) con muestras insuficientes).\")\n",
    "\n",
    "        ft_prog_folds.append(acc_ft)\n",
    "\n",
    "    # ---------- resultados finales ----------\n",
    "    if len(all_true) > 0:\n",
    "        all_true = np.concatenate(all_true)\n",
    "        all_pred = np.concatenate(all_pred)\n",
    "    else:\n",
    "        all_true = np.array([], dtype=int)\n",
    "        all_pred = np.array([], dtype=int)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTADOS FINALES\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Global folds:\", [f\"{a:.4f}\" for a in global_folds])\n",
    "    if len(global_folds) > 0:\n",
    "        print(f\"Global mean: {np.mean(global_folds):.4f}\")\n",
    "\n",
    "    print(\"Fine-tune PROGRESIVO folds:\", [(\"nan\" if (a is None or np.isnan(a)) else f\"{a:.4f}\") for a in ft_prog_folds])\n",
    "    if len(ft_prog_folds) > 0:\n",
    "        print(f\"Fine-tune PROGRESIVO mean: {np.nanmean(ft_prog_folds):.4f}\")\n",
    "        print(f\"Œî(FT-Global) mean: {np.nanmean(ft_prog_folds) - np.mean(global_folds):+.4f}\")\n",
    "\n",
    "    if all_true.size > 0:\n",
    "        plot_confusion(all_true, all_pred, CLASS_NAMES_4C,\n",
    "                       title=\"Confusion Matrix - Global Model (All Folds)\",\n",
    "                       fname=\"confusion_global_allfolds.png\")\n",
    "        print(\"\\n‚Ü≥ Matriz de confusi√≥n guardada: confusion_global_allfolds.png\")\n",
    "\n",
    "    return {\n",
    "        \"global_folds\": global_folds,\n",
    "        \"ft_prog_folds\": ft_prog_folds,\n",
    "        \"all_true\": all_true,\n",
    "        \"all_pred\": all_pred,\n",
    "        \"folds_json_path\": str(folds_json_path)\n",
    "    }\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üß† INICIANDO EXPERIMENTO CON EEGNet + FINE-TUNING PROGRESIVO (augments + SGDR + tweaks)\")\n",
    "    print(f\"üîß Configuraci√≥n: {CLASS_SCENARIO}, {len(EXPECTED_8)} canales, {WINDOW_MODE}\")\n",
    "    print(f\"‚öôÔ∏è  FT: epochs={FT_EPOCHS}, base_lr={FT_BASE_LR}, head_lr={FT_HEAD_LR}, L2SP={FT_L2SP}, patience={FT_PATIENCE}, CV={CALIB_CV_FOLDS}\")\n",
    "    run_experiment()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
