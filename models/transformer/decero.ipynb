{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92e28a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold1: 100%|██████████| 82/82 [00:12<00:00,  6.56it/s]\n",
      "Cargando test fold1: 100%|██████████| 21/21 [00:03<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train counts: {'left': np.int64(1767), 'right': np.int64(1748), 'both_fists': np.int64(1752), 'both_feet': np.int64(1761)} | total = 7028\n",
      "test  counts: {'left': np.int64(448), 'right': np.int64(446), 'both_fists': np.int64(442), 'both_feet': np.int64(450)} | total = 1786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=1.3710 | val_acc=0.3533 | val_f1m=0.3243\n",
      "Epoch 002 | train_loss=1.2879 | val_acc=0.3897 | val_f1m=0.3910\n",
      "Epoch 003 | train_loss=1.2595 | val_acc=0.3763 | val_f1m=0.3440\n",
      "Epoch 004 | train_loss=1.2336 | val_acc=0.3757 | val_f1m=0.3722\n",
      "Epoch 005 | train_loss=1.2297 | val_acc=0.4149 | val_f1m=0.4140\n",
      "Epoch 006 | train_loss=1.2192 | val_acc=0.4009 | val_f1m=0.3996\n",
      "Epoch 007 | train_loss=1.2103 | val_acc=0.3992 | val_f1m=0.3996\n",
      "Epoch 008 | train_loss=1.2072 | val_acc=0.3852 | val_f1m=0.3780\n",
      "Epoch 009 | train_loss=1.1913 | val_acc=0.4065 | val_f1m=0.4018\n",
      "Epoch 010 | train_loss=1.1832 | val_acc=0.3852 | val_f1m=0.3849\n",
      "Epoch 011 | train_loss=1.1808 | val_acc=0.4071 | val_f1m=0.4083\n",
      "Epoch 012 | train_loss=1.1699 | val_acc=0.3852 | val_f1m=0.3858\n",
      "Epoch 013 | train_loss=1.1525 | val_acc=0.3824 | val_f1m=0.3767\n",
      "Epoch 014 | train_loss=1.1547 | val_acc=0.3886 | val_f1m=0.3849\n",
      "Epoch 015 | train_loss=1.1468 | val_acc=0.3757 | val_f1m=0.3682\n",
      "Epoch 016 | train_loss=1.1294 | val_acc=0.3656 | val_f1m=0.3653\n",
      "Epoch 017 | train_loss=1.1209 | val_acc=0.3897 | val_f1m=0.3784\n",
      "Epoch 018 | train_loss=1.1043 | val_acc=0.3925 | val_f1m=0.3907\n",
      "Epoch 019 | train_loss=1.1010 | val_acc=0.3746 | val_f1m=0.3723\n",
      "Epoch 020 | train_loss=1.0854 | val_acc=0.3774 | val_f1m=0.3718\n",
      "Epoch 021 | train_loss=1.0697 | val_acc=0.3875 | val_f1m=0.3852\n",
      "Epoch 022 | train_loss=1.0668 | val_acc=0.3830 | val_f1m=0.3763\n",
      "Epoch 023 | train_loss=1.0444 | val_acc=0.3768 | val_f1m=0.3786\n",
      "Epoch 024 | train_loss=1.0242 | val_acc=0.3858 | val_f1m=0.3828\n",
      "Epoch 025 | train_loss=1.0214 | val_acc=0.3774 | val_f1m=0.3753\n",
      "Epoch 026 | train_loss=1.0026 | val_acc=0.3819 | val_f1m=0.3824\n",
      "Epoch 027 | train_loss=0.9900 | val_acc=0.3785 | val_f1m=0.3726\n",
      "Epoch 028 | train_loss=0.9699 | val_acc=0.3634 | val_f1m=0.3623\n",
      "Epoch 029 | train_loss=0.9518 | val_acc=0.3707 | val_f1m=0.3688\n",
      "Epoch 030 | train_loss=0.9370 | val_acc=0.3701 | val_f1m=0.3621\n",
      "Epoch 031 | train_loss=0.9262 | val_acc=0.3639 | val_f1m=0.3618\n",
      "Epoch 032 | train_loss=0.9049 | val_acc=0.3544 | val_f1m=0.3541\n",
      "Epoch 033 | train_loss=0.8885 | val_acc=0.3645 | val_f1m=0.3638\n",
      "Epoch 034 | train_loss=0.8726 | val_acc=0.3611 | val_f1m=0.3541\n",
      "Epoch 035 | train_loss=0.8716 | val_acc=0.3628 | val_f1m=0.3608\n",
      "Epoch 036 | train_loss=0.8384 | val_acc=0.3595 | val_f1m=0.3593\n",
      "Epoch 037 | train_loss=0.8203 | val_acc=0.3639 | val_f1m=0.3617\n",
      "Epoch 038 | train_loss=0.8112 | val_acc=0.3578 | val_f1m=0.3569\n",
      "Epoch 039 | train_loss=0.8010 | val_acc=0.3477 | val_f1m=0.3477\n",
      "Epoch 040 | train_loss=0.7830 | val_acc=0.3589 | val_f1m=0.3555\n",
      "\n",
      "=== RESULTADOS (fold 1) ===\n",
      "Acc: 0.4149 | Macro-F1: 0.4140\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[187  61  99 101]\n",
      " [ 52 190  60 144]\n",
      " [ 80  89 146 127]\n",
      " [ 79  86  67 218]]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# CNN + Transformer (PyTorch) para PhysioNet/BCI2000 MI — IMAGINERÍA (4 clases) con 8 canales.\n",
    "# Incluye reproducibilidad (seed=42) y determinismo CUDA para cuBLAS/cuDNN en notebook.\n",
    "\n",
    "# =========================\n",
    "# Reproducibilidad (poner ANTES de importar torch)\n",
    "# =========================\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'  # determinismo cuBLAS\n",
    "\n",
    "import re, json, random\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import mne\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# =========================\n",
    "# REPRODUCIBILIDAD (seed + determinismo)\n",
    "# =========================\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # determinismo cuDNN\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Evitar TF32 (puede romper determinismo en algunas GPU)\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "    # reforzar determinismo cuando sea posible\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def seed_worker(worker_id: int):\n",
    "    worker_seed = seed_everything.__defaults__[0] + worker_id if seed_everything.__defaults__ else 42 + worker_id\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "seed_everything(RANDOM_STATE)\n",
    "\n",
    "# =========================\n",
    "# CONFIG (edita a tu gusto)\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'                     # .../S###/S###R##.edf\n",
    "FOLDS_JSON = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "\n",
    "FOLD = 1\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "RESAMPLE_HZ = None              # None para mantener original\n",
    "DO_NOTCH = True                 # 60 Hz\n",
    "DO_BANDPASS = False             # band-pass (4–38) desactivado por defecto\n",
    "BP_LO, BP_HI = 4.0, 38.0\n",
    "DO_CAR = False                   # re-referencia promedio sobre 8 canales\n",
    "\n",
    "# Ventana temporal (seg)\n",
    "TMIN, TMAX = 0.5, 4.5\n",
    "\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','FCz']\n",
    "CLASS_NAMES = ['left', 'right', 'both_fists', 'both_feet']\n",
    "\n",
    "# Runs IMAGINERÍA\n",
    "IMAGERY_RUNS_LR = {4, 8, 12}    # T1=left, T2=right\n",
    "IMAGERY_RUNS_BF = {6, 10, 14}   # T1=both_fists, T2=both_feet\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Device: {DEVICE}\")\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES\n",
    "# =========================\n",
    "def normalize_ch_name(name: str) -> str:\n",
    "    \"\"\"Normaliza: quita no-alfanum, pasa a upper. 'Fc3.' -> 'FC3', 'Cz..' -> 'CZ'.\"\"\"\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', name)\n",
    "    return s.upper()\n",
    "\n",
    "NORMALIZED_TARGETS = [normalize_ch_name(c) for c in EXPECTED_8]\n",
    "\n",
    "def pick_8_channels(raw: mne.io.BaseRaw) -> mne.io.BaseRaw:\n",
    "    \"\"\"Mapea canales del EDF a EXPECTED_8 aunque tengan puntos/minúsculas.\"\"\"\n",
    "    chs = raw.info['ch_names']\n",
    "    norm_map = {normalize_ch_name(ch): ch for ch in chs}\n",
    "    picked = []\n",
    "    for target_norm, target_orig in zip(NORMALIZED_TARGETS, EXPECTED_8):\n",
    "        if target_norm in norm_map:\n",
    "            picked.append(norm_map[target_norm])\n",
    "        else:\n",
    "            raise RuntimeError(f\"Canal requerido '{target_orig}' no encontrado. Disponibles: {chs}\")\n",
    "    return raw.pick(picks=picked)\n",
    "\n",
    "def list_subject_imagery_edfs(subject_id: str) -> list:\n",
    "    \"\"\"Devuelve lista de EDFs IMAGERY para un sujeto S###: R04,R06,R08,R10,R12,R14.\"\"\"\n",
    "    subj_dir = DATA_RAW / subject_id\n",
    "    edfs = []\n",
    "    for r in [4, 6, 8, 10, 12, 14]:\n",
    "        pattern = str(subj_dir / f\"{subject_id}R{r:02d}.edf\")\n",
    "        edfs.extend(glob(pattern))\n",
    "    return sorted(edfs)\n",
    "\n",
    "def load_subject_epochs(subject_id: str, resample_hz: int, do_notch: bool, do_bandpass: bool,\n",
    "                        do_car: bool, bp_lo: float, bp_hi: float):\n",
    "    \"\"\"Carga EDFs de imaginería, aplica preproc y devuelve (X[N,8,T], y[N], sfreq).\"\"\"\n",
    "    edfs = list_subject_imagery_edfs(subject_id)\n",
    "    if len(edfs) == 0:\n",
    "        raise FileNotFoundError(f\"No imagery EDF files for {subject_id} under {DATA_RAW}\")\n",
    "\n",
    "    X_list, y_list, sfreq_list = [], [], []\n",
    "\n",
    "    for edf_path in edfs:\n",
    "        m = re.search(r\"R(\\d{2})\", Path(edf_path).name)\n",
    "        run = int(m.group(1)) if m else -1\n",
    "\n",
    "        raw = mne.io.read_raw_edf(edf_path, preload=True, verbose='ERROR')\n",
    "\n",
    "        # Selección de 8 canales\n",
    "        raw = pick_8_channels(raw)\n",
    "\n",
    "        # --- PREPROC OPCIONAL ---\n",
    "        if do_notch:\n",
    "            raw.notch_filter(freqs=[60.0], picks='all', verbose='ERROR')\n",
    "        if do_bandpass:\n",
    "            raw.filter(l_freq=bp_lo, h_freq=bp_hi, picks='all', verbose='ERROR')\n",
    "        if do_car:\n",
    "            raw.set_eeg_reference('average', projection=False, verbose='ERROR')\n",
    "\n",
    "        # Resample\n",
    "        if resample_hz is not None and resample_hz > 0:\n",
    "            raw.resample(resample_hz)\n",
    "        sfreq = raw.info['sfreq']\n",
    "\n",
    "        # Eventos (T0/T1/T2)\n",
    "        events, event_id = mne.events_from_annotations(raw, verbose='ERROR')\n",
    "\n",
    "        # Mantener solo T1/T2 (T0 ignorado)\n",
    "        keep = {k: v for k, v in event_id.items() if k in {'T1', 'T2'}}\n",
    "        if len(keep) == 0:\n",
    "            continue\n",
    "\n",
    "        epochs = mne.Epochs(raw, events=events, event_id=keep, tmin=TMIN, tmax=TMAX,\n",
    "                            baseline=None, preload=True, verbose='ERROR')\n",
    "        X = epochs.get_data()  # (n_epochs, 8, T)\n",
    "\n",
    "        # Construir y según RUN\n",
    "        ev_codes = epochs.events[:, 2]\n",
    "        inv = {v: k for k, v in keep.items()}  # id -> 'T1'/'T2'\n",
    "        y_run = []\n",
    "        for code in ev_codes:\n",
    "            lab = inv[code]\n",
    "            if run in IMAGERY_RUNS_LR:\n",
    "                y_run.append(0 if lab == 'T1' else 1)  # left/right\n",
    "            elif run in IMAGERY_RUNS_BF:\n",
    "                y_run.append(2 if lab == 'T1' else 3)  # both_fists/feet\n",
    "            else:\n",
    "                y_run.append(-1)\n",
    "        y_run = np.array(y_run, dtype=int)\n",
    "        keep_mask = y_run >= 0\n",
    "        X = X[keep_mask]\n",
    "        y = y_run[keep_mask]\n",
    "\n",
    "        if len(y) == 0:\n",
    "            continue\n",
    "\n",
    "        X_list.append(X)\n",
    "        y_list.append(y)\n",
    "        sfreq_list.append(sfreq)\n",
    "\n",
    "    if len(X_list) == 0:\n",
    "        return np.empty((0, 8, 1)), np.empty((0,), dtype=int), None\n",
    "\n",
    "    X_all = np.concatenate(X_list, axis=0)\n",
    "    y_all = np.concatenate(y_list, axis=0)\n",
    "\n",
    "    if len(set([round(s) for s in sfreq_list])) != 1:\n",
    "        raise RuntimeError(f\"Inconsistent sampling rates: {sfreq_list}\")\n",
    "\n",
    "    return X_all, y_all, sfreq_list[0]\n",
    "\n",
    "def load_fold_subjects(folds_json: Path, fold: int):\n",
    "    with open(folds_json, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    for item in data.get('folds', []):\n",
    "        if int(item.get('fold', -1)) == int(fold):\n",
    "            train_sub = list(item.get('train', []))\n",
    "            test_sub  = list(item.get('test', []))\n",
    "            return train_sub, test_sub\n",
    "    raise ValueError(f\"Fold {fold} not found in {folds_json}\")\n",
    "\n",
    "def subject_id_to_int(s: str) -> int:\n",
    "    m = re.match(r'[Ss](\\d+)', s)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "def class_count_summary(y, name):\n",
    "    bc = np.bincount(y, minlength=4)\n",
    "    print(f\"{name} counts:\", dict(zip(CLASS_NAMES, bc)), \"| total =\", bc.sum())\n",
    "\n",
    "# =========================\n",
    "# MODELO: CNN + Transformer (con dropout)\n",
    "# =========================\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k, s=1, p=0):\n",
    "        super().__init__()\n",
    "        self.dw = nn.Conv1d(in_ch, in_ch, kernel_size=k, stride=s, padding=p, groups=in_ch, bias=False)\n",
    "        self.pw = nn.Conv1d(in_ch, out_ch, kernel_size=1, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(out_ch)\n",
    "        self.act = nn.ELU()\n",
    "    def forward(self, x):\n",
    "        x = self.dw(x); x = self.pw(x); x = self.bn(x)\n",
    "        return self.act(x)\n",
    "\n",
    "class EEGCNNTransformer(nn.Module):\n",
    "    def __init__(self, n_ch=8, n_cls=4, d_model=128, n_heads=4, n_layers=2, p_drop=0.2):\n",
    "        super().__init__()\n",
    "        self.conv_t = nn.Sequential(\n",
    "            nn.Conv1d(n_ch, 32, kernel_size=129, stride=2, padding=64, bias=False),\n",
    "            nn.BatchNorm1d(32), nn.ELU(),\n",
    "            DepthwiseSeparableConv(32, 64, k=31, s=2, p=15),\n",
    "            DepthwiseSeparableConv(64, 128, k=15, s=2, p=7),\n",
    "        )\n",
    "        self.proj = nn.Conv1d(128, d_model, kernel_size=1, bias=False)\n",
    "        self.dropout = nn.Dropout(p=p_drop)\n",
    "        self.pos_encoding = None\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=2*d_model,\n",
    "            batch_first=True, activation='gelu', dropout=0.1, norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.cls = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        nn.init.normal_(self.cls, std=0.02)\n",
    "        self.head = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, n_cls))\n",
    "\n",
    "    def _positional_encoding(self, L, d):\n",
    "        pos = torch.arange(0, L, dtype=torch.float32).unsqueeze(1)\n",
    "        i   = torch.arange(0, d, dtype=torch.float32).unsqueeze(0)\n",
    "        angle = pos / torch.pow(10000, (2 * (i//2)) / d)\n",
    "        pe = torch.zeros(L, d, dtype=torch.float32)\n",
    "        pe[:, 0::2] = torch.sin(angle[:, 0::2])\n",
    "        pe[:, 1::2] = torch.cos(angle[:, 1::2])\n",
    "        return pe\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, T)\n",
    "        z = self.conv_t(x)           # (B, 128, T')\n",
    "        z = self.proj(z)             # (B, d_model, T')\n",
    "        z = self.dropout(z)          # regularización extra\n",
    "        z = z.transpose(1, 2)        # (B, T', d_model)\n",
    "        B, L, D = z.shape\n",
    "        if (self.pos_encoding is None) or (self.pos_encoding.shape[0] != L) or (self.pos_encoding.shape[1] != D):\n",
    "            self.pos_encoding = self._positional_encoding(L, D).to(z.device)\n",
    "        z = z + self.pos_encoding[None, :, :]\n",
    "        cls_tok = self.cls.expand(B, -1, -1)  # (B,1,D)\n",
    "        z = torch.cat([cls_tok, z], dim=1)    # (B, 1+L, D)\n",
    "        z = self.encoder(z)                   # (B, 1+L, D)\n",
    "        cls = z[:, 0, :]\n",
    "        return self.head(cls)\n",
    "\n",
    "# =========================\n",
    "# ENTRENAMIENTO / EVAL\n",
    "# =========================\n",
    "def standardize_per_channel(train_X, test_X):\n",
    "    \"\"\"Estandariza por canal usando SOLO estadísticas del train. Entradas (N,C,T).\"\"\"\n",
    "    C = train_X.shape[1]\n",
    "    train_X = train_X.astype(np.float32)\n",
    "    test_X  = test_X.astype(np.float32)\n",
    "    for c in range(C):\n",
    "        mu = train_X[:, c, :].mean()\n",
    "        sd = train_X[:, c, :].std()\n",
    "        sd = sd if sd > 1e-6 else 1.0\n",
    "        train_X[:, c, :] = (train_X[:, c, :] - mu) / sd\n",
    "        test_X[:, c, :]  = (test_X[:, c, :] - mu) / sd\n",
    "    return train_X, test_X\n",
    "\n",
    "def train_one_fold(fold:int, resample_hz:int, do_notch:bool, do_bandpass:bool,\n",
    "                   bp_lo:float, bp_hi:float, epochs:int, batch_size:int, lr:float,\n",
    "                   device:str=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "\n",
    "    # --- sujetos fold ---\n",
    "    train_sub, test_sub = load_fold_subjects(FOLDS_JSON, fold)\n",
    "    train_sub = [s for s in train_sub if subject_id_to_int(s) not in EXCLUDE_SUBJECTS]\n",
    "    test_sub  = [s for s in test_sub  if subject_id_to_int(s) not in EXCLUDE_SUBJECTS]\n",
    "\n",
    "    # --- carga datos ---\n",
    "    X_tr_list, y_tr_list, X_te_list, y_te_list = [], [], [], []\n",
    "    sfreq = None\n",
    "\n",
    "    for sid in tqdm(train_sub, desc=f\"Cargando train fold{fold}\"):\n",
    "        Xs, ys, sf = load_subject_epochs(sid, resample_hz, do_notch, do_bandpass, DO_CAR, bp_lo, bp_hi)\n",
    "        if len(ys) == 0: continue\n",
    "        X_tr_list.append(Xs); y_tr_list.append(ys); sfreq = sf if sfreq is None else sfreq\n",
    "\n",
    "    for sid in tqdm(test_sub, desc=f\"Cargando test fold{fold}\"):\n",
    "        Xs, ys, sf = load_subject_epochs(sid, resample_hz, do_notch, do_bandpass, DO_CAR, bp_lo, bp_hi)\n",
    "        if len(ys) == 0: continue\n",
    "        X_te_list.append(Xs); y_te_list.append(ys); sfreq = sf if sfreq is None else sfreq\n",
    "\n",
    "    if len(X_tr_list) == 0 or len(X_te_list) == 0:\n",
    "        raise RuntimeError(\"Datos insuficientes tras carga de sujetos.\")\n",
    "\n",
    "    X_tr = np.concatenate(X_tr_list, axis=0); y_tr = np.concatenate(y_tr_list, axis=0)\n",
    "    X_te = np.concatenate(X_te_list, axis=0); y_te = np.concatenate(y_te_list, axis=0)\n",
    "\n",
    "    # --- diagnósticos de desbalance ---\n",
    "    print(\"train counts:\", dict(zip(CLASS_NAMES, np.bincount(y_tr, minlength=4))), \"| total =\", len(y_tr))\n",
    "    print(\"test  counts:\", dict(zip(CLASS_NAMES, np.bincount(y_te, minlength=4))), \"| total =\", len(y_te))\n",
    "\n",
    "    # --- estandarización sin leakage ---\n",
    "    X_tr, X_te = standardize_per_channel(X_tr, X_te)\n",
    "\n",
    "    # --- DataLoaders (reproducibles) ---\n",
    "    g = torch.Generator(device=\"cpu\"); g.manual_seed(RANDOM_STATE)\n",
    "    tr_ld = DataLoader(\n",
    "        TensorDataset(torch.tensor(X_tr), torch.tensor(y_tr).long()),\n",
    "        batch_size=batch_size, shuffle=True, drop_last=False,\n",
    "        generator=g, worker_init_fn=seed_worker\n",
    "    )\n",
    "    te_ld = DataLoader(\n",
    "        TensorDataset(torch.tensor(X_te), torch.tensor(y_te).long()),\n",
    "        batch_size=batch_size, shuffle=False, drop_last=False,\n",
    "        generator=g, worker_init_fn=seed_worker\n",
    "    )\n",
    "\n",
    "    # --- Modelo, optimizador, pérdida ---\n",
    "    model = EEGCNNTransformer(n_ch=8, n_cls=4, d_model=128, n_heads=4, n_layers=2, p_drop=0.2).to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
    "    crit = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    best_acc, best_state = 0.0, None\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        tr_loss, n_seen = 0.0, 0\n",
    "        for xb, yb in tr_ld:\n",
    "            xb = xb.to(device); yb = yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            tr_loss += loss.item() * len(yb); n_seen += len(yb)\n",
    "        tr_loss /= max(1, n_seen)\n",
    "\n",
    "        # --- Eval ---\n",
    "        model.eval()\n",
    "        preds, gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in te_ld:\n",
    "                xb = xb.to(device)\n",
    "                p = model(xb).argmax(dim=1).cpu().numpy()\n",
    "                preds.append(p); gts.append(yb.numpy())\n",
    "        preds = np.concatenate(preds); gts = np.concatenate(gts)\n",
    "        acc = accuracy_score(gts, preds)\n",
    "        f1m = f1_score(gts, preds, average='macro')\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "        print(f\"Epoch {ep:03d} | train_loss={tr_loss:.4f} | val_acc={acc:.4f} | val_f1m={f1m:.4f}\")\n",
    "\n",
    "    # cargar el mejor\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # --- reporte final ---\n",
    "    model.eval()\n",
    "    preds, gts = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in te_ld:\n",
    "            xb = xb.to(device)\n",
    "            p = model(xb).argmax(dim=1).cpu().numpy()\n",
    "            preds.append(p); gts.append(yb.numpy())\n",
    "    preds = np.concatenate(preds); gts = np.concatenate(gts)\n",
    "\n",
    "    acc = accuracy_score(gts, preds)\n",
    "    f1m = f1_score(gts, preds, average='macro')\n",
    "    cm = confusion_matrix(gts, preds, labels=[0,1,2,3])\n",
    "\n",
    "    print(f\"\\n=== RESULTADOS (fold {fold}) ===\")\n",
    "    print(f\"Acc: {acc:.4f} | Macro-F1: {f1m:.4f}\")\n",
    "    print(\"Confusion matrix (rows=true, cols=pred):\")\n",
    "    print(cm)\n",
    "\n",
    "    return acc, f1m, cm, model\n",
    "\n",
    "# =========================\n",
    "# EJECUCIÓN (Notebook)\n",
    "# =========================\n",
    "acc, f1m, cm, model = train_one_fold(\n",
    "    fold=FOLD,\n",
    "    resample_hz=RESAMPLE_HZ,\n",
    "    do_notch=DO_NOTCH,\n",
    "    do_bandpass=DO_BANDPASS,\n",
    "    bp_lo=BP_LO,\n",
    "    bp_hi=BP_HI,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=LR,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af4a3a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Device: cuda\n",
      "\n",
      "============  FOLD 1  ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold1: 100%|██████████| 82/82 [00:09<00:00,  8.35it/s]\n",
      "Cargando test fold1: 100%|██████████| 21/21 [00:02<00:00,  8.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1/5] Entrenando modelo global... (n_train=6888 | n_test=1764)\n",
      "train counts: {'left': np.int64(1722), 'right': np.int64(1722), 'both_fists': np.int64(1722), 'both_feet': np.int64(1722)} | total = 6888\n",
      "test  counts: {'left': np.int64(441), 'right': np.int64(441), 'both_fists': np.int64(441), 'both_feet': np.int64(441)} | total = 1764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Época   1 | train_loss=1.3483 | val_acc=0.3656 | val_f1m=0.3498 | LR=0.000400\n",
      "  Época   2 | train_loss=1.2134 | val_acc=0.4427 | val_f1m=0.4359 | LR=0.000600\n",
      "  Época   3 | train_loss=1.1752 | val_acc=0.4598 | val_f1m=0.4615 | LR=0.000800\n",
      "  Época   4 | train_loss=1.1564 | val_acc=0.4495 | val_f1m=0.4522 | LR=0.001000\n",
      "  Época   5 | train_loss=1.1524 | val_acc=0.4507 | val_f1m=0.4365 | LR=0.001000\n",
      "  Época   6 | train_loss=1.1156 | val_acc=0.4717 | val_f1m=0.4685 | LR=0.000999\n",
      "  Época   7 | train_loss=1.1040 | val_acc=0.4745 | val_f1m=0.4572 | LR=0.000997\n",
      "  Época   8 | train_loss=1.1060 | val_acc=0.4705 | val_f1m=0.4688 | LR=0.000993\n",
      "  Época   9 | train_loss=1.1008 | val_acc=0.4422 | val_f1m=0.4248 | LR=0.000987\n",
      "  Época  10 | train_loss=1.0739 | val_acc=0.4586 | val_f1m=0.4385 | LR=0.000980\n",
      "  Época  11 | train_loss=1.0749 | val_acc=0.4830 | val_f1m=0.4818 | LR=0.000971\n",
      "  Época  12 | train_loss=1.0540 | val_acc=0.4615 | val_f1m=0.4579 | LR=0.000961\n",
      "  Época  13 | train_loss=1.0441 | val_acc=0.4683 | val_f1m=0.4621 | LR=0.000949\n",
      "  Época  14 | train_loss=1.0480 | val_acc=0.4768 | val_f1m=0.4794 | LR=0.000935\n",
      "  Época  15 | train_loss=1.0312 | val_acc=0.4461 | val_f1m=0.4343 | LR=0.000921\n",
      "  Época  16 | train_loss=1.0293 | val_acc=0.4575 | val_f1m=0.4567 | LR=0.000905\n",
      "  Época  17 | train_loss=1.0206 | val_acc=0.4728 | val_f1m=0.4731 | LR=0.000887\n",
      "  Época  18 | train_loss=1.0037 | val_acc=0.4620 | val_f1m=0.4459 | LR=0.000868\n",
      "  Época  19 | train_loss=1.0302 | val_acc=0.4773 | val_f1m=0.4707 | LR=0.000848\n",
      "  Época  20 | train_loss=1.0074 | val_acc=0.4711 | val_f1m=0.4634 | LR=0.000827\n",
      "  Época  21 | train_loss=1.0002 | val_acc=0.4858 | val_f1m=0.4847 | LR=0.000805\n",
      "  Época  22 | train_loss=0.9862 | val_acc=0.4836 | val_f1m=0.4859 | LR=0.000782\n",
      "  Época  23 | train_loss=0.9685 | val_acc=0.4722 | val_f1m=0.4782 | LR=0.000758\n",
      "  Época  24 | train_loss=0.9495 | val_acc=0.4751 | val_f1m=0.4790 | LR=0.000733\n",
      "  Época  25 | train_loss=0.9420 | val_acc=0.4841 | val_f1m=0.4773 | LR=0.000708\n",
      "  Época  26 | train_loss=0.9169 | val_acc=0.4546 | val_f1m=0.4612 | LR=0.000681\n",
      "  Época  27 | train_loss=0.9106 | val_acc=0.4756 | val_f1m=0.4740 | LR=0.000655\n",
      "  Época  28 | train_loss=0.9101 | val_acc=0.4722 | val_f1m=0.4728 | LR=0.000627\n",
      "  Época  29 | train_loss=0.8790 | val_acc=0.4722 | val_f1m=0.4681 | LR=0.000599\n",
      "  Época  30 | train_loss=0.8730 | val_acc=0.4722 | val_f1m=0.4718 | LR=0.000571\n",
      "  Época  31 | train_loss=0.8520 | val_acc=0.4904 | val_f1m=0.4892 | LR=0.000543\n",
      "  Época  32 | train_loss=0.8413 | val_acc=0.4711 | val_f1m=0.4711 | LR=0.000514\n",
      "  Época  33 | train_loss=0.8103 | val_acc=0.4677 | val_f1m=0.4659 | LR=0.000486\n",
      "  Época  34 | train_loss=0.8311 | val_acc=0.4768 | val_f1m=0.4760 | LR=0.000457\n",
      "  Época  35 | train_loss=0.8171 | val_acc=0.4796 | val_f1m=0.4747 | LR=0.000429\n",
      "  Época  36 | train_loss=0.7769 | val_acc=0.4700 | val_f1m=0.4735 | LR=0.000401\n",
      "  Época  37 | train_loss=0.7640 | val_acc=0.4824 | val_f1m=0.4835 | LR=0.000373\n",
      "  Época  38 | train_loss=0.7476 | val_acc=0.4790 | val_f1m=0.4805 | LR=0.000345\n",
      "  Época  39 | train_loss=0.7439 | val_acc=0.4802 | val_f1m=0.4776 | LR=0.000319\n",
      "  Época  40 | train_loss=0.7100 | val_acc=0.4683 | val_f1m=0.4669 | LR=0.000292\n",
      "  Época  41 | train_loss=0.7323 | val_acc=0.4819 | val_f1m=0.4802 | LR=0.000267\n",
      "  Early stopping en época 41 (mejor val_f1m=0.4892)\n",
      "[Fold 1/5] Global acc=0.4870\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.6192    0.5125    0.5608       441\n",
      "       Right     0.5107    0.5964    0.5502       441\n",
      "  Both Fists     0.3885    0.3515    0.3690       441\n",
      "   Both Feet     0.4433    0.4875    0.4644       441\n",
      "\n",
      "    accuracy                         0.4870      1764\n",
      "   macro avg     0.4904    0.4870    0.4861      1764\n",
      "weighted avg     0.4904    0.4870    0.4861      1764\n",
      "\n",
      "=== RESULTADOS (fold 1) ===\n",
      "Acc: 0.4870 | Macro-F1: 0.4861\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[226  48  97  70]\n",
      " [ 26 263  58  94]\n",
      " [ 73 107 155 106]\n",
      " [ 40  97  89 215]]\n",
      "\n",
      "============  FOLD 2  ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold2: 100%|██████████| 82/82 [00:09<00:00,  8.31it/s]\n",
      "Cargando test fold2: 100%|██████████| 21/21 [00:02<00:00,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 2/5] Entrenando modelo global... (n_train=6888 | n_test=1764)\n",
      "train counts: {'left': np.int64(1722), 'right': np.int64(1722), 'both_fists': np.int64(1722), 'both_feet': np.int64(1722)} | total = 6888\n",
      "test  counts: {'left': np.int64(441), 'right': np.int64(441), 'both_fists': np.int64(441), 'both_feet': np.int64(441)} | total = 1764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Época   1 | train_loss=1.3581 | val_acc=0.4286 | val_f1m=0.4178 | LR=0.000400\n",
      "  Época   2 | train_loss=1.2361 | val_acc=0.4864 | val_f1m=0.4735 | LR=0.000600\n",
      "  Época   3 | train_loss=1.2014 | val_acc=0.5249 | val_f1m=0.5194 | LR=0.000800\n",
      "  Época   4 | train_loss=1.1905 | val_acc=0.5283 | val_f1m=0.5318 | LR=0.001000\n",
      "  Época   5 | train_loss=1.1873 | val_acc=0.5340 | val_f1m=0.5350 | LR=0.001000\n",
      "  Época   6 | train_loss=1.1505 | val_acc=0.5368 | val_f1m=0.5404 | LR=0.000999\n",
      "  Época   7 | train_loss=1.1486 | val_acc=0.5459 | val_f1m=0.5367 | LR=0.000997\n",
      "  Época   8 | train_loss=1.1518 | val_acc=0.5193 | val_f1m=0.4985 | LR=0.000993\n",
      "  Época   9 | train_loss=1.1339 | val_acc=0.5329 | val_f1m=0.5064 | LR=0.000987\n",
      "  Época  10 | train_loss=1.1026 | val_acc=0.5482 | val_f1m=0.5315 | LR=0.000980\n",
      "  Época  11 | train_loss=1.1050 | val_acc=0.5465 | val_f1m=0.5389 | LR=0.000971\n",
      "  Época  12 | train_loss=1.0913 | val_acc=0.5527 | val_f1m=0.5528 | LR=0.000961\n",
      "  Época  13 | train_loss=1.0917 | val_acc=0.5527 | val_f1m=0.5463 | LR=0.000949\n",
      "  Época  14 | train_loss=1.0811 | val_acc=0.5255 | val_f1m=0.5248 | LR=0.000935\n",
      "  Época  15 | train_loss=1.0708 | val_acc=0.5493 | val_f1m=0.5397 | LR=0.000921\n",
      "  Época  16 | train_loss=1.0769 | val_acc=0.5493 | val_f1m=0.5420 | LR=0.000905\n",
      "  Época  17 | train_loss=1.0639 | val_acc=0.5544 | val_f1m=0.5530 | LR=0.000887\n",
      "  Época  18 | train_loss=1.0520 | val_acc=0.5283 | val_f1m=0.5117 | LR=0.000868\n",
      "  Época  19 | train_loss=1.0428 | val_acc=0.5488 | val_f1m=0.5465 | LR=0.000848\n",
      "  Época  20 | train_loss=1.0490 | val_acc=0.5459 | val_f1m=0.5383 | LR=0.000827\n",
      "  Época  21 | train_loss=1.0393 | val_acc=0.5465 | val_f1m=0.5392 | LR=0.000805\n",
      "  Época  22 | train_loss=1.0194 | val_acc=0.5454 | val_f1m=0.5431 | LR=0.000782\n",
      "  Época  23 | train_loss=1.0164 | val_acc=0.5368 | val_f1m=0.5393 | LR=0.000758\n",
      "  Época  24 | train_loss=0.9880 | val_acc=0.5624 | val_f1m=0.5643 | LR=0.000733\n",
      "  Época  25 | train_loss=0.9817 | val_acc=0.5408 | val_f1m=0.5342 | LR=0.000708\n",
      "  Época  26 | train_loss=0.9805 | val_acc=0.5618 | val_f1m=0.5633 | LR=0.000681\n",
      "  Época  27 | train_loss=0.9514 | val_acc=0.5510 | val_f1m=0.5481 | LR=0.000655\n",
      "  Época  28 | train_loss=0.9288 | val_acc=0.5544 | val_f1m=0.5512 | LR=0.000627\n",
      "  Época  29 | train_loss=0.9339 | val_acc=0.5493 | val_f1m=0.5421 | LR=0.000599\n",
      "  Época  30 | train_loss=0.9187 | val_acc=0.5368 | val_f1m=0.5329 | LR=0.000571\n",
      "  Época  31 | train_loss=0.9150 | val_acc=0.5533 | val_f1m=0.5519 | LR=0.000543\n",
      "  Época  32 | train_loss=0.9048 | val_acc=0.5351 | val_f1m=0.5288 | LR=0.000514\n",
      "  Época  33 | train_loss=0.8710 | val_acc=0.5312 | val_f1m=0.5225 | LR=0.000486\n",
      "  Época  34 | train_loss=0.8673 | val_acc=0.5368 | val_f1m=0.5343 | LR=0.000457\n",
      "  Early stopping en época 34 (mejor val_f1m=0.5643)\n",
      "[Fold 2/5] Global acc=0.5703\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.6553    0.6077    0.6306       441\n",
      "       Right     0.6372    0.6054    0.6209       441\n",
      "  Both Fists     0.5096    0.4807    0.4947       441\n",
      "   Both Feet     0.4981    0.5873    0.5390       441\n",
      "\n",
      "    accuracy                         0.5703      1764\n",
      "   macro avg     0.5750    0.5703    0.5713      1764\n",
      "weighted avg     0.5750    0.5703    0.5713      1764\n",
      "\n",
      "=== RESULTADOS (fold 2) ===\n",
      "Acc: 0.5703 | Macro-F1: 0.5713\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[268  29  77  67]\n",
      " [ 26 267  56  92]\n",
      " [ 64  63 212 102]\n",
      " [ 51  60  71 259]]\n",
      "\n",
      "============  FOLD 3  ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold3: 100%|██████████| 82/82 [00:09<00:00,  8.30it/s]\n",
      "Cargando test fold3: 100%|██████████| 21/21 [00:02<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 3/5] Entrenando modelo global... (n_train=6888 | n_test=1764)\n",
      "train counts: {'left': np.int64(1722), 'right': np.int64(1722), 'both_fists': np.int64(1722), 'both_feet': np.int64(1722)} | total = 6888\n",
      "test  counts: {'left': np.int64(441), 'right': np.int64(441), 'both_fists': np.int64(441), 'both_feet': np.int64(441)} | total = 1764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Época   1 | train_loss=1.3863 | val_acc=0.3254 | val_f1m=0.2574 | LR=0.000400\n",
      "  Época   2 | train_loss=1.2550 | val_acc=0.4677 | val_f1m=0.4658 | LR=0.000600\n",
      "  Época   3 | train_loss=1.1853 | val_acc=0.4773 | val_f1m=0.4771 | LR=0.000800\n",
      "  Época   4 | train_loss=1.1739 | val_acc=0.4575 | val_f1m=0.4612 | LR=0.001000\n",
      "  Época   5 | train_loss=1.1491 | val_acc=0.4864 | val_f1m=0.4779 | LR=0.001000\n",
      "  Época   6 | train_loss=1.1223 | val_acc=0.4881 | val_f1m=0.4881 | LR=0.000999\n",
      "  Época   7 | train_loss=1.1163 | val_acc=0.4683 | val_f1m=0.4501 | LR=0.000997\n",
      "  Época   8 | train_loss=1.1067 | val_acc=0.4688 | val_f1m=0.4621 | LR=0.000993\n",
      "  Época   9 | train_loss=1.0928 | val_acc=0.4813 | val_f1m=0.4665 | LR=0.000987\n",
      "  Época  10 | train_loss=1.0775 | val_acc=0.4813 | val_f1m=0.4804 | LR=0.000980\n",
      "  Época  11 | train_loss=1.0863 | val_acc=0.4717 | val_f1m=0.4595 | LR=0.000971\n",
      "  Época  12 | train_loss=1.0623 | val_acc=0.4586 | val_f1m=0.4511 | LR=0.000961\n",
      "  Época  13 | train_loss=1.0616 | val_acc=0.4745 | val_f1m=0.4736 | LR=0.000949\n",
      "  Época  14 | train_loss=1.0496 | val_acc=0.4739 | val_f1m=0.4797 | LR=0.000935\n",
      "  Época  15 | train_loss=1.0341 | val_acc=0.4943 | val_f1m=0.4810 | LR=0.000921\n",
      "  Época  16 | train_loss=1.0296 | val_acc=0.4938 | val_f1m=0.4927 | LR=0.000905\n",
      "  Época  17 | train_loss=1.0066 | val_acc=0.4813 | val_f1m=0.4837 | LR=0.000887\n",
      "  Época  18 | train_loss=1.0041 | val_acc=0.4734 | val_f1m=0.4731 | LR=0.000868\n",
      "  Época  19 | train_loss=1.0041 | val_acc=0.4813 | val_f1m=0.4777 | LR=0.000848\n",
      "  Época  20 | train_loss=1.0015 | val_acc=0.4802 | val_f1m=0.4805 | LR=0.000827\n",
      "  Época  21 | train_loss=0.9813 | val_acc=0.4626 | val_f1m=0.4636 | LR=0.000805\n",
      "  Época  22 | train_loss=0.9738 | val_acc=0.4813 | val_f1m=0.4814 | LR=0.000782\n",
      "  Época  23 | train_loss=0.9753 | val_acc=0.4649 | val_f1m=0.4669 | LR=0.000758\n",
      "  Época  24 | train_loss=0.9465 | val_acc=0.4836 | val_f1m=0.4857 | LR=0.000733\n",
      "  Época  25 | train_loss=0.9373 | val_acc=0.4773 | val_f1m=0.4714 | LR=0.000708\n",
      "  Época  26 | train_loss=0.9199 | val_acc=0.4626 | val_f1m=0.4670 | LR=0.000681\n",
      "  Early stopping en época 26 (mejor val_f1m=0.4927)\n",
      "[Fold 3/5] Global acc=0.4881\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5195    0.6054    0.5592       441\n",
      "       Right     0.5933    0.4830    0.5325       441\n",
      "  Both Fists     0.4202    0.4240    0.4221       441\n",
      "   Both Feet     0.4350    0.4399    0.4374       441\n",
      "\n",
      "    accuracy                         0.4881      1764\n",
      "   macro avg     0.4920    0.4881    0.4878      1764\n",
      "weighted avg     0.4920    0.4881    0.4878      1764\n",
      "\n",
      "=== RESULTADOS (fold 3) ===\n",
      "Acc: 0.4881 | Macro-F1: 0.4878\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[267  31  74  69]\n",
      " [ 44 213  92  92]\n",
      " [116  47 187  91]\n",
      " [ 87  68  92 194]]\n",
      "\n",
      "============  FOLD 4  ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold4: 100%|██████████| 83/83 [00:10<00:00,  8.27it/s]\n",
      "Cargando test fold4: 100%|██████████| 20/20 [00:02<00:00,  8.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 4/5] Entrenando modelo global... (n_train=6972 | n_test=1680)\n",
      "train counts: {'left': np.int64(1743), 'right': np.int64(1743), 'both_fists': np.int64(1743), 'both_feet': np.int64(1743)} | total = 6972\n",
      "test  counts: {'left': np.int64(420), 'right': np.int64(420), 'both_fists': np.int64(420), 'both_feet': np.int64(420)} | total = 1680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Época   1 | train_loss=1.3840 | val_acc=0.4030 | val_f1m=0.4010 | LR=0.000400\n",
      "  Época   2 | train_loss=1.2280 | val_acc=0.4929 | val_f1m=0.4864 | LR=0.000600\n",
      "  Época   3 | train_loss=1.1767 | val_acc=0.4726 | val_f1m=0.4500 | LR=0.000800\n",
      "  Época   4 | train_loss=1.1629 | val_acc=0.4994 | val_f1m=0.4880 | LR=0.001000\n",
      "  Época   5 | train_loss=1.1573 | val_acc=0.5018 | val_f1m=0.4922 | LR=0.001000\n",
      "  Época   6 | train_loss=1.1326 | val_acc=0.5018 | val_f1m=0.4944 | LR=0.000999\n",
      "  Época   7 | train_loss=1.1316 | val_acc=0.5071 | val_f1m=0.5104 | LR=0.000997\n",
      "  Época   8 | train_loss=1.1127 | val_acc=0.5131 | val_f1m=0.5082 | LR=0.000993\n",
      "  Época   9 | train_loss=1.1066 | val_acc=0.5119 | val_f1m=0.5065 | LR=0.000987\n",
      "  Época  10 | train_loss=1.0880 | val_acc=0.4940 | val_f1m=0.4961 | LR=0.000980\n",
      "  Época  11 | train_loss=1.0813 | val_acc=0.5095 | val_f1m=0.5083 | LR=0.000971\n",
      "  Época  12 | train_loss=1.0809 | val_acc=0.5244 | val_f1m=0.5223 | LR=0.000961\n",
      "  Época  13 | train_loss=1.0706 | val_acc=0.5131 | val_f1m=0.5117 | LR=0.000949\n",
      "  Época  14 | train_loss=1.0620 | val_acc=0.5054 | val_f1m=0.5095 | LR=0.000935\n",
      "  Época  15 | train_loss=1.0454 | val_acc=0.5042 | val_f1m=0.5002 | LR=0.000921\n",
      "  Época  16 | train_loss=1.0538 | val_acc=0.5107 | val_f1m=0.5099 | LR=0.000905\n",
      "  Época  17 | train_loss=1.0214 | val_acc=0.5054 | val_f1m=0.5006 | LR=0.000887\n",
      "  Época  18 | train_loss=1.0180 | val_acc=0.5173 | val_f1m=0.5168 | LR=0.000868\n",
      "  Época  19 | train_loss=0.9941 | val_acc=0.5280 | val_f1m=0.5276 | LR=0.000848\n",
      "  Época  20 | train_loss=1.0123 | val_acc=0.5054 | val_f1m=0.5069 | LR=0.000827\n",
      "  Época  21 | train_loss=0.9835 | val_acc=0.5208 | val_f1m=0.5200 | LR=0.000805\n",
      "  Época  22 | train_loss=0.9862 | val_acc=0.5042 | val_f1m=0.4987 | LR=0.000782\n",
      "  Época  23 | train_loss=0.9668 | val_acc=0.4970 | val_f1m=0.4847 | LR=0.000758\n",
      "  Época  24 | train_loss=0.9582 | val_acc=0.5060 | val_f1m=0.5027 | LR=0.000733\n",
      "  Época  25 | train_loss=0.9449 | val_acc=0.4976 | val_f1m=0.4905 | LR=0.000708\n",
      "  Época  26 | train_loss=0.9241 | val_acc=0.5018 | val_f1m=0.5015 | LR=0.000681\n",
      "  Época  27 | train_loss=0.9219 | val_acc=0.4946 | val_f1m=0.4966 | LR=0.000655\n",
      "  Época  28 | train_loss=0.9141 | val_acc=0.4964 | val_f1m=0.4980 | LR=0.000627\n",
      "  Época  29 | train_loss=0.9025 | val_acc=0.5077 | val_f1m=0.5042 | LR=0.000599\n",
      "  Early stopping en época 29 (mejor val_f1m=0.5276)\n",
      "[Fold 4/5] Global acc=0.5262\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.6475    0.5905    0.6177       420\n",
      "       Right     0.5635    0.5810    0.5721       420\n",
      "  Both Fists     0.4540    0.5524    0.4984       420\n",
      "   Both Feet     0.4533    0.3810    0.4140       420\n",
      "\n",
      "    accuracy                         0.5262      1680\n",
      "   macro avg     0.5296    0.5262    0.5255      1680\n",
      "weighted avg     0.5296    0.5262    0.5255      1680\n",
      "\n",
      "=== RESULTADOS (fold 4) ===\n",
      "Acc: 0.5262 | Macro-F1: 0.5255\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[248  22  93  57]\n",
      " [ 20 244  86  70]\n",
      " [ 56  66 232  66]\n",
      " [ 59 101 100 160]]\n",
      "\n",
      "============  FOLD 5  ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold5: 100%|██████████| 83/83 [00:09<00:00,  8.40it/s]\n",
      "Cargando test fold5: 100%|██████████| 20/20 [00:02<00:00,  8.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 5/5] Entrenando modelo global... (n_train=6972 | n_test=1680)\n",
      "train counts: {'left': np.int64(1743), 'right': np.int64(1743), 'both_fists': np.int64(1743), 'both_feet': np.int64(1743)} | total = 6972\n",
      "test  counts: {'left': np.int64(420), 'right': np.int64(420), 'both_fists': np.int64(420), 'both_feet': np.int64(420)} | total = 1680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Época   1 | train_loss=1.3911 | val_acc=0.3518 | val_f1m=0.3082 | LR=0.000400\n",
      "  Época   2 | train_loss=1.2738 | val_acc=0.5071 | val_f1m=0.5038 | LR=0.000600\n",
      "  Época   3 | train_loss=1.1954 | val_acc=0.5208 | val_f1m=0.5012 | LR=0.000800\n",
      "  Época   4 | train_loss=1.1863 | val_acc=0.5393 | val_f1m=0.5232 | LR=0.001000\n",
      "  Época   5 | train_loss=1.1657 | val_acc=0.5232 | val_f1m=0.5089 | LR=0.001000\n",
      "  Época   6 | train_loss=1.1465 | val_acc=0.5399 | val_f1m=0.5326 | LR=0.000999\n",
      "  Época   7 | train_loss=1.1450 | val_acc=0.5054 | val_f1m=0.4860 | LR=0.000997\n",
      "  Época   8 | train_loss=1.1193 | val_acc=0.5214 | val_f1m=0.5132 | LR=0.000993\n",
      "  Época   9 | train_loss=1.1186 | val_acc=0.5488 | val_f1m=0.5431 | LR=0.000987\n",
      "  Época  10 | train_loss=1.1034 | val_acc=0.5333 | val_f1m=0.5356 | LR=0.000980\n",
      "  Época  11 | train_loss=1.0964 | val_acc=0.5339 | val_f1m=0.5147 | LR=0.000971\n",
      "  Época  12 | train_loss=1.0925 | val_acc=0.5423 | val_f1m=0.5327 | LR=0.000961\n",
      "  Época  13 | train_loss=1.0821 | val_acc=0.5524 | val_f1m=0.5515 | LR=0.000949\n",
      "  Época  14 | train_loss=1.0721 | val_acc=0.5780 | val_f1m=0.5777 | LR=0.000935\n",
      "  Época  15 | train_loss=1.0550 | val_acc=0.5494 | val_f1m=0.5491 | LR=0.000921\n",
      "  Época  16 | train_loss=1.0607 | val_acc=0.5804 | val_f1m=0.5790 | LR=0.000905\n",
      "  Época  17 | train_loss=1.0381 | val_acc=0.5810 | val_f1m=0.5795 | LR=0.000887\n",
      "  Época  18 | train_loss=1.0459 | val_acc=0.5702 | val_f1m=0.5687 | LR=0.000868\n",
      "  Época  19 | train_loss=1.0318 | val_acc=0.5548 | val_f1m=0.5565 | LR=0.000848\n",
      "  Época  20 | train_loss=1.0249 | val_acc=0.5435 | val_f1m=0.5442 | LR=0.000827\n",
      "  Época  21 | train_loss=1.0145 | val_acc=0.5500 | val_f1m=0.5521 | LR=0.000805\n",
      "  Época  22 | train_loss=1.0046 | val_acc=0.5357 | val_f1m=0.5339 | LR=0.000782\n",
      "  Época  23 | train_loss=0.9861 | val_acc=0.5500 | val_f1m=0.5419 | LR=0.000758\n",
      "  Época  24 | train_loss=0.9796 | val_acc=0.5470 | val_f1m=0.5448 | LR=0.000733\n",
      "  Época  25 | train_loss=0.9614 | val_acc=0.5542 | val_f1m=0.5438 | LR=0.000708\n",
      "  Época  26 | train_loss=0.9527 | val_acc=0.5673 | val_f1m=0.5650 | LR=0.000681\n",
      "  Época  27 | train_loss=0.9408 | val_acc=0.5690 | val_f1m=0.5646 | LR=0.000655\n",
      "  Early stopping en época 27 (mejor val_f1m=0.5795)\n",
      "[Fold 5/5] Global acc=0.5780\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.6332    0.6000    0.6161       420\n",
      "       Right     0.5937    0.6714    0.6302       420\n",
      "  Both Fists     0.5432    0.4643    0.5006       420\n",
      "   Both Feet     0.5402    0.5762    0.5576       420\n",
      "\n",
      "    accuracy                         0.5780      1680\n",
      "   macro avg     0.5776    0.5780    0.5761      1680\n",
      "weighted avg     0.5776    0.5780    0.5761      1680\n",
      "\n",
      "=== RESULTADOS (fold 5) ===\n",
      "Acc: 0.5780 | Macro-F1: 0.5761\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[252  37  68  63]\n",
      " [ 21 282  41  76]\n",
      " [ 88  70 195  67]\n",
      " [ 37  86  55 242]]\n",
      "\n",
      "============================================================\n",
      "RESULTADOS FINALES (Transformer)\n",
      "============================================================\n",
      "Acc folds: ['0.4870', '0.5703', '0.4881', '0.5262', '0.5780']\n",
      "F1m folds: ['0.4861', '0.5713', '0.4878', '0.5255', '0.5761']\n",
      "Acc mean: 0.5299 | std: 0.0389\n",
      "F1m mean: 0.5294 | std: 0.0389\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# CNN + Transformer (PyTorch) para PhysioNet/BCI2000 MI — IMAGINERÍA (4 clases) con 8 canales.\n",
    "# Incluye: class-weights + label smoothing, warmup+cosine+early stopping, sampler balanceado,\n",
    "# augmentations ligeras, TTA estilo EEGNet y reproducibilidad fuerte (seed=42).\n",
    "# Ventana: 6 s (TMIN=-1.0, TMAX=5.0). Z-score por época activable con ZSCORE_PER_EPOCH.\n",
    "# Imprime métricas por fold (classification_report) y un runner de 5 folds tipo EEGNet.\n",
    "\n",
    "# =========================\n",
    "# Reproducibilidad (poner ANTES de importar torch)\n",
    "# =========================\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'  # determinismo cuBLAS\n",
    "\n",
    "import re, json, random\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import mne\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# =========================\n",
    "# REPRODUCIBILIDAD (seed + determinismo)\n",
    "# =========================\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # determinismo cuDNN\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # evitar TF32 (puede romper determinismo)\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "    # reforzar determinismo\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def seed_worker(worker_id: int):\n",
    "    worker_seed = RANDOM_STATE + worker_id\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "seed_everything(RANDOM_STATE)\n",
    "\n",
    "# =========================\n",
    "# CONFIG (edita a tu gusto)\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'                     # .../S###/S###R##.edf\n",
    "FOLDS_JSON = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "\n",
    "FOLD = 5\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "RESAMPLE_HZ = None          # None: mantiene 160 Hz original\n",
    "DO_NOTCH = True             # ganador\n",
    "DO_BANDPASS = False         # ganador\n",
    "BP_LO, BP_HI = 4.0, 38.0\n",
    "DO_CAR = False              # ganador\n",
    "\n",
    "# Normalización\n",
    "ZSCORE_PER_EPOCH = False     # <- activa/desactiva z-score por época (como EEGNet)\n",
    "\n",
    "# Hiperparámetros del modelo\n",
    "D_MODEL = 128              # (si quieres probar más capacidad: 160/192)\n",
    "N_HEADS = 4\n",
    "N_LAYERS = 2\n",
    "P_DROP = 0.2               # sube a 0.3 si ves sobreajuste\n",
    "\n",
    "# Ventana temporal (seg) — igual a EEGNet (6 s)\n",
    "TMIN, TMAX = -1.0, 5.0\n",
    "\n",
    "# Sliding windows / TTA en evaluación\n",
    "SW_MODE = 'tta'   # 'none' | 'subwin' | 'tta'\n",
    "SW_ENABLE = True  # si quieres apagar todo, pon False\n",
    "\n",
    "# Para TTA de desplazamientos cortos ±25 ms (EEGNet)\n",
    "TTA_SHIFTS_S = [-0.025, -0.0125, 0.0, 0.0125, 0.025]\n",
    "\n",
    "# (solo si usas 'subwin'; no se usa en 'tta' EEGNet)\n",
    "SW_LEN   = 2.0\n",
    "SW_STRIDE = 0.5\n",
    "\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','FCz']\n",
    "CLASS_NAMES = ['left', 'right', 'both_fists', 'both_feet']\n",
    "\n",
    "# Runs IMAGINERÍA\n",
    "IMAGERY_RUNS_LR = {4, 8, 12}    # T1=left, T2=right\n",
    "IMAGERY_RUNS_BF = {6, 10, 14}   # T1=both_fists, T2=both_feet\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Device: {DEVICE}\")\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES\n",
    "# =========================\n",
    "def normalize_ch_name(name: str) -> str:\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', name)\n",
    "    return s.upper()\n",
    "\n",
    "NORMALIZED_TARGETS = [normalize_ch_name(c) for c in EXPECTED_8]\n",
    "\n",
    "def pick_8_channels(raw: mne.io.BaseRaw) -> mne.io.BaseRaw:\n",
    "    chs = raw.info['ch_names']\n",
    "    norm_map = {normalize_ch_name(ch): ch for ch in chs}\n",
    "    picked = []\n",
    "    for target_norm, target_orig in zip(NORMALIZED_TARGETS, EXPECTED_8):\n",
    "        if target_norm in norm_map:\n",
    "            picked.append(norm_map[target_norm])\n",
    "        else:\n",
    "            raise RuntimeError(f\"Canal requerido '{target_orig}' no encontrado. Disponibles: {chs}\")\n",
    "    return raw.pick(picks=picked)\n",
    "\n",
    "def list_subject_imagery_edfs(subject_id: str) -> list:\n",
    "    subj_dir = DATA_RAW / subject_id\n",
    "    edfs = []\n",
    "    for r in [4, 6, 8, 10, 12, 14]:\n",
    "        pattern = str(subj_dir / f\"{subject_id}R{r:02d}.edf\")\n",
    "        edfs.extend(glob(pattern))\n",
    "    return sorted(edfs)\n",
    "\n",
    "def load_subject_epochs(subject_id: str, resample_hz: int, do_notch: bool, do_bandpass: bool,\n",
    "                        do_car: bool, bp_lo: float, bp_hi: float):\n",
    "    edfs = list_subject_imagery_edfs(subject_id)\n",
    "    if len(edfs) == 0:\n",
    "        raise FileNotFoundError(f\"No imagery EDF files for {subject_id} under {DATA_RAW}\")\n",
    "\n",
    "    X_list, y_list, sfreq_list = [], [], []\n",
    "\n",
    "    for edf_path in edfs:\n",
    "        m = re.search(r\"R(\\d{2})\", Path(edf_path).name)\n",
    "        run = int(m.group(1)) if m else -1\n",
    "\n",
    "        raw = mne.io.read_raw_edf(edf_path, preload=True, verbose='ERROR')\n",
    "\n",
    "        # Selección de 8 canales\n",
    "        raw = pick_8_channels(raw)\n",
    "\n",
    "        # --- PREPROC OPCIONAL ---\n",
    "        if do_notch:\n",
    "            raw.notch_filter(freqs=[60.0], picks='all', verbose='ERROR')\n",
    "        if do_bandpass:\n",
    "            raw.filter(l_freq=bp_lo, h_freq=bp_hi, picks='all', verbose='ERROR')\n",
    "        if do_car:\n",
    "            raw.set_eeg_reference('average', projection=False, verbose='ERROR')\n",
    "\n",
    "        # Resample\n",
    "        if resample_hz is not None and resample_hz > 0:\n",
    "            raw.resample(resample_hz)\n",
    "        sfreq = raw.info['sfreq']\n",
    "\n",
    "        # Eventos (T0/T1/T2)\n",
    "        events, event_id = mne.events_from_annotations(raw, verbose='ERROR')\n",
    "\n",
    "        # Mantener solo T1/T2\n",
    "        keep = {k: v for k, v in event_id.items() if k in {'T1', 'T2'}}\n",
    "        if len(keep) == 0:\n",
    "            continue\n",
    "\n",
    "        epochs = mne.Epochs(raw, events=events, event_id=keep, tmin=TMIN, tmax=TMAX,\n",
    "                            baseline=None, preload=True, verbose='ERROR')\n",
    "        X = epochs.get_data()  # (n_epochs, 8, T)\n",
    "\n",
    "        # ---- Z-SCORE POR ÉPOCA (opcional, como EEGNet) ----\n",
    "        if ZSCORE_PER_EPOCH:\n",
    "            X = X.astype(np.float32)\n",
    "            eps = 1e-6\n",
    "            mu = X.mean(axis=2, keepdims=True)              # (N,8,1)\n",
    "            sd = X.std(axis=2, keepdims=True) + eps         # (N,8,1)\n",
    "            X = (X - mu) / sd\n",
    "\n",
    "        # Construir y según RUN\n",
    "        ev_codes = epochs.events[:, 2]\n",
    "        inv = {v: k for k, v in keep.items()}  # id -> 'T1'/'T2'\n",
    "        y_run = []\n",
    "        for code in ev_codes:\n",
    "            lab = inv[code]\n",
    "            if run in IMAGERY_RUNS_LR:\n",
    "                y_run.append(0 if lab == 'T1' else 1)  # left/right\n",
    "            elif run in IMAGERY_RUNS_BF:\n",
    "                y_run.append(2 if lab == 'T1' else 3)  # both_fists/feet\n",
    "            else:\n",
    "                y_run.append(-1)\n",
    "        y_run = np.array(y_run, dtype=int)\n",
    "        keep_mask = y_run >= 0\n",
    "        X = X[keep_mask]\n",
    "        y = y_run[keep_mask]\n",
    "\n",
    "        if len(y) == 0:\n",
    "            continue\n",
    "\n",
    "        X_list.append(X)\n",
    "        y_list.append(y)\n",
    "        sfreq_list.append(sfreq)\n",
    "\n",
    "    if len(X_list) == 0:\n",
    "        return np.empty((0, 8, 1)), np.empty((0,), dtype=int), None\n",
    "\n",
    "    X_all = np.concatenate(X_list, axis=0)\n",
    "    y_all = np.concatenate(y_list, axis=0)\n",
    "\n",
    "    if len(set([round(s) for s in sfreq_list])) != 1:\n",
    "        raise RuntimeError(f\"Inconsistent sampling rates: {sfreq_list}\")\n",
    "\n",
    "    return X_all, y_all, sfreq_list[0]\n",
    "\n",
    "def load_fold_subjects(folds_json: Path, fold: int):\n",
    "    with open(folds_json, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    for item in data.get('folds', []):\n",
    "        if int(item.get('fold', -1)) == int(fold):\n",
    "            train_sub = list(item.get('train', []))\n",
    "            test_sub  = list(item.get('test', []))\n",
    "            return train_sub, test_sub\n",
    "    raise ValueError(f\"Fold {fold} not found in {folds_json}\")\n",
    "\n",
    "def subject_id_to_int(s: str) -> int:\n",
    "    m = re.match(r'[Ss](\\d+)', s)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "def class_count_summary(y, name):\n",
    "    bc = np.bincount(y, minlength=4)\n",
    "    print(f\"{name} counts:\", dict(zip(CLASS_NAMES, bc)), \"| total =\", bc.sum())\n",
    "\n",
    "# =========================\n",
    "# MODELO: CNN + Transformer\n",
    "# =========================\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k, s=1, p=0):\n",
    "        super().__init__()\n",
    "        self.dw = nn.Conv1d(in_ch, in_ch, kernel_size=k, stride=s, padding=p, groups=in_ch, bias=False)\n",
    "        self.pw = nn.Conv1d(in_ch, out_ch, kernel_size=1, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(out_ch)\n",
    "        self.act = nn.ELU()\n",
    "    def forward(self, x):\n",
    "        x = self.dw(x); x = self.pw(x); x = self.bn(x)\n",
    "        return self.act(x)\n",
    "\n",
    "class EEGCNNTransformer(nn.Module):\n",
    "    def __init__(self, n_ch=8, n_cls=4, d_model=128, n_heads=4, n_layers=2, p_drop=0.2):\n",
    "        super().__init__()\n",
    "        self.conv_t = nn.Sequential(\n",
    "            nn.Conv1d(n_ch, 32, kernel_size=129, stride=2, padding=64, bias=False),\n",
    "            nn.BatchNorm1d(32), nn.ELU(),\n",
    "            DepthwiseSeparableConv(32, 64, k=31, s=2, p=15),\n",
    "            DepthwiseSeparableConv(64, 128, k=15, s=2, p=7),\n",
    "        )\n",
    "        self.proj = nn.Conv1d(128, d_model, kernel_size=1, bias=False)\n",
    "        self.dropout = nn.Dropout(p=p_drop)\n",
    "        self.pos_encoding = None\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=2*d_model,\n",
    "            batch_first=True, activation='gelu', dropout=0.1, norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.cls = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        nn.init.normal_(self.cls, std=0.02)\n",
    "        self.head = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, n_cls))\n",
    "\n",
    "    def _positional_encoding(self, L, d):\n",
    "        pos = torch.arange(0, L, dtype=torch.float32).unsqueeze(1)\n",
    "        i   = torch.arange(0, d, dtype=torch.float32).unsqueeze(0)\n",
    "        angle = pos / torch.pow(10000, (2 * (i//2)) / d)\n",
    "        pe = torch.zeros(L, d, dtype=torch.float32)\n",
    "        pe[:, 0::2] = torch.sin(angle[:, 0::2])\n",
    "        pe[:, 1::2] = torch.cos(angle[:, 1::2])\n",
    "        return pe\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.conv_t(x)           # (B, 128, T')\n",
    "        z = self.proj(z)             # (B, d_model, T')\n",
    "        z = self.dropout(z)\n",
    "        z = z.transpose(1, 2)        # (B, T', d_model)\n",
    "        B, L, D = z.shape\n",
    "        if (self.pos_encoding is None) or (self.pos_encoding.shape[0] != L) or (self.pos_encoding.shape[1] != D):\n",
    "            self.pos_encoding = self._positional_encoding(L, D).to(z.device)\n",
    "        z = z + self.pos_encoding[None, :, :]\n",
    "        cls_tok = self.cls.expand(B, -1, -1)  # (B,1,D)\n",
    "        z = torch.cat([cls_tok, z], dim=1)    # (B, 1+L, D)\n",
    "        z = self.encoder(z)                   # (B, 1+L, D)\n",
    "        cls = z[:, 0, :]\n",
    "        return self.head(cls)\n",
    "\n",
    "# =========================\n",
    "# ENTRENAMIENTO / EVAL\n",
    "# =========================\n",
    "def standardize_per_channel(train_X, test_X):\n",
    "    C = train_X.shape[1]\n",
    "    train_X = train_X.astype(np.float32)\n",
    "    test_X  = test_X.astype(np.float32)\n",
    "    for c in range(C):\n",
    "        mu = train_X[:, c, :].mean()\n",
    "        sd = train_X[:, c, :].std()\n",
    "        sd = sd if sd > 1e-6 else 1.0\n",
    "        train_X[:, c, :] = (train_X[:, c, :] - mu) / sd\n",
    "        test_X[:, c, :]  = (test_X[:, c, :] - mu) / sd\n",
    "    return train_X, test_X\n",
    "\n",
    "# --- Augmentations ligeras ---\n",
    "def augment_batch(xb, p_jitter=0.25, p_noise=0.25, p_chdrop=0.15,\n",
    "                  max_jitter_frac=0.02, noise_std=0.02):\n",
    "    \"\"\"\n",
    "    xb: torch.Tensor (B,C,T) (se asume ya normalizado)\n",
    "    - jitter temporal (roll)\n",
    "    - ruido gaussiano\n",
    "    - channel dropout (apagar 1 canal aleatorio)\n",
    "    \"\"\"\n",
    "    B, C, T = xb.shape\n",
    "    if np.random.rand() < p_jitter:\n",
    "        max_shift = int(max(1, T*max_jitter_frac))\n",
    "        shifts = torch.randint(low=-max_shift, high=max_shift+1, size=(B,), device=xb.device)\n",
    "        for i in range(B):\n",
    "            xb[i] = torch.roll(xb[i], shifts=int(shifts[i].item()), dims=-1)\n",
    "    if np.random.rand() < p_noise:\n",
    "        xb = xb + noise_std*torch.randn_like(xb)\n",
    "    if np.random.rand() < p_chdrop:\n",
    "        k = 1\n",
    "        for i in range(B):\n",
    "            idx = torch.randperm(C, device=xb.device)[:k]\n",
    "            xb[i, idx, :] = 0.0\n",
    "    return xb\n",
    "\n",
    "def subwindow_logits(model, X, sfreq, sw_len, sw_stride, device):\n",
    "    \"\"\"\n",
    "    Promedia logits sobre sub-ventanas de longitud sw_len (seg).\n",
    "    Entrada:  X (N,C,T) estandarizado\n",
    "    Salida:   (N,4)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    wl = int(round(sw_len * sfreq))\n",
    "    st = int(round(sw_stride * sfreq))\n",
    "    wl = max(1, min(wl, X.shape[-1]))\n",
    "    st = max(1, st)\n",
    "\n",
    "    out = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(X.shape[0]):\n",
    "            x = X[i]  # (C,T)\n",
    "            acc = []\n",
    "            for s in range(0, max(1, X.shape[-1]-wl+1), st):\n",
    "                seg = x[:, s:s+wl]\n",
    "                if seg.shape[-1] < wl:\n",
    "                    pad = wl - seg.shape[-1]\n",
    "                    seg = np.pad(seg, ((0,0),(0,pad)), mode='edge')\n",
    "                xb = torch.tensor(seg[None, ...], dtype=torch.float32, device=device)\n",
    "                logit = model(xb).detach().cpu().numpy()[0]  # (4,)\n",
    "                acc.append(logit)\n",
    "            acc = np.mean(np.stack(acc, axis=0), axis=0) if len(acc) else np.zeros(4, dtype=np.float32)\n",
    "            out.append(acc)\n",
    "    return np.stack(out, axis=0)  # (N,4)\n",
    "\n",
    "def time_shift_tta_logits(model, X, sfreq, shifts_s, device):\n",
    "    \"\"\"\n",
    "    Test-Time Augmentation (EEGNet-like): 5 desplazamientos cortos ±25ms manteniendo longitud T.\n",
    "    Entrada:  X (N,C,T) estandarizado\n",
    "    Salida:   (N,4)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    T = X.shape[-1]\n",
    "    out = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(X.shape[0]):\n",
    "            x0 = X[i]  # (C,T)\n",
    "            acc = []\n",
    "            for sh in shifts_s:\n",
    "                shift = int(round(sh * sfreq))\n",
    "                if shift == 0:\n",
    "                    x = x0\n",
    "                elif shift > 0:\n",
    "                    x = np.pad(x0[:, shift:], ((0,0),(0,shift)), mode='edge')[:, :T]\n",
    "                else:\n",
    "                    shift = -shift\n",
    "                    x = np.pad(x0[:, :-shift], ((0,0),(shift,0)), mode='edge')[:, :T]\n",
    "                xb = torch.tensor(x[None, ...], dtype=torch.float32, device=device)\n",
    "                logit = model(xb).detach().cpu().numpy()[0]  # (4,)\n",
    "                acc.append(logit)\n",
    "            acc = np.mean(np.stack(acc, axis=0), axis=0)\n",
    "            out.append(acc)\n",
    "    return np.stack(out, axis=0)  # (N,4)\n",
    "\n",
    "def load_fold_subject_ids_and_counts(fold:int):\n",
    "    train_sub, test_sub = load_fold_subjects(FOLDS_JSON, fold)\n",
    "    train_sub = [s for s in train_sub if subject_id_to_int(s) not in EXCLUDE_SUBJECTS]\n",
    "    test_sub  = [s for s in test_sub  if subject_id_to_int(s) not in EXCLUDE_SUBJECTS]\n",
    "    return train_sub, test_sub\n",
    "\n",
    "def train_one_fold(fold:int, resample_hz:int, do_notch:bool, do_bandpass:bool,\n",
    "                   bp_lo:float, bp_hi:float, epochs:int, batch_size:int, lr:float,\n",
    "                   device:str=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "\n",
    "    # --- sujetos fold ---\n",
    "    train_sub, test_sub = load_fold_subject_ids_and_counts(fold)\n",
    "\n",
    "    # --- carga datos ---\n",
    "    X_tr_list, y_tr_list, X_te_list, y_te_list = [], [], [], []\n",
    "    sfreq = None\n",
    "\n",
    "    for sid in tqdm(train_sub, desc=f\"Cargando train fold{fold}\"):\n",
    "        Xs, ys, sf = load_subject_epochs(sid, resample_hz, do_notch, do_bandpass, DO_CAR, bp_lo, bp_hi)\n",
    "        if len(ys) == 0: continue\n",
    "        X_tr_list.append(Xs); y_tr_list.append(ys); sfreq = sf if sfreq is None else sfreq\n",
    "\n",
    "    for sid in tqdm(test_sub, desc=f\"Cargando test fold{fold}\"):\n",
    "        Xs, ys, sf = load_subject_epochs(sid, resample_hz, do_notch, do_bandpass, DO_CAR, bp_lo, bp_hi)\n",
    "        if len(ys) == 0: continue\n",
    "        X_te_list.append(Xs); y_te_list.append(ys); sfreq = sf if sfreq is None else sfreq\n",
    "\n",
    "    if len(X_tr_list) == 0 or len(X_te_list) == 0:\n",
    "        raise RuntimeError(\"Datos insuficientes tras carga de sujetos.\")\n",
    "\n",
    "    X_tr = np.concatenate(X_tr_list, axis=0); y_tr = np.concatenate(y_tr_list, axis=0)\n",
    "    X_te = np.concatenate(X_te_list, axis=0); y_te = np.concatenate(y_te_list, axis=0)\n",
    "\n",
    "    # --- diagnósticos de desbalance ---\n",
    "    # (y cabecera al estilo EEGNet con n_train / n_test)\n",
    "    print(f\"[Fold {fold}/5] Entrenando modelo global... (n_train={len(y_tr)} | n_test={len(y_te)})\")\n",
    "    print(\"train counts:\", dict(zip(CLASS_NAMES, np.bincount(y_tr, minlength=4))), \"| total =\", len(y_tr))\n",
    "    print(\"test  counts:\", dict(zip(CLASS_NAMES, np.bincount(y_te, minlength=4))), \"| total =\", len(y_te))\n",
    "\n",
    "    # --- Normalización ---\n",
    "    if ZSCORE_PER_EPOCH:\n",
    "        # Ya se normalizó por época dentro de load_subject_epochs → NO hacer z-score global\n",
    "        X_tr_std, X_te_std = X_tr, X_te\n",
    "    else:\n",
    "        # Sin z-score por época → aplicar z-score global por canal usando SOLO train\n",
    "        X_tr_std, X_te_std = standardize_per_channel(X_tr, X_te)\n",
    "\n",
    "    # --- DataLoaders (sampler balanceado + reproducible) ---\n",
    "    train_ds = TensorDataset(torch.tensor(X_tr_std), torch.tensor(y_tr).long())\n",
    "    class_counts = np.bincount(y_tr, minlength=4)\n",
    "    class_weights_vec = class_counts.sum() / (4.0 * np.maximum(class_counts, 1))\n",
    "    sample_weights = class_weights_vec[train_ds.tensors[1].numpy()]\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=torch.tensor(sample_weights, dtype=torch.float32),\n",
    "        num_samples=len(train_ds), replacement=True,\n",
    "        generator=torch.Generator().manual_seed(RANDOM_STATE)\n",
    "    )\n",
    "    tr_ld = DataLoader(train_ds, batch_size=batch_size, sampler=sampler,\n",
    "                       drop_last=False, worker_init_fn=seed_worker)\n",
    "\n",
    "    te_ld = DataLoader(\n",
    "        TensorDataset(torch.tensor(X_te_std), torch.tensor(y_te).long()),\n",
    "        batch_size=batch_size, shuffle=False, drop_last=False,\n",
    "        worker_init_fn=seed_worker\n",
    "    )\n",
    "\n",
    "    # --- Modelo ---\n",
    "    model = EEGCNNTransformer(\n",
    "        n_ch=8, n_cls=4, d_model=D_MODEL, n_heads=N_HEADS, n_layers=N_LAYERS, p_drop=P_DROP\n",
    "    ).to(device)\n",
    "\n",
    "    # --- Optimizador ---\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
    "\n",
    "    # (1) Class-weights + label smoothing\n",
    "    class_weights = torch.tensor(class_weights_vec, dtype=torch.float32, device=device)\n",
    "    crit = torch.nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.05)\n",
    "\n",
    "    # (2) Warmup + Cosine LR + Early stopping por macro-F1\n",
    "    from torch.optim.lr_scheduler import LambdaLR\n",
    "    total_epochs = epochs\n",
    "    warmup_epochs = max(1, min(5, epochs//10))\n",
    "    def lr_lambda(current_epoch):\n",
    "        if current_epoch < warmup_epochs:\n",
    "            return (current_epoch + 1) / warmup_epochs\n",
    "        progress = (current_epoch - warmup_epochs) / max(1, (total_epochs - warmup_epochs))\n",
    "        progress = min(1.0, max(0.0, progress))\n",
    "        return 0.5 * (1.0 + np.cos(np.pi * progress))\n",
    "    scheduler = LambdaLR(opt, lr_lambda=lr_lambda)\n",
    "\n",
    "    best_f1, best_state, patience, wait = 0.0, None, 10, 0\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        tr_loss, n_seen = 0.0, 0\n",
    "        for xb, yb in tr_ld:\n",
    "            # augs en GPU (se asume input ya normalizado)\n",
    "            xb = xb.to(device)\n",
    "            xb = augment_batch(xb)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            tr_loss += loss.item() * len(yb); n_seen += len(yb)\n",
    "        tr_loss /= max(1, n_seen)\n",
    "\n",
    "        # --- Eval en test set (mismo patrón que tu script actual) ---\n",
    "        model.eval()\n",
    "        preds, gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in te_ld:\n",
    "                xb = xb.to(device)\n",
    "                p = model(xb).argmax(dim=1).cpu().numpy()\n",
    "                preds.append(p); gts.append(yb.numpy())\n",
    "        preds = np.concatenate(preds); gts = np.concatenate(gts)\n",
    "        acc = accuracy_score(gts, preds)\n",
    "        f1m = f1_score(gts, preds, average='macro')\n",
    "\n",
    "        improved = f1m > best_f1 + 1e-4\n",
    "        if improved:\n",
    "            best_f1 = f1m\n",
    "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "\n",
    "        scheduler.step()\n",
    "        # Log estilo EEGNet (usa acc/f1m del split de test como \"val_*\" porque no tenemos val separado)\n",
    "        print(f\"  Época {ep:3d} | train_loss={tr_loss:.4f} | val_acc={acc:.4f} | val_f1m={f1m:.4f} | LR={scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        if wait >= patience:\n",
    "            print(f\"  Early stopping en época {ep} (mejor val_f1m={best_f1:.4f})\")\n",
    "            break\n",
    "\n",
    "    # cargar el mejor\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # --- reporte final con TTA/subwin/none ---\n",
    "    model.eval()\n",
    "    sfreq_used = RESAMPLE_HZ if RESAMPLE_HZ is not None else int(round(X_te_std.shape[-1] / (TMAX - TMIN)))\n",
    "\n",
    "    if (not SW_ENABLE) or SW_MODE == 'none':\n",
    "        preds, gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in te_ld:\n",
    "                xb = xb.to(device)\n",
    "                p = model(xb).argmax(dim=1).cpu().numpy()\n",
    "                preds.append(p); gts.append(yb.numpy())\n",
    "        preds = np.concatenate(preds); gts = np.concatenate(gts)\n",
    "\n",
    "    elif SW_MODE == 'subwin':\n",
    "        logits = subwindow_logits(model, X_te_std, sfreq_used, SW_LEN, SW_STRIDE, device)  # (N,4)\n",
    "        preds = logits.argmax(axis=1)\n",
    "        gts = y_te\n",
    "\n",
    "    elif SW_MODE == 'tta':\n",
    "        logits = time_shift_tta_logits(model, X_te_std, sfreq_used, TTA_SHIFTS_S, device)  # (N,4)\n",
    "        preds = logits.argmax(axis=1)\n",
    "        gts = y_te\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"SW_MODE desconocido: {SW_MODE}\")\n",
    "\n",
    "    acc = accuracy_score(gts, preds)\n",
    "    f1m = f1_score(gts, preds, average='macro')\n",
    "    cm = confusion_matrix(gts, preds, labels=[0,1,2,3])\n",
    "\n",
    "    # ---- Salida estilo EEGNet ----\n",
    "    print(f\"[Fold {fold}/5] Global acc={acc:.4f}\")\n",
    "    print(\"\\n\" + classification_report(gts, preds, target_names=[\n",
    "        'Left','Right','Both Fists','Both Feet'\n",
    "    ], digits=4))\n",
    "\n",
    "    print(f\"=== RESULTADOS (fold {fold}) ===\")\n",
    "    print(f\"Acc: {acc:.4f} | Macro-F1: {f1m:.4f}\")\n",
    "    print(\"Confusion matrix (rows=true, cols=pred):\")\n",
    "    print(cm)\n",
    "\n",
    "    return acc, f1m, cm, model\n",
    "\n",
    "# =========================\n",
    "# Runner multi-fold con resumen (igual estilo EEGNet)\n",
    "# =========================\n",
    "def run_all_folds(n_folds=5):\n",
    "    accs, f1s = [], []\n",
    "    for f in range(1, n_folds+1):\n",
    "        print(\"\\n\" + \"=\"*12 + f\"  FOLD {f}  \" + \"=\"*12)\n",
    "        acc, f1m, cm, _ = train_one_fold(\n",
    "            fold=f,\n",
    "            resample_hz=RESAMPLE_HZ,\n",
    "            do_notch=DO_NOTCH,\n",
    "            do_bandpass=DO_BANDPASS,\n",
    "            bp_lo=BP_LO,\n",
    "            bp_hi=BP_HI,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            lr=LR,\n",
    "        )\n",
    "        accs.append(acc); f1s.append(f1m)\n",
    "    if accs:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RESULTADOS FINALES (Transformer)\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"Acc folds:\", [f\"{a:.4f}\" for a in accs])\n",
    "        print(\"F1m folds:\", [f\"{a:.4f}\" for a in f1s])\n",
    "        print(f\"Acc mean: {np.mean(accs):.4f} | std: {np.std(accs):.4f}\")\n",
    "        print(f\"F1m mean: {np.mean(f1s):.4f} | std: {np.std(f1s):.4f}\")\n",
    "\n",
    "# =========================\n",
    "# EJECUCIÓN\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Opción A: correr todos los folds 1..5 con reporte por fold y resumen\n",
    "    run_all_folds(n_folds=5)\n",
    "\n",
    "    # Opción B: correr un solo fold\n",
    "    # acc, f1m, cm, model = train_one_fold(\n",
    "    #     fold=FOLD,\n",
    "    #     resample_hz=RESAMPLE_HZ,\n",
    "    #     do_notch=DO_NOTCH,\n",
    "    #     do_bandpass=DO_BANDPASS,\n",
    "    #     bp_lo=BP_LO,\n",
    "    #     bp_hi=BP_HI,\n",
    "    #     epochs=EPOCHS,\n",
    "    #     batch_size=BATCH_SIZE,\n",
    "    #     lr=LR,\n",
    "    # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d949dacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Usando dispositivo: cuda\n",
      "🧠 INICIANDO EXPERIMENTO CON CNN+Transformer (K-Fold por sujeto como EEGNet)\n",
      "🔧 Configuración: 4c, 8 canales, 6s | EPOCHS=60, BATCH=64, LR=0.001 | ZSCORE_PER_EPOCH=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold1: 100%|██████████| 67/67 [00:08<00:00,  8.35it/s]\n",
      "Cargando val fold1: 100%|██████████| 15/15 [00:01<00:00,  8.37it/s]\n",
      "Cargando test fold1: 100%|██████████| 21/21 [00:02<00:00,  8.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1/5] Entrenando modelo global... (n_train=5628 | n_val=1260 | n_test=1764)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Época   1 | train_loss=0.2225 | train_acc=0.2747 | val_acc=0.2984 | val_f1m=0.2443 | LR=0.000125\n",
      "  Época   2 | train_loss=0.2016 | train_acc=0.3625 | val_acc=0.4056 | val_f1m=0.3910 | LR=0.000250\n",
      "  Época   3 | train_loss=0.1802 | train_acc=0.4655 | val_acc=0.4143 | val_f1m=0.4060 | LR=0.000375\n",
      "  Época   4 | train_loss=0.1765 | train_acc=0.4730 | val_acc=0.4175 | val_f1m=0.4095 | LR=0.000500\n",
      "  Época   5 | train_loss=0.1699 | train_acc=0.5023 | val_acc=0.4230 | val_f1m=0.4166 | LR=0.000625\n",
      "  Época   6 | train_loss=0.1664 | train_acc=0.5263 | val_acc=0.4317 | val_f1m=0.4279 | LR=0.000750\n",
      "  Época   7 | train_loss=0.1640 | train_acc=0.5258 | val_acc=0.4365 | val_f1m=0.4340 | LR=0.000875\n",
      "  Época   8 | train_loss=0.1614 | train_acc=0.5339 | val_acc=0.4421 | val_f1m=0.4401 | LR=0.001000\n",
      "  Época   9 | train_loss=0.1648 | train_acc=0.5171 | val_acc=0.4429 | val_f1m=0.4412 | LR=0.001000\n",
      "  Época  10 | train_loss=0.1569 | train_acc=0.5437 | val_acc=0.4437 | val_f1m=0.4417 | LR=0.000999\n",
      "  Época  11 | train_loss=0.1569 | train_acc=0.5329 | val_acc=0.4405 | val_f1m=0.4388 | LR=0.000997\n",
      "  Época  12 | train_loss=0.1545 | train_acc=0.5442 | val_acc=0.4484 | val_f1m=0.4468 | LR=0.000993\n",
      "  Época  13 | train_loss=0.1507 | train_acc=0.5556 | val_acc=0.4468 | val_f1m=0.4453 | LR=0.000988\n",
      "  Época  14 | train_loss=0.1520 | train_acc=0.5549 | val_acc=0.4540 | val_f1m=0.4524 | LR=0.000982\n",
      "  Época  15 | train_loss=0.1510 | train_acc=0.5604 | val_acc=0.4532 | val_f1m=0.4517 | LR=0.000974\n",
      "  Época  16 | train_loss=0.1479 | train_acc=0.5631 | val_acc=0.4579 | val_f1m=0.4567 | LR=0.000965\n",
      "  Época  17 | train_loss=0.1422 | train_acc=0.5807 | val_acc=0.4540 | val_f1m=0.4528 | LR=0.000954\n",
      "  Época  18 | train_loss=0.1441 | train_acc=0.5766 | val_acc=0.4563 | val_f1m=0.4557 | LR=0.000942\n",
      "  Época  19 | train_loss=0.1431 | train_acc=0.5759 | val_acc=0.4587 | val_f1m=0.4584 | LR=0.000929\n",
      "  Época  20 | train_loss=0.1437 | train_acc=0.5853 | val_acc=0.4619 | val_f1m=0.4614 | LR=0.000915\n",
      "  Época  21 | train_loss=0.1422 | train_acc=0.5810 | val_acc=0.4659 | val_f1m=0.4660 | LR=0.000899\n",
      "  Época  22 | train_loss=0.1406 | train_acc=0.5839 | val_acc=0.4619 | val_f1m=0.4618 | LR=0.000883\n",
      "  Época  23 | train_loss=0.1414 | train_acc=0.5791 | val_acc=0.4643 | val_f1m=0.4637 | LR=0.000865\n",
      "  Época  24 | train_loss=0.1374 | train_acc=0.5975 | val_acc=0.4619 | val_f1m=0.4614 | LR=0.000847\n",
      "  Época  25 | train_loss=0.1367 | train_acc=0.5959 | val_acc=0.4619 | val_f1m=0.4612 | LR=0.000827\n",
      "  Época  26 | train_loss=0.1342 | train_acc=0.6050 | val_acc=0.4659 | val_f1m=0.4646 | LR=0.000807\n",
      "  Época  27 | train_loss=0.1281 | train_acc=0.6194 | val_acc=0.4635 | val_f1m=0.4626 | LR=0.000786\n",
      "  Época  28 | train_loss=0.1329 | train_acc=0.6054 | val_acc=0.4643 | val_f1m=0.4633 | LR=0.000764\n",
      "  Época  29 | train_loss=0.1286 | train_acc=0.6158 | val_acc=0.4627 | val_f1m=0.4611 | LR=0.000742\n",
      "  Época  30 | train_loss=0.1249 | train_acc=0.6270 | val_acc=0.4643 | val_f1m=0.4627 | LR=0.000719\n",
      "  Época  31 | train_loss=0.1226 | train_acc=0.6262 | val_acc=0.4611 | val_f1m=0.4599 | LR=0.000696\n",
      "  Época  32 | train_loss=0.1262 | train_acc=0.6251 | val_acc=0.4611 | val_f1m=0.4600 | LR=0.000672\n",
      "  Época  33 | train_loss=0.1203 | train_acc=0.6413 | val_acc=0.4595 | val_f1m=0.4586 | LR=0.000648\n",
      "  Early stopping en época 33 (mejor val_f1m=0.4660)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold1.png\n",
      "[Fold 1/5] Global acc=0.4943\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.6356    0.5261    0.5757       441\n",
      "       right     0.5742    0.5351    0.5540       441\n",
      "  both fists     0.3608    0.5170    0.4250       441\n",
      "   both feet     0.4944    0.3991    0.4417       441\n",
      "\n",
      "    accuracy                         0.4943      1764\n",
      "   macro avg     0.5162    0.4943    0.4991      1764\n",
      "weighted avg     0.5162    0.4943    0.4991      1764\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[232  28 139  42]\n",
      " [ 24 236 127  54]\n",
      " [ 63  66 228  84]\n",
      " [ 46  81 138 176]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold2: 100%|██████████| 67/67 [00:07<00:00,  8.41it/s]\n",
      "Cargando val fold2: 100%|██████████| 15/15 [00:01<00:00,  8.50it/s]\n",
      "Cargando test fold2: 100%|██████████| 21/21 [00:02<00:00,  8.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 2/5] Entrenando modelo global... (n_train=5628 | n_val=1260 | n_test=1764)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Época   1 | train_loss=0.2221 | train_acc=0.2790 | val_acc=0.3063 | val_f1m=0.2355 | LR=0.000125\n",
      "  Época   2 | train_loss=0.1983 | train_acc=0.3882 | val_acc=0.4302 | val_f1m=0.4300 | LR=0.000250\n",
      "  Época   3 | train_loss=0.1829 | train_acc=0.4662 | val_acc=0.4484 | val_f1m=0.4476 | LR=0.000375\n",
      "  Época   4 | train_loss=0.1810 | train_acc=0.4739 | val_acc=0.4516 | val_f1m=0.4514 | LR=0.000500\n",
      "  Época   5 | train_loss=0.1767 | train_acc=0.4803 | val_acc=0.4532 | val_f1m=0.4523 | LR=0.000625\n",
      "  Época   6 | train_loss=0.1763 | train_acc=0.4742 | val_acc=0.4698 | val_f1m=0.4689 | LR=0.000750\n",
      "  Época   7 | train_loss=0.1725 | train_acc=0.4927 | val_acc=0.4722 | val_f1m=0.4715 | LR=0.000875\n",
      "  Época   8 | train_loss=0.1758 | train_acc=0.4869 | val_acc=0.4698 | val_f1m=0.4685 | LR=0.001000\n",
      "  Época   9 | train_loss=0.1687 | train_acc=0.5084 | val_acc=0.4770 | val_f1m=0.4744 | LR=0.001000\n",
      "  Época  10 | train_loss=0.1666 | train_acc=0.5171 | val_acc=0.4810 | val_f1m=0.4783 | LR=0.000999\n",
      "  Época  11 | train_loss=0.1642 | train_acc=0.5162 | val_acc=0.4841 | val_f1m=0.4819 | LR=0.000997\n",
      "  Época  12 | train_loss=0.1667 | train_acc=0.5028 | val_acc=0.4849 | val_f1m=0.4835 | LR=0.000993\n",
      "  Época  13 | train_loss=0.1643 | train_acc=0.5156 | val_acc=0.4897 | val_f1m=0.4879 | LR=0.000988\n",
      "  Época  14 | train_loss=0.1604 | train_acc=0.5224 | val_acc=0.4921 | val_f1m=0.4905 | LR=0.000982\n",
      "  Época  15 | train_loss=0.1606 | train_acc=0.5329 | val_acc=0.4897 | val_f1m=0.4879 | LR=0.000974\n",
      "  Época  16 | train_loss=0.1585 | train_acc=0.5309 | val_acc=0.4913 | val_f1m=0.4908 | LR=0.000965\n",
      "  Época  17 | train_loss=0.1559 | train_acc=0.5421 | val_acc=0.4937 | val_f1m=0.4923 | LR=0.000954\n",
      "  Época  18 | train_loss=0.1560 | train_acc=0.5416 | val_acc=0.4921 | val_f1m=0.4910 | LR=0.000942\n",
      "  Época  19 | train_loss=0.1541 | train_acc=0.5496 | val_acc=0.4921 | val_f1m=0.4912 | LR=0.000929\n",
      "  Época  20 | train_loss=0.1527 | train_acc=0.5560 | val_acc=0.4984 | val_f1m=0.4966 | LR=0.000915\n",
      "  Época  21 | train_loss=0.1510 | train_acc=0.5460 | val_acc=0.5040 | val_f1m=0.5029 | LR=0.000899\n",
      "  Época  22 | train_loss=0.1496 | train_acc=0.5654 | val_acc=0.5024 | val_f1m=0.5015 | LR=0.000883\n",
      "  Época  23 | train_loss=0.1497 | train_acc=0.5524 | val_acc=0.5000 | val_f1m=0.4997 | LR=0.000865\n",
      "  Época  24 | train_loss=0.1444 | train_acc=0.5654 | val_acc=0.5032 | val_f1m=0.5030 | LR=0.000847\n",
      "  Época  25 | train_loss=0.1456 | train_acc=0.5762 | val_acc=0.5032 | val_f1m=0.5027 | LR=0.000827\n",
      "  Época  26 | train_loss=0.1433 | train_acc=0.5736 | val_acc=0.5024 | val_f1m=0.5017 | LR=0.000807\n",
      "  Época  27 | train_loss=0.1414 | train_acc=0.5826 | val_acc=0.5032 | val_f1m=0.5038 | LR=0.000786\n",
      "  Época  28 | train_loss=0.1443 | train_acc=0.5659 | val_acc=0.5040 | val_f1m=0.5047 | LR=0.000764\n",
      "  Época  29 | train_loss=0.1392 | train_acc=0.5810 | val_acc=0.5000 | val_f1m=0.5005 | LR=0.000742\n",
      "  Época  30 | train_loss=0.1393 | train_acc=0.5828 | val_acc=0.5032 | val_f1m=0.5037 | LR=0.000719\n",
      "  Época  31 | train_loss=0.1346 | train_acc=0.5961 | val_acc=0.5016 | val_f1m=0.5023 | LR=0.000696\n",
      "  Época  32 | train_loss=0.1305 | train_acc=0.6137 | val_acc=0.5016 | val_f1m=0.5027 | LR=0.000672\n",
      "  Época  33 | train_loss=0.1331 | train_acc=0.6025 | val_acc=0.5048 | val_f1m=0.5057 | LR=0.000648\n",
      "  Época  34 | train_loss=0.1285 | train_acc=0.6150 | val_acc=0.5040 | val_f1m=0.5049 | LR=0.000624\n",
      "  Época  35 | train_loss=0.1314 | train_acc=0.6079 | val_acc=0.5040 | val_f1m=0.5051 | LR=0.000600\n",
      "  Época  36 | train_loss=0.1247 | train_acc=0.6224 | val_acc=0.5000 | val_f1m=0.5010 | LR=0.000576\n",
      "  Época  37 | train_loss=0.1253 | train_acc=0.6249 | val_acc=0.5008 | val_f1m=0.5013 | LR=0.000552\n",
      "  Época  38 | train_loss=0.1205 | train_acc=0.6260 | val_acc=0.5008 | val_f1m=0.5010 | LR=0.000528\n",
      "  Época  39 | train_loss=0.1167 | train_acc=0.6402 | val_acc=0.5000 | val_f1m=0.5008 | LR=0.000504\n",
      "  Época  40 | train_loss=0.1182 | train_acc=0.6386 | val_acc=0.5016 | val_f1m=0.5019 | LR=0.000481\n",
      "  Época  41 | train_loss=0.1147 | train_acc=0.6461 | val_acc=0.4976 | val_f1m=0.4979 | LR=0.000458\n",
      "  Época  42 | train_loss=0.1145 | train_acc=0.6501 | val_acc=0.5032 | val_f1m=0.5033 | LR=0.000436\n",
      "  Época  43 | train_loss=0.1089 | train_acc=0.6572 | val_acc=0.4929 | val_f1m=0.4927 | LR=0.000414\n",
      "  Época  44 | train_loss=0.1087 | train_acc=0.6633 | val_acc=0.4921 | val_f1m=0.4925 | LR=0.000393\n",
      "  Época  45 | train_loss=0.1044 | train_acc=0.6715 | val_acc=0.4929 | val_f1m=0.4937 | LR=0.000373\n",
      "  Early stopping en época 45 (mejor val_f1m=0.5057)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold2.png\n",
      "[Fold 2/5] Global acc=0.5805\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.6358    0.6531    0.6443       441\n",
      "       right     0.6569    0.6077    0.6313       441\n",
      "  both fists     0.4809    0.5986    0.5333       441\n",
      "   both feet     0.5763    0.4626    0.5132       441\n",
      "\n",
      "    accuracy                         0.5805      1764\n",
      "   macro avg     0.5874    0.5805    0.5805      1764\n",
      "weighted avg     0.5874    0.5805    0.5805      1764\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[288  27  94  32]\n",
      " [ 38 268  82  53]\n",
      " [ 68  44 264  65]\n",
      " [ 59  69 109 204]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold3: 100%|██████████| 67/67 [00:07<00:00,  8.47it/s]\n",
      "Cargando val fold3: 100%|██████████| 15/15 [00:01<00:00,  8.47it/s]\n",
      "Cargando test fold3: 100%|██████████| 21/21 [00:02<00:00,  8.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 3/5] Entrenando modelo global... (n_train=5628 | n_val=1260 | n_test=1764)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Época   1 | train_loss=0.2247 | train_acc=0.2742 | val_acc=0.2976 | val_f1m=0.2007 | LR=0.000125\n",
      "  Época   2 | train_loss=0.2040 | train_acc=0.3666 | val_acc=0.4468 | val_f1m=0.4450 | LR=0.000250\n",
      "  Época   3 | train_loss=0.1837 | train_acc=0.4536 | val_acc=0.4579 | val_f1m=0.4595 | LR=0.000375\n",
      "  Época   4 | train_loss=0.1757 | train_acc=0.4890 | val_acc=0.4603 | val_f1m=0.4628 | LR=0.000500\n",
      "  Época   5 | train_loss=0.1704 | train_acc=0.5011 | val_acc=0.4619 | val_f1m=0.4654 | LR=0.000625\n",
      "  Época   6 | train_loss=0.1704 | train_acc=0.5030 | val_acc=0.4714 | val_f1m=0.4752 | LR=0.000750\n",
      "  Época   7 | train_loss=0.1684 | train_acc=0.5126 | val_acc=0.4722 | val_f1m=0.4753 | LR=0.000875\n",
      "  Época   8 | train_loss=0.1651 | train_acc=0.5229 | val_acc=0.4810 | val_f1m=0.4851 | LR=0.001000\n",
      "  Época   9 | train_loss=0.1606 | train_acc=0.5444 | val_acc=0.4817 | val_f1m=0.4865 | LR=0.001000\n",
      "  Época  10 | train_loss=0.1603 | train_acc=0.5396 | val_acc=0.4881 | val_f1m=0.4926 | LR=0.000999\n",
      "  Época  11 | train_loss=0.1595 | train_acc=0.5378 | val_acc=0.4865 | val_f1m=0.4909 | LR=0.000997\n",
      "  Época  12 | train_loss=0.1555 | train_acc=0.5482 | val_acc=0.4873 | val_f1m=0.4920 | LR=0.000993\n",
      "  Época  13 | train_loss=0.1556 | train_acc=0.5466 | val_acc=0.4929 | val_f1m=0.4975 | LR=0.000988\n",
      "  Época  14 | train_loss=0.1569 | train_acc=0.5505 | val_acc=0.4937 | val_f1m=0.4988 | LR=0.000982\n",
      "  Época  15 | train_loss=0.1555 | train_acc=0.5485 | val_acc=0.4905 | val_f1m=0.4958 | LR=0.000974\n",
      "  Época  16 | train_loss=0.1509 | train_acc=0.5595 | val_acc=0.4921 | val_f1m=0.4973 | LR=0.000965\n",
      "  Época  17 | train_loss=0.1504 | train_acc=0.5609 | val_acc=0.4913 | val_f1m=0.4957 | LR=0.000954\n",
      "  Época  18 | train_loss=0.1459 | train_acc=0.5753 | val_acc=0.4897 | val_f1m=0.4942 | LR=0.000942\n",
      "  Época  19 | train_loss=0.1493 | train_acc=0.5727 | val_acc=0.4937 | val_f1m=0.4976 | LR=0.000929\n",
      "  Época  20 | train_loss=0.1484 | train_acc=0.5762 | val_acc=0.4952 | val_f1m=0.4999 | LR=0.000915\n",
      "  Época  21 | train_loss=0.1468 | train_acc=0.5629 | val_acc=0.4952 | val_f1m=0.4994 | LR=0.000899\n",
      "  Época  22 | train_loss=0.1414 | train_acc=0.5903 | val_acc=0.4952 | val_f1m=0.4989 | LR=0.000883\n",
      "  Época  23 | train_loss=0.1392 | train_acc=0.5943 | val_acc=0.4984 | val_f1m=0.5015 | LR=0.000865\n",
      "  Época  24 | train_loss=0.1417 | train_acc=0.5887 | val_acc=0.4992 | val_f1m=0.5023 | LR=0.000847\n",
      "  Época  25 | train_loss=0.1398 | train_acc=0.5913 | val_acc=0.5032 | val_f1m=0.5065 | LR=0.000827\n",
      "  Época  26 | train_loss=0.1380 | train_acc=0.5949 | val_acc=0.5024 | val_f1m=0.5054 | LR=0.000807\n",
      "  Época  27 | train_loss=0.1348 | train_acc=0.6114 | val_acc=0.5016 | val_f1m=0.5044 | LR=0.000786\n",
      "  Época  28 | train_loss=0.1302 | train_acc=0.6167 | val_acc=0.5024 | val_f1m=0.5056 | LR=0.000764\n",
      "  Época  29 | train_loss=0.1313 | train_acc=0.6205 | val_acc=0.5024 | val_f1m=0.5058 | LR=0.000742\n",
      "  Época  30 | train_loss=0.1325 | train_acc=0.6153 | val_acc=0.5016 | val_f1m=0.5045 | LR=0.000719\n",
      "  Época  31 | train_loss=0.1315 | train_acc=0.6166 | val_acc=0.4984 | val_f1m=0.5008 | LR=0.000696\n",
      "  Época  32 | train_loss=0.1302 | train_acc=0.6263 | val_acc=0.4944 | val_f1m=0.4967 | LR=0.000672\n",
      "  Época  33 | train_loss=0.1238 | train_acc=0.6400 | val_acc=0.4952 | val_f1m=0.4975 | LR=0.000648\n",
      "  Época  34 | train_loss=0.1254 | train_acc=0.6276 | val_acc=0.4937 | val_f1m=0.4957 | LR=0.000624\n",
      "  Época  35 | train_loss=0.1211 | train_acc=0.6437 | val_acc=0.4897 | val_f1m=0.4912 | LR=0.000600\n",
      "  Época  36 | train_loss=0.1196 | train_acc=0.6624 | val_acc=0.4921 | val_f1m=0.4935 | LR=0.000576\n",
      "  Época  37 | train_loss=0.1164 | train_acc=0.6585 | val_acc=0.4944 | val_f1m=0.4958 | LR=0.000552\n",
      "  Early stopping en época 37 (mejor val_f1m=0.5065)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold3.png\n",
      "[Fold 3/5] Global acc=0.5006\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.5721    0.5215    0.5457       441\n",
      "       right     0.6531    0.5465    0.5951       441\n",
      "  both fists     0.3859    0.5329    0.4476       441\n",
      "   both feet     0.4609    0.4014    0.4291       441\n",
      "\n",
      "    accuracy                         0.5006      1764\n",
      "   macro avg     0.5180    0.5006    0.5044      1764\n",
      "weighted avg     0.5180    0.5006    0.5044      1764\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[230  25 128  58]\n",
      " [ 23 241 104  73]\n",
      " [ 85  45 235  76]\n",
      " [ 64  58 142 177]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold4: 100%|██████████| 68/68 [00:08<00:00,  8.42it/s]\n",
      "Cargando val fold4: 100%|██████████| 15/15 [00:01<00:00,  8.44it/s]\n",
      "Cargando test fold4: 100%|██████████| 20/20 [00:02<00:00,  8.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 4/5] Entrenando modelo global... (n_train=5712 | n_val=1260 | n_test=1680)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Época   1 | train_loss=0.2243 | train_acc=0.2635 | val_acc=0.3310 | val_f1m=0.2866 | LR=0.000125\n",
      "  Época   2 | train_loss=0.2013 | train_acc=0.3853 | val_acc=0.4270 | val_f1m=0.4305 | LR=0.000250\n",
      "  Época   3 | train_loss=0.1904 | train_acc=0.4268 | val_acc=0.4397 | val_f1m=0.4423 | LR=0.000375\n",
      "  Época   4 | train_loss=0.1795 | train_acc=0.4758 | val_acc=0.4429 | val_f1m=0.4452 | LR=0.000500\n",
      "  Época   5 | train_loss=0.1781 | train_acc=0.4855 | val_acc=0.4579 | val_f1m=0.4601 | LR=0.000625\n",
      "  Época   6 | train_loss=0.1710 | train_acc=0.4979 | val_acc=0.4587 | val_f1m=0.4610 | LR=0.000750\n",
      "  Época   7 | train_loss=0.1742 | train_acc=0.4911 | val_acc=0.4754 | val_f1m=0.4777 | LR=0.000875\n",
      "  Época   8 | train_loss=0.1716 | train_acc=0.5004 | val_acc=0.4810 | val_f1m=0.4834 | LR=0.001000\n",
      "  Época   9 | train_loss=0.1642 | train_acc=0.5165 | val_acc=0.4857 | val_f1m=0.4877 | LR=0.001000\n",
      "  Época  10 | train_loss=0.1656 | train_acc=0.5114 | val_acc=0.4865 | val_f1m=0.4880 | LR=0.000999\n",
      "  Época  11 | train_loss=0.1626 | train_acc=0.5226 | val_acc=0.4897 | val_f1m=0.4911 | LR=0.000997\n",
      "  Época  12 | train_loss=0.1643 | train_acc=0.5201 | val_acc=0.4849 | val_f1m=0.4867 | LR=0.000993\n",
      "  Época  13 | train_loss=0.1604 | train_acc=0.5264 | val_acc=0.4825 | val_f1m=0.4849 | LR=0.000988\n",
      "  Época  14 | train_loss=0.1571 | train_acc=0.5410 | val_acc=0.4865 | val_f1m=0.4888 | LR=0.000982\n",
      "  Época  15 | train_loss=0.1586 | train_acc=0.5382 | val_acc=0.4921 | val_f1m=0.4943 | LR=0.000974\n",
      "  Época  16 | train_loss=0.1589 | train_acc=0.5387 | val_acc=0.4960 | val_f1m=0.4983 | LR=0.000965\n",
      "  Época  17 | train_loss=0.1535 | train_acc=0.5469 | val_acc=0.5008 | val_f1m=0.5025 | LR=0.000954\n",
      "  Época  18 | train_loss=0.1539 | train_acc=0.5480 | val_acc=0.5024 | val_f1m=0.5041 | LR=0.000942\n",
      "  Época  19 | train_loss=0.1532 | train_acc=0.5501 | val_acc=0.5056 | val_f1m=0.5070 | LR=0.000929\n",
      "  Época  20 | train_loss=0.1559 | train_acc=0.5425 | val_acc=0.5063 | val_f1m=0.5077 | LR=0.000915\n",
      "  Época  21 | train_loss=0.1482 | train_acc=0.5625 | val_acc=0.5127 | val_f1m=0.5135 | LR=0.000899\n",
      "  Época  22 | train_loss=0.1470 | train_acc=0.5692 | val_acc=0.5079 | val_f1m=0.5092 | LR=0.000883\n",
      "  Época  23 | train_loss=0.1511 | train_acc=0.5511 | val_acc=0.5087 | val_f1m=0.5101 | LR=0.000865\n",
      "  Época  24 | train_loss=0.1459 | train_acc=0.5776 | val_acc=0.5087 | val_f1m=0.5098 | LR=0.000847\n",
      "  Época  25 | train_loss=0.1424 | train_acc=0.5854 | val_acc=0.5063 | val_f1m=0.5071 | LR=0.000827\n",
      "  Época  26 | train_loss=0.1400 | train_acc=0.5903 | val_acc=0.5095 | val_f1m=0.5103 | LR=0.000807\n",
      "  Época  27 | train_loss=0.1428 | train_acc=0.5795 | val_acc=0.5143 | val_f1m=0.5146 | LR=0.000786\n",
      "  Época  28 | train_loss=0.1389 | train_acc=0.5979 | val_acc=0.5127 | val_f1m=0.5134 | LR=0.000764\n",
      "  Época  29 | train_loss=0.1382 | train_acc=0.5991 | val_acc=0.5167 | val_f1m=0.5171 | LR=0.000742\n",
      "  Época  30 | train_loss=0.1370 | train_acc=0.5888 | val_acc=0.5183 | val_f1m=0.5188 | LR=0.000719\n",
      "  Época  31 | train_loss=0.1369 | train_acc=0.6010 | val_acc=0.5135 | val_f1m=0.5144 | LR=0.000696\n",
      "  Época  32 | train_loss=0.1272 | train_acc=0.6169 | val_acc=0.5111 | val_f1m=0.5118 | LR=0.000672\n",
      "  Época  33 | train_loss=0.1316 | train_acc=0.6045 | val_acc=0.5151 | val_f1m=0.5157 | LR=0.000648\n",
      "  Época  34 | train_loss=0.1276 | train_acc=0.6269 | val_acc=0.5143 | val_f1m=0.5151 | LR=0.000624\n",
      "  Época  35 | train_loss=0.1251 | train_acc=0.6336 | val_acc=0.5127 | val_f1m=0.5133 | LR=0.000600\n",
      "  Época  36 | train_loss=0.1228 | train_acc=0.6369 | val_acc=0.5071 | val_f1m=0.5079 | LR=0.000576\n",
      "  Época  37 | train_loss=0.1238 | train_acc=0.6246 | val_acc=0.5087 | val_f1m=0.5094 | LR=0.000552\n",
      "  Época  38 | train_loss=0.1198 | train_acc=0.6448 | val_acc=0.5032 | val_f1m=0.5036 | LR=0.000528\n",
      "  Época  39 | train_loss=0.1195 | train_acc=0.6451 | val_acc=0.5040 | val_f1m=0.5039 | LR=0.000504\n",
      "  Época  40 | train_loss=0.1155 | train_acc=0.6609 | val_acc=0.5040 | val_f1m=0.5040 | LR=0.000481\n",
      "  Época  41 | train_loss=0.1152 | train_acc=0.6514 | val_acc=0.5040 | val_f1m=0.5044 | LR=0.000458\n",
      "  Época  42 | train_loss=0.1077 | train_acc=0.6632 | val_acc=0.5032 | val_f1m=0.5034 | LR=0.000436\n",
      "  Early stopping en época 42 (mejor val_f1m=0.5188)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold4.png\n",
      "[Fold 4/5] Global acc=0.5238\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.6657    0.5643    0.6108       420\n",
      "       right     0.5568    0.5833    0.5698       420\n",
      "  both fists     0.4186    0.5881    0.4891       420\n",
      "   both feet     0.5136    0.3595    0.4230       420\n",
      "\n",
      "    accuracy                         0.5238      1680\n",
      "   macro avg     0.5387    0.5238    0.5232      1680\n",
      "weighted avg     0.5387    0.5238    0.5232      1680\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[237  23 117  43]\n",
      " [ 16 245 100  59]\n",
      " [ 54  78 247  41]\n",
      " [ 49  94 126 151]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold5: 100%|██████████| 68/68 [00:08<00:00,  8.42it/s]\n",
      "Cargando val fold5: 100%|██████████| 15/15 [00:01<00:00,  8.49it/s]\n",
      "Cargando test fold5: 100%|██████████| 20/20 [00:02<00:00,  8.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 5/5] Entrenando modelo global... (n_train=5712 | n_val=1260 | n_test=1680)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Época   1 | train_loss=0.2241 | train_acc=0.2742 | val_acc=0.3183 | val_f1m=0.2769 | LR=0.000125\n",
      "  Época   2 | train_loss=0.2039 | train_acc=0.3699 | val_acc=0.4238 | val_f1m=0.4264 | LR=0.000250\n",
      "  Época   3 | train_loss=0.1856 | train_acc=0.4508 | val_acc=0.4206 | val_f1m=0.4102 | LR=0.000375\n",
      "  Época   4 | train_loss=0.1807 | train_acc=0.4624 | val_acc=0.4238 | val_f1m=0.4147 | LR=0.000500\n",
      "  Época   5 | train_loss=0.1752 | train_acc=0.4933 | val_acc=0.4254 | val_f1m=0.4166 | LR=0.000625\n",
      "  Época   6 | train_loss=0.1775 | train_acc=0.4855 | val_acc=0.4278 | val_f1m=0.4172 | LR=0.000750\n",
      "  Época   7 | train_loss=0.1736 | train_acc=0.4956 | val_acc=0.4373 | val_f1m=0.4288 | LR=0.000875\n",
      "  Época   8 | train_loss=0.1700 | train_acc=0.4986 | val_acc=0.4413 | val_f1m=0.4329 | LR=0.001000\n",
      "  Época   9 | train_loss=0.1673 | train_acc=0.5130 | val_acc=0.4317 | val_f1m=0.4234 | LR=0.001000\n",
      "  Época  10 | train_loss=0.1647 | train_acc=0.5191 | val_acc=0.4333 | val_f1m=0.4257 | LR=0.000999\n",
      "  Época  11 | train_loss=0.1629 | train_acc=0.5315 | val_acc=0.4341 | val_f1m=0.4263 | LR=0.000997\n",
      "  Época  12 | train_loss=0.1583 | train_acc=0.5432 | val_acc=0.4468 | val_f1m=0.4401 | LR=0.000993\n",
      "  Época  13 | train_loss=0.1599 | train_acc=0.5357 | val_acc=0.4476 | val_f1m=0.4409 | LR=0.000988\n",
      "  Época  14 | train_loss=0.1580 | train_acc=0.5453 | val_acc=0.4500 | val_f1m=0.4432 | LR=0.000982\n",
      "  Época  15 | train_loss=0.1540 | train_acc=0.5487 | val_acc=0.4524 | val_f1m=0.4461 | LR=0.000974\n",
      "  Época  16 | train_loss=0.1548 | train_acc=0.5478 | val_acc=0.4524 | val_f1m=0.4484 | LR=0.000965\n",
      "  Época  17 | train_loss=0.1545 | train_acc=0.5513 | val_acc=0.4579 | val_f1m=0.4542 | LR=0.000954\n",
      "  Época  18 | train_loss=0.1517 | train_acc=0.5518 | val_acc=0.4595 | val_f1m=0.4566 | LR=0.000942\n",
      "  Época  19 | train_loss=0.1514 | train_acc=0.5590 | val_acc=0.4643 | val_f1m=0.4612 | LR=0.000929\n",
      "  Época  20 | train_loss=0.1442 | train_acc=0.5826 | val_acc=0.4635 | val_f1m=0.4604 | LR=0.000915\n",
      "  Época  21 | train_loss=0.1467 | train_acc=0.5672 | val_acc=0.4643 | val_f1m=0.4615 | LR=0.000899\n",
      "  Época  22 | train_loss=0.1501 | train_acc=0.5551 | val_acc=0.4698 | val_f1m=0.4667 | LR=0.000883\n",
      "  Época  23 | train_loss=0.1419 | train_acc=0.5825 | val_acc=0.4683 | val_f1m=0.4657 | LR=0.000865\n",
      "  Época  24 | train_loss=0.1423 | train_acc=0.5819 | val_acc=0.4683 | val_f1m=0.4656 | LR=0.000847\n",
      "  Época  25 | train_loss=0.1446 | train_acc=0.5805 | val_acc=0.4706 | val_f1m=0.4678 | LR=0.000827\n",
      "  Época  26 | train_loss=0.1398 | train_acc=0.5933 | val_acc=0.4706 | val_f1m=0.4675 | LR=0.000807\n",
      "  Época  27 | train_loss=0.1422 | train_acc=0.5851 | val_acc=0.4698 | val_f1m=0.4672 | LR=0.000786\n",
      "  Época  28 | train_loss=0.1335 | train_acc=0.6024 | val_acc=0.4762 | val_f1m=0.4733 | LR=0.000764\n",
      "  Época  29 | train_loss=0.1323 | train_acc=0.6014 | val_acc=0.4730 | val_f1m=0.4692 | LR=0.000742\n",
      "  Época  30 | train_loss=0.1310 | train_acc=0.6133 | val_acc=0.4754 | val_f1m=0.4721 | LR=0.000719\n",
      "  Época  31 | train_loss=0.1315 | train_acc=0.6021 | val_acc=0.4722 | val_f1m=0.4690 | LR=0.000696\n",
      "  Época  32 | train_loss=0.1259 | train_acc=0.6238 | val_acc=0.4746 | val_f1m=0.4718 | LR=0.000672\n",
      "  Época  33 | train_loss=0.1266 | train_acc=0.6218 | val_acc=0.4730 | val_f1m=0.4701 | LR=0.000648\n",
      "  Época  34 | train_loss=0.1217 | train_acc=0.6395 | val_acc=0.4698 | val_f1m=0.4675 | LR=0.000624\n",
      "  Época  35 | train_loss=0.1236 | train_acc=0.6275 | val_acc=0.4698 | val_f1m=0.4679 | LR=0.000600\n",
      "  Época  36 | train_loss=0.1201 | train_acc=0.6289 | val_acc=0.4738 | val_f1m=0.4714 | LR=0.000576\n",
      "  Época  37 | train_loss=0.1189 | train_acc=0.6383 | val_acc=0.4730 | val_f1m=0.4709 | LR=0.000552\n",
      "  Época  38 | train_loss=0.1161 | train_acc=0.6436 | val_acc=0.4722 | val_f1m=0.4704 | LR=0.000528\n",
      "  Época  39 | train_loss=0.1086 | train_acc=0.6630 | val_acc=0.4635 | val_f1m=0.4617 | LR=0.000504\n",
      "  Época  40 | train_loss=0.1118 | train_acc=0.6619 | val_acc=0.4611 | val_f1m=0.4596 | LR=0.000481\n",
      "  Early stopping en época 40 (mejor val_f1m=0.4733)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold5.png\n",
      "[Fold 5/5] Global acc=0.5601\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.6463    0.5786    0.6106       420\n",
      "       right     0.6281    0.6071    0.6174       420\n",
      "  both fists     0.4380    0.6143    0.5114       420\n",
      "   both feet     0.5987    0.4405    0.5075       420\n",
      "\n",
      "    accuracy                         0.5601      1680\n",
      "   macro avg     0.5778    0.5601    0.5617      1680\n",
      "weighted avg     0.5778    0.5601    0.5617      1680\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[243  24 113  40]\n",
      " [ 23 255  93  49]\n",
      " [ 66  61 258  35]\n",
      " [ 44  66 125 185]]\n",
      "\n",
      "============================================================\n",
      "RESULTADOS FINALES\n",
      "============================================================\n",
      "Global folds (ACC): ['0.4943', '0.5805', '0.5006', '0.5238', '0.5601']\n",
      "Global mean ACC: 0.5319\n",
      "F1 folds (MACRO): ['0.4991', '0.5805', '0.5044', '0.5232', '0.5617']\n",
      "F1 mean (MACRO): 0.5338\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# CNN+Transformer para MI (4 clases, 8 canales) con:\n",
    "# (1) Focal Loss\n",
    "# (4) Aumentos más fuertes y estables\n",
    "# (6) LR warmup+cosine, early stopping por F1 macro\n",
    "# Cambios propuestos:\n",
    "# - GroupNorm en conv (en lugar de BN)\n",
    "# - EMA de pesos para val/test\n",
    "# - Balanced sampler por sujeto/clase (WeightedRandomSampler)\n",
    "# - Dropout antes del encoder = 0.3\n",
    "# - Cosine min_factor=0.2\n",
    "# - Ensemble TTA + Subwindows en test (opcional)\n",
    "# - Val estratificada por sujeto (opcional)\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "import re, json, random\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import mne\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# =========================\n",
    "# REPRODUCIBILIDAD\n",
    "# =========================\n",
    "RANDOM_STATE = 42\n",
    "def seed_everything(seed: int = 42):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def seed_worker(worker_id: int):\n",
    "    worker_seed = RANDOM_STATE + worker_id\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "seed_everything(RANDOM_STATE)\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "FOLDS_JSON = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 64        # múltiplo de 4\n",
    "BASE_LR = 1e-3\n",
    "WARMUP_EPOCHS = 8\n",
    "PATIENCE = 12\n",
    "\n",
    "# Split de validación por fold (por sujetos)\n",
    "VAL_SUBJECT_FRAC = 0.18  # ≈ 18% de sujetos del train → val\n",
    "VAL_STRAT_SUBJECT = True # <- ACTIVADO: estratifica por etiqueta dominante de cada sujeto\n",
    "\n",
    "# Prepro\n",
    "RESAMPLE_HZ = None\n",
    "DO_NOTCH = True\n",
    "DO_BANDPASS = False       # dejamos apagado para respetar tu setup original\n",
    "BP_LO, BP_HI = 4.0, 38.0\n",
    "DO_CAR = False\n",
    "ZSCORE_PER_EPOCH = False\n",
    "\n",
    "# Modelo\n",
    "D_MODEL = 128\n",
    "N_HEADS = 4\n",
    "N_LAYERS = 2\n",
    "P_DROP = 0.2            # dropout en conv stack\n",
    "P_DROP_ENCODER = 0.3    # dropout antes del encoder (↑ respecto a 0.2)\n",
    "\n",
    "# Ventana temporal\n",
    "TMIN, TMAX = -1.0, 5.0\n",
    "\n",
    "# TTA / SUBWINDOW en TEST\n",
    "SW_MODE = 'tta'   # 'none'|'subwin'|'tta'\n",
    "SW_ENABLE = True\n",
    "TTA_SHIFTS_S = [-0.05, -0.025, 0.0, 0.025, 0.05]\n",
    "SW_LEN, SW_STRIDE = 4.5, 2.0\n",
    "COMBINE_TTA_AND_SUBWIN = False  # Promedia logits TTA y Subwindow (50/50) si True\n",
    "\n",
    "# Sampler balanceado\n",
    "USE_WEIGHTED_SAMPLER = True\n",
    "\n",
    "# EMA\n",
    "USE_EMA = True\n",
    "EMA_DECAY = 0.999  # 0.999 ~ suave y efectivo\n",
    "\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','FCz']\n",
    "CLASS_NAMES = ['left', 'right', 'both_fists', 'both_feet']\n",
    "\n",
    "IMAGERY_RUNS_LR = {4, 8, 12}\n",
    "IMAGERY_RUNS_BF = {6, 10, 14}\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Usando dispositivo: {DEVICE}\")\n",
    "print(\"🧠 INICIANDO EXPERIMENTO CON CNN+Transformer (K-Fold por sujeto como EEGNet)\")\n",
    "print(f\"🔧 Configuración: 4c, 8 canales, 6s | EPOCHS={EPOCHS}, BATCH={BATCH_SIZE}, LR={BASE_LR} | ZSCORE_PER_EPOCH={ZSCORE_PER_EPOCH}\")\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES I/O\n",
    "# =========================\n",
    "def normalize_ch_name(name: str) -> str:\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', name)\n",
    "    return s.upper()\n",
    "\n",
    "NORMALIZED_TARGETS = [normalize_ch_name(c) for c in EXPECTED_8]\n",
    "\n",
    "def pick_8_channels(raw: mne.io.BaseRaw) -> mne.io.BaseRaw:\n",
    "    chs = raw.info['ch_names']\n",
    "    norm_map = {normalize_ch_name(ch): ch for ch in chs}\n",
    "    picked = []\n",
    "    for target_norm, target_orig in zip(NORMALIZED_TARGETS, EXPECTED_8):\n",
    "        if target_norm in norm_map:\n",
    "            picked.append(norm_map[target_norm])\n",
    "        else:\n",
    "            raise RuntimeError(f\"Canal requerido '{target_orig}' no encontrado. Disponibles: {chs}\")\n",
    "    return raw.pick(picks=picked)\n",
    "\n",
    "def list_subject_imagery_edfs(subject_id: str) -> list:\n",
    "    subj_dir = DATA_RAW / subject_id\n",
    "    edfs = []\n",
    "    for r in [4, 6, 8, 10, 12, 14]:\n",
    "        edfs.extend(glob(str(subj_dir / f\"{subject_id}R{r:02d}.edf\")))\n",
    "    return sorted(edfs)\n",
    "\n",
    "def subject_id_to_int(s: str) -> int:\n",
    "    m = re.match(r'[Ss](\\d+)', s)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "def load_subject_epochs(subject_id: str, resample_hz: int, do_notch: bool, do_bandpass: bool,\n",
    "                        do_car: bool, bp_lo: float, bp_hi: float):\n",
    "    edfs = list_subject_imagery_edfs(subject_id)\n",
    "    if len(edfs) == 0:\n",
    "        return np.empty((0,8,1), dtype=np.float32), np.empty((0,), dtype=int), None\n",
    "\n",
    "    X_list, y_list, sfreq_list = [], [], []\n",
    "\n",
    "    for edf_path in edfs:\n",
    "        m = re.search(r\"R(\\d{2})\", Path(edf_path).name)\n",
    "        run = int(m.group(1)) if m else -1\n",
    "\n",
    "        raw = mne.io.read_raw_edf(edf_path, preload=True, verbose='ERROR')\n",
    "        raw = pick_8_channels(raw)\n",
    "\n",
    "        if do_notch:\n",
    "            raw.notch_filter(freqs=[60.0], picks='all', verbose='ERROR')\n",
    "        if do_bandpass:\n",
    "            raw.filter(l_freq=bp_lo, h_freq=bp_hi, picks='all', verbose='ERROR')\n",
    "        if do_car:\n",
    "            raw.set_eeg_reference('average', projection=False, verbose='ERROR')\n",
    "\n",
    "        if resample_hz is not None and resample_hz > 0:\n",
    "            raw.resample(resample_hz)\n",
    "        sfreq = raw.info['sfreq']\n",
    "\n",
    "        events, event_id = mne.events_from_annotations(raw, verbose='ERROR')\n",
    "        keep = {k: v for k, v in event_id.items() if k in {'T1', 'T2'}}\n",
    "        if len(keep) == 0:\n",
    "            continue\n",
    "\n",
    "        epochs = mne.Epochs(raw, events=events, event_id=keep, tmin=TMIN, tmax=TMAX,\n",
    "                            baseline=None, preload=True, verbose='ERROR')\n",
    "        X = epochs.get_data()\n",
    "\n",
    "        if ZSCORE_PER_EPOCH:\n",
    "            X = X.astype(np.float32)\n",
    "            eps = 1e-6\n",
    "            mu = X.mean(axis=2, keepdims=True)\n",
    "            sd = X.std(axis=2, keepdims=True) + eps\n",
    "            X = (X - mu) / sd\n",
    "\n",
    "        ev_codes = epochs.events[:, 2]\n",
    "        inv = {v: k for k, v in keep.items()}\n",
    "        y_run = []\n",
    "        for code in ev_codes:\n",
    "            lab = inv[code]\n",
    "            if run in IMAGERY_RUNS_LR:\n",
    "                y_run.append(0 if lab == 'T1' else 1)\n",
    "            elif run in IMAGERY_RUNS_BF:\n",
    "                y_run.append(2 if lab == 'T1' else 3)\n",
    "            else:\n",
    "                y_run.append(-1)\n",
    "        y_run = np.array(y_run, dtype=int)\n",
    "        mask = y_run >= 0\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        X_list.append(X[mask])\n",
    "        y_list.append(y_run[mask])\n",
    "        sfreq_list.append(sfreq)\n",
    "\n",
    "    if len(X_list) == 0:\n",
    "        return np.empty((0,8,1), dtype=np.float32), np.empty((0,), dtype=int), None\n",
    "\n",
    "    X_all = np.concatenate(X_list, axis=0).astype(np.float32)\n",
    "    y_all = np.concatenate(y_list, axis=0).astype(int)\n",
    "\n",
    "    if len(set([int(round(s)) for s in sfreq_list])) != 1:\n",
    "        raise RuntimeError(f\"Sampling rates inconsistentes: {sfreq_list}\")\n",
    "\n",
    "    return X_all, y_all, sfreq_list[0]\n",
    "\n",
    "def load_fold_subjects(folds_json: Path, fold: int):\n",
    "    with open(folds_json, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    for item in data.get('folds', []):\n",
    "        if int(item.get('fold', -1)) == int(fold):\n",
    "            return list(item.get('train', [])), list(item.get('test', []))\n",
    "    raise ValueError(f\"Fold {fold} not found in {folds_json}\")\n",
    "\n",
    "def standardize_per_channel(train_X, other_X):\n",
    "    C = train_X.shape[1]\n",
    "    train_X = train_X.astype(np.float32)\n",
    "    other_X = other_X.astype(np.float32)\n",
    "    for c in range(C):\n",
    "        mu = train_X[:, c, :].mean()\n",
    "        sd = train_X[:, c, :].std()\n",
    "        sd = sd if sd > 1e-6 else 1.0\n",
    "        train_X[:, c, :] = (train_X[:, c, :] - mu) / sd\n",
    "        other_X[:, c, :]  = (other_X[:, c, :] - mu) / sd\n",
    "    return train_X, other_X\n",
    "\n",
    "# =========================\n",
    "# MODELO (GroupNorm en conv)\n",
    "# =========================\n",
    "def make_gn(num_channels, num_groups=8):\n",
    "    # Ajusta grupos para que dividan a num_channels\n",
    "    g = min(num_groups, num_channels)\n",
    "    while num_channels % g != 0 and g > 1:\n",
    "        g -= 1\n",
    "    return nn.GroupNorm(g, num_channels)\n",
    "\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k, s=1, p=0, p_drop=0.2):\n",
    "        super().__init__()\n",
    "        self.dw = nn.Conv1d(in_ch, in_ch, kernel_size=k, stride=s, padding=p, groups=in_ch, bias=False)\n",
    "        self.pw = nn.Conv1d(in_ch, out_ch, kernel_size=1, bias=False)\n",
    "        self.norm = make_gn(out_ch)\n",
    "        self.act = nn.ELU()\n",
    "        self.dropout = nn.Dropout(p=p_drop)\n",
    "    def forward(self, x):\n",
    "        x = self.dw(x); x = self.pw(x); x = self.norm(x)\n",
    "        x = self.act(x); x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class EEGCNNTransformer(nn.Module):\n",
    "    def __init__(self, n_ch=8, n_cls=4, d_model=128, n_heads=4, n_layers=2,\n",
    "                 p_drop=0.2, p_drop_encoder=0.3):\n",
    "        super().__init__()\n",
    "        self.conv_t = nn.Sequential(\n",
    "            nn.Conv1d(n_ch, 32, kernel_size=129, stride=2, padding=64, bias=False),\n",
    "            make_gn(32),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=p_drop),\n",
    "            DepthwiseSeparableConv(32, 64, k=31, s=2, p=15, p_drop=p_drop),\n",
    "            DepthwiseSeparableConv(64, 128, k=15, s=2, p=7,  p_drop=p_drop),\n",
    "        )\n",
    "        self.proj = nn.Conv1d(128, d_model, kernel_size=1, bias=False)\n",
    "        self.dropout = nn.Dropout(p=p_drop_encoder)  # ↑ 0.3\n",
    "        self.pos_encoding = None\n",
    "        enc = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=2*d_model,\n",
    "            batch_first=True, activation='gelu', dropout=0.1, norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc, num_layers=n_layers)\n",
    "        self.cls = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        nn.init.normal_(self.cls, std=0.02)\n",
    "        self.head = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, n_cls))\n",
    "\n",
    "    def _positional_encoding(self, L, d):\n",
    "        pos = torch.arange(0, L, dtype=torch.float32).unsqueeze(1)\n",
    "        i   = torch.arange(0, d, dtype=torch.float32).unsqueeze(0)\n",
    "        angle = pos / torch.pow(10000, (2 * (i//2)) / d)\n",
    "        pe = torch.zeros(L, d, dtype=torch.float32)\n",
    "        pe[:, 0::2] = torch.sin(angle[:, 0::2])\n",
    "        pe[:, 1::2] = torch.cos(angle[:, 1::2])\n",
    "        return pe\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.conv_t(x)           # (B, 128, T')\n",
    "        z = self.proj(z)             # (B, d_model, T')\n",
    "        z = self.dropout(z)\n",
    "        z = z.transpose(1, 2)        # (B, T', d_model)\n",
    "        B, L, D = z.shape\n",
    "        if (self.pos_encoding is None) or (self.pos_encoding.shape[0] != L) or (self.pos_encoding.shape[1] != D):\n",
    "            self.pos_encoding = self._positional_encoding(L, D).to(z.device)\n",
    "        z = z + self.pos_encoding[None, :, :]\n",
    "        cls_tok = self.cls.expand(B, -1, -1)\n",
    "        z = torch.cat([cls_tok, z], dim=1)\n",
    "        z = self.encoder(z)\n",
    "        cls = z[:, 0, :]\n",
    "        return self.head(cls)\n",
    "\n",
    "# =========================\n",
    "# (1) FOCAL LOSS\n",
    "# =========================\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha: torch.Tensor, gamma: float = 1.5, reduction: str = 'mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha / alpha.sum()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    def forward(self, logits, target):\n",
    "        logp = nn.functional.log_softmax(logits, dim=-1)      # (B,C)\n",
    "        p = logp.exp()\n",
    "        idx = torch.arange(target.shape[0], device=logits.device)\n",
    "        pt = p[idx, target]\n",
    "        logpt = logp[idx, target]\n",
    "        at = self.alpha[target]\n",
    "        loss = - at * ((1 - pt) ** self.gamma) * logpt\n",
    "        if self.reduction == 'mean': return loss.mean()\n",
    "        if self.reduction == 'sum':  return loss.sum()\n",
    "        return loss\n",
    "\n",
    "# =========================\n",
    "# (4) AUGMENTS MÁS FUERTES (pero estables)\n",
    "# =========================\n",
    "def augment_batch(\n",
    "    xb,\n",
    "    p_jitter=0.35, p_noise=0.35, p_chdrop=0.15,\n",
    "    max_jitter_frac=0.03, noise_std=0.03, max_chdrop=1\n",
    "):\n",
    "    \"\"\"\n",
    "    - jitter: shift temporal pequeño (±3% de la longitud)\n",
    "    - noise: ruido gaussiano leve\n",
    "    - chdrop: apagado de 1 canal aleatorio\n",
    "    \"\"\"\n",
    "    B, C, T = xb.shape\n",
    "    if np.random.rand() < p_jitter:\n",
    "        max_shift = int(max(1, T*max_jitter_frac))\n",
    "        shifts = torch.randint(low=-max_shift, high=max_shift+1, size=(B,), device=xb.device)\n",
    "        for i in range(B):\n",
    "            xb[i] = torch.roll(xb[i], shifts=int(shifts[i].item()), dims=-1)\n",
    "    if np.random.rand() < p_noise:\n",
    "        xb = xb + noise_std*torch.randn_like(xb)\n",
    "    if np.random.rand() < p_chdrop and max_chdrop > 0:\n",
    "        k = min(max_chdrop, C)\n",
    "        for i in range(B):\n",
    "            idx = torch.randperm(C, device=xb.device)[:k]\n",
    "            xb[i, idx, :] = 0.0\n",
    "    return xb\n",
    "\n",
    "# =========================\n",
    "# EMA de pesos\n",
    "# =========================\n",
    "class ModelEMA:\n",
    "    def __init__(self, model: nn.Module, decay: float = 0.999, device=None):\n",
    "        self.ema = self._clone(model).to(device if device is not None else next(model.parameters()).device)\n",
    "        self.decay = decay\n",
    "        self._updates = 0\n",
    "        self.update(model, force=True)\n",
    "\n",
    "    def _clone(self, model):\n",
    "        ema = type(model)()\n",
    "        ema.load_state_dict(model.state_dict())\n",
    "        for p in ema.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        return ema\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model: nn.Module, force: bool = False):\n",
    "        d = self.decay\n",
    "        if self._updates < 200:\n",
    "            # warmup EMA decay para primeras iteraciones\n",
    "            d = 0.0 + (self._updates / 200.0) * self.decay\n",
    "        msd = model.state_dict()\n",
    "        esd = self.ema.state_dict()\n",
    "        for k in esd.keys():\n",
    "            if esd[k].dtype.is_floating_point:\n",
    "                esd[k].mul_(d).add_(msd[k].detach(), alpha=1.0 - d)\n",
    "            else:\n",
    "                esd[k] = msd[k]\n",
    "        self._updates += 1\n",
    "\n",
    "# =========================\n",
    "# INFERENCIA TTA / SUBWINDOW\n",
    "# =========================\n",
    "def subwindow_logits(model, X, sfreq, sw_len, sw_stride, device):\n",
    "    model.eval()\n",
    "    wl = int(round(sw_len * sfreq))\n",
    "    st = int(round(sw_stride * sfreq))\n",
    "    wl = max(1, min(wl, X.shape[-1])); st = max(1, st)\n",
    "    out = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(X.shape[0]):\n",
    "            x = X[i]; acc = []\n",
    "            for s in range(0, max(1, X.shape[-1]-wl+1), st):\n",
    "                seg = x[:, s:s+wl]\n",
    "                if seg.shape[-1] < wl:\n",
    "                    pad = wl - seg.shape[-1]\n",
    "                    seg = np.pad(seg, ((0,0),(0,pad)), mode='edge')\n",
    "                xb = torch.tensor(seg[None, ...], dtype=torch.float32, device=device)\n",
    "                logit = model(xb).detach().cpu().numpy()[0]\n",
    "                acc.append(logit)\n",
    "            acc = np.mean(np.stack(acc, axis=0), axis=0) if len(acc) else np.zeros(4, dtype=np.float32)\n",
    "            out.append(acc)\n",
    "    return np.stack(out, axis=0)\n",
    "\n",
    "def time_shift_tta_logits(model, X, sfreq, shifts_s, device):\n",
    "    model.eval()\n",
    "    T = X.shape[-1]; out = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(X.shape[0]):\n",
    "            x0 = X[i]; acc = []\n",
    "            for sh in shifts_s:\n",
    "                shift = int(round(sh * sfreq))\n",
    "                if shift == 0:\n",
    "                    x = x0\n",
    "                elif shift > 0:\n",
    "                    x = np.pad(x0[:, shift:], ((0,0),(0,shift)), mode='edge')[:, :T]\n",
    "                else:\n",
    "                    shift = -shift\n",
    "                    x = np.pad(x0[:, :-shift], ((0,0),(shift,0)), mode='edge')[:, :T]\n",
    "                xb = torch.tensor(x[None, ...], dtype=torch.float32, device=device)\n",
    "                logit = model(xb).detach().cpu().numpy()[0]\n",
    "                acc.append(logit)\n",
    "            out.append(np.mean(np.stack(acc, axis=0), axis=0))\n",
    "    return np.stack(out, axis=0)\n",
    "\n",
    "# =========================\n",
    "# Utilidades splits estratificados por sujeto\n",
    "# =========================\n",
    "def build_subject_label_map(subject_ids):\n",
    "    \"\"\"\n",
    "    Estima una etiqueta dominante por sujeto (argmax de su histograma de clases).\n",
    "    Se usa sólo para estratificar la selección de sujetos de validación.\n",
    "    \"\"\"\n",
    "    y_dom_list = []\n",
    "    for sid in subject_ids:\n",
    "        Xs, ys, _ = load_subject_epochs(sid, RESAMPLE_HZ, DO_NOTCH, DO_BANDPASS, DO_CAR, BP_LO, BP_HI)\n",
    "        if len(ys) == 0:\n",
    "            y_dom_list.append(-1)\n",
    "            continue\n",
    "        binc = np.bincount(ys, minlength=4)\n",
    "        y_dom = int(np.argmax(binc))\n",
    "        y_dom_list.append(y_dom)\n",
    "    return np.array(y_dom_list, dtype=int)\n",
    "\n",
    "# =========================\n",
    "# TRAIN/EVAL por FOLD (con train/val/test)\n",
    "# =========================\n",
    "def train_one_fold(fold:int, device):\n",
    "    # --- sujetos por fold ---\n",
    "    def load_fold_subjects_local(folds_json: Path, fold: int):\n",
    "        return load_fold_subjects(folds_json, fold)\n",
    "\n",
    "    train_sub, test_sub = load_fold_subjects_local(FOLDS_JSON, fold)\n",
    "    train_sub = [s for s in train_sub if subject_id_to_int(s) not in EXCLUDE_SUBJECTS]\n",
    "    test_sub  = [s for s in test_sub  if subject_id_to_int(s) not in EXCLUDE_SUBJECTS]\n",
    "\n",
    "    # Split de validación por sujetos (determinista y opcionalmente estratificado)\n",
    "    rng = np.random.RandomState(RANDOM_STATE + fold)\n",
    "    tr_subjects = sorted(train_sub)\n",
    "\n",
    "    if VAL_STRAT_SUBJECT and len(tr_subjects) > 1:\n",
    "        y_dom = build_subject_label_map(tr_subjects)\n",
    "        # Reemplaza -1 (sujetos sin trials) por la moda para no romper el split\n",
    "        if np.any(y_dom < 0):\n",
    "            mask = y_dom >= 0\n",
    "            moda = int(np.bincount(y_dom[mask]).argmax()) if mask.sum() > 0 else 0\n",
    "            y_dom[~mask] = moda\n",
    "        n_val_subj = max(1, int(round(len(tr_subjects) * VAL_SUBJECT_FRAC)))\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=n_val_subj, random_state=RANDOM_STATE + fold)\n",
    "        # sss.split(X, y) — X no importa, pasamos índice\n",
    "        idx = np.arange(len(tr_subjects))\n",
    "        _, val_idx = next(sss.split(idx, y_dom))\n",
    "        val_subjects = sorted([tr_subjects[i] for i in val_idx])\n",
    "        train_subjects = [s for s in tr_subjects if s not in val_subjects]\n",
    "    else:\n",
    "        tr_subjects_shuf = tr_subjects.copy()\n",
    "        rng.shuffle(tr_subjects_shuf)\n",
    "        n_val_subj = max(1, int(round(len(tr_subjects_shuf) * VAL_SUBJECT_FRAC)))\n",
    "        val_subjects = sorted(tr_subjects_shuf[:n_val_subj])\n",
    "        train_subjects = sorted(tr_subjects_shuf[n_val_subj:])\n",
    "\n",
    "    # Carga TRAIN/VAL/TEST\n",
    "    X_tr_list, y_tr_list, sub_tr_list = [], [], []\n",
    "    X_val_list, y_val_list, sub_val_list = [], [], []\n",
    "    X_te_list, y_te_list, sub_te_list = [], [], []\n",
    "    sfreq = None\n",
    "\n",
    "    for sid in tqdm(train_subjects, desc=f\"Cargando train fold{fold}\"):\n",
    "        Xs, ys, sf = load_subject_epochs(sid, RESAMPLE_HZ, DO_NOTCH, DO_BANDPASS, DO_CAR, BP_LO, BP_HI)\n",
    "        if len(ys) == 0: continue\n",
    "        X_tr_list.append(Xs); y_tr_list.append(ys)\n",
    "        sub_tr_list.append(np.full_like(ys, fill_value=subject_id_to_int(sid)))\n",
    "        sfreq = sf if sfreq is None else sfreq\n",
    "\n",
    "    for sid in tqdm(val_subjects, desc=f\"Cargando val fold{fold}\"):\n",
    "        Xs, ys, sf = load_subject_epochs(sid, RESAMPLE_HZ, DO_NOTCH, DO_BANDPASS, DO_CAR, BP_LO, BP_HI)\n",
    "        if len(ys) == 0: continue\n",
    "        X_val_list.append(Xs); y_val_list.append(ys)\n",
    "        sub_val_list.append(np.full_like(ys, fill_value=subject_id_to_int(sid)))\n",
    "        sfreq = sf if sfreq is None else sfreq\n",
    "\n",
    "    for sid in tqdm(test_sub, desc=f\"Cargando test fold{fold}\"):\n",
    "        Xs, ys, sf = load_subject_epochs(sid, RESAMPLE_HZ, DO_NOTCH, DO_BANDPASS, DO_CAR, BP_LO, BP_HI)\n",
    "        if len(ys) == 0: continue\n",
    "        X_te_list.append(Xs); y_te_list.append(ys)\n",
    "        sub_te_list.append(np.full_like(ys, fill_value=subject_id_to_int(sid)))\n",
    "        sfreq = sf if sfreq is None else sfreq\n",
    "\n",
    "    # Concatenar\n",
    "    X_tr = np.concatenate(X_tr_list, axis=0); y_tr = np.concatenate(y_tr_list, axis=0)\n",
    "    sub_tr = np.concatenate(sub_tr_list, axis=0)\n",
    "    X_val = np.concatenate(X_val_list, axis=0); y_val = np.concatenate(y_val_list, axis=0)\n",
    "    sub_val = np.concatenate(sub_val_list, axis=0)\n",
    "    X_te = np.concatenate(X_te_list, axis=0); y_te = np.concatenate(y_te_list, axis=0)\n",
    "    sub_te = np.concatenate(sub_te_list, axis=0)\n",
    "\n",
    "    print(f\"[Fold {fold}/5] Entrenando modelo global... (n_train={len(y_tr)} | n_val={len(y_val)} | n_test={len(y_te)})\")\n",
    "\n",
    "    # Normalización por canal (fit en TRAIN y aplicar a VAL/TEST)\n",
    "    if ZSCORE_PER_EPOCH:\n",
    "        X_tr_std, X_val_std, X_te_std = X_tr, X_val, X_te\n",
    "    else:\n",
    "        X_tr_std, X_val_std = standardize_per_channel(X_tr, X_val)\n",
    "        _,        X_te_std  = standardize_per_channel(X_tr, X_te)  # reutiliza stats de train\n",
    "\n",
    "    # Datasets\n",
    "    tr_ds  = TensorDataset(torch.tensor(X_tr_std),  torch.tensor(y_tr).long(),  torch.tensor(sub_tr).long())\n",
    "    val_ds = TensorDataset(torch.tensor(X_val_std), torch.tensor(y_val).long(), torch.tensor(sub_val).long())\n",
    "    te_ds  = TensorDataset(torch.tensor(X_te_std),  torch.tensor(y_te).long(),  torch.tensor(sub_te).long())\n",
    "\n",
    "    # Weighted sampler por sujeto/clase\n",
    "    def make_weighted_sampler(dataset: TensorDataset):\n",
    "        Xb, yb, sb = dataset.tensors\n",
    "        yb_np = yb.numpy()\n",
    "        sb_np = sb.numpy()\n",
    "        # frec por sujeto\n",
    "        uniq_s, cnt_s = np.unique(sb_np, return_counts=True)\n",
    "        map_s = {s:c for s,c in zip(uniq_s, cnt_s)}\n",
    "        # frec por (sujeto,clase)\n",
    "        key = sb_np.astype(np.int64) * 10 + yb_np.astype(np.int64)  # asumiendo clases <10\n",
    "        uniq_k, cnt_k = np.unique(key, return_counts=True)\n",
    "        map_k = {k:c for k,c in zip(uniq_k, cnt_k)}\n",
    "        # peso = 1 / (frec_sujeto * frec_clase_en_sujeto)\n",
    "        w = []\n",
    "        for s, y in zip(sb_np, yb_np):\n",
    "            k = int(s)*10 + int(y)\n",
    "            ws = map_s[int(s)]\n",
    "            wk = map_k[k]\n",
    "            w.append(1.0 / (float(ws) * float(wk)))\n",
    "        w = np.array(w, dtype=np.float64)\n",
    "        w = w / (w.mean() + 1e-12)\n",
    "        sampler = WeightedRandomSampler(weights=torch.tensor(w, dtype=torch.double),\n",
    "                                        num_samples=len(yb_np), replacement=True)\n",
    "        return sampler\n",
    "\n",
    "    if USE_WEIGHTED_SAMPLER:\n",
    "        tr_sampler = make_weighted_sampler(tr_ds)\n",
    "        tr_ld  = DataLoader(tr_ds, batch_size=BATCH_SIZE, sampler=tr_sampler, drop_last=False, worker_init_fn=seed_worker)\n",
    "    else:\n",
    "        tr_ld  = DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True,  drop_last=False, worker_init_fn=seed_worker)\n",
    "\n",
    "    val_ld = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, worker_init_fn=seed_worker)\n",
    "    te_ld  = DataLoader(te_ds,  batch_size=BATCH_SIZE, shuffle=False, drop_last=False, worker_init_fn=seed_worker)\n",
    "\n",
    "    # Modelo\n",
    "    model = EEGCNNTransformer(n_ch=8, n_cls=4, d_model=D_MODEL, n_heads=N_HEADS,\n",
    "                              n_layers=N_LAYERS, p_drop=P_DROP, p_drop_encoder=P_DROP_ENCODER).to(device)\n",
    "\n",
    "    # Optimizador + (1) Focal Loss (ligero upweight a both_fists)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=1e-2)\n",
    "\n",
    "    class_counts = np.bincount(y_tr, minlength=4).astype(np.float32)\n",
    "    inv = class_counts.sum() / (4.0 * np.maximum(class_counts, 1.0))\n",
    "    alpha = torch.tensor(inv, dtype=torch.float32, device=device)\n",
    "    # Aumentamos un poco el peso de both_fists para mejorar su recall\n",
    "    alpha_mean = alpha.mean().item()\n",
    "    alpha[2] = 1.5 * alpha_mean\n",
    "    crit = FocalLoss(alpha=alpha, gamma=1.5, reduction='mean')\n",
    "\n",
    "    # (6) Warmup+Cosine (min_factor 0.1)\n",
    "    from torch.optim.lr_scheduler import LambdaLR\n",
    "    total_epochs = EPOCHS\n",
    "    warmup_epochs = max(1, int(WARMUP_EPOCHS))\n",
    "    min_factor = 0.1\n",
    "    def lr_lambda(current_epoch):\n",
    "        if current_epoch < warmup_epochs:\n",
    "            return (current_epoch + 1) / warmup_epochs\n",
    "        progress = (current_epoch - warmup_epochs) / max(1, (total_epochs - warmup_epochs))\n",
    "        progress = min(1.0, max(0.0, progress))\n",
    "        return min_factor + 0.5 * (1.0 - min_factor) * (1.0 + np.cos(np.pi * progress))\n",
    "    scheduler = LambdaLR(opt, lr_lambda=lr_lambda)\n",
    "\n",
    "    # EMA\n",
    "    if USE_EMA:\n",
    "        ema = ModelEMA(model, decay=EMA_DECAY, device=device)\n",
    "    else:\n",
    "        ema = None\n",
    "\n",
    "    # Entrenamiento con early stopping por F1 macro (en VAL; usando EMA si está activo)\n",
    "    best_f1, best_state, wait = 0.0, None, 0\n",
    "    hist = {\"ep\": [], \"tr_loss\": [], \"tr_acc\": [], \"val_acc\": [], \"val_f1m\": [], \"lr\": []}\n",
    "\n",
    "    def evaluate_on(loader, use_ema=True):\n",
    "        mdl = ema.ema if (ema is not None and use_ema) else model\n",
    "        mdl.eval()\n",
    "        preds, gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _sb in loader:\n",
    "                xb = xb.to(device)\n",
    "                p = mdl(xb).argmax(dim=1).cpu().numpy()\n",
    "                preds.append(p); gts.append(yb.numpy())\n",
    "        preds = np.concatenate(preds); gts = np.concatenate(gts)\n",
    "        acc = accuracy_score(gts, preds)\n",
    "        f1m = f1_score(gts, preds, average='macro')\n",
    "        return acc, f1m\n",
    "\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        # ---- Train ----\n",
    "        model.train()\n",
    "        tr_loss, n_seen, tr_correct = 0.0, 0, 0\n",
    "        for xb, yb, _sb in tr_ld:\n",
    "            xb = xb.to(device); yb = yb.to(device)\n",
    "            xb = augment_batch(xb)  # (4) aumentos más fuertes\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            if ema is not None:\n",
    "                ema.update(model)\n",
    "\n",
    "            tr_loss += loss.item() * len(yb)\n",
    "            n_seen += len(yb)\n",
    "            tr_correct += (logits.argmax(1) == yb).sum().item()\n",
    "        tr_loss /= max(1, n_seen)\n",
    "        tr_acc = tr_correct / max(1, n_seen)\n",
    "\n",
    "        # ---- Val (con EMA si aplica) ----\n",
    "        acc, f1m = evaluate_on(val_ld, use_ema=True)\n",
    "\n",
    "        hist[\"ep\"].append(ep)\n",
    "        hist[\"tr_loss\"].append(tr_loss)\n",
    "        hist[\"tr_acc\"].append(tr_acc)\n",
    "        hist[\"val_acc\"].append(acc)\n",
    "        hist[\"val_f1m\"].append(f1m)\n",
    "        hist[\"lr\"].append(scheduler.get_last_lr()[0])\n",
    "\n",
    "        print(f\"  Época {ep:3d} | train_loss={tr_loss:.4f} | train_acc={tr_acc:.4f} | val_acc={acc:.4f} | val_f1m={f1m:.4f} | LR={scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        improved = f1m > best_f1 + 1e-4\n",
    "        if improved:\n",
    "            best_f1 = f1m\n",
    "            # guardamos el estado EMA si existe; si no, el del modelo\n",
    "            ref_model = ema.ema if ema is not None else model\n",
    "            best_state = {k: v.detach().cpu() for k, v in ref_model.state_dict().items()}\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "\n",
    "        scheduler.step()\n",
    "        if wait >= PATIENCE:\n",
    "            print(f\"  Early stopping en época {ep} (mejor val_f1m={best_f1:.4f})\")\n",
    "            break\n",
    "\n",
    "    # Cargamos mejor estado guardado (EMA si estaba activo)\n",
    "    if best_state is not None:\n",
    "        (ema.ema if (ema is not None) else model).load_state_dict(best_state)\n",
    "\n",
    "    # ---- Guardar curva ----\n",
    "    fig = plt.figure(figsize=(8,4.5))\n",
    "    ax1 = plt.gca()\n",
    "    ax1.plot(hist[\"ep\"], hist[\"tr_loss\"], label=\"train_loss\")\n",
    "    ax1.plot(hist[\"ep\"], hist[\"val_f1m\"], label=\"val_f1m\")\n",
    "    ax1.plot(hist[\"ep\"], hist[\"val_acc\"], label=\"val_acc\")\n",
    "    ax1.set_xlabel(\"Época\"); ax1.set_title(f\"Fold {fold} — Curva de entrenamiento\")\n",
    "    ax1.legend(); ax1.grid(True, alpha=0.3)\n",
    "    out_png = f\"training_curve_fold{fold}.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=140)\n",
    "    plt.close(fig)\n",
    "    print(f\"↳ Curva de entrenamiento guardada: {out_png}\")\n",
    "\n",
    "    # ---- Evaluación final en TEST (con TTA/subwindows y ensemble si aplica) ----\n",
    "    eval_model = ema.ema if (ema is not None) else model\n",
    "    eval_model.eval()\n",
    "\n",
    "    sfreq_used = RESAMPLE_HZ\n",
    "    if sfreq_used is None:\n",
    "        sfreq_used = int(round(X_te_std.shape[-1] / (TMAX - TMIN)))\n",
    "\n",
    "    if (not SW_ENABLE) or SW_MODE == 'none':\n",
    "        preds, gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _sb in te_ld:\n",
    "                xb = xb.to(device)\n",
    "                p = eval_model(xb).argmax(dim=1).cpu().numpy()\n",
    "                preds.append(p); gts.append(yb.numpy())\n",
    "        preds = np.concatenate(preds); gts = np.concatenate(gts)\n",
    "    elif SW_MODE in ('subwin', 'tta'):\n",
    "        logits_tta = None\n",
    "        logits_sw  = None\n",
    "        if SW_MODE == 'subwin':\n",
    "            logits_sw = subwindow_logits(eval_model, X_te_std, sfreq_used, SW_LEN, SW_STRIDE, device)\n",
    "        elif SW_MODE == 'tta':\n",
    "            logits_tta = time_shift_tta_logits(eval_model, X_te_std, sfreq_used, TTA_SHIFTS_S, device)\n",
    "\n",
    "        if COMBINE_TTA_AND_SUBWIN:\n",
    "            if logits_tta is None:\n",
    "                logits_tta = time_shift_tta_logits(eval_model, X_te_std, sfreq_used, TTA_SHIFTS_S, device)\n",
    "            if logits_sw is None:\n",
    "                logits_sw  = subwindow_logits(eval_model, X_te_std, sfreq_used, SW_LEN, SW_STRIDE, device)\n",
    "            logits = 0.5 * logits_tta + 0.5 * logits_sw\n",
    "        else:\n",
    "            logits = logits_tta if logits_tta is not None else logits_sw\n",
    "\n",
    "        preds = logits.argmax(axis=1); gts = y_te\n",
    "    else:\n",
    "        raise ValueError(f\"SW_MODE desconocido: {SW_MODE}\")\n",
    "\n",
    "    acc = accuracy_score(gts, preds)\n",
    "    f1m = f1_score(gts, preds, average='macro')\n",
    "    print(f\"[Fold {fold}/5] Global acc={acc:.4f}\\n\")\n",
    "    print(classification_report(gts, preds, target_names=[c.replace('_',' ') for c in CLASS_NAMES], digits=4))\n",
    "    print(\"Confusion matrix (rows=true, cols=pred):\")\n",
    "    print(confusion_matrix(gts, preds, labels=[0,1,2,3]))\n",
    "\n",
    "    return acc, f1m\n",
    "\n",
    "# =========================\n",
    "# LOOP 5 FOLDS + RESUMEN\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    acc_folds, f1_folds = [], []\n",
    "    for fold in range(1, 6):\n",
    "        acc, f1m = train_one_fold(fold, DEVICE)\n",
    "        acc_folds.append(f\"{acc:.4f}\")\n",
    "        f1_folds.append(f\"{f1m:.4f}\")\n",
    "\n",
    "    acc_mean = float(np.mean([float(a) for a in acc_folds]))\n",
    "    f1_mean  = float(np.mean([float(f) for f in f1_folds]))\n",
    "\n",
    "    print(\"\\n============================================================\")\n",
    "    print(\"RESULTADOS FINALES\")\n",
    "    print(\"============================================================\")\n",
    "    print(f\"Global folds (ACC): {acc_folds}\")\n",
    "    print(f\"Global mean ACC: {acc_mean:.4f}\")\n",
    "    print(f\"F1 folds (MACRO): {f1_folds}\")\n",
    "    print(f\"F1 mean (MACRO): {f1_mean:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d62e021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Usando dispositivo: cuda\n",
      "🧠 INICIANDO EXPERIMENTO CON CNN+Transformer (K-Fold por sujeto como EEGNet)\n",
      "🔧 Configuración: 4c, 8 canales, 6s | EPOCHS=60, BATCH=64, LR=0.001 | ZSCORE_PER_EPOCH=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold1: 100%|██████████| 67/67 [00:08<00:00,  8.34it/s]\n",
      "Cargando val fold1: 100%|██████████| 15/15 [00:01<00:00,  8.28it/s]\n",
      "Cargando test fold1: 100%|██████████| 21/21 [00:02<00:00,  8.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1/5] Entrenando modelo global... (n_train=5628 | n_val=1260 | n_test=1764)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Época   1 | train_loss=0.2228 | train_acc=0.2822 | val_acc=0.2968 | val_f1m=0.2339 | LR=0.000125\n",
      "  Época   2 | train_loss=0.2013 | train_acc=0.3701 | val_acc=0.4087 | val_f1m=0.3803 | LR=0.000250\n",
      "  Época   3 | train_loss=0.1803 | train_acc=0.4726 | val_acc=0.4302 | val_f1m=0.4253 | LR=0.000375\n",
      "  Época   4 | train_loss=0.1769 | train_acc=0.4812 | val_acc=0.4500 | val_f1m=0.4505 | LR=0.000500\n",
      "  Época   5 | train_loss=0.1705 | train_acc=0.4993 | val_acc=0.4651 | val_f1m=0.4590 | LR=0.000625\n",
      "  Época   6 | train_loss=0.1670 | train_acc=0.5258 | val_acc=0.4643 | val_f1m=0.4612 | LR=0.000750\n",
      "  Época   7 | train_loss=0.1638 | train_acc=0.5352 | val_acc=0.4452 | val_f1m=0.4363 | LR=0.000875\n",
      "  Época   8 | train_loss=0.1619 | train_acc=0.5288 | val_acc=0.4532 | val_f1m=0.4557 | LR=0.001000\n",
      "  Época   9 | train_loss=0.1647 | train_acc=0.5135 | val_acc=0.4413 | val_f1m=0.4447 | LR=0.001000\n",
      "  Época  10 | train_loss=0.1573 | train_acc=0.5439 | val_acc=0.4508 | val_f1m=0.4501 | LR=0.000999\n",
      "  Época  11 | train_loss=0.1578 | train_acc=0.5455 | val_acc=0.4508 | val_f1m=0.4510 | LR=0.000997\n",
      "  Época  12 | train_loss=0.1540 | train_acc=0.5544 | val_acc=0.4532 | val_f1m=0.4526 | LR=0.000993\n",
      "  Época  13 | train_loss=0.1510 | train_acc=0.5620 | val_acc=0.4508 | val_f1m=0.4503 | LR=0.000987\n",
      "  Época  14 | train_loss=0.1527 | train_acc=0.5609 | val_acc=0.4540 | val_f1m=0.4536 | LR=0.000980\n",
      "  Época  15 | train_loss=0.1520 | train_acc=0.5624 | val_acc=0.4524 | val_f1m=0.4522 | LR=0.000971\n",
      "  Época  16 | train_loss=0.1482 | train_acc=0.5672 | val_acc=0.4516 | val_f1m=0.4516 | LR=0.000960\n",
      "  Época  17 | train_loss=0.1421 | train_acc=0.5844 | val_acc=0.4524 | val_f1m=0.4526 | LR=0.000948\n",
      "  Época  18 | train_loss=0.1447 | train_acc=0.5736 | val_acc=0.4540 | val_f1m=0.4542 | LR=0.000935\n",
      "  Early stopping en época 18 (mejor val_f1m=0.4612)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold1.png\n",
      "[Fold 1/5] Global acc=0.4938\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.6081    0.5420    0.5731       441\n",
      "       right     0.5238    0.5737    0.5476       441\n",
      "  both fists     0.3730    0.4331    0.4008       441\n",
      "   both feet     0.5000    0.4263    0.4602       441\n",
      "\n",
      "    accuracy                         0.4938      1764\n",
      "   macro avg     0.5012    0.4938    0.4955      1764\n",
      "weighted avg     0.5012    0.4938    0.4955      1764\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[239  45 104  53]\n",
      " [ 32 253 100  56]\n",
      " [ 69 102 191  79]\n",
      " [ 53  83 117 188]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold2: 100%|██████████| 67/67 [00:08<00:00,  8.12it/s]\n",
      "Cargando val fold2: 100%|██████████| 15/15 [00:01<00:00,  8.18it/s]\n",
      "Cargando test fold2: 100%|██████████| 21/21 [00:02<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 2/5] Entrenando modelo global... (n_train=5628 | n_val=1260 | n_test=1764)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Época   1 | train_loss=0.2254 | train_acc=0.2774 | val_acc=0.2960 | val_f1m=0.1950 | LR=0.000125\n",
      "  Época   2 | train_loss=0.2034 | train_acc=0.3651 | val_acc=0.4381 | val_f1m=0.4369 | LR=0.000250\n",
      "  Época   3 | train_loss=0.1904 | train_acc=0.4314 | val_acc=0.4651 | val_f1m=0.4496 | LR=0.000375\n",
      "  Época   4 | train_loss=0.1830 | train_acc=0.4645 | val_acc=0.4635 | val_f1m=0.4623 | LR=0.000500\n",
      "  Época   5 | train_loss=0.1794 | train_acc=0.4627 | val_acc=0.4651 | val_f1m=0.4664 | LR=0.000625\n",
      "  Época   6 | train_loss=0.1770 | train_acc=0.4789 | val_acc=0.4444 | val_f1m=0.4288 | LR=0.000750\n",
      "  Época   7 | train_loss=0.1771 | train_acc=0.4815 | val_acc=0.4794 | val_f1m=0.4826 | LR=0.000875\n",
      "  Época   8 | train_loss=0.1734 | train_acc=0.4931 | val_acc=0.4484 | val_f1m=0.4509 | LR=0.001000\n",
      "  Época   9 | train_loss=0.1707 | train_acc=0.5002 | val_acc=0.4778 | val_f1m=0.4829 | LR=0.001000\n",
      "  Época  10 | train_loss=0.1664 | train_acc=0.5155 | val_acc=0.4849 | val_f1m=0.4854 | LR=0.000999\n",
      "  Época  11 | train_loss=0.1678 | train_acc=0.5128 | val_acc=0.4841 | val_f1m=0.4861 | LR=0.000997\n",
      "  Época  12 | train_loss=0.1626 | train_acc=0.5299 | val_acc=0.4889 | val_f1m=0.4918 | LR=0.000993\n",
      "  Época  13 | train_loss=0.1662 | train_acc=0.5204 | val_acc=0.4905 | val_f1m=0.4932 | LR=0.000987\n",
      "  Época  14 | train_loss=0.1624 | train_acc=0.5261 | val_acc=0.4897 | val_f1m=0.4923 | LR=0.000980\n",
      "  Época  15 | train_loss=0.1618 | train_acc=0.5286 | val_acc=0.4889 | val_f1m=0.4914 | LR=0.000971\n",
      "  Época  16 | train_loss=0.1606 | train_acc=0.5279 | val_acc=0.4897 | val_f1m=0.4919 | LR=0.000960\n",
      "  Época  17 | train_loss=0.1596 | train_acc=0.5341 | val_acc=0.4913 | val_f1m=0.4935 | LR=0.000948\n",
      "  Época  18 | train_loss=0.1573 | train_acc=0.5400 | val_acc=0.4913 | val_f1m=0.4938 | LR=0.000935\n",
      "  Época  19 | train_loss=0.1587 | train_acc=0.5467 | val_acc=0.4905 | val_f1m=0.4928 | LR=0.000920\n",
      "  Época  20 | train_loss=0.1535 | train_acc=0.5485 | val_acc=0.4905 | val_f1m=0.4929 | LR=0.000904\n",
      "  Época  21 | train_loss=0.1546 | train_acc=0.5467 | val_acc=0.4913 | val_f1m=0.4934 | LR=0.000887\n",
      "  Época  22 | train_loss=0.1508 | train_acc=0.5659 | val_acc=0.4960 | val_f1m=0.4975 | LR=0.000868\n",
      "  Época  23 | train_loss=0.1535 | train_acc=0.5554 | val_acc=0.4952 | val_f1m=0.4970 | LR=0.000848\n",
      "  Época  24 | train_loss=0.1488 | train_acc=0.5583 | val_acc=0.4952 | val_f1m=0.4971 | LR=0.000828\n",
      "  Época  25 | train_loss=0.1457 | train_acc=0.5760 | val_acc=0.4929 | val_f1m=0.4947 | LR=0.000806\n",
      "  Época  26 | train_loss=0.1447 | train_acc=0.5776 | val_acc=0.4921 | val_f1m=0.4942 | LR=0.000783\n",
      "  Época  27 | train_loss=0.1481 | train_acc=0.5636 | val_acc=0.4944 | val_f1m=0.4967 | LR=0.000759\n",
      "  Época  28 | train_loss=0.1456 | train_acc=0.5778 | val_acc=0.4937 | val_f1m=0.4958 | LR=0.000735\n",
      "  Época  29 | train_loss=0.1421 | train_acc=0.5830 | val_acc=0.4937 | val_f1m=0.4960 | LR=0.000710\n",
      "  Época  30 | train_loss=0.1435 | train_acc=0.5858 | val_acc=0.4921 | val_f1m=0.4944 | LR=0.000684\n",
      "  Época  31 | train_loss=0.1386 | train_acc=0.5906 | val_acc=0.4944 | val_f1m=0.4966 | LR=0.000658\n",
      "  Época  32 | train_loss=0.1357 | train_acc=0.6066 | val_acc=0.4960 | val_f1m=0.4983 | LR=0.000631\n",
      "  Época  33 | train_loss=0.1354 | train_acc=0.6036 | val_acc=0.4937 | val_f1m=0.4959 | LR=0.000604\n",
      "  Época  34 | train_loss=0.1329 | train_acc=0.6093 | val_acc=0.4937 | val_f1m=0.4960 | LR=0.000577\n",
      "  Época  35 | train_loss=0.1327 | train_acc=0.6041 | val_acc=0.4921 | val_f1m=0.4945 | LR=0.000550\n",
      "  Época  36 | train_loss=0.1296 | train_acc=0.6109 | val_acc=0.4889 | val_f1m=0.4915 | LR=0.000523\n",
      "  Época  37 | train_loss=0.1297 | train_acc=0.6199 | val_acc=0.4889 | val_f1m=0.4913 | LR=0.000496\n",
      "  Época  38 | train_loss=0.1273 | train_acc=0.6214 | val_acc=0.4913 | val_f1m=0.4936 | LR=0.000469\n",
      "  Época  39 | train_loss=0.1211 | train_acc=0.6393 | val_acc=0.4873 | val_f1m=0.4896 | LR=0.000442\n",
      "  Época  40 | train_loss=0.1213 | train_acc=0.6400 | val_acc=0.4905 | val_f1m=0.4924 | LR=0.000416\n",
      "  Época  41 | train_loss=0.1182 | train_acc=0.6461 | val_acc=0.4905 | val_f1m=0.4924 | LR=0.000390\n",
      "  Época  42 | train_loss=0.1177 | train_acc=0.6478 | val_acc=0.4905 | val_f1m=0.4922 | LR=0.000365\n",
      "  Época  43 | train_loss=0.1187 | train_acc=0.6434 | val_acc=0.4889 | val_f1m=0.4904 | LR=0.000341\n",
      "  Época  44 | train_loss=0.1143 | train_acc=0.6619 | val_acc=0.4905 | val_f1m=0.4923 | LR=0.000317\n",
      "  Early stopping en época 44 (mejor val_f1m=0.4983)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold2.png\n",
      "[Fold 2/5] Global acc=0.5794\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.6690    0.6463    0.6574       441\n",
      "       right     0.6401    0.6372    0.6386       441\n",
      "  both fists     0.4813    0.5556    0.5158       441\n",
      "   both feet     0.5410    0.4785    0.5078       441\n",
      "\n",
      "    accuracy                         0.5794      1764\n",
      "   macro avg     0.5829    0.5794    0.5799      1764\n",
      "weighted avg     0.5829    0.5794    0.5799      1764\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[285  22  88  46]\n",
      " [ 34 281  68  58]\n",
      " [ 69  52 245  75]\n",
      " [ 38  84 108 211]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold3: 100%|██████████| 67/67 [00:08<00:00,  8.34it/s]\n",
      "Cargando val fold3: 100%|██████████| 15/15 [00:01<00:00,  8.41it/s]\n",
      "Cargando test fold3: 100%|██████████| 21/21 [00:02<00:00,  8.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 3/5] Entrenando modelo global... (n_train=5628 | n_val=1260 | n_test=1764)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Época   1 | train_loss=0.2213 | train_acc=0.2937 | val_acc=0.3230 | val_f1m=0.2394 | LR=0.000125\n",
      "  Época   2 | train_loss=0.1972 | train_acc=0.3898 | val_acc=0.4627 | val_f1m=0.4549 | LR=0.000250\n",
      "  Época   3 | train_loss=0.1835 | train_acc=0.4595 | val_acc=0.4698 | val_f1m=0.4589 | LR=0.000375\n",
      "  Época   4 | train_loss=0.1791 | train_acc=0.4829 | val_acc=0.4778 | val_f1m=0.4768 | LR=0.000500\n",
      "  Época   5 | train_loss=0.1750 | train_acc=0.4982 | val_acc=0.4770 | val_f1m=0.4783 | LR=0.000625\n",
      "  Época   6 | train_loss=0.1718 | train_acc=0.5034 | val_acc=0.4667 | val_f1m=0.4606 | LR=0.000750\n",
      "  Época   7 | train_loss=0.1682 | train_acc=0.5114 | val_acc=0.4738 | val_f1m=0.4708 | LR=0.000875\n",
      "  Época   8 | train_loss=0.1677 | train_acc=0.5203 | val_acc=0.4905 | val_f1m=0.4921 | LR=0.001000\n",
      "  Época   9 | train_loss=0.1653 | train_acc=0.5183 | val_acc=0.5008 | val_f1m=0.5042 | LR=0.001000\n",
      "  Época  10 | train_loss=0.1617 | train_acc=0.5295 | val_acc=0.4968 | val_f1m=0.5005 | LR=0.000999\n",
      "  Época  11 | train_loss=0.1611 | train_acc=0.5334 | val_acc=0.4984 | val_f1m=0.5027 | LR=0.000997\n",
      "  Época  12 | train_loss=0.1555 | train_acc=0.5563 | val_acc=0.4968 | val_f1m=0.5015 | LR=0.000993\n",
      "  Época  13 | train_loss=0.1562 | train_acc=0.5540 | val_acc=0.4937 | val_f1m=0.4984 | LR=0.000987\n",
      "  Época  14 | train_loss=0.1587 | train_acc=0.5496 | val_acc=0.4921 | val_f1m=0.4970 | LR=0.000980\n",
      "  Época  15 | train_loss=0.1551 | train_acc=0.5567 | val_acc=0.4929 | val_f1m=0.4976 | LR=0.000971\n",
      "  Época  16 | train_loss=0.1493 | train_acc=0.5723 | val_acc=0.4952 | val_f1m=0.5000 | LR=0.000960\n",
      "  Época  17 | train_loss=0.1509 | train_acc=0.5750 | val_acc=0.4921 | val_f1m=0.4966 | LR=0.000948\n",
      "  Época  18 | train_loss=0.1523 | train_acc=0.5593 | val_acc=0.4968 | val_f1m=0.5010 | LR=0.000935\n",
      "  Época  19 | train_loss=0.1495 | train_acc=0.5679 | val_acc=0.4984 | val_f1m=0.5025 | LR=0.000920\n",
      "  Época  20 | train_loss=0.1485 | train_acc=0.5654 | val_acc=0.5008 | val_f1m=0.5048 | LR=0.000904\n",
      "  Época  21 | train_loss=0.1454 | train_acc=0.5794 | val_acc=0.5040 | val_f1m=0.5078 | LR=0.000887\n",
      "  Época  22 | train_loss=0.1452 | train_acc=0.5803 | val_acc=0.5024 | val_f1m=0.5063 | LR=0.000868\n",
      "  Época  23 | train_loss=0.1459 | train_acc=0.5764 | val_acc=0.5048 | val_f1m=0.5089 | LR=0.000848\n",
      "  Época  24 | train_loss=0.1398 | train_acc=0.5935 | val_acc=0.5063 | val_f1m=0.5106 | LR=0.000828\n",
      "  Época  25 | train_loss=0.1399 | train_acc=0.5956 | val_acc=0.5056 | val_f1m=0.5099 | LR=0.000806\n",
      "  Época  26 | train_loss=0.1378 | train_acc=0.6066 | val_acc=0.5040 | val_f1m=0.5084 | LR=0.000783\n",
      "  Época  27 | train_loss=0.1366 | train_acc=0.6048 | val_acc=0.5063 | val_f1m=0.5107 | LR=0.000759\n",
      "  Época  28 | train_loss=0.1367 | train_acc=0.6073 | val_acc=0.5048 | val_f1m=0.5091 | LR=0.000735\n",
      "  Época  29 | train_loss=0.1334 | train_acc=0.6194 | val_acc=0.5008 | val_f1m=0.5054 | LR=0.000710\n",
      "  Época  30 | train_loss=0.1327 | train_acc=0.6190 | val_acc=0.5000 | val_f1m=0.5047 | LR=0.000684\n",
      "  Época  31 | train_loss=0.1312 | train_acc=0.6166 | val_acc=0.5032 | val_f1m=0.5080 | LR=0.000658\n",
      "  Época  32 | train_loss=0.1256 | train_acc=0.6423 | val_acc=0.5040 | val_f1m=0.5091 | LR=0.000631\n",
      "  Época  33 | train_loss=0.1269 | train_acc=0.6286 | val_acc=0.5040 | val_f1m=0.5089 | LR=0.000604\n",
      "  Época  34 | train_loss=0.1224 | train_acc=0.6500 | val_acc=0.5056 | val_f1m=0.5101 | LR=0.000577\n",
      "  Época  35 | train_loss=0.1209 | train_acc=0.6482 | val_acc=0.5048 | val_f1m=0.5095 | LR=0.000550\n",
      "  Época  36 | train_loss=0.1203 | train_acc=0.6564 | val_acc=0.5056 | val_f1m=0.5103 | LR=0.000523\n",
      "  Early stopping en época 36 (mejor val_f1m=0.5106)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold3.png\n",
      "[Fold 3/5] Global acc=0.4875\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.5928    0.4853    0.5337       441\n",
      "       right     0.6209    0.5125    0.5615       441\n",
      "  both fists     0.3735    0.4853    0.4221       441\n",
      "   both feet     0.4421    0.4671    0.4542       441\n",
      "\n",
      "    accuracy                         0.4875      1764\n",
      "   macro avg     0.5073    0.4875    0.4929      1764\n",
      "weighted avg     0.5073    0.4875    0.4929      1764\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[214  26 129  72]\n",
      " [ 21 226 110  84]\n",
      " [ 75  48 214 104]\n",
      " [ 51  64 120 206]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold4: 100%|██████████| 68/68 [00:08<00:00,  8.32it/s]\n",
      "Cargando val fold4: 100%|██████████| 15/15 [00:01<00:00,  8.34it/s]\n",
      "Cargando test fold4: 100%|██████████| 20/20 [00:02<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 4/5] Entrenando modelo global... (n_train=5712 | n_val=1260 | n_test=1680)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Época   1 | train_loss=0.2248 | train_acc=0.2799 | val_acc=0.3206 | val_f1m=0.2356 | LR=0.000125\n",
      "  Época   2 | train_loss=0.2045 | train_acc=0.3641 | val_acc=0.4484 | val_f1m=0.4439 | LR=0.000250\n",
      "  Época   3 | train_loss=0.1881 | train_acc=0.4396 | val_acc=0.4611 | val_f1m=0.4624 | LR=0.000375\n",
      "  Época   4 | train_loss=0.1807 | train_acc=0.4718 | val_acc=0.4738 | val_f1m=0.4721 | LR=0.000500\n",
      "  Época   5 | train_loss=0.1742 | train_acc=0.4937 | val_acc=0.4722 | val_f1m=0.4751 | LR=0.000625\n",
      "  Época   6 | train_loss=0.1775 | train_acc=0.4795 | val_acc=0.4857 | val_f1m=0.4847 | LR=0.000750\n",
      "  Época   7 | train_loss=0.1746 | train_acc=0.4977 | val_acc=0.4651 | val_f1m=0.4615 | LR=0.000875\n",
      "  Época   8 | train_loss=0.1728 | train_acc=0.4982 | val_acc=0.4778 | val_f1m=0.4771 | LR=0.001000\n",
      "  Época   9 | train_loss=0.1687 | train_acc=0.5117 | val_acc=0.4817 | val_f1m=0.4825 | LR=0.001000\n",
      "  Época  10 | train_loss=0.1670 | train_acc=0.5186 | val_acc=0.4960 | val_f1m=0.4986 | LR=0.000999\n",
      "  Época  11 | train_loss=0.1674 | train_acc=0.5236 | val_acc=0.4952 | val_f1m=0.4964 | LR=0.000997\n",
      "  Época  12 | train_loss=0.1624 | train_acc=0.5385 | val_acc=0.5008 | val_f1m=0.5017 | LR=0.000993\n",
      "  Época  13 | train_loss=0.1616 | train_acc=0.5375 | val_acc=0.4992 | val_f1m=0.5001 | LR=0.000987\n",
      "  Época  14 | train_loss=0.1606 | train_acc=0.5322 | val_acc=0.4984 | val_f1m=0.4991 | LR=0.000980\n",
      "  Época  15 | train_loss=0.1588 | train_acc=0.5504 | val_acc=0.5016 | val_f1m=0.5021 | LR=0.000971\n",
      "  Época  16 | train_loss=0.1556 | train_acc=0.5471 | val_acc=0.5008 | val_f1m=0.5014 | LR=0.000960\n",
      "  Época  17 | train_loss=0.1588 | train_acc=0.5382 | val_acc=0.5008 | val_f1m=0.5014 | LR=0.000948\n",
      "  Época  18 | train_loss=0.1539 | train_acc=0.5534 | val_acc=0.5032 | val_f1m=0.5039 | LR=0.000935\n",
      "  Época  19 | train_loss=0.1520 | train_acc=0.5630 | val_acc=0.4976 | val_f1m=0.4990 | LR=0.000920\n",
      "  Época  20 | train_loss=0.1509 | train_acc=0.5508 | val_acc=0.4976 | val_f1m=0.4986 | LR=0.000904\n",
      "  Época  21 | train_loss=0.1535 | train_acc=0.5558 | val_acc=0.4960 | val_f1m=0.4970 | LR=0.000887\n",
      "  Época  22 | train_loss=0.1479 | train_acc=0.5692 | val_acc=0.4944 | val_f1m=0.4957 | LR=0.000868\n",
      "  Época  23 | train_loss=0.1474 | train_acc=0.5685 | val_acc=0.4937 | val_f1m=0.4948 | LR=0.000848\n",
      "  Época  24 | train_loss=0.1448 | train_acc=0.5861 | val_acc=0.4944 | val_f1m=0.4958 | LR=0.000828\n",
      "  Época  25 | train_loss=0.1447 | train_acc=0.5826 | val_acc=0.4968 | val_f1m=0.4983 | LR=0.000806\n",
      "  Época  26 | train_loss=0.1432 | train_acc=0.5858 | val_acc=0.4976 | val_f1m=0.4992 | LR=0.000783\n",
      "  Época  27 | train_loss=0.1408 | train_acc=0.5933 | val_acc=0.4976 | val_f1m=0.4991 | LR=0.000759\n",
      "  Época  28 | train_loss=0.1393 | train_acc=0.5933 | val_acc=0.5016 | val_f1m=0.5032 | LR=0.000735\n",
      "  Época  29 | train_loss=0.1352 | train_acc=0.6082 | val_acc=0.5000 | val_f1m=0.5018 | LR=0.000710\n",
      "  Época  30 | train_loss=0.1371 | train_acc=0.6038 | val_acc=0.4976 | val_f1m=0.4994 | LR=0.000684\n",
      "  Early stopping en época 30 (mejor val_f1m=0.5039)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold4.png\n",
      "[Fold 4/5] Global acc=0.5333\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.6509    0.5905    0.6192       420\n",
      "       right     0.6089    0.5524    0.5793       420\n",
      "  both fists     0.4196    0.6214    0.5010       420\n",
      "   both feet     0.5236    0.3690    0.4330       420\n",
      "\n",
      "    accuracy                         0.5333      1680\n",
      "   macro avg     0.5508    0.5333    0.5331      1680\n",
      "weighted avg     0.5508    0.5333    0.5331      1680\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[248  18 120  34]\n",
      " [ 24 232 102  62]\n",
      " [ 56  58 261  45]\n",
      " [ 53  73 139 155]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold5: 100%|██████████| 68/68 [00:08<00:00,  8.31it/s]\n",
      "Cargando val fold5: 100%|██████████| 15/15 [00:01<00:00,  8.30it/s]\n",
      "Cargando test fold5: 100%|██████████| 20/20 [00:02<00:00,  8.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 5/5] Entrenando modelo global... (n_train=5712 | n_val=1260 | n_test=1680)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Época   1 | train_loss=0.2235 | train_acc=0.2638 | val_acc=0.2960 | val_f1m=0.2855 | LR=0.000125\n",
      "  Época   2 | train_loss=0.2031 | train_acc=0.3780 | val_acc=0.3984 | val_f1m=0.3959 | LR=0.000250\n",
      "  Época   3 | train_loss=0.1842 | train_acc=0.4629 | val_acc=0.4413 | val_f1m=0.4405 | LR=0.000375\n",
      "  Época   4 | train_loss=0.1765 | train_acc=0.4879 | val_acc=0.4397 | val_f1m=0.4409 | LR=0.000500\n",
      "  Época   5 | train_loss=0.1809 | train_acc=0.4690 | val_acc=0.4254 | val_f1m=0.4172 | LR=0.000625\n",
      "  Época   6 | train_loss=0.1749 | train_acc=0.4821 | val_acc=0.4373 | val_f1m=0.4356 | LR=0.000750\n",
      "  Época   7 | train_loss=0.1715 | train_acc=0.5023 | val_acc=0.4000 | val_f1m=0.3734 | LR=0.000875\n",
      "  Época   8 | train_loss=0.1718 | train_acc=0.5058 | val_acc=0.4571 | val_f1m=0.4603 | LR=0.001000\n",
      "  Época   9 | train_loss=0.1667 | train_acc=0.5093 | val_acc=0.4524 | val_f1m=0.4541 | LR=0.001000\n",
      "  Época  10 | train_loss=0.1633 | train_acc=0.5278 | val_acc=0.4611 | val_f1m=0.4629 | LR=0.000999\n",
      "  Época  11 | train_loss=0.1611 | train_acc=0.5327 | val_acc=0.4730 | val_f1m=0.4758 | LR=0.000997\n",
      "  Época  12 | train_loss=0.1632 | train_acc=0.5254 | val_acc=0.4730 | val_f1m=0.4752 | LR=0.000993\n",
      "  Época  13 | train_loss=0.1589 | train_acc=0.5345 | val_acc=0.4738 | val_f1m=0.4757 | LR=0.000987\n",
      "  Época  14 | train_loss=0.1590 | train_acc=0.5457 | val_acc=0.4746 | val_f1m=0.4767 | LR=0.000980\n",
      "  Época  15 | train_loss=0.1553 | train_acc=0.5530 | val_acc=0.4778 | val_f1m=0.4798 | LR=0.000971\n",
      "  Época  16 | train_loss=0.1548 | train_acc=0.5508 | val_acc=0.4762 | val_f1m=0.4785 | LR=0.000960\n",
      "  Época  17 | train_loss=0.1545 | train_acc=0.5508 | val_acc=0.4722 | val_f1m=0.4741 | LR=0.000948\n",
      "  Época  18 | train_loss=0.1527 | train_acc=0.5583 | val_acc=0.4730 | val_f1m=0.4753 | LR=0.000935\n",
      "  Época  19 | train_loss=0.1507 | train_acc=0.5653 | val_acc=0.4730 | val_f1m=0.4751 | LR=0.000920\n",
      "  Época  20 | train_loss=0.1479 | train_acc=0.5725 | val_acc=0.4746 | val_f1m=0.4768 | LR=0.000904\n",
      "  Época  21 | train_loss=0.1512 | train_acc=0.5523 | val_acc=0.4738 | val_f1m=0.4761 | LR=0.000887\n",
      "  Época  22 | train_loss=0.1475 | train_acc=0.5679 | val_acc=0.4698 | val_f1m=0.4718 | LR=0.000868\n",
      "  Época  23 | train_loss=0.1458 | train_acc=0.5728 | val_acc=0.4754 | val_f1m=0.4773 | LR=0.000848\n",
      "  Época  24 | train_loss=0.1463 | train_acc=0.5679 | val_acc=0.4778 | val_f1m=0.4797 | LR=0.000828\n",
      "  Época  25 | train_loss=0.1472 | train_acc=0.5723 | val_acc=0.4778 | val_f1m=0.4794 | LR=0.000806\n",
      "  Época  26 | train_loss=0.1387 | train_acc=0.5940 | val_acc=0.4738 | val_f1m=0.4759 | LR=0.000783\n",
      "  Época  27 | train_loss=0.1419 | train_acc=0.5954 | val_acc=0.4778 | val_f1m=0.4796 | LR=0.000759\n",
      "  Early stopping en época 27 (mejor val_f1m=0.4798)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold5.png\n",
      "[Fold 5/5] Global acc=0.5726\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.6667    0.5810    0.6209       420\n",
      "       right     0.6078    0.5905    0.5990       420\n",
      "  both fists     0.4847    0.5667    0.5225       420\n",
      "   both feet     0.5590    0.5524    0.5557       420\n",
      "\n",
      "    accuracy                         0.5726      1680\n",
      "   macro avg     0.5796    0.5726    0.5745      1680\n",
      "weighted avg     0.5796    0.5726    0.5745      1680\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[244  24  98  54]\n",
      " [ 19 248  77  76]\n",
      " [ 62  67 238  53]\n",
      " [ 41  69  78 232]]\n",
      "\n",
      "============================================================\n",
      "RESULTADOS FINALES\n",
      "============================================================\n",
      "Global folds (ACC): ['0.4938', '0.5794', '0.4875', '0.5333', '0.5726']\n",
      "Global mean ACC: 0.5333\n",
      "F1 folds (MACRO): ['0.4955', '0.5799', '0.4929', '0.5331', '0.5745']\n",
      "F1 mean (MACRO): 0.5352\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# CNN+Transformer para MI (4 clases, 8 canales) con:\n",
    "# (1) Focal Loss\n",
    "# (4) Aumentos más fuertes y estables\n",
    "# (6) LR warmup+cosine, early stopping por F1 macro\n",
    "# Cambios aplicados (2,3,4,5,6,7):\n",
    "# 2) WeightedRandomSampler templado (a=0.5, b=1.0)\n",
    "# 3) Focal alpha: boost también para both_feet\n",
    "# 4) TransformerEncoderLayer(norm_first=False)\n",
    "# 5) Gradient clipping (max_norm=1.0)\n",
    "# 6) EMA más lenta (EMA_DECAY=0.9995; warmup decay a 1000 updates)\n",
    "# 7) Cosine LR min_factor=0.1\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "import re, json, random\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import mne\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# =========================\n",
    "# REPRODUCIBILIDAD\n",
    "# =========================\n",
    "RANDOM_STATE = 42\n",
    "def seed_everything(seed: int = 42):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def seed_worker(worker_id: int):\n",
    "    worker_seed = RANDOM_STATE + worker_id\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "seed_everything(RANDOM_STATE)\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "FOLDS_JSON = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 64        # múltiplo de 4\n",
    "BASE_LR = 1e-3\n",
    "WARMUP_EPOCHS = 8\n",
    "PATIENCE = 12\n",
    "\n",
    "# Split de validación por fold (por sujetos)\n",
    "VAL_SUBJECT_FRAC = 0.18  # ≈ 18% de sujetos del train → val\n",
    "VAL_STRAT_SUBJECT = True # estratifica por etiqueta dominante de cada sujeto\n",
    "\n",
    "# Prepro\n",
    "RESAMPLE_HZ = None\n",
    "DO_NOTCH = True\n",
    "DO_BANDPASS = False       # respetamos tu setup original\n",
    "BP_LO, BP_HI = 4.0, 38.0\n",
    "DO_CAR = False\n",
    "ZSCORE_PER_EPOCH = False\n",
    "\n",
    "# Modelo\n",
    "D_MODEL = 128\n",
    "N_HEADS = 4\n",
    "N_LAYERS = 2\n",
    "P_DROP = 0.2            # dropout en conv stack\n",
    "P_DROP_ENCODER = 0.3    # dropout antes del encoder\n",
    "\n",
    "# Ventana temporal\n",
    "TMIN, TMAX = -1.0, 5.0\n",
    "\n",
    "# TTA / SUBWINDOW en TEST\n",
    "SW_MODE = 'tta'   # 'none'|'subwin'|'tta'\n",
    "SW_ENABLE = True\n",
    "TTA_SHIFTS_S = [-0.05, -0.025, 0.0, 0.025, 0.05]\n",
    "SW_LEN, SW_STRIDE = 4.5, 2.0\n",
    "COMBINE_TTA_AND_SUBWIN = False  # solicitado (no cambio 1)\n",
    "\n",
    "# Sampler balanceado\n",
    "USE_WEIGHTED_SAMPLER = True\n",
    "\n",
    "# EMA\n",
    "USE_EMA = True\n",
    "EMA_DECAY = 0.9995  # (6) más lenta\n",
    "\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','FCz']\n",
    "CLASS_NAMES = ['left', 'right', 'both_fists', 'both_feet']\n",
    "\n",
    "IMAGERY_RUNS_LR = {4, 8, 12}\n",
    "IMAGERY_RUNS_BF = {6, 10, 14}\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Usando dispositivo: {DEVICE}\")\n",
    "print(\"🧠 INICIANDO EXPERIMENTO CON CNN+Transformer (K-Fold por sujeto como EEGNet)\")\n",
    "print(f\"🔧 Configuración: 4c, 8 canales, 6s | EPOCHS={EPOCHS}, BATCH={BATCH_SIZE}, LR={BASE_LR} | ZSCORE_PER_EPOCH={ZSCORE_PER_EPOCH}\")\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES I/O\n",
    "# =========================\n",
    "def normalize_ch_name(name: str) -> str:\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', name)\n",
    "    return s.upper()\n",
    "\n",
    "NORMALIZED_TARGETS = [normalize_ch_name(c) for c in EXPECTED_8]\n",
    "\n",
    "def pick_8_channels(raw: mne.io.BaseRaw) -> mne.io.BaseRaw:\n",
    "    chs = raw.info['ch_names']\n",
    "    norm_map = {normalize_ch_name(ch): ch for ch in chs}\n",
    "    picked = []\n",
    "    for target_norm, target_orig in zip(NORMALIZED_TARGETS, EXPECTED_8):\n",
    "        if target_norm in norm_map:\n",
    "            picked.append(norm_map[target_norm])\n",
    "        else:\n",
    "            raise RuntimeError(f\"Canal requerido '{target_orig}' no encontrado. Disponibles: {chs}\")\n",
    "    return raw.pick(picks=picked)\n",
    "\n",
    "def list_subject_imagery_edfs(subject_id: str) -> list:\n",
    "    subj_dir = DATA_RAW / subject_id\n",
    "    edfs = []\n",
    "    for r in [4, 6, 8, 10, 12, 14]:\n",
    "        edfs.extend(glob(str(subj_dir / f\"{subject_id}R{r:02d}.edf\")))\n",
    "    return sorted(edfs)\n",
    "\n",
    "def subject_id_to_int(s: str) -> int:\n",
    "    m = re.match(r'[Ss](\\d+)', s)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "def load_subject_epochs(subject_id: str, resample_hz: int, do_notch: bool, do_bandpass: bool,\n",
    "                        do_car: bool, bp_lo: float, bp_hi: float):\n",
    "    edfs = list_subject_imagery_edfs(subject_id)\n",
    "    if len(edfs) == 0:\n",
    "        return np.empty((0,8,1), dtype=np.float32), np.empty((0,), dtype=int), None\n",
    "\n",
    "    X_list, y_list, sfreq_list = [], [], []\n",
    "\n",
    "    for edf_path in edfs:\n",
    "        m = re.search(r\"R(\\d{2})\", Path(edf_path).name)\n",
    "        run = int(m.group(1)) if m else -1\n",
    "\n",
    "        raw = mne.io.read_raw_edf(edf_path, preload=True, verbose='ERROR')\n",
    "        raw = pick_8_channels(raw)\n",
    "\n",
    "        if do_notch:\n",
    "            raw.notch_filter(freqs=[60.0], picks='all', verbose='ERROR')\n",
    "        if do_bandpass:\n",
    "            raw.filter(l_freq=bp_lo, h_freq=bp_hi, picks='all', verbose='ERROR')\n",
    "        if do_car:\n",
    "            raw.set_eeg_reference('average', projection=False, verbose='ERROR')\n",
    "\n",
    "        if resample_hz is not None and resample_hz > 0:\n",
    "            raw.resample(resample_hz)\n",
    "        sfreq = raw.info['sfreq']\n",
    "\n",
    "        events, event_id = mne.events_from_annotations(raw, verbose='ERROR')\n",
    "        keep = {k: v for k, v in event_id.items() if k in {'T1', 'T2'}}\n",
    "        if len(keep) == 0:\n",
    "            continue\n",
    "\n",
    "        epochs = mne.Epochs(raw, events=events, event_id=keep, tmin=TMIN, tmax=TMAX,\n",
    "                            baseline=None, preload=True, verbose='ERROR')\n",
    "        X = epochs.get_data()\n",
    "\n",
    "        if ZSCORE_PER_EPOCH:\n",
    "            X = X.astype(np.float32)\n",
    "            eps = 1e-6\n",
    "            mu = X.mean(axis=2, keepdims=True)\n",
    "            sd = X.std(axis=2, keepdims=True) + eps\n",
    "            X = (X - mu) / sd\n",
    "\n",
    "        ev_codes = epochs.events[:, 2]\n",
    "        inv = {v: k for k, v in keep.items()}\n",
    "        y_run = []\n",
    "        for code in ev_codes:\n",
    "            lab = inv[code]\n",
    "            if run in IMAGERY_RUNS_LR:\n",
    "                y_run.append(0 if lab == 'T1' else 1)\n",
    "            elif run in IMAGERY_RUNS_BF:\n",
    "                y_run.append(2 if lab == 'T1' else 3)\n",
    "            else:\n",
    "                y_run.append(-1)\n",
    "        y_run = np.array(y_run, dtype=int)\n",
    "        mask = y_run >= 0\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        X_list.append(X[mask])\n",
    "        y_list.append(y_run[mask])\n",
    "        sfreq_list.append(sfreq)\n",
    "\n",
    "    if len(X_list) == 0:\n",
    "        return np.empty((0,8,1), dtype=np.float32), np.empty((0,), dtype=int), None\n",
    "\n",
    "    X_all = np.concatenate(X_list, axis=0).astype(np.float32)\n",
    "    y_all = np.concatenate(y_list, axis=0).astype(int)\n",
    "\n",
    "    if len(set([int(round(s)) for s in sfreq_list])) != 1:\n",
    "        raise RuntimeError(f\"Sampling rates inconsistentes: {sfreq_list}\")\n",
    "\n",
    "    return X_all, y_all, sfreq_list[0]\n",
    "\n",
    "def load_fold_subjects(folds_json: Path, fold: int):\n",
    "    with open(folds_json, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    for item in data.get('folds', []):\n",
    "        if int(item.get('fold', -1)) == int(fold):\n",
    "            return list(item.get('train', [])), list(item.get('test', []))\n",
    "    raise ValueError(f\"Fold {fold} not found in {folds_json}\")\n",
    "\n",
    "def standardize_per_channel(train_X, other_X):\n",
    "    C = train_X.shape[1]\n",
    "    train_X = train_X.astype(np.float32)\n",
    "    other_X = other_X.astype(np.float32)\n",
    "    for c in range(C):\n",
    "        mu = train_X[:, c, :].mean()\n",
    "        sd = train_X[:, c, :].std()\n",
    "        sd = sd if sd > 1e-6 else 1.0\n",
    "        train_X[:, c, :] = (train_X[:, c, :] - mu) / sd\n",
    "        other_X[:, c, :]  = (other_X[:, c, :] - mu) / sd\n",
    "    return train_X, other_X\n",
    "\n",
    "# =========================\n",
    "# MODELO (GroupNorm en conv)\n",
    "# =========================\n",
    "def make_gn(num_channels, num_groups=8):\n",
    "    g = min(num_groups, num_channels)\n",
    "    while num_channels % g != 0 and g > 1:\n",
    "        g -= 1\n",
    "    return nn.GroupNorm(g, num_channels)\n",
    "\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k, s=1, p=0, p_drop=0.2):\n",
    "        super().__init__()\n",
    "        self.dw = nn.Conv1d(in_ch, in_ch, kernel_size=k, stride=s, padding=p, groups=in_ch, bias=False)\n",
    "        self.pw = nn.Conv1d(in_ch, out_ch, kernel_size=1, bias=False)\n",
    "        self.norm = make_gn(out_ch)\n",
    "        self.act = nn.ELU()\n",
    "        self.dropout = nn.Dropout(p=p_drop)\n",
    "    def forward(self, x):\n",
    "        x = self.dw(x); x = self.pw(x); x = self.norm(x)\n",
    "        x = self.act(x); x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class EEGCNNTransformer(nn.Module):\n",
    "    def __init__(self, n_ch=8, n_cls=4, d_model=128, n_heads=4, n_layers=2,\n",
    "                 p_drop=0.2, p_drop_encoder=0.3):\n",
    "        super().__init__()\n",
    "        self.conv_t = nn.Sequential(\n",
    "            nn.Conv1d(n_ch, 32, kernel_size=129, stride=2, padding=64, bias=False),\n",
    "            make_gn(32),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=p_drop),\n",
    "            DepthwiseSeparableConv(32, 64, k=31, s=2, p=15, p_drop=p_drop),\n",
    "            DepthwiseSeparableConv(64, 128, k=15, s=2, p=7,  p_drop=p_drop),\n",
    "        )\n",
    "        self.proj = nn.Conv1d(128, d_model, kernel_size=1, bias=False)\n",
    "        self.dropout = nn.Dropout(p=p_drop_encoder)\n",
    "        self.pos_encoding = None\n",
    "        enc = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=2*d_model,\n",
    "            batch_first=True, activation='gelu', dropout=0.1, norm_first=True  # (4)\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc, num_layers=n_layers)\n",
    "        self.cls = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        nn.init.normal_(self.cls, std=0.02)\n",
    "        self.head = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, n_cls))\n",
    "\n",
    "    def _positional_encoding(self, L, d):\n",
    "        pos = torch.arange(0, L, dtype=torch.float32).unsqueeze(1)\n",
    "        i   = torch.arange(0, d, dtype=torch.float32).unsqueeze(0)\n",
    "        angle = pos / torch.pow(10000, (2 * (i//2)) / d)\n",
    "        pe = torch.zeros(L, d, dtype=torch.float32)\n",
    "        pe[:, 0::2] = torch.sin(angle[:, 0::2])\n",
    "        pe[:, 1::2] = torch.cos(angle[:, 1::2])\n",
    "        return pe\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.conv_t(x)           # (B, 128, T')\n",
    "        z = self.proj(z)             # (B, d_model, T')\n",
    "        z = self.dropout(z)\n",
    "        z = z.transpose(1, 2)        # (B, T', d_model)\n",
    "        B, L, D = z.shape\n",
    "        if (self.pos_encoding is None) or (self.pos_encoding.shape[0] != L) or (self.pos_encoding.shape[1] != D):\n",
    "            self.pos_encoding = self._positional_encoding(L, D).to(z.device)\n",
    "        z = z + self.pos_encoding[None, :, :]\n",
    "        cls_tok = self.cls.expand(B, -1, -1)\n",
    "        z = torch.cat([cls_tok, z], dim=1)\n",
    "        z = self.encoder(z)\n",
    "        cls = z[:, 0, :]\n",
    "        return self.head(cls)\n",
    "\n",
    "# =========================\n",
    "# (1) FOCAL LOSS\n",
    "# =========================\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha: torch.Tensor, gamma: float = 1.5, reduction: str = 'mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha / alpha.sum()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    def forward(self, logits, target):\n",
    "        logp = nn.functional.log_softmax(logits, dim=-1)      # (B,C)\n",
    "        p = logp.exp()\n",
    "        idx = torch.arange(target.shape[0], device=logits.device)\n",
    "        pt = p[idx, target]\n",
    "        logpt = logp[idx, target]\n",
    "        at = self.alpha[target]\n",
    "        loss = - at * ((1 - pt) ** self.gamma) * logpt\n",
    "        if self.reduction == 'mean': return loss.mean()\n",
    "        if self.reduction == 'sum':  return loss.sum()\n",
    "        return loss\n",
    "\n",
    "# =========================\n",
    "# (4) AUGMENTS MÁS FUERTES (pero estables)\n",
    "# =========================\n",
    "def augment_batch(\n",
    "    xb,\n",
    "    p_jitter=0.35, p_noise=0.35, p_chdrop=0.15,\n",
    "    max_jitter_frac=0.03, noise_std=0.03, max_chdrop=1\n",
    "):\n",
    "    B, C, T = xb.shape\n",
    "    if np.random.rand() < p_jitter:\n",
    "        max_shift = int(max(1, T*max_jitter_frac))\n",
    "        shifts = torch.randint(low=-max_shift, high=max_shift+1, size=(B,), device=xb.device)\n",
    "        for i in range(B):\n",
    "            xb[i] = torch.roll(xb[i], shifts=int(shifts[i].item()), dims=-1)\n",
    "    if np.random.rand() < p_noise:\n",
    "        xb = xb + noise_std*torch.randn_like(xb)\n",
    "    if np.random.rand() < p_chdrop and max_chdrop > 0:\n",
    "        k = min(max_chdrop, C)\n",
    "        for i in range(B):\n",
    "            idx = torch.randperm(C, device=xb.device)[:k]\n",
    "            xb[i, idx, :] = 0.0\n",
    "    return xb\n",
    "\n",
    "# =========================\n",
    "# EMA de pesos\n",
    "# =========================\n",
    "class ModelEMA:\n",
    "    def __init__(self, model: nn.Module, decay: float = 0.9995, device=None):\n",
    "        self.ema = self._clone(model).to(device if device is not None else next(model.parameters()).device)\n",
    "        self.decay = decay\n",
    "        self._updates = 0\n",
    "        self.update(model, force=True)\n",
    "\n",
    "    def _clone(self, model):\n",
    "        ema = type(model)()\n",
    "        ema.load_state_dict(model.state_dict())\n",
    "        for p in ema.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        return ema\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model: nn.Module, force: bool = False):\n",
    "        d = self.decay\n",
    "        # (6) warmup del decay extendido a 1000 updates\n",
    "        if self._updates < 1000:\n",
    "            d = (self._updates / 1000.0) * self.decay\n",
    "        msd = model.state_dict()\n",
    "        esd = self.ema.state_dict()\n",
    "        for k in esd.keys():\n",
    "            if esd[k].dtype.is_floating_point:\n",
    "                esd[k].mul_(d).add_(msd[k].detach(), alpha=1.0 - d)\n",
    "            else:\n",
    "                esd[k] = msd[k]\n",
    "        self._updates += 1\n",
    "\n",
    "# =========================\n",
    "# INFERENCIA TTA / SUBWINDOW\n",
    "# =========================\n",
    "def subwindow_logits(model, X, sfreq, sw_len, sw_stride, device):\n",
    "    model.eval()\n",
    "    wl = int(round(sw_len * sfreq))\n",
    "    st = int(round(sw_stride * sfreq))\n",
    "    wl = max(1, min(wl, X.shape[-1])); st = max(1, st)\n",
    "    out = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(X.shape[0]):\n",
    "            x = X[i]; acc = []\n",
    "            for s in range(0, max(1, X.shape[-1]-wl+1), st):\n",
    "                seg = x[:, s:s+wl]\n",
    "                if seg.shape[-1] < wl:\n",
    "                    pad = wl - seg.shape[-1]\n",
    "                    seg = np.pad(seg, ((0,0),(0,pad)), mode='edge')\n",
    "                xb = torch.tensor(seg[None, ...], dtype=torch.float32, device=device)\n",
    "                logit = model(xb).detach().cpu().numpy()[0]\n",
    "                acc.append(logit)\n",
    "            acc = np.mean(np.stack(acc, axis=0), axis=0) if len(acc) else np.zeros(4, dtype=np.float32)\n",
    "            out.append(acc)\n",
    "    return np.stack(out, axis=0)\n",
    "\n",
    "def time_shift_tta_logits(model, X, sfreq, shifts_s, device):\n",
    "    model.eval()\n",
    "    T = X.shape[-1]; out = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(X.shape[0]):\n",
    "            x0 = X[i]; acc = []\n",
    "            for sh in shifts_s:\n",
    "                shift = int(round(sh * sfreq))\n",
    "                if shift == 0:\n",
    "                    x = x0\n",
    "                elif shift > 0:\n",
    "                    x = np.pad(x0[:, shift:], ((0,0),(0,shift)), mode='edge')[:, :T]\n",
    "                else:\n",
    "                    shift = -shift\n",
    "                    x = np.pad(x0[:, :-shift], ((0,0),(shift,0)), mode='edge')[:, :T]\n",
    "                xb = torch.tensor(x[None, ...], dtype=torch.float32, device=device)\n",
    "                logit = model(xb).detach().cpu().numpy()[0]\n",
    "                acc.append(logit)\n",
    "            out.append(np.mean(np.stack(acc, axis=0), axis=0))\n",
    "    return np.stack(out, axis=0)\n",
    "\n",
    "# =========================\n",
    "# Utilidades splits estratificados por sujeto\n",
    "# =========================\n",
    "def build_subject_label_map(subject_ids):\n",
    "    y_dom_list = []\n",
    "    for sid in subject_ids:\n",
    "        Xs, ys, _ = load_subject_epochs(sid, RESAMPLE_HZ, DO_NOTCH, DO_BANDPASS, DO_CAR, BP_LO, BP_HI)\n",
    "        if len(ys) == 0:\n",
    "            y_dom_list.append(-1)\n",
    "            continue\n",
    "        binc = np.bincount(ys, minlength=4)\n",
    "        y_dom = int(np.argmax(binc))\n",
    "        y_dom_list.append(y_dom)\n",
    "    return np.array(y_dom_list, dtype=int)\n",
    "\n",
    "# =========================\n",
    "# TRAIN/EVAL por FOLD (con train/val/test)\n",
    "# =========================\n",
    "def train_one_fold(fold:int, device):\n",
    "    def load_fold_subjects_local(folds_json: Path, fold: int):\n",
    "        return load_fold_subjects(folds_json, fold)\n",
    "\n",
    "    train_sub, test_sub = load_fold_subjects_local(FOLDS_JSON, fold)\n",
    "    train_sub = [s for s in train_sub if subject_id_to_int(s) not in EXCLUDE_SUBJECTS]\n",
    "    test_sub  = [s for s in test_sub  if subject_id_to_int(s) not in EXCLUDE_SUBJECTS]\n",
    "\n",
    "    # Split de validación por sujetos (determinista y opcionalmente estratificado)\n",
    "    rng = np.random.RandomState(RANDOM_STATE + fold)\n",
    "    tr_subjects = sorted(train_sub)\n",
    "\n",
    "    if VAL_STRAT_SUBJECT and len(tr_subjects) > 1:\n",
    "        y_dom = build_subject_label_map(tr_subjects)\n",
    "        if np.any(y_dom < 0):\n",
    "            mask = y_dom >= 0\n",
    "            moda = int(np.bincount(y_dom[mask]).argmax()) if mask.sum() > 0 else 0\n",
    "            y_dom[~mask] = moda\n",
    "        n_val_subj = max(1, int(round(len(tr_subjects) * VAL_SUBJECT_FRAC)))\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=n_val_subj, random_state=RANDOM_STATE + fold)\n",
    "        idx = np.arange(len(tr_subjects))\n",
    "        _, val_idx = next(sss.split(idx, y_dom))\n",
    "        val_subjects = sorted([tr_subjects[i] for i in val_idx])\n",
    "        train_subjects = [s for s in tr_subjects if s not in val_subjects]\n",
    "    else:\n",
    "        tr_subjects_shuf = tr_subjects.copy()\n",
    "        rng.shuffle(tr_subjects_shuf)\n",
    "        n_val_subj = max(1, int(round(len(tr_subjects_shuf) * VAL_SUBJECT_FRAC)))\n",
    "        val_subjects = sorted(tr_subjects_shuf[:n_val_subj])\n",
    "        train_subjects = sorted(tr_subjects_shuf[n_val_subj:])\n",
    "\n",
    "    # Carga TRAIN/VAL/TEST\n",
    "    X_tr_list, y_tr_list, sub_tr_list = [], [], []\n",
    "    X_val_list, y_val_list, sub_val_list = [], [], []\n",
    "    X_te_list, y_te_list, sub_te_list = [], [], []\n",
    "    sfreq = None\n",
    "\n",
    "    for sid in tqdm(train_subjects, desc=f\"Cargando train fold{fold}\"):\n",
    "        Xs, ys, sf = load_subject_epochs(sid, RESAMPLE_HZ, DO_NOTCH, DO_BANDPASS, DO_CAR, BP_LO, BP_HI)\n",
    "        if len(ys) == 0: continue\n",
    "        X_tr_list.append(Xs); y_tr_list.append(ys)\n",
    "        sub_tr_list.append(np.full_like(ys, fill_value=subject_id_to_int(sid)))\n",
    "        sfreq = sf if sfreq is None else sfreq\n",
    "\n",
    "    for sid in tqdm(val_subjects, desc=f\"Cargando val fold{fold}\"):\n",
    "        Xs, ys, sf = load_subject_epochs(sid, RESAMPLE_HZ, DO_NOTCH, DO_BANDPASS, DO_CAR, BP_LO, BP_HI)\n",
    "        if len(ys) == 0: continue\n",
    "        X_val_list.append(Xs); y_val_list.append(ys)\n",
    "        sub_val_list.append(np.full_like(ys, fill_value=subject_id_to_int(sid)))\n",
    "        sfreq = sf if sfreq is None else sfreq\n",
    "\n",
    "    for sid in tqdm(test_sub, desc=f\"Cargando test fold{fold}\"):\n",
    "        Xs, ys, sf = load_subject_epochs(sid, RESAMPLE_HZ, DO_NOTCH, DO_BANDPASS, DO_CAR, BP_LO, BP_HI)\n",
    "        if len(ys) == 0: continue\n",
    "        X_te_list.append(Xs); y_te_list.append(ys)\n",
    "        sub_te_list.append(np.full_like(ys, fill_value=subject_id_to_int(sid)))\n",
    "        sfreq = sf if sfreq is None else sfreq\n",
    "\n",
    "    # Concatenar\n",
    "    X_tr = np.concatenate(X_tr_list, axis=0); y_tr = np.concatenate(y_tr_list, axis=0)\n",
    "    sub_tr = np.concatenate(sub_tr_list, axis=0)\n",
    "    X_val = np.concatenate(X_val_list, axis=0); y_val = np.concatenate(y_val_list, axis=0)\n",
    "    sub_val = np.concatenate(sub_val_list, axis=0)\n",
    "    X_te = np.concatenate(X_te_list, axis=0); y_te = np.concatenate(y_te_list, axis=0)\n",
    "    sub_te = np.concatenate(sub_te_list, axis=0)\n",
    "\n",
    "    print(f\"[Fold {fold}/5] Entrenando modelo global... (n_train={len(y_tr)} | n_val={len(y_val)} | n_test={len(y_te)})\")\n",
    "\n",
    "    # Normalización por canal (fit en TRAIN y aplicar a VAL/TEST)\n",
    "    if ZSCORE_PER_EPOCH:\n",
    "        X_tr_std, X_val_std, X_te_std = X_tr, X_val, X_te\n",
    "    else:\n",
    "        X_tr_std, X_val_std = standardize_per_channel(X_tr, X_val)\n",
    "        _,        X_te_std  = standardize_per_channel(X_tr, X_te)\n",
    "\n",
    "    # Datasets\n",
    "    tr_ds  = TensorDataset(torch.tensor(X_tr_std),  torch.tensor(y_tr).long(),  torch.tensor(sub_tr).long())\n",
    "    val_ds = TensorDataset(torch.tensor(X_val_std), torch.tensor(y_val).long(), torch.tensor(sub_val).long())\n",
    "    te_ds  = TensorDataset(torch.tensor(X_te_std),  torch.tensor(y_te).long(),  torch.tensor(sub_te).long())\n",
    "\n",
    "    # (2) Weighted sampler templado: w = (1/ws)^a * (1/wk)^b\n",
    "    def make_weighted_sampler(dataset: TensorDataset):\n",
    "        _Xb, yb, sb = dataset.tensors\n",
    "        yb_np = yb.numpy()\n",
    "        sb_np = sb.numpy()\n",
    "        uniq_s, cnt_s = np.unique(sb_np, return_counts=True)\n",
    "        map_s = {s:c for s,c in zip(uniq_s, cnt_s)}\n",
    "        key = sb_np.astype(np.int64) * 10 + yb_np.astype(np.int64)\n",
    "        uniq_k, cnt_k = np.unique(key, return_counts=True)\n",
    "        map_k = {k:c for k,c in zip(uniq_k, cnt_k)}\n",
    "        a, b = 0.8, 1.0\n",
    "        w = []\n",
    "        for s, y in zip(sb_np, yb_np):\n",
    "            k = int(s)*10 + int(y)\n",
    "            ws = float(map_s[int(s)])\n",
    "            wk = float(map_k[k])\n",
    "            w.append((ws ** (-a)) * (wk ** (-b)))\n",
    "        w = np.array(w, dtype=np.float64)\n",
    "        w = w / (w.mean() + 1e-12)\n",
    "        sampler = WeightedRandomSampler(weights=torch.tensor(w, dtype=torch.double),\n",
    "                                        num_samples=len(yb_np), replacement=True)\n",
    "        return sampler\n",
    "\n",
    "    if USE_WEIGHTED_SAMPLER:\n",
    "        tr_sampler = make_weighted_sampler(tr_ds)\n",
    "        tr_ld  = DataLoader(tr_ds, batch_size=BATCH_SIZE, sampler=tr_sampler, drop_last=False, worker_init_fn=seed_worker)\n",
    "    else:\n",
    "        tr_ld  = DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True,  drop_last=False, worker_init_fn=seed_worker)\n",
    "\n",
    "    val_ld = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, worker_init_fn=seed_worker)\n",
    "    te_ld  = DataLoader(te_ds,  batch_size=BATCH_SIZE, shuffle=False, drop_last=False, worker_init_fn=seed_worker)\n",
    "\n",
    "    # Modelo\n",
    "    model = EEGCNNTransformer(n_ch=8, n_cls=4, d_model=D_MODEL, n_heads=N_HEADS,\n",
    "                              n_layers=N_LAYERS, p_drop=P_DROP, p_drop_encoder=P_DROP_ENCODER).to(device)\n",
    "\n",
    "    # Optimizador + Focal Loss (3) alpha tweaking\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=1e-2)\n",
    "\n",
    "    class_counts = np.bincount(y_tr, minlength=4).astype(np.float32)\n",
    "    inv = class_counts.sum() / (4.0 * np.maximum(class_counts, 1.0))\n",
    "    alpha = torch.tensor(inv, dtype=torch.float32, device=device)\n",
    "    alpha_mean = alpha.mean().item()\n",
    "    alpha[2] = 1.5 * alpha_mean      # both_fists (ya estaba)\n",
    "    alpha[3] = 1.15 * alpha_mean      # (3) pequeño empujón a both_feet\n",
    "    crit = FocalLoss(alpha=alpha, gamma=1.5, reduction='mean')\n",
    "\n",
    "    # (7) Warmup+Cosine (min_factor=0.1)\n",
    "    from torch.optim.lr_scheduler import LambdaLR\n",
    "    total_epochs = EPOCHS\n",
    "    warmup_epochs = max(1, int(WARMUP_EPOCHS))\n",
    "    min_factor = 0.1\n",
    "    def lr_lambda(current_epoch):\n",
    "        if current_epoch < warmup_epochs:\n",
    "            return (current_epoch + 1) / warmup_epochs\n",
    "        progress = (current_epoch - warmup_epochs) / max(1, (total_epochs - warmup_epochs))\n",
    "        progress = min(1.0, max(0.0, progress))\n",
    "        return min_factor + 0.5 * (1.0 - min_factor) * (1.0 + np.cos(np.pi * progress))\n",
    "    scheduler = LambdaLR(opt, lr_lambda=lr_lambda)\n",
    "\n",
    "    # EMA\n",
    "    if USE_EMA:\n",
    "        ema = ModelEMA(model, decay=EMA_DECAY, device=device)\n",
    "    else:\n",
    "        ema = None\n",
    "\n",
    "    # Entrenamiento con early stopping por F1 macro (EMA en val/test si aplica)\n",
    "    best_f1, best_state, wait = 0.0, None, 0\n",
    "    hist = {\"ep\": [], \"tr_loss\": [], \"tr_acc\": [], \"val_acc\": [], \"val_f1m\": [], \"lr\": []}\n",
    "\n",
    "    def evaluate_on(loader, use_ema=True):\n",
    "        mdl = ema.ema if (ema is not None and use_ema) else model\n",
    "        mdl.eval()\n",
    "        preds, gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _sb in loader:\n",
    "                xb = xb.to(device)\n",
    "                p = mdl(xb).argmax(dim=1).cpu().numpy()\n",
    "                preds.append(p); gts.append(yb.numpy())\n",
    "        preds = np.concatenate(preds); gts = np.concatenate(gts)\n",
    "        acc = accuracy_score(gts, preds)\n",
    "        f1m = f1_score(gts, preds, average='macro')\n",
    "        return acc, f1m\n",
    "\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        # ---- Train ----\n",
    "        model.train()\n",
    "        tr_loss, n_seen, tr_correct = 0.0, 0, 0\n",
    "        for xb, yb, _sb in tr_ld:\n",
    "            xb = xb.to(device); yb = yb.to(device)\n",
    "            xb = augment_batch(xb)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            loss.backward()\n",
    "            # (5) Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            opt.step()\n",
    "            if ema is not None:\n",
    "                ema.update(model)\n",
    "\n",
    "            tr_loss += loss.item() * len(yb)\n",
    "            n_seen += len(yb)\n",
    "            tr_correct += (logits.argmax(1) == yb).sum().item()\n",
    "        tr_loss /= max(1, n_seen)\n",
    "        tr_acc = tr_correct / max(1, n_seen)\n",
    "\n",
    "        # ---- Val (EMA si aplica) ----\n",
    "        acc, f1m = evaluate_on(val_ld, use_ema=True)\n",
    "\n",
    "        hist[\"ep\"].append(ep)\n",
    "        hist[\"tr_loss\"].append(tr_loss)\n",
    "        hist[\"tr_acc\"].append(tr_acc)\n",
    "        hist[\"val_acc\"].append(acc)\n",
    "        hist[\"val_f1m\"].append(f1m)\n",
    "        hist[\"lr\"].append(scheduler.get_last_lr()[0])\n",
    "\n",
    "        print(f\"  Época {ep:3d} | train_loss={tr_loss:.4f} | train_acc={tr_acc:.4f} | val_acc={acc:.4f} | val_f1m={f1m:.4f} | LR={scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        improved = f1m > best_f1 + 1e-4\n",
    "        if improved:\n",
    "            best_f1 = f1m\n",
    "            ref_model = ema.ema if ema is not None else model\n",
    "            best_state = {k: v.detach().cpu() for k, v in ref_model.state_dict().items()}\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "\n",
    "        scheduler.step()\n",
    "        if wait >= PATIENCE:\n",
    "            print(f\"  Early stopping en época {ep} (mejor val_f1m={best_f1:.4f})\")\n",
    "            break\n",
    "\n",
    "    # Cargamos mejor estado guardado (EMA si estaba activo)\n",
    "    if best_state is not None:\n",
    "        (ema.ema if (ema is not None) else model).load_state_dict(best_state)\n",
    "\n",
    "    # ---- Guardar curva ----\n",
    "    fig = plt.figure(figsize=(8,4.5))\n",
    "    ax1 = plt.gca()\n",
    "    ax1.plot(hist[\"ep\"], hist[\"tr_loss\"], label=\"train_loss\")\n",
    "    ax1.plot(hist[\"ep\"], hist[\"val_f1m\"], label=\"val_f1m\")\n",
    "    ax1.plot(hist[\"ep\"], hist[\"val_acc\"], label=\"val_acc\")\n",
    "    ax1.set_xlabel(\"Época\"); ax1.set_title(f\"Fold {fold} — Curva de entrenamiento\")\n",
    "    ax1.legend(); ax1.grid(True, alpha=0.3)\n",
    "    out_png = f\"training_curve_fold{fold}.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=140)\n",
    "    plt.close(fig)\n",
    "    print(f\"↳ Curva de entrenamiento guardada: {out_png}\")\n",
    "\n",
    "    # ---- Evaluación final en TEST ----\n",
    "    eval_model = ema.ema if (ema is not None) else model\n",
    "    eval_model.eval()\n",
    "\n",
    "    sfreq_used = RESAMPLE_HZ\n",
    "    if sfreq_used is None:\n",
    "        sfreq_used = int(round(X_te_std.shape[-1] / (TMAX - TMIN)))\n",
    "\n",
    "    if (not SW_ENABLE) or SW_MODE == 'none':\n",
    "        preds, gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _sb in te_ld:\n",
    "                xb = xb.to(device)\n",
    "                p = eval_model(xb).argmax(dim=1).cpu().numpy()\n",
    "                preds.append(p); gts.append(yb.numpy())\n",
    "        preds = np.concatenate(preds); gts = np.concatenate(gts)\n",
    "    elif SW_MODE in ('subwin', 'tta'):\n",
    "        logits_tta = None\n",
    "        logits_sw  = None\n",
    "        if SW_MODE == 'subwin':\n",
    "            logits_sw = subwindow_logits(eval_model, X_te_std, sfreq_used, SW_LEN, SW_STRIDE, device)\n",
    "        elif SW_MODE == 'tta':\n",
    "            logits_tta = time_shift_tta_logits(eval_model, X_te_std, sfreq_used, TTA_SHIFTS_S, device)\n",
    "\n",
    "        if COMBINE_TTA_AND_SUBWIN:\n",
    "            if logits_tta is None:\n",
    "                logits_tta = time_shift_tta_logits(eval_model, X_te_std, sfreq_used, TTA_SHIFTS_S, device)\n",
    "            if logits_sw is None:\n",
    "                logits_sw  = subwindow_logits(eval_model, X_te_std, sfreq_used, SW_LEN, SW_STRIDE, device)\n",
    "            logits = 0.5 * logits_tta + 0.5 * logits_sw\n",
    "        else:\n",
    "            logits = logits_tta if logits_tta is not None else logits_sw\n",
    "\n",
    "        preds = logits.argmax(axis=1); gts = y_te\n",
    "    else:\n",
    "        raise ValueError(f\"SW_MODE desconocido: {SW_MODE}\")\n",
    "\n",
    "    acc = accuracy_score(gts, preds)\n",
    "    f1m = f1_score(gts, preds, average='macro')\n",
    "    print(f\"[Fold {fold}/5] Global acc={acc:.4f}\\n\")\n",
    "    print(classification_report(gts, preds, target_names=[c.replace('_',' ') for c in CLASS_NAMES], digits=4))\n",
    "    print(\"Confusion matrix (rows=true, cols=pred):\")\n",
    "    print(confusion_matrix(gts, preds, labels=[0,1,2,3]))\n",
    "\n",
    "    return acc, f1m\n",
    "\n",
    "# =========================\n",
    "# LOOP 5 FOLDS + RESUMEN\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    acc_folds, f1_folds = [], []\n",
    "    for fold in range(1, 6):\n",
    "        acc, f1m = train_one_fold(fold, DEVICE)\n",
    "        acc_folds.append(f\"{acc:.4f}\")\n",
    "        f1_folds.append(f\"{f1m:.4f}\")\n",
    "\n",
    "    acc_mean = float(np.mean([float(a) for a in acc_folds]))\n",
    "    f1_mean  = float(np.mean([float(f) for f in f1_folds]))\n",
    "\n",
    "    print(\"\\n============================================================\")\n",
    "    print(\"RESULTADOS FINALES\")\n",
    "    print(\"============================================================\")\n",
    "    print(f\"Global folds (ACC): {acc_folds}\")\n",
    "    print(f\"Global mean ACC: {acc_mean:.4f}\")\n",
    "    print(f\"F1 folds (MACRO): {f1_folds}\")\n",
    "    print(f\"F1 mean (MACRO): {f1_mean:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b0fe5b",
   "metadata": {},
   "source": [
    "# MEJOR 54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3552953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Usando dispositivo: cuda\n",
      "🧠 INICIANDO EXPERIMENTO CON CNN+Transformer (K-Fold por sujeto como EEGNet)\n",
      "🔧 Configuración: 4c, 8 canales, 6s | EPOCHS=60, BATCH=64, LR=0.001 | ZSCORE_PER_EPOCH=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold1: 100%|██████████| 67/67 [00:08<00:00,  8.35it/s]\n",
      "Cargando val fold1: 100%|██████████| 15/15 [00:01<00:00,  8.34it/s]\n",
      "Cargando test fold1: 100%|██████████| 21/21 [00:02<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1/5] Entrenando modelo global... (n_train=5628 | n_val=1260 | n_test=1764)\n",
      "  Época   1 | train_loss=0.2326 | train_acc=0.2772 | val_acc=0.2627 | val_f1m=0.2225 | LR=0.000125\n",
      "  Época   2 | train_loss=0.2126 | train_acc=0.3257 | val_acc=0.3825 | val_f1m=0.3362 | LR=0.000250\n",
      "  Época   3 | train_loss=0.1839 | train_acc=0.4577 | val_acc=0.4381 | val_f1m=0.4333 | LR=0.000375\n",
      "  Época   4 | train_loss=0.1797 | train_acc=0.4844 | val_acc=0.4571 | val_f1m=0.4561 | LR=0.000500\n",
      "  Época   5 | train_loss=0.1724 | train_acc=0.5137 | val_acc=0.4508 | val_f1m=0.4461 | LR=0.000625\n",
      "  Época   6 | train_loss=0.1673 | train_acc=0.5348 | val_acc=0.4476 | val_f1m=0.4405 | LR=0.000750\n",
      "  Época   7 | train_loss=0.1650 | train_acc=0.5311 | val_acc=0.4397 | val_f1m=0.4288 | LR=0.000875\n",
      "  Época   8 | train_loss=0.1653 | train_acc=0.5359 | val_acc=0.4611 | val_f1m=0.4632 | LR=0.001000\n",
      "  Época   9 | train_loss=0.1664 | train_acc=0.5226 | val_acc=0.4484 | val_f1m=0.4503 | LR=0.001000\n",
      "  Época  10 | train_loss=0.1582 | train_acc=0.5414 | val_acc=0.4571 | val_f1m=0.4575 | LR=0.000999\n",
      "  Época  11 | train_loss=0.1579 | train_acc=0.5458 | val_acc=0.4635 | val_f1m=0.4634 | LR=0.000997\n",
      "  Época  12 | train_loss=0.1565 | train_acc=0.5576 | val_acc=0.4683 | val_f1m=0.4682 | LR=0.000993\n",
      "  Época  13 | train_loss=0.1520 | train_acc=0.5618 | val_acc=0.4675 | val_f1m=0.4676 | LR=0.000987\n",
      "  Época  14 | train_loss=0.1534 | train_acc=0.5684 | val_acc=0.4683 | val_f1m=0.4684 | LR=0.000980\n",
      "  Época  15 | train_loss=0.1531 | train_acc=0.5730 | val_acc=0.4675 | val_f1m=0.4676 | LR=0.000971\n",
      "  Época  16 | train_loss=0.1471 | train_acc=0.5832 | val_acc=0.4706 | val_f1m=0.4706 | LR=0.000960\n",
      "  Época  17 | train_loss=0.1446 | train_acc=0.5855 | val_acc=0.4746 | val_f1m=0.4744 | LR=0.000948\n",
      "  Época  18 | train_loss=0.1466 | train_acc=0.5732 | val_acc=0.4762 | val_f1m=0.4761 | LR=0.000935\n",
      "  Época  19 | train_loss=0.1452 | train_acc=0.5876 | val_acc=0.4778 | val_f1m=0.4776 | LR=0.000920\n",
      "  Época  20 | train_loss=0.1438 | train_acc=0.5864 | val_acc=0.4802 | val_f1m=0.4799 | LR=0.000904\n",
      "  Época  21 | train_loss=0.1454 | train_acc=0.5823 | val_acc=0.4794 | val_f1m=0.4787 | LR=0.000887\n",
      "  Época  22 | train_loss=0.1424 | train_acc=0.5904 | val_acc=0.4770 | val_f1m=0.4760 | LR=0.000868\n",
      "  Época  23 | train_loss=0.1415 | train_acc=0.5885 | val_acc=0.4762 | val_f1m=0.4752 | LR=0.000848\n",
      "  Época  24 | train_loss=0.1392 | train_acc=0.6025 | val_acc=0.4794 | val_f1m=0.4785 | LR=0.000828\n",
      "  Época  25 | train_loss=0.1388 | train_acc=0.6016 | val_acc=0.4794 | val_f1m=0.4786 | LR=0.000806\n",
      "  Época  26 | train_loss=0.1380 | train_acc=0.6111 | val_acc=0.4794 | val_f1m=0.4783 | LR=0.000783\n",
      "  Época  27 | train_loss=0.1313 | train_acc=0.6171 | val_acc=0.4817 | val_f1m=0.4806 | LR=0.000759\n",
      "  Época  28 | train_loss=0.1351 | train_acc=0.6125 | val_acc=0.4778 | val_f1m=0.4766 | LR=0.000735\n",
      "  Época  29 | train_loss=0.1328 | train_acc=0.6103 | val_acc=0.4770 | val_f1m=0.4758 | LR=0.000710\n",
      "  Época  30 | train_loss=0.1260 | train_acc=0.6317 | val_acc=0.4802 | val_f1m=0.4788 | LR=0.000684\n",
      "  Época  31 | train_loss=0.1248 | train_acc=0.6276 | val_acc=0.4770 | val_f1m=0.4757 | LR=0.000658\n",
      "  Época  32 | train_loss=0.1287 | train_acc=0.6190 | val_acc=0.4794 | val_f1m=0.4781 | LR=0.000631\n",
      "  Época  33 | train_loss=0.1243 | train_acc=0.6338 | val_acc=0.4778 | val_f1m=0.4765 | LR=0.000604\n",
      "  Época  34 | train_loss=0.1203 | train_acc=0.6354 | val_acc=0.4802 | val_f1m=0.4788 | LR=0.000577\n",
      "  Época  35 | train_loss=0.1205 | train_acc=0.6468 | val_acc=0.4802 | val_f1m=0.4785 | LR=0.000550\n",
      "  Época  36 | train_loss=0.1166 | train_acc=0.6507 | val_acc=0.4817 | val_f1m=0.4801 | LR=0.000523\n",
      "  Época  37 | train_loss=0.1126 | train_acc=0.6672 | val_acc=0.4817 | val_f1m=0.4803 | LR=0.000496\n",
      "  Época  38 | train_loss=0.1103 | train_acc=0.6693 | val_acc=0.4770 | val_f1m=0.4758 | LR=0.000469\n",
      "  Época  39 | train_loss=0.1107 | train_acc=0.6622 | val_acc=0.4762 | val_f1m=0.4751 | LR=0.000442\n",
      "  Early stopping en época 39 (mejor val_f1m=0.4806)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold1.png\n",
      "[Fold 1/5] Global acc=0.4994\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.6199    0.5510    0.5834       441\n",
      "       right     0.5441    0.5601    0.5520       441\n",
      "  both fists     0.3892    0.4422    0.4140       441\n",
      "   both feet     0.4700    0.4444    0.4569       441\n",
      "\n",
      "    accuracy                         0.4994      1764\n",
      "   macro avg     0.5058    0.4994    0.5016      1764\n",
      "weighted avg     0.5058    0.4994    0.5016      1764\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[243  42 101  55]\n",
      " [ 28 247 100  66]\n",
      " [ 63  83 195 100]\n",
      " [ 58  82 105 196]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold2: 100%|██████████| 67/67 [00:08<00:00,  8.24it/s]\n",
      "Cargando val fold2: 100%|██████████| 15/15 [00:01<00:00,  8.36it/s]\n",
      "Cargando test fold2: 100%|██████████| 21/21 [00:02<00:00,  8.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 2/5] Entrenando modelo global... (n_train=5628 | n_val=1260 | n_test=1764)\n",
      "  Época   1 | train_loss=0.2281 | train_acc=0.2681 | val_acc=0.2865 | val_f1m=0.2064 | LR=0.000125\n",
      "  Época   2 | train_loss=0.2145 | train_acc=0.3339 | val_acc=0.4103 | val_f1m=0.3789 | LR=0.000250\n",
      "  Época   3 | train_loss=0.1877 | train_acc=0.4597 | val_acc=0.4476 | val_f1m=0.4424 | LR=0.000375\n",
      "  Época   4 | train_loss=0.1827 | train_acc=0.4696 | val_acc=0.4659 | val_f1m=0.4621 | LR=0.000500\n",
      "  Época   5 | train_loss=0.1765 | train_acc=0.4888 | val_acc=0.4921 | val_f1m=0.4929 | LR=0.000625\n",
      "  Época   6 | train_loss=0.1761 | train_acc=0.4883 | val_acc=0.4881 | val_f1m=0.4785 | LR=0.000750\n",
      "  Época   7 | train_loss=0.1764 | train_acc=0.4986 | val_acc=0.4643 | val_f1m=0.4580 | LR=0.000875\n",
      "  Época   8 | train_loss=0.1739 | train_acc=0.5084 | val_acc=0.4659 | val_f1m=0.4536 | LR=0.001000\n",
      "  Época   9 | train_loss=0.1720 | train_acc=0.4986 | val_acc=0.4937 | val_f1m=0.4930 | LR=0.001000\n",
      "  Época  10 | train_loss=0.1692 | train_acc=0.5155 | val_acc=0.4778 | val_f1m=0.4797 | LR=0.000999\n",
      "  Época  11 | train_loss=0.1651 | train_acc=0.5316 | val_acc=0.4968 | val_f1m=0.4984 | LR=0.000997\n",
      "  Época  12 | train_loss=0.1665 | train_acc=0.5290 | val_acc=0.4944 | val_f1m=0.4965 | LR=0.000993\n",
      "  Época  13 | train_loss=0.1617 | train_acc=0.5313 | val_acc=0.4984 | val_f1m=0.5005 | LR=0.000987\n",
      "  Época  14 | train_loss=0.1617 | train_acc=0.5354 | val_acc=0.4976 | val_f1m=0.4996 | LR=0.000980\n",
      "  Época  15 | train_loss=0.1607 | train_acc=0.5338 | val_acc=0.4968 | val_f1m=0.4987 | LR=0.000971\n",
      "  Época  16 | train_loss=0.1595 | train_acc=0.5430 | val_acc=0.4968 | val_f1m=0.4987 | LR=0.000960\n",
      "  Época  17 | train_loss=0.1591 | train_acc=0.5309 | val_acc=0.4944 | val_f1m=0.4965 | LR=0.000948\n",
      "  Época  18 | train_loss=0.1557 | train_acc=0.5453 | val_acc=0.4929 | val_f1m=0.4947 | LR=0.000935\n",
      "  Época  19 | train_loss=0.1566 | train_acc=0.5426 | val_acc=0.4937 | val_f1m=0.4952 | LR=0.000920\n",
      "  Época  20 | train_loss=0.1545 | train_acc=0.5537 | val_acc=0.4976 | val_f1m=0.4992 | LR=0.000904\n",
      "  Época  21 | train_loss=0.1503 | train_acc=0.5592 | val_acc=0.4984 | val_f1m=0.5000 | LR=0.000887\n",
      "  Época  22 | train_loss=0.1541 | train_acc=0.5508 | val_acc=0.5000 | val_f1m=0.5013 | LR=0.000868\n",
      "  Época  23 | train_loss=0.1504 | train_acc=0.5681 | val_acc=0.5032 | val_f1m=0.5042 | LR=0.000848\n",
      "  Época  24 | train_loss=0.1531 | train_acc=0.5506 | val_acc=0.5008 | val_f1m=0.5019 | LR=0.000828\n",
      "  Época  25 | train_loss=0.1480 | train_acc=0.5746 | val_acc=0.5008 | val_f1m=0.5016 | LR=0.000806\n",
      "  Época  26 | train_loss=0.1436 | train_acc=0.5896 | val_acc=0.5016 | val_f1m=0.5021 | LR=0.000783\n",
      "  Época  27 | train_loss=0.1461 | train_acc=0.5684 | val_acc=0.5008 | val_f1m=0.5016 | LR=0.000759\n",
      "  Época  28 | train_loss=0.1448 | train_acc=0.5798 | val_acc=0.5016 | val_f1m=0.5021 | LR=0.000735\n",
      "  Época  29 | train_loss=0.1447 | train_acc=0.5784 | val_acc=0.5016 | val_f1m=0.5017 | LR=0.000710\n",
      "  Época  30 | train_loss=0.1400 | train_acc=0.5920 | val_acc=0.5000 | val_f1m=0.5000 | LR=0.000684\n",
      "  Época  31 | train_loss=0.1416 | train_acc=0.5800 | val_acc=0.5024 | val_f1m=0.5025 | LR=0.000658\n",
      "  Época  32 | train_loss=0.1333 | train_acc=0.6079 | val_acc=0.5040 | val_f1m=0.5039 | LR=0.000631\n",
      "  Época  33 | train_loss=0.1350 | train_acc=0.6100 | val_acc=0.5016 | val_f1m=0.5017 | LR=0.000604\n",
      "  Época  34 | train_loss=0.1339 | train_acc=0.6114 | val_acc=0.5016 | val_f1m=0.5020 | LR=0.000577\n",
      "  Época  35 | train_loss=0.1301 | train_acc=0.6137 | val_acc=0.5032 | val_f1m=0.5033 | LR=0.000550\n",
      "  Early stopping en época 35 (mejor val_f1m=0.5042)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold2.png\n",
      "[Fold 2/5] Global acc=0.5805\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.6525    0.6259    0.6389       441\n",
      "       right     0.6521    0.6417    0.6469       441\n",
      "  both fists     0.4807    0.5351    0.5064       441\n",
      "   both feet     0.5505    0.5193    0.5344       441\n",
      "\n",
      "    accuracy                         0.5805      1764\n",
      "   macro avg     0.5839    0.5805    0.5817      1764\n",
      "weighted avg     0.5839    0.5805    0.5817      1764\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[276  21  93  51]\n",
      " [ 34 283  65  59]\n",
      " [ 75  53 236  77]\n",
      " [ 38  77  97 229]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold3: 100%|██████████| 67/67 [00:08<00:00,  8.37it/s]\n",
      "Cargando val fold3: 100%|██████████| 15/15 [00:01<00:00,  8.40it/s]\n",
      "Cargando test fold3: 100%|██████████| 21/21 [00:02<00:00,  8.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 3/5] Entrenando modelo global... (n_train=5628 | n_val=1260 | n_test=1764)\n",
      "  Época   1 | train_loss=0.2284 | train_acc=0.2509 | val_acc=0.2786 | val_f1m=0.2070 | LR=0.000125\n",
      "  Época   2 | train_loss=0.2205 | train_acc=0.2996 | val_acc=0.3230 | val_f1m=0.2591 | LR=0.000250\n",
      "  Época   3 | train_loss=0.1958 | train_acc=0.4232 | val_acc=0.4619 | val_f1m=0.4565 | LR=0.000375\n",
      "  Época   4 | train_loss=0.1820 | train_acc=0.4869 | val_acc=0.4643 | val_f1m=0.4600 | LR=0.000500\n",
      "  Época   5 | train_loss=0.1747 | train_acc=0.5080 | val_acc=0.4643 | val_f1m=0.4611 | LR=0.000625\n",
      "  Época   6 | train_loss=0.1756 | train_acc=0.4973 | val_acc=0.4889 | val_f1m=0.4889 | LR=0.000750\n",
      "  Época   7 | train_loss=0.1709 | train_acc=0.5162 | val_acc=0.4754 | val_f1m=0.4776 | LR=0.000875\n",
      "  Época   8 | train_loss=0.1686 | train_acc=0.5240 | val_acc=0.4849 | val_f1m=0.4766 | LR=0.001000\n",
      "  Época   9 | train_loss=0.1650 | train_acc=0.5339 | val_acc=0.4937 | val_f1m=0.4983 | LR=0.001000\n",
      "  Época  10 | train_loss=0.1663 | train_acc=0.5268 | val_acc=0.4984 | val_f1m=0.5010 | LR=0.000999\n",
      "  Época  11 | train_loss=0.1622 | train_acc=0.5457 | val_acc=0.5135 | val_f1m=0.5154 | LR=0.000997\n",
      "  Época  12 | train_loss=0.1591 | train_acc=0.5482 | val_acc=0.5151 | val_f1m=0.5174 | LR=0.000993\n",
      "  Época  13 | train_loss=0.1568 | train_acc=0.5585 | val_acc=0.5127 | val_f1m=0.5150 | LR=0.000987\n",
      "  Época  14 | train_loss=0.1554 | train_acc=0.5602 | val_acc=0.5135 | val_f1m=0.5157 | LR=0.000980\n",
      "  Época  15 | train_loss=0.1571 | train_acc=0.5569 | val_acc=0.5167 | val_f1m=0.5188 | LR=0.000971\n",
      "  Época  16 | train_loss=0.1532 | train_acc=0.5613 | val_acc=0.5159 | val_f1m=0.5177 | LR=0.000960\n",
      "  Época  17 | train_loss=0.1542 | train_acc=0.5636 | val_acc=0.5103 | val_f1m=0.5120 | LR=0.000948\n",
      "  Época  18 | train_loss=0.1517 | train_acc=0.5727 | val_acc=0.5119 | val_f1m=0.5135 | LR=0.000935\n",
      "  Época  19 | train_loss=0.1533 | train_acc=0.5608 | val_acc=0.5127 | val_f1m=0.5143 | LR=0.000920\n",
      "  Época  20 | train_loss=0.1484 | train_acc=0.5773 | val_acc=0.5111 | val_f1m=0.5128 | LR=0.000904\n",
      "  Época  21 | train_loss=0.1488 | train_acc=0.5665 | val_acc=0.5143 | val_f1m=0.5159 | LR=0.000887\n",
      "  Época  22 | train_loss=0.1464 | train_acc=0.5828 | val_acc=0.5167 | val_f1m=0.5182 | LR=0.000868\n",
      "  Época  23 | train_loss=0.1448 | train_acc=0.5947 | val_acc=0.5206 | val_f1m=0.5225 | LR=0.000848\n",
      "  Época  24 | train_loss=0.1445 | train_acc=0.5943 | val_acc=0.5206 | val_f1m=0.5225 | LR=0.000828\n",
      "  Época  25 | train_loss=0.1460 | train_acc=0.5853 | val_acc=0.5214 | val_f1m=0.5233 | LR=0.000806\n",
      "  Época  26 | train_loss=0.1409 | train_acc=0.5999 | val_acc=0.5246 | val_f1m=0.5265 | LR=0.000783\n",
      "  Época  27 | train_loss=0.1375 | train_acc=0.6041 | val_acc=0.5198 | val_f1m=0.5217 | LR=0.000759\n",
      "  Época  28 | train_loss=0.1393 | train_acc=0.6068 | val_acc=0.5190 | val_f1m=0.5211 | LR=0.000735\n",
      "  Época  29 | train_loss=0.1347 | train_acc=0.6238 | val_acc=0.5230 | val_f1m=0.5253 | LR=0.000710\n",
      "  Época  30 | train_loss=0.1381 | train_acc=0.6107 | val_acc=0.5190 | val_f1m=0.5212 | LR=0.000684\n",
      "  Época  31 | train_loss=0.1317 | train_acc=0.6174 | val_acc=0.5222 | val_f1m=0.5244 | LR=0.000658\n",
      "  Época  32 | train_loss=0.1283 | train_acc=0.6336 | val_acc=0.5198 | val_f1m=0.5221 | LR=0.000631\n",
      "  Época  33 | train_loss=0.1295 | train_acc=0.6242 | val_acc=0.5206 | val_f1m=0.5229 | LR=0.000604\n",
      "  Época  34 | train_loss=0.1280 | train_acc=0.6237 | val_acc=0.5206 | val_f1m=0.5228 | LR=0.000577\n",
      "  Época  35 | train_loss=0.1286 | train_acc=0.6244 | val_acc=0.5214 | val_f1m=0.5235 | LR=0.000550\n",
      "  Época  36 | train_loss=0.1245 | train_acc=0.6453 | val_acc=0.5222 | val_f1m=0.5243 | LR=0.000523\n",
      "  Época  37 | train_loss=0.1230 | train_acc=0.6443 | val_acc=0.5214 | val_f1m=0.5235 | LR=0.000496\n",
      "  Época  38 | train_loss=0.1194 | train_acc=0.6434 | val_acc=0.5198 | val_f1m=0.5219 | LR=0.000469\n",
      "  Early stopping en época 38 (mejor val_f1m=0.5265)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold3.png\n",
      "[Fold 3/5] Global acc=0.4955\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.5821    0.5306    0.5552       441\n",
      "       right     0.6472    0.5034    0.5663       441\n",
      "  both fists     0.3981    0.4649    0.4289       441\n",
      "   both feet     0.4226    0.4830    0.4508       441\n",
      "\n",
      "    accuracy                         0.4955      1764\n",
      "   macro avg     0.5125    0.4955    0.5003      1764\n",
      "weighted avg     0.5125    0.4955    0.5003      1764\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[234  21 111  75]\n",
      " [ 25 222  93 101]\n",
      " [ 83  38 205 115]\n",
      " [ 60  62 106 213]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold4: 100%|██████████| 68/68 [00:08<00:00,  8.28it/s]\n",
      "Cargando val fold4: 100%|██████████| 15/15 [00:01<00:00,  8.35it/s]\n",
      "Cargando test fold4: 100%|██████████| 20/20 [00:02<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 4/5] Entrenando modelo global... (n_train=5712 | n_val=1260 | n_test=1680)\n",
      "  Época   1 | train_loss=0.2296 | train_acc=0.2553 | val_acc=0.2762 | val_f1m=0.1851 | LR=0.000125\n",
      "  Época   2 | train_loss=0.2137 | train_acc=0.3381 | val_acc=0.4167 | val_f1m=0.4011 | LR=0.000250\n",
      "  Época   3 | train_loss=0.1933 | train_acc=0.4405 | val_acc=0.4762 | val_f1m=0.4725 | LR=0.000375\n",
      "  Época   4 | train_loss=0.1807 | train_acc=0.4944 | val_acc=0.4738 | val_f1m=0.4669 | LR=0.000500\n",
      "  Época   5 | train_loss=0.1771 | train_acc=0.4904 | val_acc=0.4730 | val_f1m=0.4696 | LR=0.000625\n",
      "  Época   6 | train_loss=0.1761 | train_acc=0.4981 | val_acc=0.4841 | val_f1m=0.4874 | LR=0.000750\n",
      "  Época   7 | train_loss=0.1746 | train_acc=0.5077 | val_acc=0.4865 | val_f1m=0.4865 | LR=0.000875\n",
      "  Época   8 | train_loss=0.1710 | train_acc=0.5065 | val_acc=0.4857 | val_f1m=0.4889 | LR=0.001000\n",
      "  Época   9 | train_loss=0.1664 | train_acc=0.5340 | val_acc=0.4770 | val_f1m=0.4766 | LR=0.001000\n",
      "  Época  10 | train_loss=0.1696 | train_acc=0.5203 | val_acc=0.5048 | val_f1m=0.5052 | LR=0.000999\n",
      "  Época  11 | train_loss=0.1644 | train_acc=0.5334 | val_acc=0.4889 | val_f1m=0.4907 | LR=0.000997\n",
      "  Época  12 | train_loss=0.1608 | train_acc=0.5348 | val_acc=0.4881 | val_f1m=0.4896 | LR=0.000993\n",
      "  Época  13 | train_loss=0.1601 | train_acc=0.5448 | val_acc=0.4913 | val_f1m=0.4930 | LR=0.000987\n",
      "  Época  14 | train_loss=0.1594 | train_acc=0.5417 | val_acc=0.4913 | val_f1m=0.4933 | LR=0.000980\n",
      "  Época  15 | train_loss=0.1586 | train_acc=0.5452 | val_acc=0.4937 | val_f1m=0.4957 | LR=0.000971\n",
      "  Época  16 | train_loss=0.1590 | train_acc=0.5525 | val_acc=0.4905 | val_f1m=0.4926 | LR=0.000960\n",
      "  Época  17 | train_loss=0.1547 | train_acc=0.5548 | val_acc=0.4905 | val_f1m=0.4924 | LR=0.000948\n",
      "  Época  18 | train_loss=0.1566 | train_acc=0.5530 | val_acc=0.4897 | val_f1m=0.4916 | LR=0.000935\n",
      "  Época  19 | train_loss=0.1601 | train_acc=0.5497 | val_acc=0.4929 | val_f1m=0.4947 | LR=0.000920\n",
      "  Época  20 | train_loss=0.1526 | train_acc=0.5636 | val_acc=0.4952 | val_f1m=0.4967 | LR=0.000904\n",
      "  Época  21 | train_loss=0.1503 | train_acc=0.5676 | val_acc=0.4968 | val_f1m=0.4983 | LR=0.000887\n",
      "  Época  22 | train_loss=0.1521 | train_acc=0.5653 | val_acc=0.4992 | val_f1m=0.5006 | LR=0.000868\n",
      "  Early stopping en época 22 (mejor val_f1m=0.5052)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold4.png\n",
      "[Fold 4/5] Global acc=0.5304\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.6471    0.5762    0.6096       420\n",
      "       right     0.6000    0.5571    0.5778       420\n",
      "  both fists     0.4269    0.5976    0.4980       420\n",
      "   both feet     0.5000    0.3905    0.4385       420\n",
      "\n",
      "    accuracy                         0.5304      1680\n",
      "   macro avg     0.5435    0.5304    0.5310      1680\n",
      "weighted avg     0.5435    0.5304    0.5310      1680\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[242  21 118  39]\n",
      " [ 28 234  90  68]\n",
      " [ 58  54 251  57]\n",
      " [ 46  81 129 164]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold5: 100%|██████████| 68/68 [00:08<00:00,  8.30it/s]\n",
      "Cargando val fold5: 100%|██████████| 15/15 [00:01<00:00,  8.38it/s]\n",
      "Cargando test fold5: 100%|██████████| 20/20 [00:02<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 5/5] Entrenando modelo global... (n_train=5712 | n_val=1260 | n_test=1680)\n",
      "  Época   1 | train_loss=0.2275 | train_acc=0.2700 | val_acc=0.2579 | val_f1m=0.1263 | LR=0.000125\n",
      "  Época   2 | train_loss=0.2264 | train_acc=0.2775 | val_acc=0.3063 | val_f1m=0.2825 | LR=0.000250\n",
      "  Época   3 | train_loss=0.2047 | train_acc=0.3699 | val_acc=0.4087 | val_f1m=0.4094 | LR=0.000375\n",
      "  Época   4 | train_loss=0.1864 | train_acc=0.4604 | val_acc=0.4167 | val_f1m=0.3820 | LR=0.000500\n",
      "  Época   5 | train_loss=0.1767 | train_acc=0.4911 | val_acc=0.4206 | val_f1m=0.4213 | LR=0.000625\n",
      "  Época   6 | train_loss=0.1744 | train_acc=0.4995 | val_acc=0.4381 | val_f1m=0.4302 | LR=0.000750\n",
      "  Época   7 | train_loss=0.1728 | train_acc=0.4977 | val_acc=0.4476 | val_f1m=0.4490 | LR=0.000875\n",
      "  Época   8 | train_loss=0.1681 | train_acc=0.5186 | val_acc=0.4587 | val_f1m=0.4609 | LR=0.001000\n",
      "  Época   9 | train_loss=0.1673 | train_acc=0.5215 | val_acc=0.4643 | val_f1m=0.4620 | LR=0.001000\n",
      "  Época  10 | train_loss=0.1674 | train_acc=0.5273 | val_acc=0.4667 | val_f1m=0.4671 | LR=0.000999\n",
      "  Época  11 | train_loss=0.1616 | train_acc=0.5397 | val_acc=0.4698 | val_f1m=0.4712 | LR=0.000997\n",
      "  Época  12 | train_loss=0.1654 | train_acc=0.5320 | val_acc=0.4706 | val_f1m=0.4721 | LR=0.000993\n",
      "  Época  13 | train_loss=0.1595 | train_acc=0.5464 | val_acc=0.4714 | val_f1m=0.4729 | LR=0.000987\n",
      "  Época  14 | train_loss=0.1575 | train_acc=0.5550 | val_acc=0.4706 | val_f1m=0.4723 | LR=0.000980\n",
      "  Época  15 | train_loss=0.1569 | train_acc=0.5487 | val_acc=0.4690 | val_f1m=0.4704 | LR=0.000971\n",
      "  Época  16 | train_loss=0.1582 | train_acc=0.5471 | val_acc=0.4714 | val_f1m=0.4729 | LR=0.000960\n",
      "  Época  17 | train_loss=0.1554 | train_acc=0.5618 | val_acc=0.4722 | val_f1m=0.4733 | LR=0.000948\n",
      "  Época  18 | train_loss=0.1539 | train_acc=0.5636 | val_acc=0.4698 | val_f1m=0.4708 | LR=0.000935\n",
      "  Época  19 | train_loss=0.1556 | train_acc=0.5550 | val_acc=0.4698 | val_f1m=0.4707 | LR=0.000920\n",
      "  Época  20 | train_loss=0.1498 | train_acc=0.5765 | val_acc=0.4714 | val_f1m=0.4725 | LR=0.000904\n",
      "  Época  21 | train_loss=0.1512 | train_acc=0.5714 | val_acc=0.4746 | val_f1m=0.4755 | LR=0.000887\n",
      "  Época  22 | train_loss=0.1467 | train_acc=0.5770 | val_acc=0.4738 | val_f1m=0.4749 | LR=0.000868\n",
      "  Época  23 | train_loss=0.1418 | train_acc=0.5902 | val_acc=0.4770 | val_f1m=0.4781 | LR=0.000848\n",
      "  Época  24 | train_loss=0.1480 | train_acc=0.5749 | val_acc=0.4802 | val_f1m=0.4813 | LR=0.000828\n",
      "  Época  25 | train_loss=0.1430 | train_acc=0.5833 | val_acc=0.4817 | val_f1m=0.4830 | LR=0.000806\n",
      "  Época  26 | train_loss=0.1437 | train_acc=0.5805 | val_acc=0.4778 | val_f1m=0.4788 | LR=0.000783\n",
      "  Época  27 | train_loss=0.1410 | train_acc=0.6012 | val_acc=0.4770 | val_f1m=0.4780 | LR=0.000759\n",
      "  Época  28 | train_loss=0.1411 | train_acc=0.5879 | val_acc=0.4770 | val_f1m=0.4780 | LR=0.000735\n",
      "  Época  29 | train_loss=0.1400 | train_acc=0.5980 | val_acc=0.4762 | val_f1m=0.4771 | LR=0.000710\n",
      "  Época  30 | train_loss=0.1351 | train_acc=0.6112 | val_acc=0.4754 | val_f1m=0.4762 | LR=0.000684\n",
      "  Época  31 | train_loss=0.1336 | train_acc=0.6042 | val_acc=0.4746 | val_f1m=0.4754 | LR=0.000658\n",
      "  Época  32 | train_loss=0.1339 | train_acc=0.6115 | val_acc=0.4714 | val_f1m=0.4722 | LR=0.000631\n",
      "  Época  33 | train_loss=0.1320 | train_acc=0.6124 | val_acc=0.4714 | val_f1m=0.4718 | LR=0.000604\n",
      "  Época  34 | train_loss=0.1301 | train_acc=0.6197 | val_acc=0.4706 | val_f1m=0.4707 | LR=0.000577\n",
      "  Época  35 | train_loss=0.1249 | train_acc=0.6366 | val_acc=0.4738 | val_f1m=0.4735 | LR=0.000550\n",
      "  Época  36 | train_loss=0.1268 | train_acc=0.6285 | val_acc=0.4722 | val_f1m=0.4715 | LR=0.000523\n",
      "  Época  37 | train_loss=0.1235 | train_acc=0.6350 | val_acc=0.4746 | val_f1m=0.4737 | LR=0.000496\n",
      "  Early stopping en época 37 (mejor val_f1m=0.4830)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold5.png\n",
      "[Fold 5/5] Global acc=0.5821\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.6794    0.6357    0.6568       420\n",
      "       right     0.6253    0.6357    0.6305       420\n",
      "  both fists     0.4966    0.5167    0.5064       420\n",
      "   both feet     0.5366    0.5405    0.5386       420\n",
      "\n",
      "    accuracy                         0.5821      1680\n",
      "   macro avg     0.5845    0.5821    0.5831      1680\n",
      "weighted avg     0.5845    0.5821    0.5831      1680\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[267  25  74  54]\n",
      " [ 20 267  59  74]\n",
      " [ 65  70 217  68]\n",
      " [ 41  65  87 227]]\n",
      "\n",
      "============================================================\n",
      "RESULTADOS FINALES\n",
      "============================================================\n",
      "Global folds (ACC): ['0.4994', '0.5805', '0.4955', '0.5304', '0.5821']\n",
      "Global mean ACC: 0.5376\n",
      "F1 folds (MACRO): ['0.5016', '0.5817', '0.5003', '0.5310', '0.5831']\n",
      "F1 mean (MACRO): 0.5395\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "import re, json, random\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import mne\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# =========================\n",
    "# REPRODUCIBILIDAD\n",
    "# =========================\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "def seed_worker(worker_id: int):\n",
    "    worker_seed = RANDOM_STATE + worker_id\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "\n",
    "seed_everything(RANDOM_STATE)\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "FOLDS_JSON = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 64        # múltiplo de 4\n",
    "BASE_LR = 1e-3\n",
    "WARMUP_EPOCHS = 8\n",
    "PATIENCE = 12\n",
    "\n",
    "# Split de validación por fold (por sujetos)\n",
    "VAL_SUBJECT_FRAC = 0.18  # ≈ 18% de sujetos del train → val\n",
    "VAL_STRAT_SUBJECT = True # estratifica por etiqueta dominante de cada sujeto\n",
    "\n",
    "# Prepro\n",
    "RESAMPLE_HZ = None\n",
    "DO_NOTCH = True\n",
    "DO_BANDPASS = False       # respetamos tu setup original\n",
    "BP_LO, BP_HI = 4.0, 38.0\n",
    "DO_CAR = False\n",
    "ZSCORE_PER_EPOCH = False\n",
    "\n",
    "# Modelo\n",
    "D_MODEL = 128\n",
    "N_HEADS = 4\n",
    "N_LAYERS = 2\n",
    "P_DROP = 0.2            # dropout en conv stack\n",
    "P_DROP_ENCODER = 0.3    # dropout antes del encoder\n",
    "\n",
    "# Ventana temporal\n",
    "TMIN, TMAX = -1.0, 5.0\n",
    "\n",
    "# TTA / SUBWINDOW en TEST\n",
    "SW_MODE = 'tta'   # 'none'|'subwin'|'tta'\n",
    "SW_ENABLE = True\n",
    "TTA_SHIFTS_S = [-0.075, -0.05, -0.025, 0.0, 0.025, 0.05, 0.075]\n",
    "SW_LEN, SW_STRIDE = 4.5, 1.5\n",
    "COMBINE_TTA_AND_SUBWIN = False  # solicitado (no cambio 1)\n",
    "\n",
    "# Sampler balanceado\n",
    "USE_WEIGHTED_SAMPLER = True\n",
    "\n",
    "# EMA\n",
    "USE_EMA = True\n",
    "EMA_DECAY = 0.9995  # (6) más lenta\n",
    "\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','FCz']\n",
    "CLASS_NAMES = ['left', 'right', 'both_fists', 'both_feet']\n",
    "\n",
    "IMAGERY_RUNS_LR = {4, 8, 12}\n",
    "IMAGERY_RUNS_BF = {6, 10, 14}\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Usando dispositivo: {DEVICE}\")\n",
    "print(\"🧠 INICIANDO EXPERIMENTO CON CNN+Transformer (K-Fold por sujeto como EEGNet)\")\n",
    "print(f\"🔧 Configuración: 4c, 8 canales, 6s | EPOCHS={EPOCHS}, BATCH={BATCH_SIZE}, LR={BASE_LR} | ZSCORE_PER_EPOCH={ZSCORE_PER_EPOCH}\")\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES I/O\n",
    "# =========================\n",
    "def normalize_ch_name(name: str) -> str:\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', name)\n",
    "    return s.upper()\n",
    "\n",
    "\n",
    "NORMALIZED_TARGETS = [normalize_ch_name(c) for c in EXPECTED_8]\n",
    "\n",
    "\n",
    "def pick_8_channels(raw: mne.io.BaseRaw) -> mne.io.BaseRaw:\n",
    "    chs = raw.info['ch_names']\n",
    "    norm_map = {normalize_ch_name(ch): ch for ch in chs}\n",
    "    picked = []\n",
    "    for target_norm, target_orig in zip(NORMALIZED_TARGETS, EXPECTED_8):\n",
    "        if target_norm in norm_map:\n",
    "            picked.append(norm_map[target_norm])\n",
    "        else:\n",
    "            raise RuntimeError(f\"Canal requerido '{target_orig}' no encontrado. Disponibles: {chs}\")\n",
    "    return raw.pick(picks=picked)\n",
    "\n",
    "\n",
    "def list_subject_imagery_edfs(subject_id: str) -> list:\n",
    "    subj_dir = DATA_RAW / subject_id\n",
    "    edfs = []\n",
    "    for r in [4, 6, 8, 10, 12, 14]:\n",
    "        edfs.extend(glob(str(subj_dir / f\"{subject_id}R{r:02d}.edf\")))\n",
    "    return sorted(edfs)\n",
    "\n",
    "\n",
    "def subject_id_to_int(s: str) -> int:\n",
    "    m = re.match(r'[Ss](\\d+)', s)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "\n",
    "def load_subject_epochs(subject_id: str, resample_hz: int, do_notch: bool, do_bandpass: bool,\n",
    "                        do_car: bool, bp_lo: float, bp_hi: float):\n",
    "    edfs = list_subject_imagery_edfs(subject_id)\n",
    "    if len(edfs) == 0:\n",
    "        return np.empty((0,8,1), dtype=np.float32), np.empty((0,), dtype=int), None\n",
    "\n",
    "    X_list, y_list, sfreq_list = [], [], []\n",
    "\n",
    "    for edf_path in edfs:\n",
    "        m = re.search(r\"R(\\d{2})\", Path(edf_path).name)\n",
    "        run = int(m.group(1)) if m else -1\n",
    "\n",
    "        raw = mne.io.read_raw_edf(edf_path, preload=True, verbose='ERROR')\n",
    "        raw = pick_8_channels(raw)\n",
    "\n",
    "        if do_notch:\n",
    "            raw.notch_filter(freqs=[60.0], picks='all', verbose='ERROR')\n",
    "        if do_bandpass:\n",
    "            raw.filter(l_freq=bp_lo, h_freq=bp_hi, picks='all', verbose='ERROR')\n",
    "        if do_car:\n",
    "            raw.set_eeg_reference('average', projection=False, verbose='ERROR')\n",
    "\n",
    "        if resample_hz is not None and resample_hz > 0:\n",
    "            raw.resample(resample_hz)\n",
    "        sfreq = raw.info['sfreq']\n",
    "\n",
    "        events, event_id = mne.events_from_annotations(raw, verbose='ERROR')\n",
    "        keep = {k: v for k, v in event_id.items() if k in {'T1', 'T2'}}\n",
    "        if len(keep) == 0:\n",
    "            continue\n",
    "\n",
    "        epochs = mne.Epochs(raw, events=events, event_id=keep, tmin=TMIN, tmax=TMAX,\n",
    "                            baseline=None, preload=True, verbose='ERROR')\n",
    "        X = epochs.get_data()\n",
    "\n",
    "        if ZSCORE_PER_EPOCH:\n",
    "            X = X.astype(np.float32)\n",
    "            eps = 1e-6\n",
    "            mu = X.mean(axis=2, keepdims=True)\n",
    "            sd = X.std(axis=2, keepdims=True) + eps\n",
    "            X = (X - mu) / sd\n",
    "\n",
    "        ev_codes = epochs.events[:, 2]\n",
    "        inv = {v: k for k, v in keep.items()}\n",
    "        y_run = []\n",
    "        for code in ev_codes:\n",
    "            lab = inv[code]\n",
    "            if run in IMAGERY_RUNS_LR:\n",
    "                y_run.append(0 if lab == 'T1' else 1)\n",
    "            elif run in IMAGERY_RUNS_BF:\n",
    "                y_run.append(2 if lab == 'T1' else 3)\n",
    "            else:\n",
    "                y_run.append(-1)\n",
    "        y_run = np.array(y_run, dtype=int)\n",
    "        mask = y_run >= 0\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        X_list.append(X[mask])\n",
    "        y_list.append(y_run[mask])\n",
    "        sfreq_list.append(sfreq)\n",
    "\n",
    "    if len(X_list) == 0:\n",
    "        return np.empty((0,8,1), dtype=np.float32), np.empty((0,), dtype=int), None\n",
    "\n",
    "    X_all = np.concatenate(X_list, axis=0).astype(np.float32)\n",
    "    y_all = np.concatenate(y_list, axis=0).astype(int)\n",
    "\n",
    "    if len(set([int(round(s)) for s in sfreq_list])) != 1:\n",
    "        raise RuntimeError(f\"Sampling rates inconsistentes: {sfreq_list}\")\n",
    "\n",
    "    return X_all, y_all, sfreq_list[0]\n",
    "\n",
    "\n",
    "def load_fold_subjects(folds_json: Path, fold: int):\n",
    "    with open(folds_json, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    for item in data.get('folds', []):\n",
    "        if int(item.get('fold', -1)) == int(fold):\n",
    "            return list(item.get('train', [])), list(item.get('test', []))\n",
    "    raise ValueError(f\"Fold {fold} not found in {folds_json}\")\n",
    "\n",
    "\n",
    "def standardize_per_channel(train_X, other_X):\n",
    "    C = train_X.shape[1]\n",
    "    train_X = train_X.astype(np.float32)\n",
    "    other_X = other_X.astype(np.float32)\n",
    "    for c in range(C):\n",
    "        mu = train_X[:, c, :].mean()\n",
    "        sd = train_X[:, c, :].std()\n",
    "        sd = sd if sd > 1e-6 else 1.0\n",
    "        train_X[:, c, :] = (train_X[:, c, :] - mu) / sd\n",
    "        other_X[:, c, :] = (other_X[:, c, :] - mu) / sd\n",
    "    return train_X, other_X\n",
    "\n",
    "# =========================\n",
    "# MODELO (GroupNorm en conv)\n",
    "# =========================\n",
    "def make_gn(num_channels, num_groups=8):\n",
    "    g = min(num_groups, num_channels)\n",
    "    while num_channels % g != 0 and g > 1:\n",
    "        g -= 1\n",
    "    return nn.GroupNorm(g, num_channels)\n",
    "\n",
    "\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k, s=1, p=0, p_drop=0.2):\n",
    "        super().__init__()\n",
    "        self.dw = nn.Conv1d(in_ch, in_ch, kernel_size=k, stride=s, padding=p, groups=in_ch, bias=False)\n",
    "        self.pw = nn.Conv1d(in_ch, out_ch, kernel_size=1, bias=False)\n",
    "        self.norm = make_gn(out_ch)\n",
    "        self.act = nn.ELU()\n",
    "        self.dropout = nn.Dropout(p=p_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dw(x); x = self.pw(x); x = self.norm(x)\n",
    "        x = self.act(x); x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EEGCNNTransformer(nn.Module):\n",
    "    def __init__(self, n_ch=8, n_cls=4, d_model=128, n_heads=4, n_layers=2,\n",
    "                 p_drop=0.2, p_drop_encoder=0.3):\n",
    "        super().__init__()\n",
    "        self.conv_t = nn.Sequential(\n",
    "            nn.Conv1d(n_ch, 32, kernel_size=129, stride=2, padding=64, bias=False),\n",
    "            make_gn(32),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=p_drop),\n",
    "            DepthwiseSeparableConv(32, 64, k=31, s=2, p=15, p_drop=p_drop),\n",
    "            DepthwiseSeparableConv(64, 128, k=15, s=2, p=7,  p_drop=p_drop),\n",
    "        )\n",
    "        self.proj = nn.Conv1d(128, d_model, kernel_size=1, bias=False)\n",
    "        self.dropout = nn.Dropout(p=p_drop_encoder)\n",
    "        self.pos_encoding = None\n",
    "        enc = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=2*d_model,\n",
    "            batch_first=True, activation='gelu', dropout=0.1, norm_first=False  # (4)\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc, num_layers=n_layers)\n",
    "        self.cls = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        nn.init.normal_(self.cls, std=0.02)\n",
    "        self.head = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, n_cls))\n",
    "\n",
    "    def _positional_encoding(self, L, d):\n",
    "        pos = torch.arange(0, L, dtype=torch.float32).unsqueeze(1)\n",
    "        i   = torch.arange(0, d, dtype=torch.float32).unsqueeze(0)\n",
    "        angle = pos / torch.pow(10000, (2 * (i//2)) / d)\n",
    "        pe = torch.zeros(L, d, dtype=torch.float32)\n",
    "        pe[:, 0::2] = torch.sin(angle[:, 0::2])\n",
    "        pe[:, 1::2] = torch.cos(angle[:, 1::2])\n",
    "        return pe\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.conv_t(x)           # (B, 128, T')\n",
    "        z = self.proj(z)             # (B, d_model, T')\n",
    "        z = self.dropout(z)\n",
    "        z = z.transpose(1, 2)        # (B, T', d_model)\n",
    "        B, L, D = z.shape\n",
    "        if (self.pos_encoding is None) or (self.pos_encoding.shape[0] != L) or (self.pos_encoding.shape[1] != D):\n",
    "            self.pos_encoding = self._positional_encoding(L, D).to(z.device)\n",
    "        z = z + self.pos_encoding[None, :, :]\n",
    "        cls_tok = self.cls.expand(B, -1, -1)\n",
    "        z = torch.cat([cls_tok, z], dim=1)\n",
    "        z = self.encoder(z)\n",
    "        cls = z[:, 0, :]\n",
    "        return self.head(cls)\n",
    "\n",
    "# =========================\n",
    "# (1) FOCAL LOSS\n",
    "# =========================\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha: torch.Tensor, gamma: float = 1.5, reduction: str = 'mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha / alpha.sum()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        logp = nn.functional.log_softmax(logits, dim=-1)      # (B,C)\n",
    "        p = logp.exp()\n",
    "        idx = torch.arange(target.shape[0], device=logits.device)\n",
    "        pt = p[idx, target]\n",
    "        logpt = logp[idx, target]\n",
    "        at = self.alpha[target]\n",
    "        loss = - at * ((1 - pt) ** self.gamma) * logpt\n",
    "        if self.reduction == 'mean': return loss.mean()\n",
    "        if self.reduction == 'sum':  return loss.sum()\n",
    "        return loss\n",
    "\n",
    "# =========================\n",
    "# (4) AUGMENTS MÁS FUERTES (pero estables)\n",
    "# =========================\n",
    "def augment_batch(\n",
    "    xb,\n",
    "    p_jitter=0.35, p_noise=0.35, p_chdrop=0.15,\n",
    "    max_jitter_frac=0.03, noise_std=0.03, max_chdrop=1\n",
    "):\n",
    "    B, C, T = xb.shape\n",
    "    if np.random.rand() < p_jitter:\n",
    "        max_shift = int(max(1, T*max_jitter_frac))\n",
    "        shifts = torch.randint(low=-max_shift, high=max_shift+1, size=(B,), device=xb.device)\n",
    "        for i in range(B):\n",
    "            xb[i] = torch.roll(xb[i], shifts=int(shifts[i].item()), dims=-1)\n",
    "    if np.random.rand() < p_noise:\n",
    "        xb = xb + noise_std*torch.randn_like(xb)\n",
    "    if np.random.rand() < p_chdrop and max_chdrop > 0:\n",
    "        k = min(max_chdrop, C)\n",
    "        for i in range(B):\n",
    "            idx = torch.randperm(C, device=xb.device)[:k]\n",
    "            xb[i, idx, :] = 0.0\n",
    "    return xb\n",
    "\n",
    "# =========================\n",
    "# EMA de pesos\n",
    "# =========================\n",
    "class ModelEMA:\n",
    "    def __init__(self, model: nn.Module, decay: float = 0.9995, device=None):\n",
    "        self.ema = self._clone(model).to(device if device is not None else next(model.parameters()).device)\n",
    "        self.decay = decay\n",
    "        self._updates = 0\n",
    "        self.update(model, force=True)\n",
    "\n",
    "    def _clone(self, model):\n",
    "        ema = type(model)()\n",
    "        ema.load_state_dict(model.state_dict())\n",
    "        for p in ema.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        return ema\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model: nn.Module, force: bool = False):\n",
    "        d = self.decay\n",
    "        # (6) warmup del decay extendido a 1000 updates\n",
    "        if self._updates < 1000:\n",
    "            d = (self._updates / 1000.0) * self.decay\n",
    "        msd = model.state_dict()\n",
    "        esd = self.ema.state_dict()\n",
    "        for k in esd.keys():\n",
    "            if esd[k].dtype.is_floating_point:\n",
    "                esd[k].mul_(d).add_(msd[k].detach(), alpha=1.0 - d)\n",
    "            else:\n",
    "                esd[k] = msd[k]\n",
    "        self._updates += 1\n",
    "\n",
    "# =========================\n",
    "# INFERENCIA TTA / SUBWINDOW\n",
    "# =========================\n",
    "def subwindow_logits(model, X, sfreq, sw_len, sw_stride, device):\n",
    "    model.eval()\n",
    "    wl = int(round(sw_len * sfreq))\n",
    "    st = int(round(sw_stride * sfreq))\n",
    "    wl = max(1, min(wl, X.shape[-1])); st = max(1, st)\n",
    "    out = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(X.shape[0]):\n",
    "            x = X[i]; acc = []\n",
    "            for s in range(0, max(1, X.shape[-1]-wl+1), st):\n",
    "                seg = x[:, s:s+wl]\n",
    "                if seg.shape[-1] < wl:\n",
    "                    pad = wl - seg.shape[-1]\n",
    "                    seg = np.pad(seg, ((0,0),(0,pad)), mode='edge')\n",
    "                xb = torch.tensor(seg[None, ...], dtype=torch.float32, device=device)\n",
    "                logit = model(xb).detach().cpu().numpy()[0]\n",
    "                acc.append(logit)\n",
    "            acc = np.mean(np.stack(acc, axis=0), axis=0) if len(acc) else np.zeros(4, dtype=np.float32)\n",
    "            out.append(acc)\n",
    "    return np.stack(out, axis=0)\n",
    "\n",
    "\n",
    "def time_shift_tta_logits(model, X, sfreq, shifts_s, device):\n",
    "    model.eval()\n",
    "    T = X.shape[-1]; out = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(X.shape[0]):\n",
    "            x0 = X[i]; acc = []\n",
    "            for sh in shifts_s:\n",
    "                shift = int(round(sh * sfreq))\n",
    "                if shift == 0:\n",
    "                    x = x0\n",
    "                elif shift > 0:\n",
    "                    x = np.pad(x0[:, shift:], ((0,0),(0,shift)), mode='edge')[:, :T]\n",
    "                else:\n",
    "                    shift = -shift\n",
    "                    x = np.pad(x0[:, :-shift], ((0,0),(shift,0)), mode='edge')[:, :T]\n",
    "                xb = torch.tensor(x[None, ...], dtype=torch.float32, device=device)\n",
    "                logit = model(xb).detach().cpu().numpy()[0]\n",
    "                acc.append(logit)\n",
    "            out.append(np.mean(np.stack(acc, axis=0), axis=0))\n",
    "    return np.stack(out, axis=0)\n",
    "\n",
    "# =========================\n",
    "# Utilidades splits estratificados por sujeto\n",
    "# =========================\n",
    "def build_subject_label_map(subject_ids):\n",
    "    y_dom_list = []\n",
    "    for sid in subject_ids:\n",
    "        Xs, ys, _ = load_subject_epochs(sid, RESAMPLE_HZ, DO_NOTCH, DO_BANDPASS, DO_CAR, BP_LO, BP_HI)\n",
    "        if len(ys) == 0:\n",
    "            y_dom_list.append(-1)\n",
    "            continue\n",
    "        binc = np.bincount(ys, minlength=4)\n",
    "        y_dom = int(np.argmax(binc))\n",
    "        y_dom_list.append(y_dom)\n",
    "    return np.array(y_dom_list, dtype=int)\n",
    "\n",
    "# =========================\n",
    "# TRAIN/EVAL por FOLD (con train/val/test)\n",
    "# =========================\n",
    "def train_one_fold(fold:int, device):\n",
    "    def load_fold_subjects_local(folds_json: Path, fold: int):\n",
    "        return load_fold_subjects(folds_json, fold)\n",
    "\n",
    "    train_sub, test_sub = load_fold_subjects_local(FOLDS_JSON, fold)\n",
    "    train_sub = [s for s in train_sub if subject_id_to_int(s) not in EXCLUDE_SUBJECTS]\n",
    "    test_sub  = [s for s in test_sub  if subject_id_to_int(s) not in EXCLUDE_SUBJECTS]\n",
    "\n",
    "    # Split de validación por sujetos (determinista y opcionalmente estratificado)\n",
    "    rng = np.random.RandomState(RANDOM_STATE + fold)\n",
    "    tr_subjects = sorted(train_sub)\n",
    "\n",
    "    if VAL_STRAT_SUBJECT and len(tr_subjects) > 1:\n",
    "        y_dom = build_subject_label_map(tr_subjects)\n",
    "        if np.any(y_dom < 0):\n",
    "            mask = y_dom >= 0\n",
    "            moda = int(np.bincount(y_dom[mask]).argmax()) if mask.sum() > 0 else 0\n",
    "            y_dom[~mask] = moda\n",
    "        n_val_subj = max(1, int(round(len(tr_subjects) * VAL_SUBJECT_FRAC)))\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=n_val_subj, random_state=RANDOM_STATE + fold)\n",
    "        # sss.split(X, y) — X no importa, pasamos índice\n",
    "        idx = np.arange(len(tr_subjects))\n",
    "        _, val_idx = next(sss.split(idx, y_dom))\n",
    "        val_subjects = sorted([tr_subjects[i] for i in val_idx])\n",
    "        train_subjects = [s for s in tr_subjects if s not in val_subjects]\n",
    "    else:\n",
    "        tr_subjects_shuf = tr_subjects.copy()\n",
    "        rng.shuffle(tr_subjects_shuf)\n",
    "        n_val_subj = max(1, int(round(len(tr_subjects_shuf) * VAL_SUBJECT_FRAC)))\n",
    "        val_subjects = sorted(tr_subjects_shuf[:n_val_subj])\n",
    "        train_subjects = sorted(tr_subjects_shuf[n_val_subj:])\n",
    "\n",
    "    # Carga TRAIN/VAL/TEST\n",
    "    X_tr_list, y_tr_list, sub_tr_list = [], [], []\n",
    "    X_val_list, y_val_list, sub_val_list = [], [], []\n",
    "    X_te_list, y_te_list, sub_te_list = [], [], []\n",
    "    sfreq = None\n",
    "\n",
    "    for sid in tqdm(train_subjects, desc=f\"Cargando train fold{fold}\"):\n",
    "        Xs, ys, sf = load_subject_epochs(sid, RESAMPLE_HZ, DO_NOTCH, DO_BANDPASS, DO_CAR, BP_LO, BP_HI)\n",
    "        if len(ys) == 0: continue\n",
    "        X_tr_list.append(Xs); y_tr_list.append(ys)\n",
    "        sub_tr_list.append(np.full_like(ys, fill_value=subject_id_to_int(sid)))\n",
    "        sfreq = sf if sfreq is None else sfreq\n",
    "\n",
    "    for sid in tqdm(val_subjects, desc=f\"Cargando val fold{fold}\"):\n",
    "        Xs, ys, sf = load_subject_epochs(sid, RESAMPLE_HZ, DO_NOTCH, DO_BANDPASS, DO_CAR, BP_LO, BP_HI)\n",
    "        if len(ys) == 0: continue\n",
    "        X_val_list.append(Xs); y_val_list.append(ys)\n",
    "        sub_val_list.append(np.full_like(ys, fill_value=subject_id_to_int(sid)))\n",
    "        sfreq = sf if sfreq is None else sfreq\n",
    "\n",
    "    for sid in tqdm(test_sub, desc=f\"Cargando test fold{fold}\"):\n",
    "        Xs, ys, sf = load_subject_epochs(sid, RESAMPLE_HZ, DO_NOTCH, DO_BANDPASS, DO_CAR, BP_LO, BP_HI)\n",
    "        if len(ys) == 0: continue\n",
    "        X_te_list.append(Xs); y_te_list.append(ys)\n",
    "        sub_te_list.append(np.full_like(ys, fill_value=subject_id_to_int(sid)))\n",
    "        sfreq = sf if sfreq is None else sfreq\n",
    "\n",
    "    # Concatenar\n",
    "    X_tr = np.concatenate(X_tr_list, axis=0); y_tr = np.concatenate(y_tr_list, axis=0)\n",
    "    sub_tr = np.concatenate(sub_tr_list, axis=0)\n",
    "    X_val = np.concatenate(X_val_list, axis=0); y_val = np.concatenate(y_val_list, axis=0)\n",
    "    sub_val = np.concatenate(sub_val_list, axis=0)\n",
    "    X_te = np.concatenate(X_te_list, axis=0); y_te = np.concatenate(y_te_list, axis=0)\n",
    "    sub_te = np.concatenate(sub_te_list, axis=0)\n",
    "\n",
    "    print(f\"[Fold {fold}/5] Entrenando modelo global... (n_train={len(y_tr)} | n_val={len(y_val)} | n_test={len(y_te)})\")\n",
    "\n",
    "    # Normalización por canal (fit en TRAIN y aplicar a VAL/TEST)\n",
    "    if ZSCORE_PER_EPOCH:\n",
    "        X_tr_std, X_val_std, X_te_std = X_tr, X_val, X_te\n",
    "    else:\n",
    "        X_tr_std, X_val_std = standardize_per_channel(X_tr, X_val)\n",
    "        _,        X_te_std  = standardize_per_channel(X_tr, X_te)  # reutiliza stats de train\n",
    "\n",
    "    # Datasets\n",
    "    tr_ds  = TensorDataset(torch.tensor(X_tr_std),  torch.tensor(y_tr).long(),  torch.tensor(sub_tr).long())\n",
    "    val_ds = TensorDataset(torch.tensor(X_val_std), torch.tensor(y_val).long(), torch.tensor(sub_val).long())\n",
    "    te_ds  = TensorDataset(torch.tensor(X_te_std),  torch.tensor(y_te).long(),  torch.tensor(sub_te).long())\n",
    "\n",
    "    # (2) Weighted sampler templado: w = (1/ws)^a * (1/wk)^b\n",
    "    def make_weighted_sampler(dataset: TensorDataset):\n",
    "        _Xb, yb, sb = dataset.tensors\n",
    "        yb_np = yb.numpy()\n",
    "        sb_np = sb.numpy()\n",
    "        uniq_s, cnt_s = np.unique(sb_np, return_counts=True)\n",
    "        map_s = {s:c for s,c in zip(uniq_s, cnt_s)}\n",
    "        key = sb_np.astype(np.int64) * 10 + yb_np.astype(np.int64)\n",
    "        uniq_k, cnt_k = np.unique(key, return_counts=True)\n",
    "        map_k = {k:c for k,c in zip(uniq_k, cnt_k)}\n",
    "        a, b = 0.8, 1.0\n",
    "        w = []\n",
    "        for s, y in zip(sb_np, yb_np):\n",
    "            k = int(s)*10 + int(y)\n",
    "            ws = float(map_s[int(s)])\n",
    "            wk = float(map_k[k])\n",
    "            w.append((ws ** (-a)) * (wk ** (-b)))\n",
    "        w = np.array(w, dtype=np.float64)\n",
    "        w = w / (w.mean() + 1e-12)\n",
    "        sampler = WeightedRandomSampler(weights=torch.tensor(w, dtype=torch.double),\n",
    "                                        num_samples=len(yb_np), replacement=True)\n",
    "        return sampler\n",
    "\n",
    "    if USE_WEIGHTED_SAMPLER:\n",
    "        tr_sampler = make_weighted_sampler(tr_ds)\n",
    "        tr_ld  = DataLoader(tr_ds, batch_size=BATCH_SIZE, sampler=tr_sampler, drop_last=False, worker_init_fn=seed_worker)\n",
    "    else:\n",
    "        tr_ld  = DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True,  drop_last=False, worker_init_fn=seed_worker)\n",
    "\n",
    "    val_ld = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, worker_init_fn=seed_worker)\n",
    "    te_ld  = DataLoader(te_ds,  batch_size=BATCH_SIZE, shuffle=False, drop_last=False, worker_init_fn=seed_worker)\n",
    "\n",
    "    # Modelo\n",
    "    model = EEGCNNTransformer(n_ch=8, n_cls=4, d_model=D_MODEL, n_heads=N_HEADS,\n",
    "                              n_layers=N_LAYERS, p_drop=P_DROP, p_drop_encoder=P_DROP_ENCODER).to(device)\n",
    "\n",
    "    # Optimizador + Focal Loss (3) alpha tweaking\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=1e-2)\n",
    "\n",
    "    class_counts = np.bincount(y_tr, minlength=4).astype(np.float32)\n",
    "    inv = class_counts.sum() / (4.0 * np.maximum(class_counts, 1.0))\n",
    "    alpha = torch.tensor(inv, dtype=torch.float32, device=device)\n",
    "    alpha_mean = alpha.mean().item()\n",
    "    alpha[2] = 1.25 * alpha_mean  # both_fists (ya estaba)\n",
    "    alpha[3] = 1.05 * alpha_mean # (3) pequeño empujón a both_feet\n",
    "    crit = FocalLoss(alpha=alpha, gamma=1.5, reduction='mean')\n",
    "\n",
    "    # (7) Warmup+Cosine (min_factor=0.1)\n",
    "    from torch.optim.lr_scheduler import LambdaLR\n",
    "    total_epochs = EPOCHS\n",
    "    warmup_epochs = max(1, int(WARMUP_EPOCHS))\n",
    "    min_factor = 0.1\n",
    "\n",
    "    def lr_lambda(current_epoch):\n",
    "        if current_epoch < warmup_epochs:\n",
    "            return (current_epoch + 1) / warmup_epochs\n",
    "        progress = (current_epoch - warmup_epochs) / max(1, (total_epochs - warmup_epochs))\n",
    "        progress = min(1.0, max(0.0, progress))\n",
    "        return min_factor + 0.5 * (1.0 - min_factor) * (1.0 + np.cos(np.pi * progress))\n",
    "\n",
    "    scheduler = LambdaLR(opt, lr_lambda=lr_lambda)\n",
    "\n",
    "    # EMA\n",
    "    if USE_EMA:\n",
    "        ema = ModelEMA(model, decay=EMA_DECAY, device=device)\n",
    "    else:\n",
    "        ema = None\n",
    "\n",
    "    # Entrenamiento con early stopping por F1 macro (EMA en val/test si aplica)\n",
    "    best_f1, best_state, wait = 0.0, None, 0\n",
    "    hist = {\"ep\": [], \"tr_loss\": [], \"tr_acc\": [], \"val_acc\": [], \"val_f1m\": [], \"lr\": []}\n",
    "\n",
    "    def evaluate_on(loader, use_ema=True):\n",
    "        mdl = ema.ema if (ema is not None and use_ema) else model\n",
    "        mdl.eval()\n",
    "        preds, gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _sb in loader:\n",
    "                xb = xb.to(device)\n",
    "                p = mdl(xb).argmax(dim=1).cpu().numpy()\n",
    "                preds.append(p); gts.append(yb.numpy())\n",
    "        preds = np.concatenate(preds); gts = np.concatenate(gts)\n",
    "        acc = accuracy_score(gts, preds)\n",
    "        f1m = f1_score(gts, preds, average='macro')\n",
    "        return acc, f1m\n",
    "\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        # ---- Train ----\n",
    "        model.train()\n",
    "        tr_loss, n_seen, tr_correct = 0.0, 0, 0\n",
    "        for xb, yb, _sb in tr_ld:\n",
    "            xb = xb.to(device); yb = yb.to(device)\n",
    "            xb = augment_batch(xb)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            loss.backward()\n",
    "            # (5) Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            opt.step()\n",
    "            if ema is not None:\n",
    "                ema.update(model)\n",
    "\n",
    "            tr_loss += loss.item() * len(yb)\n",
    "            n_seen += len(yb)\n",
    "            tr_correct += (logits.argmax(1) == yb).sum().item()\n",
    "        tr_loss /= max(1, n_seen)\n",
    "        tr_acc = tr_correct / max(1, n_seen)\n",
    "\n",
    "        # ---- Val (EMA si aplica) ----\n",
    "        acc, f1m = evaluate_on(val_ld, use_ema=True)\n",
    "\n",
    "        hist[\"ep\"].append(ep)\n",
    "        hist[\"tr_loss\"].append(tr_loss)\n",
    "        hist[\"tr_acc\"].append(tr_acc)\n",
    "        hist[\"val_acc\"].append(acc)\n",
    "        hist[\"val_f1m\"].append(f1m)\n",
    "        hist[\"lr\"].append(scheduler.get_last_lr()[0])\n",
    "\n",
    "        print(f\"  Época {ep:3d} | train_loss={tr_loss:.4f} | train_acc={tr_acc:.4f} | val_acc={acc:.4f} | val_f1m={f1m:.4f} | LR={scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        improved = f1m > best_f1 + 1e-4\n",
    "        if improved:\n",
    "            best_f1 = f1m\n",
    "            ref_model = ema.ema if ema is not None else model\n",
    "            best_state = {k: v.detach().cpu() for k, v in ref_model.state_dict().items()}\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "\n",
    "        scheduler.step()\n",
    "        if wait >= PATIENCE:\n",
    "            print(f\"  Early stopping en época {ep} (mejor val_f1m={best_f1:.4f})\")\n",
    "            break\n",
    "\n",
    "    # Cargamos mejor estado guardado (EMA si estaba activo)\n",
    "    if best_state is not None:\n",
    "        (ema.ema if (ema is not None) else model).load_state_dict(best_state)\n",
    "\n",
    "    # ---- Guardar curva ----\n",
    "    fig = plt.figure(figsize=(8,4.5))\n",
    "    ax1 = plt.gca()\n",
    "    ax1.plot(hist[\"ep\"], hist[\"tr_loss\"], label=\"train_loss\")\n",
    "    ax1.plot(hist[\"ep\"], hist[\"tr_acc\"], label=\"tr_acc\")\n",
    "    ax1.plot(hist[\"ep\"], hist[\"val_acc\"], label=\"val_acc\")\n",
    "    ax1.set_xlabel(\"Época\"); ax1.set_title(f\"Fold {fold} — Curva de entrenamiento\")\n",
    "    ax1.legend(); ax1.grid(True, alpha=0.3)\n",
    "    out_png = f\"training_curve_fold{fold}.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=140)\n",
    "    plt.close(fig)\n",
    "    print(f\"↳ Curva de entrenamiento guardada: {out_png}\")\n",
    "\n",
    "    # ---- Evaluación final en TEST ----\n",
    "    eval_model = ema.ema if (ema is not None) else model\n",
    "    eval_model.eval()\n",
    "\n",
    "    sfreq_used = RESAMPLE_HZ\n",
    "    if sfreq_used is None:\n",
    "        sfreq_used = int(round(X_te_std.shape[-1] / (TMAX - TMIN)))\n",
    "\n",
    "    if (not SW_ENABLE) or SW_MODE == 'none':\n",
    "        preds, gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _sb in te_ld:\n",
    "                xb = xb.to(device)\n",
    "                p = eval_model(xb).argmax(dim=1).cpu().numpy()\n",
    "                preds.append(p); gts.append(yb.numpy())\n",
    "        preds = np.concatenate(preds); gts = np.concatenate(gts)\n",
    "    elif SW_MODE in ('subwin', 'tta'):\n",
    "        logits_tta = None\n",
    "        logits_sw  = None\n",
    "        if SW_MODE == 'subwin':\n",
    "            logits_sw = subwindow_logits(eval_model, X_te_std, sfreq_used, SW_LEN, SW_STRIDE, device)\n",
    "        elif SW_MODE == 'tta':\n",
    "            logits_tta = time_shift_tta_logits(eval_model, X_te_std, sfreq_used, TTA_SHIFTS_S, device)\n",
    "\n",
    "        if COMBINE_TTA_AND_SUBWIN:\n",
    "            if logits_tta is None:\n",
    "                logits_tta = time_shift_tta_logits(eval_model, X_te_std, sfreq_used, TTA_SHIFTS_S, device)\n",
    "            if logits_sw is None:\n",
    "                logits_sw  = subwindow_logits(eval_model, X_te_std, sfreq_used, SW_LEN, SW_STRIDE, device)\n",
    "            logits = 0.5 * logits_tta + 0.5 * logits_sw\n",
    "        else:\n",
    "            logits = logits_tta if logits_tta is not None else logits_sw\n",
    "\n",
    "        preds = logits.argmax(axis=1); gts = y_te\n",
    "    else:\n",
    "        raise ValueError(f\"SW_MODE desconocido: {SW_MODE}\")\n",
    "\n",
    "    acc = accuracy_score(gts, preds)\n",
    "    f1m = f1_score(gts, preds, average='macro')\n",
    "    print(f\"[Fold {fold}/5] Global acc={acc:.4f}\\n\")\n",
    "    print(classification_report(gts, preds, target_names=[c.replace('_',' ') for c in CLASS_NAMES], digits=4))\n",
    "    print(\"Confusion matrix (rows=true, cols=pred):\")\n",
    "    print(confusion_matrix(gts, preds, labels=[0,1,2,3]))\n",
    "\n",
    "    return acc, f1m\n",
    "\n",
    "# =========================\n",
    "# LOOP 5 FOLDS + RESUMEN\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    acc_folds, f1_folds = [], []\n",
    "    for fold in range(1, 6):\n",
    "        acc, f1m = train_one_fold(fold, DEVICE)\n",
    "        acc_folds.append(f\"{acc:.4f}\")\n",
    "        f1_folds.append(f\"{f1m:.4f}\")\n",
    "\n",
    "    acc_mean = float(np.mean([float(a) for a in acc_folds]))\n",
    "    f1_mean  = float(np.mean([float(f) for f in f1_folds]))\n",
    "\n",
    "    print(\"\\n============================================================\")\n",
    "    print(\"RESULTADOS FINALES\")\n",
    "    print(\"============================================================\")\n",
    "    print(f\"Global folds (ACC): {acc_folds}\")\n",
    "    print(f\"Global mean ACC: {acc_mean:.4f}\")\n",
    "    print(f\"F1 folds (MACRO): {f1_folds}\")\n",
    "    print(f\"F1 mean (MACRO): {f1_mean:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e2f86a",
   "metadata": {},
   "source": [
    "## FINETUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fc2cf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Usando dispositivo: cuda\n",
      "🧠 INICIANDO EXPERIMENTO CON CNN+Transformer (K-Fold por sujeto como EEGNet)\n",
      "🔧 Configuración: 4c, 8 canales, 6s | EPOCHS=60, BATCH=64, LR=0.001 | ZSCORE_PER_EPOCH=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold1: 100%|██████████| 67/67 [00:08<00:00,  8.29it/s]\n",
      "Cargando val fold1: 100%|██████████| 15/15 [00:01<00:00,  8.49it/s]\n",
      "Cargando test fold1: 100%|██████████| 21/21 [00:02<00:00,  7.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1/5] Entrenando modelo global... (n_train=5628 | n_val=1260 | n_test=1764)\n",
      "  Época   1 | train_loss=0.2326 | train_acc=0.2772 | val_acc=0.2627 | val_f1m=0.2225 | LR=0.000125\n",
      "  Época   2 | train_loss=0.2126 | train_acc=0.3257 | val_acc=0.3825 | val_f1m=0.3362 | LR=0.000250\n",
      "  Época   3 | train_loss=0.1839 | train_acc=0.4577 | val_acc=0.4381 | val_f1m=0.4333 | LR=0.000375\n",
      "  Época   4 | train_loss=0.1797 | train_acc=0.4844 | val_acc=0.4571 | val_f1m=0.4561 | LR=0.000500\n",
      "  Época   5 | train_loss=0.1724 | train_acc=0.5137 | val_acc=0.4508 | val_f1m=0.4461 | LR=0.000625\n",
      "  Época   6 | train_loss=0.1673 | train_acc=0.5348 | val_acc=0.4476 | val_f1m=0.4405 | LR=0.000750\n",
      "  Época   7 | train_loss=0.1650 | train_acc=0.5311 | val_acc=0.4397 | val_f1m=0.4288 | LR=0.000875\n",
      "  Época   8 | train_loss=0.1653 | train_acc=0.5359 | val_acc=0.4611 | val_f1m=0.4632 | LR=0.001000\n",
      "  Época   9 | train_loss=0.1664 | train_acc=0.5226 | val_acc=0.4484 | val_f1m=0.4503 | LR=0.001000\n",
      "  Época  10 | train_loss=0.1582 | train_acc=0.5414 | val_acc=0.4571 | val_f1m=0.4575 | LR=0.000999\n",
      "  Época  11 | train_loss=0.1579 | train_acc=0.5458 | val_acc=0.4635 | val_f1m=0.4634 | LR=0.000997\n",
      "  Época  12 | train_loss=0.1565 | train_acc=0.5576 | val_acc=0.4683 | val_f1m=0.4682 | LR=0.000993\n",
      "  Época  13 | train_loss=0.1520 | train_acc=0.5618 | val_acc=0.4675 | val_f1m=0.4676 | LR=0.000987\n",
      "  Época  14 | train_loss=0.1534 | train_acc=0.5684 | val_acc=0.4683 | val_f1m=0.4684 | LR=0.000980\n",
      "  Época  15 | train_loss=0.1531 | train_acc=0.5730 | val_acc=0.4675 | val_f1m=0.4676 | LR=0.000971\n",
      "  Época  16 | train_loss=0.1471 | train_acc=0.5832 | val_acc=0.4706 | val_f1m=0.4706 | LR=0.000960\n",
      "  Época  17 | train_loss=0.1446 | train_acc=0.5855 | val_acc=0.4746 | val_f1m=0.4744 | LR=0.000948\n",
      "  Época  18 | train_loss=0.1466 | train_acc=0.5732 | val_acc=0.4762 | val_f1m=0.4761 | LR=0.000935\n",
      "  Época  19 | train_loss=0.1452 | train_acc=0.5876 | val_acc=0.4778 | val_f1m=0.4776 | LR=0.000920\n",
      "  Época  20 | train_loss=0.1438 | train_acc=0.5864 | val_acc=0.4802 | val_f1m=0.4799 | LR=0.000904\n",
      "  Época  21 | train_loss=0.1454 | train_acc=0.5823 | val_acc=0.4794 | val_f1m=0.4787 | LR=0.000887\n",
      "  Época  22 | train_loss=0.1424 | train_acc=0.5904 | val_acc=0.4770 | val_f1m=0.4760 | LR=0.000868\n",
      "  Época  23 | train_loss=0.1415 | train_acc=0.5885 | val_acc=0.4762 | val_f1m=0.4752 | LR=0.000848\n",
      "  Época  24 | train_loss=0.1392 | train_acc=0.6025 | val_acc=0.4794 | val_f1m=0.4785 | LR=0.000828\n",
      "  Época  25 | train_loss=0.1388 | train_acc=0.6016 | val_acc=0.4794 | val_f1m=0.4786 | LR=0.000806\n",
      "  Época  26 | train_loss=0.1380 | train_acc=0.6111 | val_acc=0.4794 | val_f1m=0.4783 | LR=0.000783\n",
      "  Época  27 | train_loss=0.1313 | train_acc=0.6171 | val_acc=0.4817 | val_f1m=0.4806 | LR=0.000759\n",
      "  Época  28 | train_loss=0.1351 | train_acc=0.6125 | val_acc=0.4778 | val_f1m=0.4766 | LR=0.000735\n",
      "  Época  29 | train_loss=0.1328 | train_acc=0.6103 | val_acc=0.4770 | val_f1m=0.4758 | LR=0.000710\n",
      "  Época  30 | train_loss=0.1260 | train_acc=0.6317 | val_acc=0.4802 | val_f1m=0.4788 | LR=0.000684\n",
      "  Época  31 | train_loss=0.1248 | train_acc=0.6276 | val_acc=0.4770 | val_f1m=0.4757 | LR=0.000658\n",
      "  Época  32 | train_loss=0.1287 | train_acc=0.6190 | val_acc=0.4794 | val_f1m=0.4781 | LR=0.000631\n",
      "  Época  33 | train_loss=0.1243 | train_acc=0.6338 | val_acc=0.4778 | val_f1m=0.4765 | LR=0.000604\n",
      "  Época  34 | train_loss=0.1203 | train_acc=0.6354 | val_acc=0.4802 | val_f1m=0.4788 | LR=0.000577\n",
      "  Época  35 | train_loss=0.1205 | train_acc=0.6468 | val_acc=0.4802 | val_f1m=0.4785 | LR=0.000550\n",
      "  Época  36 | train_loss=0.1166 | train_acc=0.6507 | val_acc=0.4817 | val_f1m=0.4801 | LR=0.000523\n",
      "  Época  37 | train_loss=0.1126 | train_acc=0.6672 | val_acc=0.4817 | val_f1m=0.4803 | LR=0.000496\n",
      "  Época  38 | train_loss=0.1103 | train_acc=0.6693 | val_acc=0.4770 | val_f1m=0.4758 | LR=0.000469\n",
      "  Época  39 | train_loss=0.1107 | train_acc=0.6622 | val_acc=0.4762 | val_f1m=0.4751 | LR=0.000442\n",
      "  Early stopping en época 39 (mejor val_f1m=0.4806)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold1.png\n",
      "[Fold 1/5] Global acc=0.4994\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.6199    0.5510    0.5834       441\n",
      "       right     0.5441    0.5601    0.5520       441\n",
      "  both fists     0.3892    0.4422    0.4140       441\n",
      "   both feet     0.4700    0.4444    0.4569       441\n",
      "\n",
      "    accuracy                         0.4994      1764\n",
      "   macro avg     0.5058    0.4994    0.5016      1764\n",
      "weighted avg     0.5058    0.4994    0.5016      1764\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[243  42 101  55]\n",
      " [ 28 247 100  66]\n",
      " [ 63  83 195 100]\n",
      " [ 58  82 105 196]]\n",
      "↳ Matriz de confusión guardada: confusion_global_fold1.png\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5686\n",
      "  Δ(FT-Global) = +0.0692\n",
      "Confusion matrix FT (rows=true, cols=pred):\n",
      "[[273  32  85  51]\n",
      " [ 29 254  88  70]\n",
      " [ 56  59 246  80]\n",
      " [ 57  56  98 230]]\n",
      "↳ Matriz de confusión FT guardada: confusion_ft_fold1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold2: 100%|██████████| 67/67 [00:07<00:00,  8.46it/s]\n",
      "Cargando val fold2: 100%|██████████| 15/15 [00:01<00:00,  8.51it/s]\n",
      "Cargando test fold2: 100%|██████████| 21/21 [00:02<00:00,  8.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 2/5] Entrenando modelo global... (n_train=5628 | n_val=1260 | n_test=1764)\n",
      "  Época   1 | train_loss=0.2279 | train_acc=0.2733 | val_acc=0.3056 | val_f1m=0.2554 | LR=0.000125\n",
      "  Época   2 | train_loss=0.2164 | train_acc=0.3218 | val_acc=0.3889 | val_f1m=0.3563 | LR=0.000250\n",
      "  Época   3 | train_loss=0.1937 | train_acc=0.4271 | val_acc=0.4349 | val_f1m=0.4283 | LR=0.000375\n",
      "  Época   4 | train_loss=0.1842 | train_acc=0.4701 | val_acc=0.4603 | val_f1m=0.4592 | LR=0.000500\n",
      "  Época   5 | train_loss=0.1778 | train_acc=0.4989 | val_acc=0.4667 | val_f1m=0.4591 | LR=0.000625\n",
      "  Época   6 | train_loss=0.1776 | train_acc=0.4893 | val_acc=0.4619 | val_f1m=0.4597 | LR=0.000750\n",
      "  Época   7 | train_loss=0.1742 | train_acc=0.5032 | val_acc=0.4770 | val_f1m=0.4738 | LR=0.000875\n",
      "  Época   8 | train_loss=0.1750 | train_acc=0.4957 | val_acc=0.4746 | val_f1m=0.4715 | LR=0.001000\n",
      "  Época   9 | train_loss=0.1705 | train_acc=0.5215 | val_acc=0.4730 | val_f1m=0.4739 | LR=0.001000\n",
      "  Época  10 | train_loss=0.1645 | train_acc=0.5270 | val_acc=0.4881 | val_f1m=0.4889 | LR=0.000999\n",
      "  Época  11 | train_loss=0.1688 | train_acc=0.5215 | val_acc=0.4944 | val_f1m=0.4952 | LR=0.000997\n",
      "  Época  12 | train_loss=0.1625 | train_acc=0.5350 | val_acc=0.5000 | val_f1m=0.5009 | LR=0.000993\n",
      "  Época  13 | train_loss=0.1608 | train_acc=0.5297 | val_acc=0.5024 | val_f1m=0.5029 | LR=0.000987\n",
      "  Época  14 | train_loss=0.1645 | train_acc=0.5316 | val_acc=0.5016 | val_f1m=0.5023 | LR=0.000980\n",
      "  Época  15 | train_loss=0.1593 | train_acc=0.5458 | val_acc=0.5024 | val_f1m=0.5033 | LR=0.000971\n",
      "  Época  16 | train_loss=0.1612 | train_acc=0.5359 | val_acc=0.5000 | val_f1m=0.5006 | LR=0.000960\n",
      "  Época  17 | train_loss=0.1580 | train_acc=0.5366 | val_acc=0.5032 | val_f1m=0.5041 | LR=0.000948\n",
      "  Época  18 | train_loss=0.1545 | train_acc=0.5617 | val_acc=0.5032 | val_f1m=0.5036 | LR=0.000935\n",
      "  Época  19 | train_loss=0.1566 | train_acc=0.5482 | val_acc=0.5024 | val_f1m=0.5031 | LR=0.000920\n",
      "  Época  20 | train_loss=0.1469 | train_acc=0.5853 | val_acc=0.5032 | val_f1m=0.5038 | LR=0.000904\n",
      "  Época  21 | train_loss=0.1570 | train_acc=0.5421 | val_acc=0.5040 | val_f1m=0.5044 | LR=0.000887\n",
      "  Época  22 | train_loss=0.1501 | train_acc=0.5757 | val_acc=0.5040 | val_f1m=0.5045 | LR=0.000868\n",
      "  Época  23 | train_loss=0.1498 | train_acc=0.5697 | val_acc=0.5063 | val_f1m=0.5066 | LR=0.000848\n",
      "  Época  24 | train_loss=0.1481 | train_acc=0.5766 | val_acc=0.5063 | val_f1m=0.5067 | LR=0.000828\n",
      "  Época  25 | train_loss=0.1472 | train_acc=0.5736 | val_acc=0.5048 | val_f1m=0.5052 | LR=0.000806\n",
      "  Época  26 | train_loss=0.1436 | train_acc=0.5780 | val_acc=0.4992 | val_f1m=0.5001 | LR=0.000783\n",
      "  Época  27 | train_loss=0.1422 | train_acc=0.5796 | val_acc=0.4944 | val_f1m=0.4952 | LR=0.000759\n",
      "  Época  28 | train_loss=0.1430 | train_acc=0.5819 | val_acc=0.4952 | val_f1m=0.4958 | LR=0.000735\n",
      "  Época  29 | train_loss=0.1498 | train_acc=0.5638 | val_acc=0.4968 | val_f1m=0.4976 | LR=0.000710\n",
      "  Época  30 | train_loss=0.1377 | train_acc=0.5928 | val_acc=0.4937 | val_f1m=0.4943 | LR=0.000684\n",
      "  Época  31 | train_loss=0.1371 | train_acc=0.6032 | val_acc=0.4960 | val_f1m=0.4969 | LR=0.000658\n",
      "  Época  32 | train_loss=0.1327 | train_acc=0.6098 | val_acc=0.4968 | val_f1m=0.4978 | LR=0.000631\n",
      "  Época  33 | train_loss=0.1354 | train_acc=0.6055 | val_acc=0.4976 | val_f1m=0.4986 | LR=0.000604\n",
      "  Época  34 | train_loss=0.1300 | train_acc=0.6221 | val_acc=0.4992 | val_f1m=0.5006 | LR=0.000577\n",
      "  Época  35 | train_loss=0.1283 | train_acc=0.6217 | val_acc=0.4984 | val_f1m=0.4999 | LR=0.000550\n",
      "  Early stopping en época 35 (mejor val_f1m=0.5066)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold2.png\n",
      "[Fold 2/5] Global acc=0.5816\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.6379    0.6871    0.6616       441\n",
      "       right     0.6353    0.6281    0.6317       441\n",
      "  both fists     0.5112    0.5170    0.5141       441\n",
      "   both feet     0.5356    0.4943    0.5142       441\n",
      "\n",
      "    accuracy                         0.5816      1764\n",
      "   macro avg     0.5800    0.5816    0.5804      1764\n",
      "weighted avg     0.5800    0.5816    0.5804      1764\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[303  25  72  41]\n",
      " [ 35 277  64  65]\n",
      " [ 74  56 228  83]\n",
      " [ 63  78  82 218]]\n",
      "↳ Matriz de confusión guardada: confusion_global_fold2.png\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.6417\n",
      "  Δ(FT-Global) = +0.0601\n",
      "Confusion matrix FT (rows=true, cols=pred):\n",
      "[[315  14  80  32]\n",
      " [ 31 293  67  50]\n",
      " [ 58  36 274  73]\n",
      " [ 50  54  87 250]]\n",
      "↳ Matriz de confusión FT guardada: confusion_ft_fold2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold3: 100%|██████████| 67/67 [00:07<00:00,  8.46it/s]\n",
      "Cargando val fold3: 100%|██████████| 15/15 [00:01<00:00,  8.42it/s]\n",
      "Cargando test fold3: 100%|██████████| 21/21 [00:02<00:00,  8.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 3/5] Entrenando modelo global... (n_train=5628 | n_val=1260 | n_test=1764)\n",
      "  Época   1 | train_loss=0.2271 | train_acc=0.2788 | val_acc=0.3048 | val_f1m=0.2318 | LR=0.000125\n",
      "  Época   2 | train_loss=0.2121 | train_acc=0.3431 | val_acc=0.4151 | val_f1m=0.4163 | LR=0.000250\n",
      "  Época   3 | train_loss=0.1900 | train_acc=0.4437 | val_acc=0.4508 | val_f1m=0.4547 | LR=0.000375\n",
      "  Época   4 | train_loss=0.1801 | train_acc=0.4813 | val_acc=0.4635 | val_f1m=0.4623 | LR=0.000500\n",
      "  Época   5 | train_loss=0.1720 | train_acc=0.5146 | val_acc=0.4849 | val_f1m=0.4840 | LR=0.000625\n",
      "  Época   6 | train_loss=0.1697 | train_acc=0.5251 | val_acc=0.4937 | val_f1m=0.4964 | LR=0.000750\n",
      "  Época   7 | train_loss=0.1700 | train_acc=0.5229 | val_acc=0.4857 | val_f1m=0.4877 | LR=0.000875\n",
      "  Época   8 | train_loss=0.1673 | train_acc=0.5393 | val_acc=0.4937 | val_f1m=0.4947 | LR=0.001000\n",
      "  Época   9 | train_loss=0.1642 | train_acc=0.5304 | val_acc=0.5143 | val_f1m=0.5126 | LR=0.001000\n",
      "  Época  10 | train_loss=0.1624 | train_acc=0.5418 | val_acc=0.4976 | val_f1m=0.4990 | LR=0.000999\n",
      "  Época  11 | train_loss=0.1576 | train_acc=0.5570 | val_acc=0.5111 | val_f1m=0.5127 | LR=0.000997\n",
      "  Época  12 | train_loss=0.1584 | train_acc=0.5551 | val_acc=0.5103 | val_f1m=0.5128 | LR=0.000993\n",
      "  Época  13 | train_loss=0.1601 | train_acc=0.5396 | val_acc=0.5119 | val_f1m=0.5146 | LR=0.000987\n",
      "  Época  14 | train_loss=0.1559 | train_acc=0.5597 | val_acc=0.5127 | val_f1m=0.5153 | LR=0.000980\n",
      "  Época  15 | train_loss=0.1563 | train_acc=0.5640 | val_acc=0.5151 | val_f1m=0.5177 | LR=0.000971\n",
      "  Época  16 | train_loss=0.1562 | train_acc=0.5563 | val_acc=0.5175 | val_f1m=0.5201 | LR=0.000960\n",
      "  Época  17 | train_loss=0.1541 | train_acc=0.5698 | val_acc=0.5206 | val_f1m=0.5234 | LR=0.000948\n",
      "  Época  18 | train_loss=0.1506 | train_acc=0.5794 | val_acc=0.5198 | val_f1m=0.5224 | LR=0.000935\n",
      "  Época  19 | train_loss=0.1509 | train_acc=0.5695 | val_acc=0.5198 | val_f1m=0.5224 | LR=0.000920\n",
      "  Época  20 | train_loss=0.1492 | train_acc=0.5684 | val_acc=0.5159 | val_f1m=0.5184 | LR=0.000904\n",
      "  Época  21 | train_loss=0.1457 | train_acc=0.5842 | val_acc=0.5159 | val_f1m=0.5181 | LR=0.000887\n",
      "  Época  22 | train_loss=0.1446 | train_acc=0.5956 | val_acc=0.5167 | val_f1m=0.5189 | LR=0.000868\n",
      "  Época  23 | train_loss=0.1463 | train_acc=0.5798 | val_acc=0.5175 | val_f1m=0.5196 | LR=0.000848\n",
      "  Época  24 | train_loss=0.1480 | train_acc=0.5796 | val_acc=0.5143 | val_f1m=0.5163 | LR=0.000828\n",
      "  Época  25 | train_loss=0.1422 | train_acc=0.5979 | val_acc=0.5119 | val_f1m=0.5137 | LR=0.000806\n",
      "  Época  26 | train_loss=0.1381 | train_acc=0.6029 | val_acc=0.5119 | val_f1m=0.5137 | LR=0.000783\n",
      "  Época  27 | train_loss=0.1398 | train_acc=0.5963 | val_acc=0.5143 | val_f1m=0.5161 | LR=0.000759\n",
      "  Época  28 | train_loss=0.1420 | train_acc=0.5917 | val_acc=0.5127 | val_f1m=0.5144 | LR=0.000735\n",
      "  Época  29 | train_loss=0.1320 | train_acc=0.6192 | val_acc=0.5119 | val_f1m=0.5134 | LR=0.000710\n",
      "  Early stopping en época 29 (mejor val_f1m=0.5234)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold3.png\n",
      "[Fold 3/5] Global acc=0.4870\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.5603    0.5374    0.5486       441\n",
      "       right     0.6324    0.4875    0.5506       441\n",
      "  both fists     0.3859    0.4331    0.4081       441\n",
      "   both feet     0.4269    0.4898    0.4562       441\n",
      "\n",
      "    accuracy                         0.4870      1764\n",
      "   macro avg     0.5013    0.4870    0.4909      1764\n",
      "weighted avg     0.5013    0.4870    0.4909      1764\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[237  24 113  67]\n",
      " [ 31 215  93 102]\n",
      " [ 88  41 191 121]\n",
      " [ 67  60  98 216]]\n",
      "↳ Matriz de confusión guardada: confusion_global_fold3.png\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5527\n",
      "  Δ(FT-Global) = +0.0658\n",
      "Confusion matrix FT (rows=true, cols=pred):\n",
      "[[261  30  93  57]\n",
      " [ 31 267  78  65]\n",
      " [ 77  43 229  92]\n",
      " [ 68  65  90 218]]\n",
      "↳ Matriz de confusión FT guardada: confusion_ft_fold3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold4: 100%|██████████| 68/68 [00:08<00:00,  8.47it/s]\n",
      "Cargando val fold4: 100%|██████████| 15/15 [00:01<00:00,  8.46it/s]\n",
      "Cargando test fold4: 100%|██████████| 20/20 [00:02<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 4/5] Entrenando modelo global... (n_train=5712 | n_val=1260 | n_test=1680)\n",
      "  Época   1 | train_loss=0.2284 | train_acc=0.2549 | val_acc=0.2794 | val_f1m=0.1887 | LR=0.000125\n",
      "  Época   2 | train_loss=0.2121 | train_acc=0.3410 | val_acc=0.3960 | val_f1m=0.3735 | LR=0.000250\n",
      "  Época   3 | train_loss=0.1935 | train_acc=0.4301 | val_acc=0.4429 | val_f1m=0.4377 | LR=0.000375\n",
      "  Época   4 | train_loss=0.1836 | train_acc=0.4729 | val_acc=0.4794 | val_f1m=0.4802 | LR=0.000500\n",
      "  Época   5 | train_loss=0.1762 | train_acc=0.5051 | val_acc=0.4865 | val_f1m=0.4867 | LR=0.000625\n",
      "  Época   6 | train_loss=0.1741 | train_acc=0.5068 | val_acc=0.4794 | val_f1m=0.4797 | LR=0.000750\n",
      "  Época   7 | train_loss=0.1763 | train_acc=0.5009 | val_acc=0.4825 | val_f1m=0.4826 | LR=0.000875\n",
      "  Época   8 | train_loss=0.1705 | train_acc=0.5254 | val_acc=0.4817 | val_f1m=0.4722 | LR=0.001000\n",
      "  Época   9 | train_loss=0.1674 | train_acc=0.5205 | val_acc=0.4929 | val_f1m=0.4927 | LR=0.001000\n",
      "  Época  10 | train_loss=0.1648 | train_acc=0.5277 | val_acc=0.4944 | val_f1m=0.4971 | LR=0.000999\n",
      "  Época  11 | train_loss=0.1627 | train_acc=0.5418 | val_acc=0.5119 | val_f1m=0.5133 | LR=0.000997\n",
      "  Época  12 | train_loss=0.1670 | train_acc=0.5313 | val_acc=0.5127 | val_f1m=0.5141 | LR=0.000993\n",
      "  Época  13 | train_loss=0.1628 | train_acc=0.5357 | val_acc=0.5127 | val_f1m=0.5141 | LR=0.000987\n",
      "  Época  14 | train_loss=0.1603 | train_acc=0.5488 | val_acc=0.5095 | val_f1m=0.5107 | LR=0.000980\n",
      "  Época  15 | train_loss=0.1571 | train_acc=0.5523 | val_acc=0.5079 | val_f1m=0.5088 | LR=0.000971\n",
      "  Época  16 | train_loss=0.1585 | train_acc=0.5518 | val_acc=0.5063 | val_f1m=0.5072 | LR=0.000960\n",
      "  Época  17 | train_loss=0.1575 | train_acc=0.5592 | val_acc=0.5040 | val_f1m=0.5050 | LR=0.000948\n",
      "  Época  18 | train_loss=0.1577 | train_acc=0.5527 | val_acc=0.5040 | val_f1m=0.5050 | LR=0.000935\n",
      "  Época  19 | train_loss=0.1589 | train_acc=0.5467 | val_acc=0.5048 | val_f1m=0.5060 | LR=0.000920\n",
      "  Época  20 | train_loss=0.1578 | train_acc=0.5515 | val_acc=0.5048 | val_f1m=0.5060 | LR=0.000904\n",
      "  Época  21 | train_loss=0.1543 | train_acc=0.5623 | val_acc=0.5032 | val_f1m=0.5046 | LR=0.000887\n",
      "  Época  22 | train_loss=0.1536 | train_acc=0.5574 | val_acc=0.5040 | val_f1m=0.5056 | LR=0.000868\n",
      "  Época  23 | train_loss=0.1488 | train_acc=0.5765 | val_acc=0.5056 | val_f1m=0.5073 | LR=0.000848\n",
      "  Época  24 | train_loss=0.1484 | train_acc=0.5842 | val_acc=0.5048 | val_f1m=0.5066 | LR=0.000828\n",
      "  Early stopping en época 24 (mejor val_f1m=0.5141)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold4.png\n",
      "[Fold 4/5] Global acc=0.5268\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.6375    0.6071    0.6220       420\n",
      "       right     0.5688    0.5905    0.5794       420\n",
      "  both fists     0.4350    0.5500    0.4858       420\n",
      "   both feet     0.4824    0.3595    0.4120       420\n",
      "\n",
      "    accuracy                         0.5268      1680\n",
      "   macro avg     0.5309    0.5268    0.5248      1680\n",
      "weighted avg     0.5309    0.5268    0.5248      1680\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[255  17 109  39]\n",
      " [ 30 248  78  64]\n",
      " [ 59  71 231  59]\n",
      " [ 56 100 113 151]]\n",
      "↳ Matriz de confusión guardada: confusion_global_fold4.png\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5976\n",
      "  Δ(FT-Global) = +0.0708\n",
      "Confusion matrix FT (rows=true, cols=pred):\n",
      "[[281  19  77  43]\n",
      " [ 27 269  65  59]\n",
      " [ 52  55 245  68]\n",
      " [ 57  71  83 209]]\n",
      "↳ Matriz de confusión FT guardada: confusion_ft_fold4.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold5: 100%|██████████| 68/68 [00:08<00:00,  8.49it/s]\n",
      "Cargando val fold5: 100%|██████████| 15/15 [00:01<00:00,  8.61it/s]\n",
      "Cargando test fold5: 100%|██████████| 20/20 [00:02<00:00,  8.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 5/5] Entrenando modelo global... (n_train=5712 | n_val=1260 | n_test=1680)\n",
      "  Época   1 | train_loss=0.2280 | train_acc=0.2640 | val_acc=0.2659 | val_f1m=0.2042 | LR=0.000125\n",
      "  Época   2 | train_loss=0.2091 | train_acc=0.3528 | val_acc=0.4159 | val_f1m=0.4055 | LR=0.000250\n",
      "  Época   3 | train_loss=0.1839 | train_acc=0.4732 | val_acc=0.4254 | val_f1m=0.4233 | LR=0.000375\n",
      "  Época   4 | train_loss=0.1776 | train_acc=0.4839 | val_acc=0.4413 | val_f1m=0.4399 | LR=0.000500\n",
      "  Época   5 | train_loss=0.1759 | train_acc=0.4998 | val_acc=0.4571 | val_f1m=0.4528 | LR=0.000625\n",
      "  Época   6 | train_loss=0.1751 | train_acc=0.4989 | val_acc=0.4325 | val_f1m=0.4330 | LR=0.000750\n",
      "  Época   7 | train_loss=0.1735 | train_acc=0.5019 | val_acc=0.4683 | val_f1m=0.4706 | LR=0.000875\n",
      "  Época   8 | train_loss=0.1705 | train_acc=0.5075 | val_acc=0.4571 | val_f1m=0.4602 | LR=0.001000\n",
      "  Época   9 | train_loss=0.1692 | train_acc=0.5131 | val_acc=0.4476 | val_f1m=0.4471 | LR=0.001000\n",
      "  Época  10 | train_loss=0.1681 | train_acc=0.5173 | val_acc=0.4603 | val_f1m=0.4612 | LR=0.000999\n",
      "  Época  11 | train_loss=0.1635 | train_acc=0.5264 | val_acc=0.4595 | val_f1m=0.4626 | LR=0.000997\n",
      "  Época  12 | train_loss=0.1636 | train_acc=0.5247 | val_acc=0.4611 | val_f1m=0.4636 | LR=0.000993\n",
      "  Época  13 | train_loss=0.1569 | train_acc=0.5637 | val_acc=0.4627 | val_f1m=0.4652 | LR=0.000987\n",
      "  Época  14 | train_loss=0.1622 | train_acc=0.5413 | val_acc=0.4635 | val_f1m=0.4659 | LR=0.000980\n",
      "  Época  15 | train_loss=0.1630 | train_acc=0.5292 | val_acc=0.4627 | val_f1m=0.4646 | LR=0.000971\n",
      "  Época  16 | train_loss=0.1550 | train_acc=0.5502 | val_acc=0.4603 | val_f1m=0.4619 | LR=0.000960\n",
      "  Época  17 | train_loss=0.1561 | train_acc=0.5508 | val_acc=0.4603 | val_f1m=0.4621 | LR=0.000948\n",
      "  Época  18 | train_loss=0.1544 | train_acc=0.5579 | val_acc=0.4595 | val_f1m=0.4613 | LR=0.000935\n",
      "  Época  19 | train_loss=0.1540 | train_acc=0.5553 | val_acc=0.4595 | val_f1m=0.4613 | LR=0.000920\n",
      "  Early stopping en época 19 (mejor val_f1m=0.4706)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold5.png\n",
      "[Fold 5/5] Global acc=0.5518\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.6206    0.6310    0.6257       420\n",
      "       right     0.6071    0.5738    0.5900       420\n",
      "  both fists     0.4697    0.4976    0.4832       420\n",
      "   both feet     0.5158    0.5048    0.5102       420\n",
      "\n",
      "    accuracy                         0.5518      1680\n",
      "   macro avg     0.5533    0.5518    0.5523      1680\n",
      "weighted avg     0.5533    0.5518    0.5523      1680\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[265  20  80  55]\n",
      " [ 29 241  71  79]\n",
      " [ 81  65 209  65]\n",
      " [ 52  71  85 212]]\n",
      "↳ Matriz de confusión guardada: confusion_global_fold5.png\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.6238\n",
      "  Δ(FT-Global) = +0.0720\n",
      "Confusion matrix FT (rows=true, cols=pred):\n",
      "[[279  17  78  46]\n",
      " [ 20 277  68  55]\n",
      " [ 59  53 244  64]\n",
      " [ 44  58  70 248]]\n",
      "↳ Matriz de confusión FT guardada: confusion_ft_fold5.png\n",
      "\n",
      "============================================================\n",
      "RESULTADOS FINALES\n",
      "============================================================\n",
      "Global folds (ACC): ['0.4994', '0.5816', '0.4870', '0.5268', '0.5518']\n",
      "Global mean ACC: 0.5293\n",
      "F1 folds (MACRO): ['0.5016', '0.5804', '0.4909', '0.5248', '0.5523']\n",
      "F1 mean (MACRO): 0.5300\n",
      "Fine-tune PROGRESIVO folds: ['0.5686', '0.6417', '0.5527', '0.5976', '0.6238']\n",
      "Fine-tune PROGRESIVO mean: 0.5969\n",
      "Δ(FT-Global) mean: +0.0676\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "import re, json, random\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import mne\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "\n",
    "# =========================\n",
    "# REPRODUCIBILIDAD\n",
    "# =========================\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def seed_worker(worker_id: int):\n",
    "    worker_seed = RANDOM_STATE + worker_id\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "seed_everything(RANDOM_STATE)\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "FOLDS_JSON = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 64\n",
    "BASE_LR = 1e-3\n",
    "WARMUP_EPOCHS = 8\n",
    "PATIENCE = 12\n",
    "\n",
    "# Split de validación por fold (por sujetos)\n",
    "VAL_SUBJECT_FRAC = 0.18\n",
    "VAL_STRAT_SUBJECT = True\n",
    "\n",
    "# Prepro global\n",
    "RESAMPLE_HZ = None\n",
    "DO_NOTCH = True\n",
    "DO_BANDPASS = False\n",
    "BP_LO, BP_HI = 4.0, 38.0\n",
    "DO_CAR = False\n",
    "ZSCORE_PER_EPOCH = False\n",
    "\n",
    "# Modelo\n",
    "D_MODEL = 128\n",
    "N_HEADS = 4\n",
    "N_LAYERS = 2\n",
    "P_DROP = 0.2\n",
    "P_DROP_ENCODER = 0.3\n",
    "\n",
    "# Ventana temporal\n",
    "TMIN, TMAX = -1.0, 5.0\n",
    "\n",
    "# TTA / SUBWINDOW en TEST\n",
    "SW_MODE = 'tta'   # 'none'|'subwin'|'tta'\n",
    "SW_ENABLE = True\n",
    "TTA_SHIFTS_S = [-0.075, -0.05, -0.025, 0.0, 0.025, 0.05, 0.075]\n",
    "SW_LEN, SW_STRIDE = 4.5, 1.5\n",
    "COMBINE_TTA_AND_SUBWIN = False\n",
    "\n",
    "# Sampler balanceado\n",
    "USE_WEIGHTED_SAMPLER = True\n",
    "\n",
    "# EMA (solo GLOBAL). En FT se desactiva.\n",
    "USE_EMA = True\n",
    "EMA_DECAY = 0.9995\n",
    "\n",
    "# Fine-tuning (por sujeto)\n",
    "FT_N_FOLDS = 4\n",
    "FT_FREEZE_EPOCHS = 8           # etapa 1 (congelar backbone)\n",
    "FT_UNFREEZE_EPOCHS = 8         # etapa 2 (descongelar)\n",
    "FT_PATIENCE = 6                # early stopping en FT\n",
    "FT_BATCH = 64\n",
    "FT_LR_HEAD = 1e-3              # cabeza (5x aprox del backbone)\n",
    "FT_LR_BACKBONE = 2e-4          # backbone\n",
    "FT_WD = 1e-3                   # weight decay menor en FT\n",
    "FT_AUG = dict(p_jitter=0.25, p_noise=0.25, p_chdrop=0.10, max_jitter_frac=0.02, noise_std=0.02, max_chdrop=1)\n",
    "FT_ALPHA_BOOST_FISTS = 1.25\n",
    "FT_ALPHA_BOOST_FEET  = 1.05\n",
    "\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','FCz']\n",
    "CLASS_NAMES = ['left', 'right', 'both_fists', 'both_feet']\n",
    "\n",
    "IMAGERY_RUNS_LR = {4, 8, 12}\n",
    "IMAGERY_RUNS_BF = {6, 10, 14}\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Usando dispositivo: {DEVICE}\")\n",
    "print(\"🧠 INICIANDO EXPERIMENTO CON CNN+Transformer (K-Fold por sujeto como EEGNet)\")\n",
    "print(f\"🔧 Configuración: 4c, 8 canales, 6s | EPOCHS={EPOCHS}, BATCH={BATCH_SIZE}, LR={BASE_LR} | ZSCORE_PER_EPOCH={ZSCORE_PER_EPOCH}\")\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES I/O\n",
    "# =========================\n",
    "def normalize_ch_name(name: str) -> str:\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', name)\n",
    "    return s.upper()\n",
    "\n",
    "NORMALIZED_TARGETS = [normalize_ch_name(c) for c in EXPECTED_8]\n",
    "\n",
    "def pick_8_channels(raw: mne.io.BaseRaw) -> mne.io.BaseRaw:\n",
    "    chs = raw.info['ch_names']\n",
    "    norm_map = {normalize_ch_name(ch): ch for ch in chs}\n",
    "    picked = []\n",
    "    for target_norm, target_orig in zip(NORMALIZED_TARGETS, EXPECTED_8):\n",
    "        if target_norm in norm_map:\n",
    "            picked.append(norm_map[target_norm])\n",
    "        else:\n",
    "            raise RuntimeError(f\"Canal requerido '{target_orig}' no encontrado. Disponibles: {chs}\")\n",
    "    return raw.pick(picks=picked)\n",
    "\n",
    "def list_subject_imagery_edfs(subject_id: str) -> list:\n",
    "    subj_dir = DATA_RAW / subject_id\n",
    "    edfs = []\n",
    "    for r in [4, 6, 8, 10, 12, 14]:\n",
    "        edfs.extend(glob(str(subj_dir / f\"{subject_id}R{r:02d}.edf\")))\n",
    "    return sorted(edfs)\n",
    "\n",
    "def subject_id_to_int(s: str) -> int:\n",
    "    m = re.match(r'[Ss](\\d+)', s)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "def load_subject_epochs(subject_id: str, resample_hz: int, do_notch: bool, do_bandpass: bool,\n",
    "                        do_car: bool, bp_lo: float, bp_hi: float):\n",
    "    edfs = list_subject_imagery_edfs(subject_id)\n",
    "    if len(edfs) == 0:\n",
    "        return np.empty((0,8,1), dtype=np.float32), np.empty((0,), dtype=int), None\n",
    "\n",
    "    X_list, y_list, sfreq_list = [], [], []\n",
    "    for edf_path in edfs:\n",
    "        m = re.search(r\"R(\\d{2})\", Path(edf_path).name)\n",
    "        run = int(m.group(1)) if m else -1\n",
    "\n",
    "        raw = mne.io.read_raw_edf(edf_path, preload=True, verbose='ERROR')\n",
    "        raw = pick_8_channels(raw)\n",
    "\n",
    "        if do_notch:\n",
    "            raw.notch_filter(freqs=[60.0], picks='all', verbose='ERROR')\n",
    "        if do_bandpass:\n",
    "            raw.filter(l_freq=bp_lo, h_freq=bp_hi, picks='all', verbose='ERROR')\n",
    "        if do_car:\n",
    "            raw.set_eeg_reference('average', projection=False, verbose='ERROR')\n",
    "        if resample_hz is not None and resample_hz > 0:\n",
    "            raw.resample(resample_hz)\n",
    "\n",
    "        sfreq = raw.info['sfreq']\n",
    "        events, event_id = mne.events_from_annotations(raw, verbose='ERROR')\n",
    "        keep = {k: v for k, v in event_id.items() if k in {'T1', 'T2'}}\n",
    "        if len(keep) == 0:\n",
    "            continue\n",
    "\n",
    "        epochs = mne.Epochs(raw, events=events, event_id=keep, tmin=TMIN, tmax=TMAX,\n",
    "                            baseline=None, preload=True, verbose='ERROR')\n",
    "        X = epochs.get_data()\n",
    "\n",
    "        if ZSCORE_PER_EPOCH:\n",
    "            X = X.astype(np.float32)\n",
    "            eps = 1e-6\n",
    "            mu = X.mean(axis=2, keepdims=True)\n",
    "            sd = X.std(axis=2, keepdims=True) + eps\n",
    "            X = (X - mu) / sd\n",
    "\n",
    "        ev_codes = epochs.events[:, 2]\n",
    "        inv = {v: k for k, v in keep.items()}\n",
    "        y_run = []\n",
    "        for code in ev_codes:\n",
    "            lab = inv[code]\n",
    "            if run in IMAGERY_RUNS_LR:\n",
    "                y_run.append(0 if lab == 'T1' else 1)\n",
    "            elif run in IMAGERY_RUNS_BF:\n",
    "                y_run.append(2 if lab == 'T1' else 3)\n",
    "            else:\n",
    "                y_run.append(-1)\n",
    "        y_run = np.array(y_run, dtype=int)\n",
    "        mask = y_run >= 0\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        X_list.append(X[mask])\n",
    "        y_list.append(y_run[mask])\n",
    "        sfreq_list.append(sfreq)\n",
    "\n",
    "    if len(X_list) == 0:\n",
    "        return np.empty((0,8,1), dtype=np.float32), np.empty((0,), dtype=int), None\n",
    "\n",
    "    X_all = np.concatenate(X_list, axis=0).astype(np.float32)\n",
    "    y_all = np.concatenate(y_list, axis=0).astype(int)\n",
    "\n",
    "    if len(set([int(round(s)) for s in sfreq_list])) != 1:\n",
    "        raise RuntimeError(f\"Sampling rates inconsistentes: {sfreq_list}\")\n",
    "\n",
    "    return X_all, y_all, sfreq_list[0]\n",
    "\n",
    "def load_fold_subjects(folds_json: Path, fold: int):\n",
    "    with open(folds_json, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    for item in data.get('folds', []):\n",
    "        if int(item.get('fold', -1)) == int(fold):\n",
    "            return list(item.get('train', [])), list(item.get('test', []))\n",
    "    raise ValueError(f\"Fold {fold} not found in {folds_json}\")\n",
    "\n",
    "def standardize_per_channel(train_X, other_X):\n",
    "    C = train_X.shape[1]\n",
    "    train_X = train_X.astype(np.float32)\n",
    "    other_X = other_X.astype(np.float32)\n",
    "    for c in range(C):\n",
    "        mu = train_X[:, c, :].mean()\n",
    "        sd = train_X[:, c, :].std()\n",
    "        sd = sd if sd > 1e-6 else 1.0\n",
    "        train_X[:, c, :] = (train_X[:, c, :] - mu) / sd\n",
    "        other_X[:, c, :] = (other_X[:, c, :] - mu) / sd\n",
    "    return train_X, other_X\n",
    "\n",
    "# =========================\n",
    "# MODELO (GroupNorm en conv)\n",
    "# =========================\n",
    "def make_gn(num_channels, num_groups=8):\n",
    "    g = min(num_groups, num_channels)\n",
    "    while num_channels % g != 0 and g > 1:\n",
    "        g -= 1\n",
    "    return nn.GroupNorm(g, num_channels)\n",
    "\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k, s=1, p=0, p_drop=0.2):\n",
    "        super().__init__()\n",
    "        self.dw = nn.Conv1d(in_ch, in_ch, kernel_size=k, stride=s, padding=p, groups=in_ch, bias=False)\n",
    "        self.pw = nn.Conv1d(in_ch, out_ch, kernel_size=1, bias=False)\n",
    "        self.norm = make_gn(out_ch)\n",
    "        self.act = nn.ELU()\n",
    "        self.dropout = nn.Dropout(p=p_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dw(x); x = self.pw(x); x = self.norm(x)\n",
    "        x = self.act(x); x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class EEGCNNTransformer(nn.Module):\n",
    "    def __init__(self, n_ch=8, n_cls=4, d_model=128, n_heads=4, n_layers=2,\n",
    "                 p_drop=0.2, p_drop_encoder=0.3):\n",
    "        super().__init__()\n",
    "        self.conv_t = nn.Sequential(\n",
    "            nn.Conv1d(n_ch, 32, kernel_size=129, stride=2, padding=64, bias=False),\n",
    "            make_gn(32),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=p_drop),\n",
    "            DepthwiseSeparableConv(32, 64, k=31, s=2, p=15, p_drop=p_drop),\n",
    "            DepthwiseSeparableConv(64, 128, k=15, s=2, p=7,  p_drop=p_drop),\n",
    "        )\n",
    "        self.proj = nn.Conv1d(128, d_model, kernel_size=1, bias=False)\n",
    "        self.dropout = nn.Dropout(p=p_drop_encoder)\n",
    "        self.pos_encoding = None\n",
    "        enc = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=2*d_model,\n",
    "            batch_first=True, activation='gelu', dropout=0.1, norm_first=False\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc, num_layers=n_layers)\n",
    "        self.cls = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        nn.init.normal_(self.cls, std=0.02)\n",
    "        self.head = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, n_cls))\n",
    "\n",
    "    def _positional_encoding(self, L, d):\n",
    "        pos = torch.arange(0, L, dtype=torch.float32).unsqueeze(1)\n",
    "        i   = torch.arange(0, d, dtype=torch.float32).unsqueeze(0)\n",
    "        angle = pos / torch.pow(10000, (2 * (i//2)) / d)\n",
    "        pe = torch.zeros(L, d, dtype=torch.float32)\n",
    "        pe[:, 0::2] = torch.sin(angle[:, 0::2])\n",
    "        pe[:, 1::2] = torch.cos(angle[:, 1::2])\n",
    "        return pe\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.conv_t(x)           # (B, 128, T')\n",
    "        z = self.proj(z)             # (B, d_model, T')\n",
    "        z = self.dropout(z)\n",
    "        z = z.transpose(1, 2)        # (B, T', d_model)\n",
    "        B, L, D = z.shape\n",
    "        if (self.pos_encoding is None) or (self.pos_encoding.shape[0] != L) or (self.pos_encoding.shape[1] != D):\n",
    "            self.pos_encoding = self._positional_encoding(L, D).to(z.device)\n",
    "        z = z + self.pos_encoding[None, :, :]\n",
    "        cls_tok = self.cls.expand(B, -1, -1)\n",
    "        z = torch.cat([cls_tok, z], dim=1)\n",
    "        z = self.encoder(z)\n",
    "        cls = z[:, 0, :]\n",
    "        return self.head(cls)\n",
    "\n",
    "# =========================\n",
    "# FOCAL LOSS\n",
    "# =========================\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha: torch.Tensor, gamma: float = 1.5, reduction: str = 'mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha / alpha.sum()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        logp = nn.functional.log_softmax(logits, dim=-1)      # (B,C)\n",
    "        p = logp.exp()\n",
    "        idx = torch.arange(target.shape[0], device=logits.device)\n",
    "        pt = p[idx, target]\n",
    "        logpt = logp[idx, target]\n",
    "        at = self.alpha[target]\n",
    "        loss = - at * ((1 - pt) ** self.gamma) * logpt\n",
    "        if self.reduction == 'mean': return loss.mean()\n",
    "        if self.reduction == 'sum':  return loss.sum()\n",
    "        return loss\n",
    "\n",
    "# =========================\n",
    "# AUGMENTS\n",
    "# =========================\n",
    "def augment_batch(\n",
    "    xb,\n",
    "    p_jitter=0.35, p_noise=0.35, p_chdrop=0.15,\n",
    "    max_jitter_frac=0.03, noise_std=0.03, max_chdrop=1\n",
    "):\n",
    "    B, C, T = xb.shape\n",
    "    if np.random.rand() < p_jitter:\n",
    "        max_shift = int(max(1, T*max_jitter_frac))\n",
    "        shifts = torch.randint(low=-max_shift, high=max_shift+1, size=(B,), device=xb.device)\n",
    "        for i in range(B):\n",
    "            xb[i] = torch.roll(xb[i], shifts=int(shifts[i].item()), dims=-1)\n",
    "    if np.random.rand() < p_noise:\n",
    "        xb = xb + noise_std*torch.randn_like(xb)\n",
    "    if np.random.rand() < p_chdrop and max_chdrop > 0:\n",
    "        k = min(max_chdrop, C)\n",
    "        for i in range(B):\n",
    "            idx = torch.randperm(C, device=xb.device)[:k]\n",
    "            xb[i, idx, :] = 0.0\n",
    "    return xb\n",
    "\n",
    "# Versión suave para FT\n",
    "def augment_batch_ft(xb):\n",
    "    return augment_batch(xb, **FT_AUG)\n",
    "\n",
    "# =========================\n",
    "# EMA de pesos (solo global)\n",
    "# =========================\n",
    "class ModelEMA:\n",
    "    def __init__(self, model: nn.Module, decay: float = 0.9995, device=None):\n",
    "        self.ema = self._clone(model).to(device if device is not None else next(model.parameters()).device)\n",
    "        self.decay = decay\n",
    "        self._updates = 0\n",
    "        self.update(model, force=True)\n",
    "\n",
    "    def _clone(self, model):\n",
    "        ema = type(model)()\n",
    "        ema.load_state_dict(model.state_dict())\n",
    "        for p in ema.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        return ema\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model: nn.Module, force: bool = False):\n",
    "        d = self.decay\n",
    "        if self._updates < 1000:\n",
    "            d = (self._updates / 1000.0) * self.decay\n",
    "        msd = model.state_dict()\n",
    "        esd = self.ema.state_dict()\n",
    "        for k in esd.keys():\n",
    "            if esd[k].dtype.is_floating_point:\n",
    "                esd[k].mul_(d).add_(msd[k].detach(), alpha=1.0 - d)\n",
    "            else:\n",
    "                esd[k] = msd[k]\n",
    "        self._updates += 1\n",
    "\n",
    "# =========================\n",
    "# INFERENCIA TTA / SUBWINDOW\n",
    "# =========================\n",
    "def subwindow_logits(model, X, sfreq, sw_len, sw_stride, device):\n",
    "    model.eval()\n",
    "    wl = int(round(sw_len * sfreq))\n",
    "    st = int(round(sw_stride * sfreq))\n",
    "    wl = max(1, min(wl, X.shape[-1])); st = max(1, st)\n",
    "    out = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(X.shape[0]):\n",
    "            x = X[i]; acc = []\n",
    "            for s in range(0, max(1, X.shape[-1]-wl+1), st):\n",
    "                seg = x[:, s:s+wl]\n",
    "                if seg.shape[-1] < wl:\n",
    "                    pad = wl - seg.shape[-1]\n",
    "                    seg = np.pad(seg, ((0,0),(0,pad)), mode='edge')\n",
    "                xb = torch.tensor(seg[None, ...], dtype=torch.float32, device=device)\n",
    "                logit = model(xb).detach().cpu().numpy()[0]\n",
    "                acc.append(logit)\n",
    "            acc = np.mean(np.stack(acc, axis=0), axis=0) if len(acc) else np.zeros(4, dtype=np.float32)\n",
    "            out.append(acc)\n",
    "    return np.stack(out, axis=0)\n",
    "\n",
    "def time_shift_tta_logits(model, X, sfreq, shifts_s, device):\n",
    "    model.eval()\n",
    "    T = X.shape[-1]; out = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(X.shape[0]):\n",
    "            x0 = X[i]; acc = []\n",
    "            for sh in shifts_s:\n",
    "                shift = int(round(sh * sfreq))\n",
    "                if shift == 0:\n",
    "                    x = x0\n",
    "                elif shift > 0:\n",
    "                    x = np.pad(x0[:, shift:], ((0,0),(0,shift)), mode='edge')[:, :T]\n",
    "                else:\n",
    "                    shift = -shift\n",
    "                    x = np.pad(x0[:, :-shift], ((0,0),(shift,0)), mode='edge')[:, :T]\n",
    "                xb = torch.tensor(x[None, ...], dtype=torch.float32, device=device)\n",
    "                logit = model(xb).detach().cpu().numpy()[0]\n",
    "                acc.append(logit)\n",
    "            out.append(np.mean(np.stack(acc, axis=0), axis=0))\n",
    "    return np.stack(out, axis=0)\n",
    "\n",
    "# =========================\n",
    "# Utilidades splits estratificados por sujeto\n",
    "# =========================\n",
    "def build_subject_label_map(subject_ids):\n",
    "    y_dom_list = []\n",
    "    for sid in subject_ids:\n",
    "        Xs, ys, _ = load_subject_epochs(sid, RESAMPLE_HZ, DO_NOTCH, DO_BANDPASS, DO_CAR, BP_LO, BP_HI)\n",
    "        if len(ys) == 0:\n",
    "            y_dom_list.append(-1)\n",
    "            continue\n",
    "        binc = np.bincount(ys, minlength=4)\n",
    "        y_dom = int(np.argmax(binc))\n",
    "        y_dom_list.append(y_dom)\n",
    "    return np.array(y_dom_list, dtype=int)\n",
    "\n",
    "# =========================\n",
    "# TRAIN/EVAL GLOBAL POR FOLD\n",
    "# =========================\n",
    "def train_one_fold(fold:int, device):\n",
    "    def load_fold_subjects_local(folds_json: Path, fold: int):\n",
    "        return load_fold_subjects(folds_json, fold)\n",
    "\n",
    "    train_sub, test_sub = load_fold_subjects_local(FOLDS_JSON, fold)\n",
    "    train_sub = [s for s in train_sub if subject_id_to_int(s) not in EXCLUDE_SUBJECTS]\n",
    "    test_sub  = [s for s in test_sub  if subject_id_to_int(s) not in EXCLUDE_SUBJECTS]\n",
    "\n",
    "    rng = np.random.RandomState(RANDOM_STATE + fold)\n",
    "    tr_subjects = sorted(train_sub)\n",
    "\n",
    "    if VAL_STRAT_SUBJECT and len(tr_subjects) > 1:\n",
    "        y_dom = build_subject_label_map(tr_subjects)\n",
    "        if np.any(y_dom < 0):\n",
    "            mask = y_dom >= 0\n",
    "            moda = int(np.bincount(y_dom[mask]).argmax()) if mask.sum() > 0 else 0\n",
    "            y_dom[~mask] = moda\n",
    "        n_val_subj = max(1, int(round(len(tr_subjects) * VAL_SUBJECT_FRAC)))\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=n_val_subj, random_state=RANDOM_STATE + fold)\n",
    "        idx = np.arange(len(tr_subjects))\n",
    "        _, val_idx = next(sss.split(idx, y_dom))\n",
    "        val_subjects = sorted([tr_subjects[i] for i in val_idx])\n",
    "        train_subjects = [s for s in tr_subjects if s not in val_subjects]\n",
    "    else:\n",
    "        tr_subjects_shuf = tr_subjects.copy()\n",
    "        rng.shuffle(tr_subjects_shuf)\n",
    "        n_val_subj = max(1, int(round(len(tr_subjects_shuf) * VAL_SUBJECT_FRAC)))\n",
    "        val_subjects = sorted(tr_subjects_shuf[:n_val_subj])\n",
    "        train_subjects = sorted(tr_subjects_shuf[n_val_subj:])\n",
    "\n",
    "    # Carga TRAIN/VAL/TEST\n",
    "    X_tr_list, y_tr_list, sub_tr_list = [], [], []\n",
    "    X_val_list, y_val_list, sub_val_list = [], [], []\n",
    "    X_te_list, y_te_list, sub_te_list = [], [], []\n",
    "    sfreq = None\n",
    "\n",
    "    for sid in tqdm(train_subjects, desc=f\"Cargando train fold{fold}\"):\n",
    "        Xs, ys, sf = load_subject_epochs(sid, RESAMPLE_HZ, DO_NOTCH, DO_BANDPASS, DO_CAR, BP_LO, BP_HI)\n",
    "        if len(ys) == 0: continue\n",
    "        X_tr_list.append(Xs); y_tr_list.append(ys)\n",
    "        sub_tr_list.append(np.full_like(ys, fill_value=subject_id_to_int(sid)))\n",
    "        sfreq = sf if sfreq is None else sfreq\n",
    "\n",
    "    for sid in tqdm(val_subjects, desc=f\"Cargando val fold{fold}\"):\n",
    "        Xs, ys, sf = load_subject_epochs(sid, RESAMPLE_HZ, DO_NOTCH, DO_BANDPASS, DO_CAR, BP_LO, BP_HI)\n",
    "        if len(ys) == 0: continue\n",
    "        X_val_list.append(Xs); y_val_list.append(ys)\n",
    "        sub_val_list.append(np.full_like(ys, fill_value=subject_id_to_int(sid)))\n",
    "        sfreq = sf if sfreq is None else sfreq\n",
    "\n",
    "    for sid in tqdm(test_sub, desc=f\"Cargando test fold{fold}\"):\n",
    "        Xs, ys, sf = load_subject_epochs(sid, RESAMPLE_HZ, DO_NOTCH, DO_BANDPASS, DO_CAR, BP_LO, BP_HI)\n",
    "        if len(ys) == 0: continue\n",
    "        X_te_list.append(Xs); y_te_list.append(ys)\n",
    "        sub_te_list.append(np.full_like(ys, fill_value=subject_id_to_int(sid)))\n",
    "        sfreq = sf if sfreq is None else sfreq\n",
    "\n",
    "    # Concatenar\n",
    "    X_tr = np.concatenate(X_tr_list, axis=0); y_tr = np.concatenate(y_tr_list, axis=0)\n",
    "    sub_tr = np.concatenate(sub_tr_list, axis=0)\n",
    "    X_val = np.concatenate(X_val_list, axis=0); y_val = np.concatenate(y_val_list, axis=0)\n",
    "    sub_val = np.concatenate(sub_val_list, axis=0)\n",
    "    X_te = np.concatenate(X_te_list, axis=0); y_te = np.concatenate(y_te_list, axis=0)\n",
    "    sub_te = np.concatenate(sub_te_list, axis=0)\n",
    "\n",
    "    print(f\"[Fold {fold}/5] Entrenando modelo global... (n_train={len(y_tr)} | n_val={len(y_val)} | n_test={len(y_te)})\")\n",
    "\n",
    "    # Normalización por canal (fit en TRAIN y aplicar a VAL/TEST)\n",
    "    if ZSCORE_PER_EPOCH:\n",
    "        X_tr_std, X_val_std, X_te_std = X_tr, X_val, X_te\n",
    "    else:\n",
    "        X_tr_std, X_val_std = standardize_per_channel(X_tr, X_val)\n",
    "        _,        X_te_std  = standardize_per_channel(X_tr, X_te)\n",
    "\n",
    "    # Datasets\n",
    "    tr_ds  = TensorDataset(torch.tensor(X_tr_std),  torch.tensor(y_tr).long(),  torch.tensor(sub_tr).long())\n",
    "    val_ds = TensorDataset(torch.tensor(X_val_std), torch.tensor(y_val).long(), torch.tensor(sub_val).long())\n",
    "    te_ds  = TensorDataset(torch.tensor(X_te_std),  torch.tensor(y_te).long(),  torch.tensor(sub_te).long())\n",
    "\n",
    "    # Weighted sampler templado: w = (1/ws)^a * (1/wk)^b\n",
    "    def make_weighted_sampler(dataset: TensorDataset):\n",
    "        _Xb, yb, sb = dataset.tensors\n",
    "        yb_np = yb.numpy()\n",
    "        sb_np = sb.numpy()\n",
    "        uniq_s, cnt_s = np.unique(sb_np, return_counts=True)\n",
    "        map_s = {s:c for s,c in zip(uniq_s, cnt_s)}\n",
    "        key = sb_np.astype(np.int64) * 10 + yb_np.astype(np.int64)\n",
    "        uniq_k, cnt_k = np.unique(key, return_counts=True)\n",
    "        map_k = {k:c for k,c in zip(uniq_k, cnt_k)}\n",
    "        a, b = 0.8, 1.0\n",
    "        w = []\n",
    "        for s, y in zip(sb_np, yb_np):\n",
    "            k = int(s)*10 + int(y)\n",
    "            ws = float(map_s[int(s)])\n",
    "            wk = float(map_k[k])\n",
    "            w.append((ws ** (-a)) * (wk ** (-b)))\n",
    "        w = np.array(w, dtype=np.float64)\n",
    "        w = w / (w.mean() + 1e-12)\n",
    "        sampler = WeightedRandomSampler(weights=torch.tensor(w, dtype=torch.double),\n",
    "                                        num_samples=len(yb_np), replacement=True)\n",
    "        return sampler\n",
    "\n",
    "    if USE_WEIGHTED_SAMPLER:\n",
    "        tr_sampler = make_weighted_sampler(tr_ds)\n",
    "        tr_ld  = DataLoader(tr_ds, batch_size=BATCH_SIZE, sampler=tr_sampler, drop_last=False, worker_init_fn=seed_worker)\n",
    "    else:\n",
    "        tr_ld  = DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True,  drop_last=False, worker_init_fn=seed_worker)\n",
    "\n",
    "    val_ld = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, worker_init_fn=seed_worker)\n",
    "    te_ld  = DataLoader(te_ds,  batch_size=BATCH_SIZE, shuffle=False, drop_last=False, worker_init_fn=seed_worker)\n",
    "\n",
    "    # Modelo\n",
    "    model = EEGCNNTransformer(n_ch=8, n_cls=4, d_model=D_MODEL, n_heads=N_HEADS,\n",
    "                              n_layers=N_LAYERS, p_drop=P_DROP, p_drop_encoder=P_DROP_ENCODER).to(device)\n",
    "\n",
    "    # Optimizador + Focal Loss\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=1e-2)\n",
    "    class_counts = np.bincount(y_tr, minlength=4).astype(np.float32)\n",
    "    inv = class_counts.sum() / (4.0 * np.maximum(class_counts, 1.0))\n",
    "    alpha = torch.tensor(inv, dtype=torch.float32, device=device)\n",
    "    alpha_mean = alpha.mean().item()\n",
    "    alpha[2] = 1.25 * alpha_mean  # both_fists\n",
    "    alpha[3] = 1.05 * alpha_mean  # both_feet\n",
    "    crit = FocalLoss(alpha=alpha, gamma=1.5, reduction='mean')\n",
    "\n",
    "    # LR scheduler Warmup+Cosine\n",
    "    from torch.optim.lr_scheduler import LambdaLR\n",
    "    total_epochs = EPOCHS\n",
    "    warmup_epochs = max(1, int(WARMUP_EPOCHS))\n",
    "    min_factor = 0.1\n",
    "    def lr_lambda(current_epoch):\n",
    "        if current_epoch < warmup_epochs:\n",
    "            return (current_epoch + 1) / warmup_epochs\n",
    "        progress = (current_epoch - warmup_epochs) / max(1, (total_epochs - warmup_epochs))\n",
    "        progress = min(1.0, max(0.0, progress))\n",
    "        return min_factor + 0.5 * (1.0 - min_factor) * (1.0 + np.cos(np.pi * progress))\n",
    "    scheduler = LambdaLR(opt, lr_lambda=lr_lambda)\n",
    "\n",
    "    # EMA (solo global)\n",
    "    if USE_EMA:\n",
    "        ema = ModelEMA(model, decay=EMA_DECAY, device=device)\n",
    "    else:\n",
    "        ema = None\n",
    "\n",
    "    # Entrenamiento global con early stopping por F1 macro\n",
    "    best_f1, best_state, wait = 0.0, None, 0\n",
    "    hist = {\"ep\": [], \"tr_loss\": [], \"tr_acc\": [], \"val_acc\": [], \"val_f1m\": [], \"lr\": []}\n",
    "\n",
    "    def evaluate_on(loader, use_ema=True):\n",
    "        mdl = ema.ema if (ema is not None and use_ema) else model\n",
    "        mdl.eval()\n",
    "        preds, gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _sb in loader:\n",
    "                xb = xb.to(device)\n",
    "                p = mdl(xb).argmax(dim=1).cpu().numpy()\n",
    "                preds.append(p); gts.append(yb.numpy())\n",
    "        preds = np.concatenate(preds); gts = np.concatenate(gts)\n",
    "        acc = accuracy_score(gts, preds)\n",
    "        f1m = f1_score(gts, preds, average='macro')\n",
    "        return acc, f1m\n",
    "\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        tr_loss, n_seen, tr_correct = 0.0, 0, 0\n",
    "        for xb, yb, _sb in tr_ld:\n",
    "            xb = xb.to(device); yb = yb.to(device)\n",
    "            xb = augment_batch(xb)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            opt.step()\n",
    "            if ema is not None:\n",
    "                ema.update(model)\n",
    "\n",
    "            tr_loss += loss.item() * len(yb)\n",
    "            n_seen += len(yb)\n",
    "            tr_correct += (logits.argmax(1) == yb).sum().item()\n",
    "        tr_loss /= max(1, n_seen)\n",
    "        tr_acc = tr_correct / max(1, n_seen)\n",
    "\n",
    "        acc, f1m = evaluate_on(val_ld, use_ema=True)\n",
    "\n",
    "        hist[\"ep\"].append(ep)\n",
    "        hist[\"tr_loss\"].append(tr_loss)\n",
    "        hist[\"tr_acc\"].append(tr_acc)\n",
    "        hist[\"val_acc\"].append(acc)\n",
    "        hist[\"val_f1m\"].append(f1m)\n",
    "        hist[\"lr\"].append(scheduler.get_last_lr()[0])\n",
    "\n",
    "        print(f\"  Época {ep:3d} | train_loss={tr_loss:.4f} | train_acc={tr_acc:.4f} | val_acc={acc:.4f} | val_f1m={f1m:.4f} | LR={scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        improved = f1m > best_f1 + 1e-4\n",
    "        if improved:\n",
    "            best_f1 = f1m\n",
    "            ref_model = ema.ema if ema is not None else model\n",
    "            best_state = {k: v.detach().cpu() for k, v in ref_model.state_dict().items()}\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "\n",
    "        scheduler.step()\n",
    "        if wait >= PATIENCE:\n",
    "            print(f\"  Early stopping en época {ep} (mejor val_f1m={best_f1:.4f})\")\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        (ema.ema if (ema is not None) else model).load_state_dict(best_state)\n",
    "\n",
    "    # ---- Guardar curva ----\n",
    "    fig = plt.figure(figsize=(8,4.5))\n",
    "    ax1 = plt.gca()\n",
    "    ax1.plot(hist[\"ep\"], hist[\"tr_loss\"], label=\"train_loss\")\n",
    "    ax1.plot(hist[\"ep\"], hist[\"tr_acc\"], label=\"tr_acc\")\n",
    "    ax1.plot(hist[\"ep\"], hist[\"val_acc\"], label=\"val_acc\")\n",
    "    ax1.set_xlabel(\"Época\"); ax1.set_title(f\"Fold {fold} — Curva de entrenamiento\")\n",
    "    ax1.legend(); ax1.grid(True, alpha=0.3)\n",
    "    out_png = f\"training_curve_fold{fold}.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=140)\n",
    "    plt.close(fig)\n",
    "    print(f\"↳ Curva de entrenamiento guardada: {out_png}\")\n",
    "\n",
    "    # ---- Evaluación final en TEST (global) ----\n",
    "    eval_model = ema.ema if (ema is not None) else model\n",
    "    eval_model.eval()\n",
    "\n",
    "    sfreq_used = RESAMPLE_HZ\n",
    "    if sfreq_used is None:\n",
    "        sfreq_used = int(round(X_te_std.shape[-1] / (TMAX - TMIN)))\n",
    "\n",
    "    if (not SW_ENABLE) or SW_MODE == 'none':\n",
    "        preds, gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _sb in te_ld:\n",
    "                xb = xb.to(device)\n",
    "                p = eval_model(xb).argmax(dim=1).cpu().numpy()\n",
    "                preds.append(p); gts.append(yb.numpy())\n",
    "        preds = np.concatenate(preds); gts = np.concatenate(gts)\n",
    "    elif SW_MODE in ('subwin', 'tta'):\n",
    "        logits_tta = None\n",
    "        logits_sw  = None\n",
    "        if SW_MODE == 'subwin':\n",
    "            logits_sw = subwindow_logits(eval_model, X_te_std, sfreq_used, SW_LEN, SW_STRIDE, device)\n",
    "        elif SW_MODE == 'tta':\n",
    "            logits_tta = time_shift_tta_logits(eval_model, X_te_std, sfreq_used, TTA_SHIFTS_S, device)\n",
    "\n",
    "        if COMBINE_TTA_AND_SUBWIN:\n",
    "            if logits_tta is None:\n",
    "                logits_tta = time_shift_tta_logits(eval_model, X_te_std, sfreq_used, TTA_SHIFTS_S, device)\n",
    "            if logits_sw is None:\n",
    "                logits_sw  = subwindow_logits(eval_model, X_te_std, sfreq_used, SW_LEN, SW_STRIDE, device)\n",
    "            logits = 0.5 * logits_tta + 0.5 * logits_sw\n",
    "        else:\n",
    "            logits = logits_tta if logits_tta is not None else logits_sw\n",
    "\n",
    "        preds = logits.argmax(axis=1); gts = y_te\n",
    "    else:\n",
    "        raise ValueError(f\"SW_MODE desconocido: {SW_MODE}\")\n",
    "\n",
    "    acc = accuracy_score(gts, preds)\n",
    "    f1m = f1_score(gts, preds, average='macro')\n",
    "    print(f\"[Fold {fold}/5] Global acc={acc:.4f}\\n\")\n",
    "    print(classification_report(gts, preds, target_names=[c.replace('_',' ') for c in CLASS_NAMES], digits=4))\n",
    "    print(\"Confusion matrix (rows=true, cols=pred):\")\n",
    "    cm = confusion_matrix(gts, preds, labels=[0,1,2,3])\n",
    "    print(cm)\n",
    "    print(f\"↳ Matriz de confusión guardada: confusion_global_fold{fold}.png\")\n",
    "    # plot cm\n",
    "    fig = plt.figure(figsize=(4.8,4.2))\n",
    "    plt.imshow(cm, cmap='Blues'); plt.title(f\"Confusion — Fold {fold} Global\")\n",
    "    plt.xlabel(\"pred\"); plt.ylabel(\"true\")\n",
    "    plt.colorbar(); plt.tight_layout()\n",
    "    plt.savefig(f\"confusion_global_fold{fold}.png\", dpi=140); plt.close(fig)\n",
    "\n",
    "    # =========================\n",
    "    # FINE-TUNING PROGRESIVO POR SUJETO (4-fold CV interno)\n",
    "    # =========================\n",
    "    # agrupamos por sujeto en el TEST\n",
    "    subjects = np.unique(sub_te)\n",
    "    subj_to_idx = {s: np.where(sub_te == s)[0] for s in subjects}\n",
    "\n",
    "    def make_ft_optimizer(model):\n",
    "        # dos grupos de params: head (lr alto), backbone (lr bajo)\n",
    "        head_params = list(model.head.parameters())\n",
    "        backbone_params = [p for n,p in model.named_parameters() if not n.startswith('head.')]\n",
    "        return torch.optim.AdamW([\n",
    "            {'params': backbone_params, 'lr': FT_LR_BACKBONE},\n",
    "            {'params': head_params,     'lr': FT_LR_HEAD}\n",
    "        ], weight_decay=FT_WD)\n",
    "\n",
    "    def freeze_backbone(model, freeze=True):\n",
    "        for n,p in model.named_parameters():\n",
    "            if not n.startswith('head.'):\n",
    "                p.requires_grad_(not freeze)\n",
    "\n",
    "    def ft_make_criterion_from_counts(counts):\n",
    "        inv = counts.sum() / (4.0 * np.maximum(counts.astype(np.float32), 1.0))\n",
    "        a = torch.tensor(inv, dtype=torch.float32, device=device)\n",
    "        am = a.mean().item()\n",
    "        a[2] = FT_ALPHA_BOOST_FISTS * am\n",
    "        a[3] = FT_ALPHA_BOOST_FEET  * am\n",
    "        return FocalLoss(alpha=a, gamma=1.5, reduction='mean')\n",
    "\n",
    "    def evaluate_tensor(model_t, X, y):\n",
    "        model_t.eval()\n",
    "        with torch.no_grad():\n",
    "            xb = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "            p = model_t(xb).argmax(1).cpu().numpy()\n",
    "        return accuracy_score(y, p), f1_score(y, p, average='macro'), p\n",
    "\n",
    "    # Para reporte agregado\n",
    "    ft_correct, ft_total = 0, 0\n",
    "    all_true, all_pred = [], []\n",
    "\n",
    "    # usamos como base los pesos del mejor global (EMA si existía)\n",
    "    base_state = (ema.ema if (ema is not None) else model).state_dict()\n",
    "\n",
    "    for s in subjects:\n",
    "        idx = subj_to_idx[s]\n",
    "        Xs, ys = X_te_std[idx], y_te[idx]\n",
    "\n",
    "        if len(np.unique(ys)) < 2 or len(ys) < 20:\n",
    "            # sujeto con muy pocos ejemplos/clases: eval directo\n",
    "            eval_model = EEGCNNTransformer(n_ch=8, n_cls=4, d_model=D_MODEL, n_heads=N_HEADS,\n",
    "                                           n_layers=N_LAYERS, p_drop=P_DROP, p_drop_encoder=P_DROP_ENCODER).to(device)\n",
    "            eval_model.load_state_dict(base_state)\n",
    "            acc_s, f1_s, pred_s = evaluate_tensor(eval_model, Xs, ys)\n",
    "            ft_correct += int(acc_s*len(ys)); ft_total += len(ys)\n",
    "            all_true.append(ys); all_pred.append(pred_s)\n",
    "            continue\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=FT_N_FOLDS, shuffle=True, random_state=RANDOM_STATE + fold + int(s))\n",
    "        for tr_idx, te_idx in skf.split(Xs, ys):\n",
    "            Xtr, ytr = Xs[tr_idx].copy(), ys[tr_idx].copy()\n",
    "            Xte_s, yte_s = Xs[te_idx].copy(), ys[te_idx].copy()\n",
    "\n",
    "            # estandarización por sujeto (fit en FT-train, aplicar a FT-test)\n",
    "            Xtr_std, Xte_std = standardize_per_channel(Xtr, Xte_s)\n",
    "\n",
    "            # modelo fresh desde el global\n",
    "            ft_model = EEGCNNTransformer(n_ch=8, n_cls=4, d_model=D_MODEL, n_heads=N_HEADS,\n",
    "                                         n_layers=N_LAYERS, p_drop=P_DROP, p_drop_encoder=P_DROP_ENCODER).to(device)\n",
    "            ft_model.load_state_dict(base_state)\n",
    "\n",
    "            # optim + criterio (α de este sujeto/split)\n",
    "            opt_ft = make_ft_optimizer(ft_model)\n",
    "            alpha_counts = np.bincount(ytr, minlength=4)\n",
    "            ft_crit = ft_make_criterion_from_counts(alpha_counts)\n",
    "\n",
    "            # loaders FT\n",
    "            ds_tr = TensorDataset(torch.tensor(Xtr_std), torch.tensor(ytr).long())\n",
    "            ds_te = TensorDataset(torch.tensor(Xte_std), torch.tensor(yte_s).long())\n",
    "            ld_tr = DataLoader(ds_tr, batch_size=FT_BATCH, shuffle=True,  drop_last=False, worker_init_fn=seed_worker)\n",
    "            ld_te = DataLoader(ds_te, batch_size=FT_BATCH, shuffle=False, drop_last=False, worker_init_fn=seed_worker)\n",
    "\n",
    "            # ETAPA 1: freeze backbone\n",
    "            freeze_backbone(ft_model, True)\n",
    "            best_f1_s, best_state_s, wait_s = -1, None, 0\n",
    "            for ep in range(1, FT_FREEZE_EPOCHS+1):\n",
    "                ft_model.train()\n",
    "                for xb, yb in ld_tr:\n",
    "                    xb = xb.to(device); yb = yb.to(device)\n",
    "                    xb = augment_batch_ft(xb)\n",
    "                    opt_ft.zero_grad()\n",
    "                    logits = ft_model(xb)\n",
    "                    loss = ft_crit(logits, yb)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(ft_model.parameters(), 1.0)\n",
    "                    opt_ft.step()\n",
    "\n",
    "                # eval fold\n",
    "                acc_s, f1_s = evaluate_tensor(ft_model, Xte_std, yte_s)[:2]\n",
    "                improved = f1_s > best_f1_s + 1e-4\n",
    "                if improved:\n",
    "                    best_f1_s = f1_s\n",
    "                    best_state_s = {k: v.detach().cpu() for k,v in ft_model.state_dict().items()}\n",
    "                    wait_s = 0\n",
    "                else:\n",
    "                    wait_s += 1\n",
    "                if wait_s >= FT_PATIENCE:\n",
    "                    break\n",
    "\n",
    "            if best_state_s is not None:\n",
    "                ft_model.load_state_dict(best_state_s)\n",
    "\n",
    "            # ETAPA 2: unfreeze (todo entrenable) con early stopping\n",
    "            freeze_backbone(ft_model, False)\n",
    "            best_f1_s2, best_state_s2, wait_s2 = -1, None, 0\n",
    "            for ep in range(1, FT_UNFREEZE_EPOCHS+1):\n",
    "                ft_model.train()\n",
    "                for xb, yb in ld_tr:\n",
    "                    xb = xb.to(device); yb = yb.to(device)\n",
    "                    xb = augment_batch_ft(xb)\n",
    "                    opt_ft.zero_grad()\n",
    "                    logits = ft_model(xb)\n",
    "                    loss = ft_crit(logits, yb)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(ft_model.parameters(), 1.0)\n",
    "                    opt_ft.step()\n",
    "\n",
    "                acc_s, f1_s = evaluate_tensor(ft_model, Xte_std, yte_s)[:2]\n",
    "                improved = f1_s > best_f1_s2 + 1e-4\n",
    "                if improved:\n",
    "                    best_f1_s2 = f1_s\n",
    "                    best_state_s2 = {k: v.detach().cpu() for k,v in ft_model.state_dict().items()}\n",
    "                    wait_s2 = 0\n",
    "                else:\n",
    "                    wait_s2 += 1\n",
    "                if wait_s2 >= FT_PATIENCE:\n",
    "                    break\n",
    "\n",
    "            if best_state_s2 is not None:\n",
    "                ft_model.load_state_dict(best_state_s2)\n",
    "\n",
    "            # métricas del fold del sujeto\n",
    "            acc_s, f1_s, pred_s = evaluate_tensor(ft_model, Xte_std, yte_s)\n",
    "            ft_correct += int(acc_s*len(yte_s)); ft_total += len(yte_s)\n",
    "            all_true.append(yte_s); all_pred.append(pred_s)\n",
    "\n",
    "    # resumen FT por sujeto\n",
    "    all_true = np.concatenate(all_true)\n",
    "    all_pred = np.concatenate(all_pred)\n",
    "    ft_acc = accuracy_score(all_true, all_pred)\n",
    "    ft_f1m = f1_score(all_true, all_pred, average='macro')\n",
    "    print(f\"  Fine-tuning PROGRESIVO (por sujeto, {FT_N_FOLDS}-fold CV) acc={ft_acc:.4f}\")\n",
    "    print(f\"  Δ(FT-Global) = {ft_acc - acc:+.4f}\")\n",
    "    cm_ft = confusion_matrix(all_true, all_pred, labels=[0,1,2,3])\n",
    "    print(\"Confusion matrix FT (rows=true, cols=pred):\")\n",
    "    print(cm_ft)\n",
    "    fig = plt.figure(figsize=(4.8,4.2))\n",
    "    plt.imshow(cm_ft, cmap='Greens'); plt.title(f\"Confusion — Fold {fold} FT\")\n",
    "    plt.xlabel(\"pred\"); plt.ylabel(\"true\")\n",
    "    plt.colorbar(); plt.tight_layout()\n",
    "    plt.savefig(f\"confusion_ft_fold{fold}.png\", dpi=140); plt.close(fig)\n",
    "    print(f\"↳ Matriz de confusión FT guardada: confusion_ft_fold{fold}.png\")\n",
    "\n",
    "    return acc, f1m, ft_acc\n",
    "\n",
    "# =========================\n",
    "# LOOP 5 FOLDS + RESUMEN\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    acc_folds, f1_folds, ft_acc_folds = [], [], []\n",
    "    for fold in range(1, 6):\n",
    "        acc, f1m, ft_acc = train_one_fold(fold, DEVICE)\n",
    "        acc_folds.append(f\"{acc:.4f}\")\n",
    "        f1_folds.append(f\"{f1m:.4f}\")\n",
    "        ft_acc_folds.append(f\"{ft_acc:.4f}\")\n",
    "\n",
    "    acc_mean = float(np.mean([float(a) for a in acc_folds]))\n",
    "    f1_mean  = float(np.mean([float(f) for f in f1_folds]))\n",
    "    ft_acc_mean = float(np.mean([float(a) for a in ft_acc_folds]))\n",
    "    delta_mean = ft_acc_mean - acc_mean\n",
    "\n",
    "    print(\"\\n============================================================\")\n",
    "    print(\"RESULTADOS FINALES\")\n",
    "    print(\"============================================================\")\n",
    "    print(f\"Global folds (ACC): {acc_folds}\")\n",
    "    print(f\"Global mean ACC: {acc_mean:.4f}\")\n",
    "    print(f\"F1 folds (MACRO): {f1_folds}\")\n",
    "    print(f\"F1 mean (MACRO): {f1_mean:.4f}\")\n",
    "    print(f\"Fine-tune PROGRESIVO folds: {ft_acc_folds}\")\n",
    "    print(f\"Fine-tune PROGRESIVO mean: {ft_acc_mean:.4f}\")\n",
    "    print(f\"Δ(FT-Global) mean: {delta_mean:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465e0421",
   "metadata": {},
   "source": [
    "## DOS CLASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ccd4ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Usando dispositivo: cuda\n",
      "🧠 INICIANDO EXPERIMENTO CON CNN+Transformer (K-Fold por sujeto) — 2 clases (left/right)\n",
      "🔧 Configuración: 2c (L/R), 8 canales, 6s | EPOCHS=60, BATCH=64, LR=0.001 | ZSCORE_PER_EPOCH=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold1: 100%|██████████| 67/67 [00:03<00:00, 17.11it/s]\n",
      "Cargando val fold1: 100%|██████████| 15/15 [00:00<00:00, 17.16it/s]\n",
      "Cargando test fold1: 100%|██████████| 21/21 [00:01<00:00, 17.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1/5] Entrenando modelo global... (n_train=2814 | n_val=630 | n_test=882)\n",
      "  Época   1 | train_loss=0.1543 | train_acc=0.5064 | val_acc=0.5825 | val_f1m=0.5682 | LR=0.000125\n",
      "  Época   2 | train_loss=0.1046 | train_acc=0.6834 | val_acc=0.7063 | val_f1m=0.7060 | LR=0.000250\n",
      "  Época   3 | train_loss=0.0808 | train_acc=0.7893 | val_acc=0.7429 | val_f1m=0.7428 | LR=0.000375\n",
      "  Época   4 | train_loss=0.0776 | train_acc=0.8149 | val_acc=0.7587 | val_f1m=0.7587 | LR=0.000500\n",
      "  Época   5 | train_loss=0.0701 | train_acc=0.8262 | val_acc=0.7333 | val_f1m=0.7309 | LR=0.000625\n",
      "  Época   6 | train_loss=0.0698 | train_acc=0.8344 | val_acc=0.7730 | val_f1m=0.7729 | LR=0.000750\n",
      "  Época   7 | train_loss=0.0678 | train_acc=0.8312 | val_acc=0.7619 | val_f1m=0.7619 | LR=0.000875\n",
      "  Época   8 | train_loss=0.0652 | train_acc=0.8426 | val_acc=0.7190 | val_f1m=0.7131 | LR=0.001000\n",
      "  Época   9 | train_loss=0.0688 | train_acc=0.8323 | val_acc=0.7651 | val_f1m=0.7650 | LR=0.001000\n",
      "  Época  10 | train_loss=0.0730 | train_acc=0.8127 | val_acc=0.7476 | val_f1m=0.7458 | LR=0.000999\n",
      "  Época  11 | train_loss=0.0589 | train_acc=0.8568 | val_acc=0.7619 | val_f1m=0.7608 | LR=0.000997\n",
      "  Época  12 | train_loss=0.0606 | train_acc=0.8547 | val_acc=0.7667 | val_f1m=0.7665 | LR=0.000993\n",
      "  Época  13 | train_loss=0.0575 | train_acc=0.8621 | val_acc=0.7683 | val_f1m=0.7681 | LR=0.000987\n",
      "  Época  14 | train_loss=0.0612 | train_acc=0.8515 | val_acc=0.7778 | val_f1m=0.7776 | LR=0.000980\n",
      "  Época  15 | train_loss=0.0570 | train_acc=0.8685 | val_acc=0.7651 | val_f1m=0.7651 | LR=0.000971\n",
      "  Época  16 | train_loss=0.0593 | train_acc=0.8518 | val_acc=0.7619 | val_f1m=0.7610 | LR=0.000960\n",
      "  Época  17 | train_loss=0.0573 | train_acc=0.8575 | val_acc=0.7889 | val_f1m=0.7887 | LR=0.000948\n",
      "  Época  18 | train_loss=0.0578 | train_acc=0.8586 | val_acc=0.7667 | val_f1m=0.7665 | LR=0.000935\n",
      "  Época  19 | train_loss=0.0504 | train_acc=0.8753 | val_acc=0.7746 | val_f1m=0.7740 | LR=0.000920\n",
      "  Época  20 | train_loss=0.0571 | train_acc=0.8611 | val_acc=0.7571 | val_f1m=0.7566 | LR=0.000904\n",
      "  Época  21 | train_loss=0.0518 | train_acc=0.8738 | val_acc=0.7810 | val_f1m=0.7809 | LR=0.000887\n",
      "  Época  22 | train_loss=0.0475 | train_acc=0.8881 | val_acc=0.7698 | val_f1m=0.7698 | LR=0.000868\n",
      "  Época  23 | train_loss=0.0470 | train_acc=0.8952 | val_acc=0.7714 | val_f1m=0.7713 | LR=0.000848\n",
      "  Época  24 | train_loss=0.0522 | train_acc=0.8703 | val_acc=0.7714 | val_f1m=0.7713 | LR=0.000828\n",
      "  Época  25 | train_loss=0.0457 | train_acc=0.8959 | val_acc=0.7698 | val_f1m=0.7697 | LR=0.000806\n",
      "  Época  26 | train_loss=0.0437 | train_acc=0.8994 | val_acc=0.7714 | val_f1m=0.7713 | LR=0.000783\n",
      "  Época  27 | train_loss=0.0489 | train_acc=0.8881 | val_acc=0.7683 | val_f1m=0.7682 | LR=0.000759\n",
      "  Época  28 | train_loss=0.0470 | train_acc=0.8856 | val_acc=0.7714 | val_f1m=0.7713 | LR=0.000735\n",
      "  Época  29 | train_loss=0.0465 | train_acc=0.8927 | val_acc=0.7714 | val_f1m=0.7713 | LR=0.000710\n",
      "  Early stopping en época 29 (mejor val_f1m=0.7887)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold1.png\n",
      "[Fold 1/5] Global acc=0.7993\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.7882    0.8186    0.8031       441\n",
      "       right     0.8113    0.7800    0.7954       441\n",
      "\n",
      "    accuracy                         0.7993       882\n",
      "   macro avg     0.7998    0.7993    0.7992       882\n",
      "weighted avg     0.7998    0.7993    0.7992       882\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[361  80]\n",
      " [ 97 344]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold2: 100%|██████████| 67/67 [00:04<00:00, 15.84it/s]\n",
      "Cargando val fold2: 100%|██████████| 15/15 [00:00<00:00, 15.76it/s]\n",
      "Cargando test fold2: 100%|██████████| 21/21 [00:01<00:00, 16.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 2/5] Entrenando modelo global... (n_train=2814 | n_val=630 | n_test=882)\n",
      "  Época   1 | train_loss=0.1289 | train_acc=0.5235 | val_acc=0.5762 | val_f1m=0.5722 | LR=0.000125\n",
      "  Época   2 | train_loss=0.1076 | train_acc=0.6731 | val_acc=0.7111 | val_f1m=0.7111 | LR=0.000250\n",
      "  Época   3 | train_loss=0.0869 | train_acc=0.7793 | val_acc=0.7286 | val_f1m=0.7242 | LR=0.000375\n",
      "  Época   4 | train_loss=0.0806 | train_acc=0.7864 | val_acc=0.7540 | val_f1m=0.7494 | LR=0.000500\n",
      "  Época   5 | train_loss=0.0825 | train_acc=0.7896 | val_acc=0.7952 | val_f1m=0.7952 | LR=0.000625\n",
      "  Época   6 | train_loss=0.0732 | train_acc=0.8145 | val_acc=0.7603 | val_f1m=0.7603 | LR=0.000750\n",
      "  Época   7 | train_loss=0.0748 | train_acc=0.8099 | val_acc=0.7778 | val_f1m=0.7747 | LR=0.000875\n",
      "  Época   8 | train_loss=0.0749 | train_acc=0.8106 | val_acc=0.7508 | val_f1m=0.7428 | LR=0.001000\n",
      "  Época   9 | train_loss=0.0761 | train_acc=0.8106 | val_acc=0.7794 | val_f1m=0.7792 | LR=0.001000\n",
      "  Época  10 | train_loss=0.0692 | train_acc=0.8276 | val_acc=0.7905 | val_f1m=0.7900 | LR=0.000999\n",
      "  Época  11 | train_loss=0.0654 | train_acc=0.8397 | val_acc=0.7857 | val_f1m=0.7851 | LR=0.000997\n",
      "  Época  12 | train_loss=0.0687 | train_acc=0.8294 | val_acc=0.7714 | val_f1m=0.7711 | LR=0.000993\n",
      "  Época  13 | train_loss=0.0654 | train_acc=0.8394 | val_acc=0.7698 | val_f1m=0.7697 | LR=0.000987\n",
      "  Época  14 | train_loss=0.0687 | train_acc=0.8266 | val_acc=0.7857 | val_f1m=0.7847 | LR=0.000980\n",
      "  Época  15 | train_loss=0.0627 | train_acc=0.8486 | val_acc=0.7825 | val_f1m=0.7824 | LR=0.000971\n",
      "  Época  16 | train_loss=0.0607 | train_acc=0.8518 | val_acc=0.7889 | val_f1m=0.7887 | LR=0.000960\n",
      "  Época  17 | train_loss=0.0668 | train_acc=0.8422 | val_acc=0.7921 | val_f1m=0.7912 | LR=0.000948\n",
      "  Early stopping en época 17 (mejor val_f1m=0.7952)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold2.png\n",
      "[Fold 2/5] Global acc=0.8231\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.8118    0.8413    0.8263       441\n",
      "       right     0.8353    0.8050    0.8199       441\n",
      "\n",
      "    accuracy                         0.8231       882\n",
      "   macro avg     0.8236    0.8231    0.8231       882\n",
      "weighted avg     0.8236    0.8231    0.8231       882\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[371  70]\n",
      " [ 86 355]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold3: 100%|██████████| 67/67 [00:03<00:00, 16.77it/s]\n",
      "Cargando val fold3: 100%|██████████| 15/15 [00:00<00:00, 16.84it/s]\n",
      "Cargando test fold3: 100%|██████████| 21/21 [00:01<00:00, 16.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 3/5] Entrenando modelo global... (n_train=2814 | n_val=630 | n_test=882)\n",
      "  Época   1 | train_loss=0.1278 | train_acc=0.5181 | val_acc=0.5270 | val_f1m=0.4369 | LR=0.000125\n",
      "  Época   2 | train_loss=0.1172 | train_acc=0.6027 | val_acc=0.7524 | val_f1m=0.7523 | LR=0.000250\n",
      "  Época   3 | train_loss=0.0903 | train_acc=0.7672 | val_acc=0.7984 | val_f1m=0.7973 | LR=0.000375\n",
      "  Época   4 | train_loss=0.0877 | train_acc=0.7832 | val_acc=0.7921 | val_f1m=0.7921 | LR=0.000500\n",
      "  Época   5 | train_loss=0.0747 | train_acc=0.8198 | val_acc=0.7937 | val_f1m=0.7935 | LR=0.000625\n",
      "  Época   6 | train_loss=0.0789 | train_acc=0.8021 | val_acc=0.7746 | val_f1m=0.7739 | LR=0.000750\n",
      "  Época   7 | train_loss=0.0700 | train_acc=0.8298 | val_acc=0.7921 | val_f1m=0.7910 | LR=0.000875\n",
      "  Época   8 | train_loss=0.0758 | train_acc=0.8117 | val_acc=0.7746 | val_f1m=0.7738 | LR=0.001000\n",
      "  Época   9 | train_loss=0.0723 | train_acc=0.8244 | val_acc=0.7889 | val_f1m=0.7876 | LR=0.001000\n",
      "  Época  10 | train_loss=0.0695 | train_acc=0.8362 | val_acc=0.8032 | val_f1m=0.8029 | LR=0.000999\n",
      "  Época  11 | train_loss=0.0702 | train_acc=0.8287 | val_acc=0.7794 | val_f1m=0.7784 | LR=0.000997\n",
      "  Época  12 | train_loss=0.0710 | train_acc=0.8348 | val_acc=0.8095 | val_f1m=0.8091 | LR=0.000993\n",
      "  Época  13 | train_loss=0.0605 | train_acc=0.8571 | val_acc=0.8143 | val_f1m=0.8143 | LR=0.000987\n",
      "  Época  14 | train_loss=0.0664 | train_acc=0.8387 | val_acc=0.8048 | val_f1m=0.8042 | LR=0.000980\n",
      "  Época  15 | train_loss=0.0621 | train_acc=0.8550 | val_acc=0.8016 | val_f1m=0.8016 | LR=0.000971\n",
      "  Época  16 | train_loss=0.0636 | train_acc=0.8380 | val_acc=0.8063 | val_f1m=0.8063 | LR=0.000960\n",
      "  Época  17 | train_loss=0.0564 | train_acc=0.8664 | val_acc=0.8063 | val_f1m=0.8060 | LR=0.000948\n",
      "  Época  18 | train_loss=0.0578 | train_acc=0.8699 | val_acc=0.8000 | val_f1m=0.7997 | LR=0.000935\n",
      "  Época  19 | train_loss=0.0600 | train_acc=0.8518 | val_acc=0.7905 | val_f1m=0.7902 | LR=0.000920\n",
      "  Época  20 | train_loss=0.0600 | train_acc=0.8564 | val_acc=0.7937 | val_f1m=0.7928 | LR=0.000904\n",
      "  Época  21 | train_loss=0.0566 | train_acc=0.8600 | val_acc=0.8063 | val_f1m=0.8063 | LR=0.000887\n",
      "  Época  22 | train_loss=0.0596 | train_acc=0.8550 | val_acc=0.7968 | val_f1m=0.7968 | LR=0.000868\n",
      "  Época  23 | train_loss=0.0539 | train_acc=0.8650 | val_acc=0.7905 | val_f1m=0.7905 | LR=0.000848\n",
      "  Época  24 | train_loss=0.0572 | train_acc=0.8532 | val_acc=0.7905 | val_f1m=0.7905 | LR=0.000828\n",
      "  Época  25 | train_loss=0.0524 | train_acc=0.8802 | val_acc=0.7905 | val_f1m=0.7905 | LR=0.000806\n",
      "  Early stopping en época 25 (mejor val_f1m=0.8143)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold3.png\n",
      "[Fold 3/5] Global acc=0.7857\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.7903    0.7778    0.7840       441\n",
      "       right     0.7812    0.7937    0.7874       441\n",
      "\n",
      "    accuracy                         0.7857       882\n",
      "   macro avg     0.7858    0.7857    0.7857       882\n",
      "weighted avg     0.7858    0.7857    0.7857       882\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[343  98]\n",
      " [ 91 350]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold4: 100%|██████████| 68/68 [00:04<00:00, 16.79it/s]\n",
      "Cargando val fold4: 100%|██████████| 15/15 [00:00<00:00, 16.74it/s]\n",
      "Cargando test fold4: 100%|██████████| 20/20 [00:01<00:00, 16.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 4/5] Entrenando modelo global... (n_train=2856 | n_val=630 | n_test=840)\n",
      "  Época   1 | train_loss=0.1285 | train_acc=0.5081 | val_acc=0.5524 | val_f1m=0.4999 | LR=0.000125\n",
      "  Época   2 | train_loss=0.1099 | train_acc=0.6516 | val_acc=0.7238 | val_f1m=0.7233 | LR=0.000250\n",
      "  Época   3 | train_loss=0.0932 | train_acc=0.7528 | val_acc=0.7603 | val_f1m=0.7599 | LR=0.000375\n",
      "  Época   4 | train_loss=0.0815 | train_acc=0.7899 | val_acc=0.7524 | val_f1m=0.7523 | LR=0.000500\n",
      "  Época   5 | train_loss=0.0778 | train_acc=0.8057 | val_acc=0.7651 | val_f1m=0.7649 | LR=0.000625\n",
      "  Época   6 | train_loss=0.0837 | train_acc=0.7847 | val_acc=0.7714 | val_f1m=0.7684 | LR=0.000750\n",
      "  Época   7 | train_loss=0.0745 | train_acc=0.8144 | val_acc=0.7857 | val_f1m=0.7842 | LR=0.000875\n",
      "  Época   8 | train_loss=0.0732 | train_acc=0.8113 | val_acc=0.7794 | val_f1m=0.7784 | LR=0.001000\n",
      "  Época   9 | train_loss=0.0708 | train_acc=0.8176 | val_acc=0.7698 | val_f1m=0.7693 | LR=0.001000\n",
      "  Época  10 | train_loss=0.0698 | train_acc=0.8197 | val_acc=0.7698 | val_f1m=0.7693 | LR=0.000999\n",
      "  Época  11 | train_loss=0.0732 | train_acc=0.8113 | val_acc=0.7778 | val_f1m=0.7763 | LR=0.000997\n",
      "  Época  12 | train_loss=0.0701 | train_acc=0.8207 | val_acc=0.7746 | val_f1m=0.7744 | LR=0.000993\n",
      "  Época  13 | train_loss=0.0653 | train_acc=0.8298 | val_acc=0.7794 | val_f1m=0.7794 | LR=0.000987\n",
      "  Época  14 | train_loss=0.0632 | train_acc=0.8431 | val_acc=0.7857 | val_f1m=0.7857 | LR=0.000980\n",
      "  Época  15 | train_loss=0.0640 | train_acc=0.8393 | val_acc=0.7952 | val_f1m=0.7951 | LR=0.000971\n",
      "  Época  16 | train_loss=0.0700 | train_acc=0.8211 | val_acc=0.7841 | val_f1m=0.7836 | LR=0.000960\n",
      "  Época  17 | train_loss=0.0658 | train_acc=0.8347 | val_acc=0.7762 | val_f1m=0.7761 | LR=0.000948\n",
      "  Época  18 | train_loss=0.0643 | train_acc=0.8354 | val_acc=0.7857 | val_f1m=0.7857 | LR=0.000935\n",
      "  Época  19 | train_loss=0.0622 | train_acc=0.8494 | val_acc=0.7937 | val_f1m=0.7935 | LR=0.000920\n",
      "  Época  20 | train_loss=0.0601 | train_acc=0.8547 | val_acc=0.7921 | val_f1m=0.7915 | LR=0.000904\n",
      "  Época  21 | train_loss=0.0616 | train_acc=0.8501 | val_acc=0.7968 | val_f1m=0.7968 | LR=0.000887\n",
      "  Época  22 | train_loss=0.0598 | train_acc=0.8519 | val_acc=0.7984 | val_f1m=0.7978 | LR=0.000868\n",
      "  Época  23 | train_loss=0.0598 | train_acc=0.8508 | val_acc=0.8000 | val_f1m=0.7994 | LR=0.000848\n",
      "  Época  24 | train_loss=0.0594 | train_acc=0.8554 | val_acc=0.7984 | val_f1m=0.7979 | LR=0.000828\n",
      "  Época  25 | train_loss=0.0570 | train_acc=0.8627 | val_acc=0.7984 | val_f1m=0.7979 | LR=0.000806\n",
      "  Época  26 | train_loss=0.0561 | train_acc=0.8564 | val_acc=0.7968 | val_f1m=0.7962 | LR=0.000783\n",
      "  Época  27 | train_loss=0.0580 | train_acc=0.8676 | val_acc=0.7952 | val_f1m=0.7947 | LR=0.000759\n",
      "  Época  28 | train_loss=0.0551 | train_acc=0.8680 | val_acc=0.7937 | val_f1m=0.7930 | LR=0.000735\n",
      "  Época  29 | train_loss=0.0541 | train_acc=0.8697 | val_acc=0.7937 | val_f1m=0.7930 | LR=0.000710\n",
      "  Época  30 | train_loss=0.0497 | train_acc=0.8803 | val_acc=0.7937 | val_f1m=0.7930 | LR=0.000684\n",
      "  Época  31 | train_loss=0.0530 | train_acc=0.8722 | val_acc=0.7952 | val_f1m=0.7947 | LR=0.000658\n",
      "  Época  32 | train_loss=0.0515 | train_acc=0.8775 | val_acc=0.7937 | val_f1m=0.7931 | LR=0.000631\n",
      "  Época  33 | train_loss=0.0471 | train_acc=0.8866 | val_acc=0.7937 | val_f1m=0.7931 | LR=0.000604\n",
      "  Época  34 | train_loss=0.0513 | train_acc=0.8789 | val_acc=0.7952 | val_f1m=0.7947 | LR=0.000577\n",
      "  Época  35 | train_loss=0.0471 | train_acc=0.8880 | val_acc=0.7984 | val_f1m=0.7980 | LR=0.000550\n",
      "  Early stopping en época 35 (mejor val_f1m=0.7994)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold4.png\n",
      "[Fold 4/5] Global acc=0.8202\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.8195    0.8214    0.8205       420\n",
      "       right     0.8210    0.8190    0.8200       420\n",
      "\n",
      "    accuracy                         0.8202       840\n",
      "   macro avg     0.8202    0.8202    0.8202       840\n",
      "weighted avg     0.8202    0.8202    0.8202       840\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[345  75]\n",
      " [ 76 344]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold5: 100%|██████████| 68/68 [00:03<00:00, 17.03it/s]\n",
      "Cargando val fold5: 100%|██████████| 15/15 [00:00<00:00, 16.61it/s]\n",
      "Cargando test fold5: 100%|██████████| 20/20 [00:01<00:00, 16.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 5/5] Entrenando modelo global... (n_train=2856 | n_val=630 | n_test=840)\n",
      "  Época   1 | train_loss=0.1265 | train_acc=0.5224 | val_acc=0.5524 | val_f1m=0.5190 | LR=0.000125\n",
      "  Época   2 | train_loss=0.1102 | train_acc=0.6579 | val_acc=0.6968 | val_f1m=0.6950 | LR=0.000250\n",
      "  Época   3 | train_loss=0.0859 | train_acc=0.7763 | val_acc=0.7206 | val_f1m=0.7205 | LR=0.000375\n",
      "  Época   4 | train_loss=0.0811 | train_acc=0.7994 | val_acc=0.7317 | val_f1m=0.7316 | LR=0.000500\n",
      "  Época   5 | train_loss=0.0791 | train_acc=0.8029 | val_acc=0.7286 | val_f1m=0.7286 | LR=0.000625\n",
      "  Época   6 | train_loss=0.0750 | train_acc=0.8193 | val_acc=0.6952 | val_f1m=0.6830 | LR=0.000750\n",
      "  Época   7 | train_loss=0.0791 | train_acc=0.7980 | val_acc=0.7587 | val_f1m=0.7586 | LR=0.000875\n",
      "  Época   8 | train_loss=0.0720 | train_acc=0.8186 | val_acc=0.7524 | val_f1m=0.7512 | LR=0.001000\n",
      "  Época   9 | train_loss=0.0726 | train_acc=0.8165 | val_acc=0.7651 | val_f1m=0.7650 | LR=0.001000\n",
      "  Época  10 | train_loss=0.0696 | train_acc=0.8246 | val_acc=0.7635 | val_f1m=0.7634 | LR=0.000999\n",
      "  Época  11 | train_loss=0.0683 | train_acc=0.8277 | val_acc=0.7651 | val_f1m=0.7651 | LR=0.000997\n",
      "  Época  12 | train_loss=0.0669 | train_acc=0.8274 | val_acc=0.7746 | val_f1m=0.7738 | LR=0.000993\n",
      "  Época  13 | train_loss=0.0620 | train_acc=0.8515 | val_acc=0.7778 | val_f1m=0.7773 | LR=0.000987\n",
      "  Época  14 | train_loss=0.0681 | train_acc=0.8337 | val_acc=0.7730 | val_f1m=0.7729 | LR=0.000980\n",
      "  Época  15 | train_loss=0.0648 | train_acc=0.8295 | val_acc=0.7492 | val_f1m=0.7492 | LR=0.000971\n",
      "  Época  16 | train_loss=0.0623 | train_acc=0.8417 | val_acc=0.7905 | val_f1m=0.7905 | LR=0.000960\n",
      "  Época  17 | train_loss=0.0578 | train_acc=0.8578 | val_acc=0.7302 | val_f1m=0.7288 | LR=0.000948\n",
      "  Época  18 | train_loss=0.0595 | train_acc=0.8498 | val_acc=0.7683 | val_f1m=0.7682 | LR=0.000935\n",
      "  Época  19 | train_loss=0.0562 | train_acc=0.8631 | val_acc=0.7444 | val_f1m=0.7444 | LR=0.000920\n",
      "  Época  20 | train_loss=0.0590 | train_acc=0.8547 | val_acc=0.7651 | val_f1m=0.7651 | LR=0.000904\n",
      "  Época  21 | train_loss=0.0590 | train_acc=0.8585 | val_acc=0.7746 | val_f1m=0.7746 | LR=0.000887\n",
      "  Época  22 | train_loss=0.0579 | train_acc=0.8655 | val_acc=0.7730 | val_f1m=0.7730 | LR=0.000868\n",
      "  Época  23 | train_loss=0.0542 | train_acc=0.8718 | val_acc=0.7730 | val_f1m=0.7730 | LR=0.000848\n",
      "  Época  24 | train_loss=0.0545 | train_acc=0.8610 | val_acc=0.7714 | val_f1m=0.7714 | LR=0.000828\n",
      "  Época  25 | train_loss=0.0556 | train_acc=0.8641 | val_acc=0.7698 | val_f1m=0.7698 | LR=0.000806\n",
      "  Época  26 | train_loss=0.0518 | train_acc=0.8778 | val_acc=0.7714 | val_f1m=0.7714 | LR=0.000783\n",
      "  Época  27 | train_loss=0.0558 | train_acc=0.8536 | val_acc=0.7730 | val_f1m=0.7730 | LR=0.000759\n",
      "  Época  28 | train_loss=0.0532 | train_acc=0.8708 | val_acc=0.7730 | val_f1m=0.7730 | LR=0.000735\n",
      "  Early stopping en época 28 (mejor val_f1m=0.7905)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold5.png\n",
      "[Fold 5/5] Global acc=0.8286\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.8286    0.8286    0.8286       420\n",
      "       right     0.8286    0.8286    0.8286       420\n",
      "\n",
      "    accuracy                         0.8286       840\n",
      "   macro avg     0.8286    0.8286    0.8286       840\n",
      "weighted avg     0.8286    0.8286    0.8286       840\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[348  72]\n",
      " [ 72 348]]\n",
      "\n",
      "============================================================\n",
      "RESULTADOS FINALES (2 clases: left/right)\n",
      "============================================================\n",
      "Global folds (ACC): ['0.7993', '0.8231', '0.7857', '0.8202', '0.8286']\n",
      "Global mean ACC: 0.8114\n",
      "F1 folds (MACRO): ['0.7992', '0.8231', '0.7857', '0.8202', '0.8286']\n",
      "F1 mean (MACRO): 0.8114\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "import re, json, random\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import mne\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# =========================\n",
    "# REPRODUCIBILIDAD\n",
    "# =========================\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def seed_worker(worker_id: int):\n",
    "    worker_seed = RANDOM_STATE + worker_id\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "seed_everything(RANDOM_STATE)\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "FOLDS_JSON = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 64        # múltiplo de 4\n",
    "BASE_LR = 1e-3\n",
    "WARMUP_EPOCHS = 8\n",
    "PATIENCE = 12\n",
    "\n",
    "# Split de validación por fold (por sujetos)\n",
    "VAL_SUBJECT_FRAC = 0.18  # ≈ 18% de sujetos del train → val\n",
    "VAL_STRAT_SUBJECT = True # estratifica por etiqueta dominante de cada sujeto\n",
    "\n",
    "# Prepro\n",
    "RESAMPLE_HZ = None\n",
    "DO_NOTCH = True\n",
    "DO_BANDPASS = False       # respetamos tu setup original\n",
    "BP_LO, BP_HI = 4.0, 38.0\n",
    "DO_CAR = False\n",
    "ZSCORE_PER_EPOCH = False\n",
    "\n",
    "# Modelo\n",
    "D_MODEL = 128\n",
    "N_HEADS = 4\n",
    "N_LAYERS = 2\n",
    "P_DROP = 0.2            # dropout en conv stack\n",
    "P_DROP_ENCODER = 0.3    # dropout antes del encoder\n",
    "\n",
    "# Ventana temporal\n",
    "TMIN, TMAX = -1.0, 5.0\n",
    "\n",
    "# TTA / SUBWINDOW en TEST\n",
    "SW_MODE = 'tta'   # 'none'|'subwin'|'tta'\n",
    "SW_ENABLE = True\n",
    "TTA_SHIFTS_S = [-0.075, -0.05, -0.025, 0.0, 0.025, 0.05, 0.075]\n",
    "SW_LEN, SW_STRIDE = 4.5, 1.5\n",
    "COMBINE_TTA_AND_SUBWIN = False\n",
    "\n",
    "# Sampler balanceado\n",
    "USE_WEIGHTED_SAMPLER = True\n",
    "\n",
    "# EMA\n",
    "USE_EMA = True\n",
    "EMA_DECAY = 0.9995\n",
    "\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','FCz']\n",
    "# -----> SOLO 2 CLASES\n",
    "CLASS_NAMES = ['left', 'right']\n",
    "\n",
    "# Solo usaremos los runs de imaginación L/R; ignoramos los de fists/feet\n",
    "IMAGERY_RUNS_LR = {4, 8, 12}\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Usando dispositivo: {DEVICE}\")\n",
    "print(\"🧠 INICIANDO EXPERIMENTO CON CNN+Transformer (K-Fold por sujeto) — 2 clases (left/right)\")\n",
    "print(f\"🔧 Configuración: 2c (L/R), 8 canales, 6s | EPOCHS={EPOCHS}, BATCH={BATCH_SIZE}, LR={BASE_LR} | ZSCORE_PER_EPOCH={ZSCORE_PER_EPOCH}\")\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES I/O\n",
    "# =========================\n",
    "def normalize_ch_name(name: str) -> str:\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', name)\n",
    "    return s.upper()\n",
    "\n",
    "NORMALIZED_TARGETS = [normalize_ch_name(c) for c in EXPECTED_8]\n",
    "\n",
    "def pick_8_channels(raw: mne.io.BaseRaw) -> mne.io.BaseRaw:\n",
    "    chs = raw.info['ch_names']\n",
    "    norm_map = {normalize_ch_name(ch): ch for ch in chs}\n",
    "    picked = []\n",
    "    for target_norm, target_orig in zip(NORMALIZED_TARGETS, EXPECTED_8):\n",
    "        if target_norm in norm_map:\n",
    "            picked.append(norm_map[target_norm])\n",
    "        else:\n",
    "            raise RuntimeError(f\"Canal requerido '{target_orig}' no encontrado. Disponibles: {chs}\")\n",
    "    return raw.pick(picks=picked)\n",
    "\n",
    "def list_subject_imagery_edfs(subject_id: str) -> list:\n",
    "    subj_dir = DATA_RAW / subject_id\n",
    "    edfs = []\n",
    "    for r in [4, 6, 8, 10, 12, 14]:\n",
    "        edfs.extend(glob(str(subj_dir / f\"{subject_id}R{r:02d}.edf\")))\n",
    "    return sorted(edfs)\n",
    "\n",
    "def subject_id_to_int(s: str) -> int:\n",
    "    m = re.match(r'[Ss](\\d+)', s)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "def load_subject_epochs(subject_id: str, resample_hz: int, do_notch: bool, do_bandpass: bool,\n",
    "                        do_car: bool, bp_lo: float, bp_hi: float):\n",
    "    edfs = list_subject_imagery_edfs(subject_id)\n",
    "    if len(edfs) == 0:\n",
    "        return np.empty((0,8,1), dtype=np.float32), np.empty((0,), dtype=int), None\n",
    "\n",
    "    X_list, y_list, sfreq_list = [], [], []\n",
    "\n",
    "    for edf_path in edfs:\n",
    "        m = re.search(r\"R(\\d{2})\", Path(edf_path).name)\n",
    "        run = int(m.group(1)) if m else -1\n",
    "\n",
    "        # Usar SOLO runs L/R\n",
    "        if run not in IMAGERY_RUNS_LR:\n",
    "            continue\n",
    "\n",
    "        raw = mne.io.read_raw_edf(edf_path, preload=True, verbose='ERROR')\n",
    "        raw = pick_8_channels(raw)\n",
    "\n",
    "        if do_notch:\n",
    "            raw.notch_filter(freqs=[60.0], picks='all', verbose='ERROR')\n",
    "        if do_bandpass:\n",
    "            raw.filter(l_freq=bp_lo, h_freq=bp_hi, picks='all', verbose='ERROR')\n",
    "        if do_car:\n",
    "            raw.set_eeg_reference('average', projection=False, verbose='ERROR')\n",
    "\n",
    "        if resample_hz is not None and resample_hz > 0:\n",
    "            raw.resample(resample_hz)\n",
    "        sfreq = raw.info['sfreq']\n",
    "\n",
    "        events, event_id = mne.events_from_annotations(raw, verbose='ERROR')\n",
    "        # T1 = left, T2 = right\n",
    "        keep = {k: v for k, v in event_id.items() if k in {'T1', 'T2'}}\n",
    "        if len(keep) == 0:\n",
    "            continue\n",
    "\n",
    "        epochs = mne.Epochs(raw, events=events, event_id=keep, tmin=TMIN, tmax=TMAX,\n",
    "                            baseline=None, preload=True, verbose='ERROR')\n",
    "        X = epochs.get_data()\n",
    "\n",
    "        if ZSCORE_PER_EPOCH:\n",
    "            X = X.astype(np.float32)\n",
    "            eps = 1e-6\n",
    "            mu = X.mean(axis=2, keepdims=True)\n",
    "            sd = X.std(axis=2, keepdims=True) + eps\n",
    "            X = (X - mu) / sd\n",
    "\n",
    "        ev_codes = epochs.events[:, 2]\n",
    "        inv = {v: k for k, v in keep.items()}\n",
    "        y_run = []\n",
    "        for code in ev_codes:\n",
    "            lab = inv[code]\n",
    "            y_run.append(0 if lab == 'T1' else 1)  # 0=left, 1=right\n",
    "        y_run = np.array(y_run, dtype=int)\n",
    "\n",
    "        X_list.append(X)\n",
    "        y_list.append(y_run)\n",
    "        sfreq_list.append(sfreq)\n",
    "\n",
    "    if len(X_list) == 0:\n",
    "        return np.empty((0,8,1), dtype=np.float32), np.empty((0,), dtype=int), None\n",
    "\n",
    "    X_all = np.concatenate(X_list, axis=0).astype(np.float32)\n",
    "    y_all = np.concatenate(y_list, axis=0).astype(int)\n",
    "\n",
    "    if len(set([int(round(s)) for s in sfreq_list])) != 1:\n",
    "        raise RuntimeError(f\"Sampling rates inconsistentes: {sfreq_list}\")\n",
    "\n",
    "    return X_all, y_all, sfreq_list[0]\n",
    "\n",
    "def load_fold_subjects(folds_json: Path, fold: int):\n",
    "    with open(folds_json, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    for item in data.get('folds', []):\n",
    "        if int(item.get('fold', -1)) == int(fold):\n",
    "            return list(item.get('train', [])), list(item.get('test', []))\n",
    "    raise ValueError(f\"Fold {fold} not found in {folds_json}\")\n",
    "\n",
    "def standardize_per_channel(train_X, other_X):\n",
    "    C = train_X.shape[1]\n",
    "    train_X = train_X.astype(np.float32)\n",
    "    other_X = other_X.astype(np.float32)\n",
    "    for c in range(C):\n",
    "        mu = train_X[:, c, :].mean()\n",
    "        sd = train_X[:, c, :].std()\n",
    "        sd = sd if sd > 1e-6 else 1.0\n",
    "        train_X[:, c, :] = (train_X[:, c, :] - mu) / sd\n",
    "        other_X[:, c, :] = (other_X[:, c, :] - mu) / sd\n",
    "    return train_X, other_X\n",
    "\n",
    "# =========================\n",
    "# MODELO (GroupNorm en conv)\n",
    "# =========================\n",
    "def make_gn(num_channels, num_groups=8):\n",
    "    g = min(num_groups, num_channels)\n",
    "    while num_channels % g != 0 and g > 1:\n",
    "        g -= 1\n",
    "    return nn.GroupNorm(g, num_channels)\n",
    "\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k, s=1, p=0, p_drop=0.2):\n",
    "        super().__init__()\n",
    "        self.dw = nn.Conv1d(in_ch, in_ch, kernel_size=k, stride=s, padding=p, groups=in_ch, bias=False)\n",
    "        self.pw = nn.Conv1d(in_ch, out_ch, kernel_size=1, bias=False)\n",
    "        self.norm = make_gn(out_ch)\n",
    "        self.act = nn.ELU()\n",
    "        self.dropout = nn.Dropout(p=p_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dw(x); x = self.pw(x); x = self.norm(x)\n",
    "        x = self.act(x); x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class EEGCNNTransformer(nn.Module):\n",
    "    def __init__(self, n_ch=8, n_cls=2, d_model=128, n_heads=4, n_layers=2,\n",
    "                 p_drop=0.2, p_drop_encoder=0.3):\n",
    "        super().__init__()\n",
    "        self.conv_t = nn.Sequential(\n",
    "            nn.Conv1d(n_ch, 32, kernel_size=129, stride=2, padding=64, bias=False),\n",
    "            make_gn(32),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=p_drop),\n",
    "            DepthwiseSeparableConv(32, 64, k=31, s=2, p=15, p_drop=p_drop),\n",
    "            DepthwiseSeparableConv(64, 128, k=15, s=2, p=7,  p_drop=p_drop),\n",
    "        )\n",
    "        self.proj = nn.Conv1d(128, d_model, kernel_size=1, bias=False)\n",
    "        self.dropout = nn.Dropout(p=p_drop_encoder)\n",
    "        self.pos_encoding = None\n",
    "        enc = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=2*d_model,\n",
    "            batch_first=True, activation='gelu', dropout=0.1, norm_first=False\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc, num_layers=n_layers)\n",
    "        self.cls = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        nn.init.normal_(self.cls, std=0.02)\n",
    "        self.head = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, n_cls))\n",
    "\n",
    "    def _positional_encoding(self, L, d):\n",
    "        pos = torch.arange(0, L, dtype=torch.float32).unsqueeze(1)\n",
    "        i   = torch.arange(0, d, dtype=torch.float32).unsqueeze(0)\n",
    "        angle = pos / torch.pow(10000, (2 * (i//2)) / d)\n",
    "        pe = torch.zeros(L, d, dtype=torch.float32)\n",
    "        pe[:, 0::2] = torch.sin(angle[:, 0::2])\n",
    "        pe[:, 1::2] = torch.cos(angle[:, 1::2])\n",
    "        return pe\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.conv_t(x)           # (B, 128, T')\n",
    "        z = self.proj(z)             # (B, d_model, T')\n",
    "        z = self.dropout(z)\n",
    "        z = z.transpose(1, 2)        # (B, T', d_model)\n",
    "        B, L, D = z.shape\n",
    "        if (self.pos_encoding is None) or (self.pos_encoding.shape[0] != L) or (self.pos_encoding.shape[1] != D):\n",
    "            self.pos_encoding = self._positional_encoding(L, D).to(z.device)\n",
    "        z = z + self.pos_encoding[None, :, :]\n",
    "        cls_tok = self.cls.expand(B, -1, -1)\n",
    "        z = torch.cat([cls_tok, z], dim=1)\n",
    "        z = self.encoder(z)\n",
    "        cls = z[:, 0, :]\n",
    "        return self.head(cls)\n",
    "\n",
    "# =========================\n",
    "# FOCAL LOSS (ajustada a 2 clases)\n",
    "# =========================\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha: torch.Tensor, gamma: float = 1.5, reduction: str = 'mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha / alpha.sum()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        logp = nn.functional.log_softmax(logits, dim=-1)      # (B,C)\n",
    "        p = logp.exp()\n",
    "        idx = torch.arange(target.shape[0], device=logits.device)\n",
    "        pt = p[idx, target]\n",
    "        logpt = logp[idx, target]\n",
    "        at = self.alpha[target]\n",
    "        loss = - at * ((1 - pt) ** self.gamma) * logpt\n",
    "        if self.reduction == 'mean': return loss.mean()\n",
    "        if self.reduction == 'sum':  return loss.sum()\n",
    "        return loss\n",
    "\n",
    "# =========================\n",
    "# AUGMENTS\n",
    "# =========================\n",
    "def augment_batch(\n",
    "    xb,\n",
    "    p_jitter=0.35, p_noise=0.35, p_chdrop=0.15,\n",
    "    max_jitter_frac=0.03, noise_std=0.03, max_chdrop=1\n",
    "):\n",
    "    B, C, T = xb.shape\n",
    "    if np.random.rand() < p_jitter:\n",
    "        max_shift = int(max(1, T*max_jitter_frac))\n",
    "        shifts = torch.randint(low=-max_shift, high=max_shift+1, size=(B,), device=xb.device)\n",
    "        for i in range(B):\n",
    "            xb[i] = torch.roll(xb[i], shifts=int(shifts[i].item()), dims=-1)\n",
    "    if np.random.rand() < p_noise:\n",
    "        xb = xb + noise_std*torch.randn_like(xb)\n",
    "    if np.random.rand() < p_chdrop and max_chdrop > 0:\n",
    "        k = min(max_chdrop, C)\n",
    "        for i in range(B):\n",
    "            idx = torch.randperm(C, device=xb.device)[:k]\n",
    "            xb[i, idx, :] = 0.0\n",
    "    return xb\n",
    "\n",
    "# =========================\n",
    "# EMA de pesos\n",
    "# =========================\n",
    "class ModelEMA:\n",
    "    def __init__(self, model: nn.Module, decay: float = 0.9995, device=None):\n",
    "        self.ema = self._clone(model).to(device if device is not None else next(model.parameters()).device)\n",
    "        self.decay = decay\n",
    "        self._updates = 0\n",
    "        self.update(model, force=True)\n",
    "\n",
    "    def _clone(self, model):\n",
    "        ema = type(model)()\n",
    "        ema.load_state_dict(model.state_dict())\n",
    "        for p in ema.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        return ema\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model: nn.Module, force: bool = False):\n",
    "        d = self.decay\n",
    "        if self._updates < 1000:\n",
    "            d = (self._updates / 1000.0) * self.decay\n",
    "        msd = model.state_dict()\n",
    "        esd = self.ema.state_dict()\n",
    "        for k in esd.keys():\n",
    "            if esd[k].dtype.is_floating_point:\n",
    "                esd[k].mul_(d).add_(msd[k].detach(), alpha=1.0 - d)\n",
    "            else:\n",
    "                esd[k] = msd[k]\n",
    "        self._updates += 1\n",
    "\n",
    "# =========================\n",
    "# INFERENCIA TTA / SUBWINDOW\n",
    "# =========================\n",
    "def subwindow_logits(model, X, sfreq, sw_len, sw_stride, device):\n",
    "    model.eval()\n",
    "    wl = int(round(sw_len * sfreq))\n",
    "    st = int(round(sw_stride * sfreq))\n",
    "    wl = max(1, min(wl, X.shape[-1])); st = max(1, st)\n",
    "    out = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(X.shape[0]):\n",
    "            x = X[i]; acc = []\n",
    "            for s in range(0, max(1, X.shape[-1]-wl+1), st):\n",
    "                seg = x[:, s:s+wl]\n",
    "                if seg.shape[-1] < wl:\n",
    "                    pad = wl - seg.shape[-1]\n",
    "                    seg = np.pad(seg, ((0,0),(0,pad)), mode='edge')\n",
    "                xb = torch.tensor(seg[None, ...], dtype=torch.float32, device=device)\n",
    "                logit = model(xb).detach().cpu().numpy()[0]\n",
    "                acc.append(logit)\n",
    "            acc = np.mean(np.stack(acc, axis=0), axis=0) if len(acc) else np.zeros(2, dtype=np.float32)\n",
    "            out.append(acc)\n",
    "    return np.stack(out, axis=0)\n",
    "\n",
    "def time_shift_tta_logits(model, X, sfreq, shifts_s, device):\n",
    "    model.eval()\n",
    "    T = X.shape[-1]; out = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(X.shape[0]):\n",
    "            x0 = X[i]; acc = []\n",
    "            for sh in shifts_s:\n",
    "                shift = int(round(sh * sfreq))\n",
    "                if shift == 0:\n",
    "                    x = x0\n",
    "                elif shift > 0:\n",
    "                    x = np.pad(x0[:, shift:], ((0,0),(0,shift)), mode='edge')[:, :T]\n",
    "                else:\n",
    "                    shift = -shift\n",
    "                    x = np.pad(x0[:, :-shift], ((0,0),(shift,0)), mode='edge')[:, :T]\n",
    "                xb = torch.tensor(x[None, ...], dtype=torch.float32, device=device)\n",
    "                logit = model(xb).detach().cpu().numpy()[0]\n",
    "                acc.append(logit)\n",
    "            out.append(np.mean(np.stack(acc, axis=0), axis=0))\n",
    "    return np.stack(out, axis=0)\n",
    "\n",
    "# =========================\n",
    "# Utilidades splits estratificados por sujeto\n",
    "# =========================\n",
    "def build_subject_label_map(subject_ids):\n",
    "    y_dom_list = []\n",
    "    for sid in subject_ids:\n",
    "        Xs, ys, _ = load_subject_epochs(sid, RESAMPLE_HZ, DO_NOTCH, DO_BANDPASS, DO_CAR, BP_LO, BP_HI)\n",
    "        if len(ys) == 0:\n",
    "            y_dom_list.append(-1)\n",
    "            continue\n",
    "        binc = np.bincount(ys, minlength=2)\n",
    "        y_dom = int(np.argmax(binc))\n",
    "        y_dom_list.append(y_dom)\n",
    "    return np.array(y_dom_list, dtype=int)\n",
    "\n",
    "# =========================\n",
    "# TRAIN/EVAL por FOLD (con train/val/test)\n",
    "# =========================\n",
    "def train_one_fold(fold:int, device):\n",
    "    def load_fold_subjects_local(folds_json: Path, fold: int):\n",
    "        return load_fold_subjects(folds_json, fold)\n",
    "\n",
    "    train_sub, test_sub = load_fold_subjects_local(FOLDS_JSON, fold)\n",
    "    train_sub = [s for s in train_sub if subject_id_to_int(s) not in EXCLUDE_SUBJECTS]\n",
    "    test_sub  = [s for s in test_sub  if subject_id_to_int(s) not in EXCLUDE_SUBJECTS]\n",
    "\n",
    "    # Split de validación por sujetos (determinista y opcionalmente estratificado)\n",
    "    rng = np.random.RandomState(RANDOM_STATE + fold)\n",
    "    tr_subjects = sorted(train_sub)\n",
    "\n",
    "    if VAL_STRAT_SUBJECT and len(tr_subjects) > 1:\n",
    "        y_dom = build_subject_label_map(tr_subjects)\n",
    "        if np.any(y_dom < 0):\n",
    "            mask = y_dom >= 0\n",
    "            moda = int(np.bincount(y_dom[mask]).argmax()) if mask.sum() > 0 else 0\n",
    "            y_dom[~mask] = moda\n",
    "        n_val_subj = max(1, int(round(len(tr_subjects) * VAL_SUBJECT_FRAC)))\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=n_val_subj, random_state=RANDOM_STATE + fold)\n",
    "        idx = np.arange(len(tr_subjects))\n",
    "        _, val_idx = next(sss.split(idx, y_dom))\n",
    "        val_subjects = sorted([tr_subjects[i] for i in val_idx])\n",
    "        train_subjects = [s for s in tr_subjects if s not in val_subjects]\n",
    "    else:\n",
    "        tr_subjects_shuf = tr_subjects.copy()\n",
    "        rng.shuffle(tr_subjects_shuf)\n",
    "        n_val_subj = max(1, int(round(len(tr_subjects_shuf) * VAL_SUBJECT_FRAC)))\n",
    "        val_subjects = sorted(tr_subjects_shuf[:n_val_subj])\n",
    "        train_subjects = sorted(tr_subjects_shuf[n_val_subj:])\n",
    "\n",
    "    # Carga TRAIN/VAL/TEST\n",
    "    X_tr_list, y_tr_list, sub_tr_list = [], [], []\n",
    "    X_val_list, y_val_list, sub_val_list = [], [], []\n",
    "    X_te_list, y_te_list, sub_te_list = [], [], []\n",
    "    sfreq = None\n",
    "\n",
    "    for sid in tqdm(train_subjects, desc=f\"Cargando train fold{fold}\"):\n",
    "        Xs, ys, sf = load_subject_epochs(sid, RESAMPLE_HZ, DO_NOTCH, DO_BANDPASS, DO_CAR, BP_LO, BP_HI)\n",
    "        if len(ys) == 0: continue\n",
    "        X_tr_list.append(Xs); y_tr_list.append(ys)\n",
    "        sub_tr_list.append(np.full_like(ys, fill_value=subject_id_to_int(sid)))\n",
    "        sfreq = sf if sfreq is None else sfreq\n",
    "\n",
    "    for sid in tqdm(val_subjects, desc=f\"Cargando val fold{fold}\"):\n",
    "        Xs, ys, sf = load_subject_epochs(sid, RESAMPLE_HZ, DO_NOTCH, DO_BANDPASS, DO_CAR, BP_LO, BP_HI)\n",
    "        if len(ys) == 0: continue\n",
    "        X_val_list.append(Xs); y_val_list.append(ys)\n",
    "        sub_val_list.append(np.full_like(ys, fill_value=subject_id_to_int(sid)))\n",
    "        sfreq = sf if sfreq is None else sfreq\n",
    "\n",
    "    for sid in tqdm(test_sub, desc=f\"Cargando test fold{fold}\"):\n",
    "        Xs, ys, sf = load_subject_epochs(sid, RESAMPLE_HZ, DO_NOTCH, DO_BANDPASS, DO_CAR, BP_LO, BP_HI)\n",
    "        if len(ys) == 0: continue\n",
    "        X_te_list.append(Xs); y_te_list.append(ys)\n",
    "        sub_te_list.append(np.full_like(ys, fill_value=subject_id_to_int(sid)))\n",
    "        sfreq = sf if sfreq is None else sfreq\n",
    "\n",
    "    # Concatenar\n",
    "    X_tr = np.concatenate(X_tr_list, axis=0); y_tr = np.concatenate(y_tr_list, axis=0)\n",
    "    sub_tr = np.concatenate(sub_tr_list, axis=0)\n",
    "    X_val = np.concatenate(X_val_list, axis=0); y_val = np.concatenate(y_val_list, axis=0)\n",
    "    sub_val = np.concatenate(sub_val_list, axis=0)\n",
    "    X_te = np.concatenate(X_te_list, axis=0); y_te = np.concatenate(y_te_list, axis=0)\n",
    "    sub_te = np.concatenate(sub_te_list, axis=0)\n",
    "\n",
    "    print(f\"[Fold {fold}/5] Entrenando modelo global... (n_train={len(y_tr)} | n_val={len(y_val)} | n_test={len(y_te)})\")\n",
    "\n",
    "    # Normalización por canal (fit en TRAIN y aplicar a VAL/TEST)\n",
    "    if ZSCORE_PER_EPOCH:\n",
    "        X_tr_std, X_val_std, X_te_std = X_tr, X_val, X_te\n",
    "    else:\n",
    "        X_tr_std, X_val_std = standardize_per_channel(X_tr, X_val)\n",
    "        _,        X_te_std  = standardize_per_channel(X_tr, X_te)\n",
    "\n",
    "    # Datasets\n",
    "    tr_ds  = TensorDataset(torch.tensor(X_tr_std),  torch.tensor(y_tr).long(),  torch.tensor(sub_tr).long())\n",
    "    val_ds = TensorDataset(torch.tensor(X_val_std), torch.tensor(y_val).long(), torch.tensor(sub_val).long())\n",
    "    te_ds  = TensorDataset(torch.tensor(X_te_std),  torch.tensor(y_te).long(),  torch.tensor(sub_te).long())\n",
    "\n",
    "    # Weighted sampler templado (por sujeto/label)\n",
    "    def make_weighted_sampler(dataset: TensorDataset):\n",
    "        _Xb, yb, sb = dataset.tensors\n",
    "        yb_np = yb.numpy()\n",
    "        sb_np = sb.numpy()\n",
    "        uniq_s, cnt_s = np.unique(sb_np, return_counts=True)\n",
    "        map_s = {s:c for s,c in zip(uniq_s, cnt_s)}\n",
    "        key = sb_np.astype(np.int64) * 10 + yb_np.astype(np.int64)\n",
    "        uniq_k, cnt_k = np.unique(key, return_counts=True)\n",
    "        map_k = {k:c for k,c in zip(uniq_k, cnt_k)}\n",
    "        a, b = 0.8, 1.0\n",
    "        w = []\n",
    "        for s, y in zip(sb_np, yb_np):\n",
    "            k = int(s)*10 + int(y)\n",
    "            ws = float(map_s[int(s)])\n",
    "            wk = float(map_k[k])\n",
    "            w.append((ws ** (-a)) * (wk ** (-b)))\n",
    "        w = np.array(w, dtype=np.float64)\n",
    "        w = w / (w.mean() + 1e-12)\n",
    "        sampler = WeightedRandomSampler(weights=torch.tensor(w, dtype=torch.double),\n",
    "                                        num_samples=len(yb_np), replacement=True)\n",
    "        return sampler\n",
    "\n",
    "    if USE_WEIGHTED_SAMPLER:\n",
    "        tr_sampler = make_weighted_sampler(tr_ds)\n",
    "        tr_ld  = DataLoader(tr_ds, batch_size=BATCH_SIZE, sampler=tr_sampler, drop_last=False, worker_init_fn=seed_worker)\n",
    "    else:\n",
    "        tr_ld  = DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True,  drop_last=False, worker_init_fn=seed_worker)\n",
    "\n",
    "    val_ld = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, worker_init_fn=seed_worker)\n",
    "    te_ld  = DataLoader(te_ds,  batch_size=BATCH_SIZE, shuffle=False, drop_last=False, worker_init_fn=seed_worker)\n",
    "\n",
    "    # Modelo (n_cls=2)\n",
    "    model = EEGCNNTransformer(n_ch=8, n_cls=2, d_model=D_MODEL, n_heads=N_HEADS,\n",
    "                              n_layers=N_LAYERS, p_drop=P_DROP, p_drop_encoder=P_DROP_ENCODER).to(device)\n",
    "\n",
    "    # Optimizador + Focal Loss (alpha para 2 clases)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=1e-2)\n",
    "\n",
    "    class_counts = np.bincount(y_tr, minlength=2).astype(np.float32)\n",
    "    inv = class_counts.sum() / (2.0 * np.maximum(class_counts, 1.0))\n",
    "    alpha = torch.tensor(inv, dtype=torch.float32, device=device)\n",
    "    crit = FocalLoss(alpha=alpha, gamma=1.5, reduction='mean')\n",
    "\n",
    "    # Warmup+Cosine (min_factor=0.1)\n",
    "    from torch.optim.lr_scheduler import LambdaLR\n",
    "    total_epochs = EPOCHS\n",
    "    warmup_epochs = max(1, int(WARMUP_EPOCHS))\n",
    "    min_factor = 0.1\n",
    "\n",
    "    def lr_lambda(current_epoch):\n",
    "        if current_epoch < warmup_epochs:\n",
    "            return (current_epoch + 1) / warmup_epochs\n",
    "        progress = (current_epoch - warmup_epochs) / max(1, (total_epochs - warmup_epochs))\n",
    "        progress = min(1.0, max(0.0, progress))\n",
    "        return min_factor + 0.5 * (1.0 - min_factor) * (1.0 + np.cos(np.pi * progress))\n",
    "\n",
    "    scheduler = LambdaLR(opt, lr_lambda=lr_lambda)\n",
    "\n",
    "    # EMA\n",
    "    if USE_EMA:\n",
    "        ema = ModelEMA(model, decay=EMA_DECAY, device=device)\n",
    "    else:\n",
    "        ema = None\n",
    "\n",
    "    # Entrenamiento con early stopping por F1 macro (EMA en val/test si aplica)\n",
    "    best_f1, best_state, wait = 0.0, None, 0\n",
    "    hist = {\"ep\": [], \"tr_loss\": [], \"tr_acc\": [], \"val_acc\": [], \"val_f1m\": [], \"lr\": []}\n",
    "\n",
    "    def evaluate_on(loader, use_ema=True):\n",
    "        mdl = ema.ema if (ema is not None and use_ema) else model\n",
    "        mdl.eval()\n",
    "        preds, gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _sb in loader:\n",
    "                xb = xb.to(device)\n",
    "                p = mdl(xb).argmax(dim=1).cpu().numpy()\n",
    "                preds.append(p); gts.append(yb.numpy())\n",
    "        preds = np.concatenate(preds); gts = np.concatenate(gts)\n",
    "        acc = accuracy_score(gts, preds)\n",
    "        f1m = f1_score(gts, preds, average='macro')\n",
    "        return acc, f1m\n",
    "\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        # ---- Train ----\n",
    "        model.train()\n",
    "        tr_loss, n_seen, tr_correct = 0.0, 0, 0\n",
    "        for xb, yb, _sb in tr_ld:\n",
    "            xb = xb.to(device); yb = yb.to(device)\n",
    "            xb = augment_batch(xb)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            opt.step()\n",
    "            if ema is not None:\n",
    "                ema.update(model)\n",
    "\n",
    "            tr_loss += loss.item() * len(yb)\n",
    "            n_seen += len(yb)\n",
    "            tr_correct += (logits.argmax(1) == yb).sum().item()\n",
    "        tr_loss /= max(1, n_seen)\n",
    "        tr_acc = tr_correct / max(1, n_seen)\n",
    "\n",
    "        # ---- Val (EMA si aplica) ----\n",
    "        acc, f1m = evaluate_on(val_ld, use_ema=True)\n",
    "\n",
    "        hist[\"ep\"].append(ep)\n",
    "        hist[\"tr_loss\"].append(tr_loss)\n",
    "        hist[\"tr_acc\"].append(tr_acc)\n",
    "        hist[\"val_acc\"].append(acc)\n",
    "        hist[\"val_f1m\"].append(f1m)\n",
    "        hist[\"lr\"].append(scheduler.get_last_lr()[0])\n",
    "\n",
    "        print(f\"  Época {ep:3d} | train_loss={tr_loss:.4f} | train_acc={tr_acc:.4f} | val_acc={acc:.4f} | val_f1m={f1m:.4f} | LR={scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        improved = f1m > best_f1 + 1e-4\n",
    "        if improved:\n",
    "            best_f1 = f1m\n",
    "            ref_model = ema.ema if ema is not None else model\n",
    "            best_state = {k: v.detach().cpu() for k, v in ref_model.state_dict().items()}\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "\n",
    "        scheduler.step()\n",
    "        if wait >= PATIENCE:\n",
    "            print(f\"  Early stopping en época {ep} (mejor val_f1m={best_f1:.4f})\")\n",
    "            break\n",
    "\n",
    "    # Cargamos mejor estado guardado (EMA si estaba activo)\n",
    "    if best_state is not None:\n",
    "        (ema.ema if (ema is not None) else model).load_state_dict(best_state)\n",
    "\n",
    "    # ---- Guardar curva ----\n",
    "    fig = plt.figure(figsize=(8,4.5))\n",
    "    ax1 = plt.gca()\n",
    "    ax1.plot(hist[\"ep\"], hist[\"tr_loss\"], label=\"train_loss\")\n",
    "    ax1.plot(hist[\"ep\"], hist[\"tr_acc\"], label=\"tr_acc\")\n",
    "    ax1.plot(hist[\"ep\"], hist[\"val_acc\"], label=\"val_acc\")\n",
    "    ax1.set_xlabel(\"Época\"); ax1.set_title(f\"Fold {fold} — Curva de entrenamiento (2 clases)\")\n",
    "    ax1.legend(); ax1.grid(True, alpha=0.3)\n",
    "    out_png = f\"training_curve_fold{fold}.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=140)\n",
    "    plt.close(fig)\n",
    "    print(f\"↳ Curva de entrenamiento guardada: {out_png}\")\n",
    "\n",
    "    # ---- Evaluación final en TEST ----\n",
    "    eval_model = ema.ema if (ema is not None) else model\n",
    "    eval_model.eval()\n",
    "\n",
    "    sfreq_used = RESAMPLE_HZ\n",
    "    if sfreq_used is None:\n",
    "        sfreq_used = int(round(X_te_std.shape[-1] / (TMAX - TMIN)))\n",
    "\n",
    "    if (not SW_ENABLE) or SW_MODE == 'none':\n",
    "        preds, gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _sb in te_ld:\n",
    "                xb = xb.to(device)\n",
    "                p = eval_model(xb).argmax(dim=1).cpu().numpy()\n",
    "                preds.append(p); gts.append(yb.numpy())\n",
    "        preds = np.concatenate(preds); gts = np.concatenate(gts)\n",
    "    elif SW_MODE in ('subwin', 'tta'):\n",
    "        logits_tta = None\n",
    "        logits_sw  = None\n",
    "        if SW_MODE == 'subwin':\n",
    "            logits_sw = subwindow_logits(eval_model, X_te_std, sfreq_used, SW_LEN, SW_STRIDE, device)\n",
    "        elif SW_MODE == 'tta':\n",
    "            logits_tta = time_shift_tta_logits(eval_model, X_te_std, sfreq_used, TTA_SHIFTS_S, device)\n",
    "\n",
    "        if COMBINE_TTA_AND_SUBWIN:\n",
    "            if logits_tta is None:\n",
    "                logits_tta = time_shift_tta_logits(eval_model, X_te_std, sfreq_used, TTA_SHIFTS_S, device)\n",
    "            if logits_sw is None:\n",
    "                logits_sw  = subwindow_logits(eval_model, X_te_std, sfreq_used, SW_LEN, SW_STRIDE, device)\n",
    "            logits = 0.5 * logits_tta + 0.5 * logits_sw\n",
    "        else:\n",
    "            logits = logits_tta if logits_tta is not None else logits_sw\n",
    "\n",
    "        preds = logits.argmax(axis=1); gts = y_te\n",
    "    else:\n",
    "        raise ValueError(f\"SW_MODE desconocido: {SW_MODE}\")\n",
    "\n",
    "    acc = accuracy_score(gts, preds)\n",
    "    f1m = f1_score(gts, preds, average='macro')\n",
    "    print(f\"[Fold {fold}/5] Global acc={acc:.4f}\\n\")\n",
    "    print(classification_report(gts, preds, target_names=[c.replace('_',' ') for c in CLASS_NAMES], digits=4))\n",
    "    print(\"Confusion matrix (rows=true, cols=pred):\")\n",
    "    print(confusion_matrix(gts, preds, labels=[0,1]))\n",
    "\n",
    "    return acc, f1m\n",
    "\n",
    "# =========================\n",
    "# LOOP 5 FOLDS + RESUMEN\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    acc_folds, f1_folds = [], []\n",
    "    for fold in range(1, 6):\n",
    "        acc, f1m = train_one_fold(fold, DEVICE)\n",
    "        acc_folds.append(f\"{acc:.4f}\")\n",
    "        f1_folds.append(f\"{f1m:.4f}\")\n",
    "\n",
    "    acc_mean = float(np.mean([float(a) for a in acc_folds]))\n",
    "    f1_mean  = float(np.mean([float(f) for f in f1_folds]))\n",
    "\n",
    "    print(\"\\n============================================================\")\n",
    "    print(\"RESULTADOS FINALES (2 clases: left/right)\")\n",
    "    print(\"============================================================\")\n",
    "    print(f\"Global folds (ACC): {acc_folds}\")\n",
    "    print(f\"Global mean ACC: {acc_mean:.4f}\")\n",
    "    print(f\"F1 folds (MACRO): {f1_folds}\")\n",
    "    print(f\"F1 mean (MACRO): {f1_mean:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca8ae58",
   "metadata": {},
   "source": [
    "## DOS CLASES FINETUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f33e25a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Usando dispositivo: cuda\n",
      "🧠 INICIANDO EXPERIMENTO CON CNN+Transformer (K-Fold por sujeto)\n",
      "🔧 Configuración: 2c (L/R), 8 canales, 6s | EPOCHS=60, BATCH=64, LR=0.001 | ZSCORE_PER_EPOCH=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold1: 100%|██████████| 67/67 [00:03<00:00, 16.94it/s]\n",
      "Cargando val fold1: 100%|██████████| 15/15 [00:00<00:00, 17.04it/s]\n",
      "Cargando test fold1: 100%|██████████| 21/21 [00:01<00:00, 16.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1/5] Entrenando modelo global... (n_train=2814 | n_val=630 | n_test=882)\n",
      "  Época   1 | train_loss=0.1543 | train_acc=0.5064 | val_acc=0.5825 | val_f1m=0.5682 | LR=0.000125\n",
      "  Época   2 | train_loss=0.1046 | train_acc=0.6834 | val_acc=0.7063 | val_f1m=0.7060 | LR=0.000250\n",
      "  Época   3 | train_loss=0.0808 | train_acc=0.7893 | val_acc=0.7429 | val_f1m=0.7428 | LR=0.000375\n",
      "  Época   4 | train_loss=0.0776 | train_acc=0.8149 | val_acc=0.7587 | val_f1m=0.7587 | LR=0.000500\n",
      "  Época   5 | train_loss=0.0701 | train_acc=0.8262 | val_acc=0.7333 | val_f1m=0.7309 | LR=0.000625\n",
      "  Época   6 | train_loss=0.0698 | train_acc=0.8344 | val_acc=0.7730 | val_f1m=0.7729 | LR=0.000750\n",
      "  Época   7 | train_loss=0.0678 | train_acc=0.8312 | val_acc=0.7619 | val_f1m=0.7619 | LR=0.000875\n",
      "  Época   8 | train_loss=0.0652 | train_acc=0.8426 | val_acc=0.7190 | val_f1m=0.7131 | LR=0.001000\n",
      "  Época   9 | train_loss=0.0688 | train_acc=0.8323 | val_acc=0.7651 | val_f1m=0.7650 | LR=0.001000\n",
      "  Época  10 | train_loss=0.0730 | train_acc=0.8127 | val_acc=0.7476 | val_f1m=0.7458 | LR=0.000999\n",
      "  Época  11 | train_loss=0.0589 | train_acc=0.8568 | val_acc=0.7619 | val_f1m=0.7608 | LR=0.000997\n",
      "  Época  12 | train_loss=0.0606 | train_acc=0.8547 | val_acc=0.7667 | val_f1m=0.7665 | LR=0.000993\n",
      "  Época  13 | train_loss=0.0575 | train_acc=0.8621 | val_acc=0.7683 | val_f1m=0.7681 | LR=0.000987\n",
      "  Época  14 | train_loss=0.0612 | train_acc=0.8515 | val_acc=0.7778 | val_f1m=0.7776 | LR=0.000980\n",
      "  Época  15 | train_loss=0.0570 | train_acc=0.8685 | val_acc=0.7651 | val_f1m=0.7651 | LR=0.000971\n",
      "  Época  16 | train_loss=0.0593 | train_acc=0.8518 | val_acc=0.7619 | val_f1m=0.7610 | LR=0.000960\n",
      "  Época  17 | train_loss=0.0573 | train_acc=0.8575 | val_acc=0.7889 | val_f1m=0.7887 | LR=0.000948\n",
      "  Época  18 | train_loss=0.0578 | train_acc=0.8586 | val_acc=0.7667 | val_f1m=0.7665 | LR=0.000935\n",
      "  Época  19 | train_loss=0.0504 | train_acc=0.8753 | val_acc=0.7746 | val_f1m=0.7740 | LR=0.000920\n",
      "  Época  20 | train_loss=0.0571 | train_acc=0.8611 | val_acc=0.7571 | val_f1m=0.7566 | LR=0.000904\n",
      "  Época  21 | train_loss=0.0518 | train_acc=0.8738 | val_acc=0.7810 | val_f1m=0.7809 | LR=0.000887\n",
      "  Época  22 | train_loss=0.0475 | train_acc=0.8881 | val_acc=0.7698 | val_f1m=0.7698 | LR=0.000868\n",
      "  Época  23 | train_loss=0.0470 | train_acc=0.8952 | val_acc=0.7714 | val_f1m=0.7713 | LR=0.000848\n",
      "  Época  24 | train_loss=0.0522 | train_acc=0.8703 | val_acc=0.7714 | val_f1m=0.7713 | LR=0.000828\n",
      "  Época  25 | train_loss=0.0457 | train_acc=0.8959 | val_acc=0.7698 | val_f1m=0.7697 | LR=0.000806\n",
      "  Época  26 | train_loss=0.0437 | train_acc=0.8994 | val_acc=0.7714 | val_f1m=0.7713 | LR=0.000783\n",
      "  Época  27 | train_loss=0.0489 | train_acc=0.8881 | val_acc=0.7683 | val_f1m=0.7682 | LR=0.000759\n",
      "  Época  28 | train_loss=0.0470 | train_acc=0.8856 | val_acc=0.7714 | val_f1m=0.7713 | LR=0.000735\n",
      "  Época  29 | train_loss=0.0465 | train_acc=0.8927 | val_acc=0.7714 | val_f1m=0.7713 | LR=0.000710\n",
      "  Early stopping en época 29 (mejor val_f1m=0.7887)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold1.png\n",
      "[Fold 1/5] Global acc=0.7993\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.7882    0.8186    0.8031       441\n",
      "       right     0.8113    0.7800    0.7954       441\n",
      "\n",
      "    accuracy                         0.7993       882\n",
      "   macro avg     0.7998    0.7993    0.7992       882\n",
      "weighted avg     0.7998    0.7993    0.7992       882\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[361  80]\n",
      " [ 97 344]]\n",
      "↳ Matriz de confusión guardada: confusion_global_fold1.png\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.8537\n",
      "  Δ(FT-Global) = +0.0544\n",
      "Confusion matrix FT (rows=true, cols=pred):\n",
      "[[371  70]\n",
      " [ 59 382]]\n",
      "↳ Matriz de confusión FT guardada: confusion_ft_fold1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold2: 100%|██████████| 67/67 [00:03<00:00, 17.01it/s]\n",
      "Cargando val fold2: 100%|██████████| 15/15 [00:00<00:00, 17.27it/s]\n",
      "Cargando test fold2: 100%|██████████| 21/21 [00:01<00:00, 17.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 2/5] Entrenando modelo global... (n_train=2814 | n_val=630 | n_test=882)\n",
      "  Época   1 | train_loss=0.1262 | train_acc=0.5448 | val_acc=0.6270 | val_f1m=0.6247 | LR=0.000125\n",
      "  Época   2 | train_loss=0.1055 | train_acc=0.6795 | val_acc=0.7429 | val_f1m=0.7423 | LR=0.000250\n",
      "  Época   3 | train_loss=0.0851 | train_acc=0.7758 | val_acc=0.7857 | val_f1m=0.7857 | LR=0.000375\n",
      "  Época   4 | train_loss=0.0827 | train_acc=0.7772 | val_acc=0.7810 | val_f1m=0.7806 | LR=0.000500\n",
      "  Época   5 | train_loss=0.0752 | train_acc=0.8063 | val_acc=0.7698 | val_f1m=0.7698 | LR=0.000625\n",
      "  Época   6 | train_loss=0.0748 | train_acc=0.8109 | val_acc=0.8000 | val_f1m=0.7999 | LR=0.000750\n",
      "  Época   7 | train_loss=0.0756 | train_acc=0.7950 | val_acc=0.7778 | val_f1m=0.7776 | LR=0.000875\n",
      "  Época   8 | train_loss=0.0740 | train_acc=0.8042 | val_acc=0.7714 | val_f1m=0.7713 | LR=0.001000\n",
      "  Época   9 | train_loss=0.0709 | train_acc=0.8227 | val_acc=0.7508 | val_f1m=0.7506 | LR=0.001000\n",
      "  Época  10 | train_loss=0.0717 | train_acc=0.8223 | val_acc=0.7635 | val_f1m=0.7634 | LR=0.000999\n",
      "  Época  11 | train_loss=0.0699 | train_acc=0.8230 | val_acc=0.7667 | val_f1m=0.7665 | LR=0.000997\n",
      "  Época  12 | train_loss=0.0674 | train_acc=0.8365 | val_acc=0.7762 | val_f1m=0.7756 | LR=0.000993\n",
      "  Época  13 | train_loss=0.0708 | train_acc=0.8316 | val_acc=0.7746 | val_f1m=0.7746 | LR=0.000987\n",
      "  Época  14 | train_loss=0.0678 | train_acc=0.8447 | val_acc=0.7778 | val_f1m=0.7776 | LR=0.000980\n",
      "  Época  15 | train_loss=0.0648 | train_acc=0.8287 | val_acc=0.7698 | val_f1m=0.7691 | LR=0.000971\n",
      "  Época  16 | train_loss=0.0697 | train_acc=0.8223 | val_acc=0.7603 | val_f1m=0.7603 | LR=0.000960\n",
      "  Época  17 | train_loss=0.0594 | train_acc=0.8547 | val_acc=0.7952 | val_f1m=0.7950 | LR=0.000948\n",
      "  Época  18 | train_loss=0.0676 | train_acc=0.8312 | val_acc=0.7794 | val_f1m=0.7792 | LR=0.000935\n",
      "  Early stopping en época 18 (mejor val_f1m=0.7999)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold2.png\n",
      "[Fold 2/5] Global acc=0.8390\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.8315    0.8503    0.8408       441\n",
      "       right     0.8469    0.8277    0.8372       441\n",
      "\n",
      "    accuracy                         0.8390       882\n",
      "   macro avg     0.8392    0.8390    0.8390       882\n",
      "weighted avg     0.8392    0.8390    0.8390       882\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[375  66]\n",
      " [ 76 365]]\n",
      "↳ Matriz de confusión guardada: confusion_global_fold2.png\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.9025\n",
      "  Δ(FT-Global) = +0.0635\n",
      "Confusion matrix FT (rows=true, cols=pred):\n",
      "[[399  42]\n",
      " [ 44 397]]\n",
      "↳ Matriz de confusión FT guardada: confusion_ft_fold2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold3: 100%|██████████| 67/67 [00:04<00:00, 16.46it/s]\n",
      "Cargando val fold3: 100%|██████████| 15/15 [00:00<00:00, 16.79it/s]\n",
      "Cargando test fold3: 100%|██████████| 21/21 [00:01<00:00, 16.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 3/5] Entrenando modelo global... (n_train=2814 | n_val=630 | n_test=882)\n",
      "  Época   1 | train_loss=0.1298 | train_acc=0.5156 | val_acc=0.6000 | val_f1m=0.5860 | LR=0.000125\n",
      "  Época   2 | train_loss=0.1162 | train_acc=0.6006 | val_acc=0.7365 | val_f1m=0.7361 | LR=0.000250\n",
      "  Época   3 | train_loss=0.0930 | train_acc=0.7559 | val_acc=0.7619 | val_f1m=0.7614 | LR=0.000375\n",
      "  Época   4 | train_loss=0.0809 | train_acc=0.7999 | val_acc=0.7730 | val_f1m=0.7721 | LR=0.000500\n",
      "  Época   5 | train_loss=0.0843 | train_acc=0.7846 | val_acc=0.7921 | val_f1m=0.7920 | LR=0.000625\n",
      "  Época   6 | train_loss=0.0759 | train_acc=0.8113 | val_acc=0.8063 | val_f1m=0.8062 | LR=0.000750\n",
      "  Época   7 | train_loss=0.0734 | train_acc=0.8120 | val_acc=0.7825 | val_f1m=0.7819 | LR=0.000875\n",
      "  Época   8 | train_loss=0.0696 | train_acc=0.8333 | val_acc=0.8016 | val_f1m=0.8009 | LR=0.001000\n",
      "  Época   9 | train_loss=0.0709 | train_acc=0.8262 | val_acc=0.7905 | val_f1m=0.7903 | LR=0.001000\n",
      "  Época  10 | train_loss=0.0666 | train_acc=0.8412 | val_acc=0.8000 | val_f1m=0.7998 | LR=0.000999\n",
      "  Época  11 | train_loss=0.0697 | train_acc=0.8308 | val_acc=0.7937 | val_f1m=0.7934 | LR=0.000997\n",
      "  Época  12 | train_loss=0.0649 | train_acc=0.8330 | val_acc=0.8048 | val_f1m=0.8048 | LR=0.000993\n",
      "  Época  13 | train_loss=0.0646 | train_acc=0.8454 | val_acc=0.7857 | val_f1m=0.7851 | LR=0.000987\n",
      "  Época  14 | train_loss=0.0631 | train_acc=0.8447 | val_acc=0.8000 | val_f1m=0.7999 | LR=0.000980\n",
      "  Época  15 | train_loss=0.0604 | train_acc=0.8479 | val_acc=0.7921 | val_f1m=0.7919 | LR=0.000971\n",
      "  Época  16 | train_loss=0.0629 | train_acc=0.8451 | val_acc=0.7921 | val_f1m=0.7916 | LR=0.000960\n",
      "  Época  17 | train_loss=0.0578 | train_acc=0.8646 | val_acc=0.7841 | val_f1m=0.7841 | LR=0.000948\n",
      "  Época  18 | train_loss=0.0580 | train_acc=0.8696 | val_acc=0.7937 | val_f1m=0.7935 | LR=0.000935\n",
      "  Early stopping en época 18 (mejor val_f1m=0.8062)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold3.png\n",
      "[Fold 3/5] Global acc=0.7937\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.7797    0.8186    0.7987       441\n",
      "       right     0.8091    0.7687    0.7884       441\n",
      "\n",
      "    accuracy                         0.7937       882\n",
      "   macro avg     0.7944    0.7937    0.7935       882\n",
      "weighted avg     0.7944    0.7937    0.7935       882\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[361  80]\n",
      " [102 339]]\n",
      "↳ Matriz de confusión guardada: confusion_global_fold3.png\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.8515\n",
      "  Δ(FT-Global) = +0.0578\n",
      "Confusion matrix FT (rows=true, cols=pred):\n",
      "[[377  64]\n",
      " [ 67 374]]\n",
      "↳ Matriz de confusión FT guardada: confusion_ft_fold3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold4: 100%|██████████| 68/68 [00:04<00:00, 16.57it/s]\n",
      "Cargando val fold4: 100%|██████████| 15/15 [00:00<00:00, 16.47it/s]\n",
      "Cargando test fold4: 100%|██████████| 20/20 [00:01<00:00, 16.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 4/5] Entrenando modelo global... (n_train=2856 | n_val=630 | n_test=840)\n",
      "  Época   1 | train_loss=0.1266 | train_acc=0.5284 | val_acc=0.5857 | val_f1m=0.5583 | LR=0.000125\n",
      "  Época   2 | train_loss=0.1082 | train_acc=0.6569 | val_acc=0.7063 | val_f1m=0.7043 | LR=0.000250\n",
      "  Época   3 | train_loss=0.0931 | train_acc=0.7486 | val_acc=0.7603 | val_f1m=0.7597 | LR=0.000375\n",
      "  Época   4 | train_loss=0.0826 | train_acc=0.7927 | val_acc=0.7524 | val_f1m=0.7512 | LR=0.000500\n",
      "  Época   5 | train_loss=0.0843 | train_acc=0.7777 | val_acc=0.7825 | val_f1m=0.7825 | LR=0.000625\n",
      "  Época   6 | train_loss=0.0792 | train_acc=0.8029 | val_acc=0.7476 | val_f1m=0.7422 | LR=0.000750\n",
      "  Época   7 | train_loss=0.0748 | train_acc=0.8141 | val_acc=0.7698 | val_f1m=0.7691 | LR=0.000875\n",
      "  Época   8 | train_loss=0.0791 | train_acc=0.7969 | val_acc=0.7857 | val_f1m=0.7857 | LR=0.001000\n",
      "  Época   9 | train_loss=0.0668 | train_acc=0.8309 | val_acc=0.8143 | val_f1m=0.8142 | LR=0.001000\n",
      "  Época  10 | train_loss=0.0726 | train_acc=0.8190 | val_acc=0.7540 | val_f1m=0.7482 | LR=0.000999\n",
      "  Época  11 | train_loss=0.0734 | train_acc=0.8148 | val_acc=0.7794 | val_f1m=0.7783 | LR=0.000997\n",
      "  Época  12 | train_loss=0.0724 | train_acc=0.8169 | val_acc=0.7762 | val_f1m=0.7761 | LR=0.000993\n",
      "  Época  13 | train_loss=0.0655 | train_acc=0.8393 | val_acc=0.8016 | val_f1m=0.8014 | LR=0.000987\n",
      "  Época  14 | train_loss=0.0699 | train_acc=0.8235 | val_acc=0.7841 | val_f1m=0.7840 | LR=0.000980\n",
      "  Época  15 | train_loss=0.0655 | train_acc=0.8361 | val_acc=0.7873 | val_f1m=0.7869 | LR=0.000971\n",
      "  Época  16 | train_loss=0.0669 | train_acc=0.8428 | val_acc=0.7619 | val_f1m=0.7601 | LR=0.000960\n",
      "  Época  17 | train_loss=0.0658 | train_acc=0.8277 | val_acc=0.7746 | val_f1m=0.7746 | LR=0.000948\n",
      "  Época  18 | train_loss=0.0611 | train_acc=0.8557 | val_acc=0.7857 | val_f1m=0.7848 | LR=0.000935\n",
      "  Época  19 | train_loss=0.0619 | train_acc=0.8466 | val_acc=0.7937 | val_f1m=0.7934 | LR=0.000920\n",
      "  Época  20 | train_loss=0.0593 | train_acc=0.8606 | val_acc=0.7905 | val_f1m=0.7901 | LR=0.000904\n",
      "  Época  21 | train_loss=0.0608 | train_acc=0.8557 | val_acc=0.7810 | val_f1m=0.7809 | LR=0.000887\n",
      "  Early stopping en época 21 (mejor val_f1m=0.8142)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold4.png\n",
      "[Fold 4/5] Global acc=0.8214\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.7885    0.8786    0.8311       420\n",
      "       right     0.8629    0.7643    0.8106       420\n",
      "\n",
      "    accuracy                         0.8214       840\n",
      "   macro avg     0.8257    0.8214    0.8208       840\n",
      "weighted avg     0.8257    0.8214    0.8208       840\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[369  51]\n",
      " [ 99 321]]\n",
      "↳ Matriz de confusión guardada: confusion_global_fold4.png\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.8798\n",
      "  Δ(FT-Global) = +0.0583\n",
      "Confusion matrix FT (rows=true, cols=pred):\n",
      "[[368  52]\n",
      " [ 49 371]]\n",
      "↳ Matriz de confusión FT guardada: confusion_ft_fold4.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando train fold5: 100%|██████████| 68/68 [00:04<00:00, 16.52it/s]\n",
      "Cargando val fold5: 100%|██████████| 15/15 [00:00<00:00, 16.80it/s]\n",
      "Cargando test fold5: 100%|██████████| 20/20 [00:01<00:00, 16.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 5/5] Entrenando modelo global... (n_train=2856 | n_val=630 | n_test=840)\n",
      "  Época   1 | train_loss=0.1309 | train_acc=0.5224 | val_acc=0.5635 | val_f1m=0.5252 | LR=0.000125\n",
      "  Época   2 | train_loss=0.1096 | train_acc=0.6551 | val_acc=0.6571 | val_f1m=0.6536 | LR=0.000250\n",
      "  Época   3 | train_loss=0.0879 | train_acc=0.7749 | val_acc=0.7413 | val_f1m=0.7397 | LR=0.000375\n",
      "  Época   4 | train_loss=0.0841 | train_acc=0.7903 | val_acc=0.7444 | val_f1m=0.7422 | LR=0.000500\n",
      "  Época   5 | train_loss=0.0771 | train_acc=0.8064 | val_acc=0.7381 | val_f1m=0.7377 | LR=0.000625\n",
      "  Época   6 | train_loss=0.0722 | train_acc=0.8204 | val_acc=0.7286 | val_f1m=0.7214 | LR=0.000750\n",
      "  Época   7 | train_loss=0.0783 | train_acc=0.8074 | val_acc=0.7413 | val_f1m=0.7406 | LR=0.000875\n",
      "  Época   8 | train_loss=0.0726 | train_acc=0.8204 | val_acc=0.7698 | val_f1m=0.7692 | LR=0.001000\n",
      "  Época   9 | train_loss=0.0682 | train_acc=0.8368 | val_acc=0.7444 | val_f1m=0.7441 | LR=0.001000\n",
      "  Época  10 | train_loss=0.0709 | train_acc=0.8193 | val_acc=0.7587 | val_f1m=0.7585 | LR=0.000999\n",
      "  Época  11 | train_loss=0.0695 | train_acc=0.8288 | val_acc=0.7556 | val_f1m=0.7554 | LR=0.000997\n",
      "  Época  12 | train_loss=0.0669 | train_acc=0.8309 | val_acc=0.7698 | val_f1m=0.7697 | LR=0.000993\n",
      "  Época  13 | train_loss=0.0663 | train_acc=0.8330 | val_acc=0.7571 | val_f1m=0.7571 | LR=0.000987\n",
      "  Época  14 | train_loss=0.0651 | train_acc=0.8386 | val_acc=0.7698 | val_f1m=0.7690 | LR=0.000980\n",
      "  Época  15 | train_loss=0.0621 | train_acc=0.8400 | val_acc=0.7540 | val_f1m=0.7535 | LR=0.000971\n",
      "  Época  16 | train_loss=0.0604 | train_acc=0.8543 | val_acc=0.7603 | val_f1m=0.7601 | LR=0.000960\n",
      "  Época  17 | train_loss=0.0632 | train_acc=0.8477 | val_acc=0.7571 | val_f1m=0.7568 | LR=0.000948\n",
      "  Época  18 | train_loss=0.0614 | train_acc=0.8519 | val_acc=0.7667 | val_f1m=0.7662 | LR=0.000935\n",
      "  Época  19 | train_loss=0.0602 | train_acc=0.8543 | val_acc=0.7873 | val_f1m=0.7871 | LR=0.000920\n",
      "  Época  20 | train_loss=0.0585 | train_acc=0.8592 | val_acc=0.7810 | val_f1m=0.7810 | LR=0.000904\n",
      "  Época  21 | train_loss=0.0556 | train_acc=0.8610 | val_acc=0.7889 | val_f1m=0.7889 | LR=0.000887\n",
      "  Época  22 | train_loss=0.0570 | train_acc=0.8519 | val_acc=0.7778 | val_f1m=0.7777 | LR=0.000868\n",
      "  Época  23 | train_loss=0.0606 | train_acc=0.8480 | val_acc=0.7762 | val_f1m=0.7762 | LR=0.000848\n",
      "  Época  24 | train_loss=0.0566 | train_acc=0.8578 | val_acc=0.7746 | val_f1m=0.7746 | LR=0.000828\n",
      "  Época  25 | train_loss=0.0554 | train_acc=0.8589 | val_acc=0.7778 | val_f1m=0.7777 | LR=0.000806\n",
      "  Época  26 | train_loss=0.0545 | train_acc=0.8641 | val_acc=0.7762 | val_f1m=0.7762 | LR=0.000783\n",
      "  Época  27 | train_loss=0.0560 | train_acc=0.8659 | val_acc=0.7762 | val_f1m=0.7761 | LR=0.000759\n",
      "  Época  28 | train_loss=0.0480 | train_acc=0.8841 | val_acc=0.7794 | val_f1m=0.7793 | LR=0.000735\n",
      "  Época  29 | train_loss=0.0456 | train_acc=0.8918 | val_acc=0.7762 | val_f1m=0.7761 | LR=0.000710\n",
      "  Época  30 | train_loss=0.0478 | train_acc=0.8915 | val_acc=0.7778 | val_f1m=0.7777 | LR=0.000684\n",
      "  Época  31 | train_loss=0.0519 | train_acc=0.8722 | val_acc=0.7810 | val_f1m=0.7809 | LR=0.000658\n",
      "  Época  32 | train_loss=0.0416 | train_acc=0.8999 | val_acc=0.7810 | val_f1m=0.7809 | LR=0.000631\n",
      "  Época  33 | train_loss=0.0427 | train_acc=0.9023 | val_acc=0.7810 | val_f1m=0.7809 | LR=0.000604\n",
      "  Early stopping en época 33 (mejor val_f1m=0.7889)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold5.png\n",
      "[Fold 5/5] Global acc=0.8405\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left     0.8373    0.8452    0.8412       420\n",
      "       right     0.8438    0.8357    0.8397       420\n",
      "\n",
      "    accuracy                         0.8405       840\n",
      "   macro avg     0.8405    0.8405    0.8405       840\n",
      "weighted avg     0.8405    0.8405    0.8405       840\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[355  65]\n",
      " [ 69 351]]\n",
      "↳ Matriz de confusión guardada: confusion_global_fold5.png\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.8679\n",
      "  Δ(FT-Global) = +0.0274\n",
      "Confusion matrix FT (rows=true, cols=pred):\n",
      "[[365  55]\n",
      " [ 56 364]]\n",
      "↳ Matriz de confusión FT guardada: confusion_ft_fold5.png\n",
      "\n",
      "============================================================\n",
      "RESULTADOS FINALES (2 clases: left/right)\n",
      "============================================================\n",
      "Global folds (ACC): ['0.7993', '0.8390', '0.7937', '0.8214', '0.8405']\n",
      "Global mean ACC: 0.8188\n",
      "F1 folds (MACRO): ['0.7992', '0.8390', '0.7935', '0.8208', '0.8405']\n",
      "F1 mean (MACRO): 0.8186\n",
      "Fine-tune PROGRESIVO folds: ['0.8537', '0.9025', '0.8515', '0.8798', '0.8679']\n",
      "Fine-tune PROGRESIVO mean: 0.8711\n",
      "Δ(FT-Global) mean: +0.0523\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "import re, json, random\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import mne\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "\n",
    "# =========================\n",
    "# REPRODUCIBILIDAD\n",
    "# =========================\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def seed_worker(worker_id: int):\n",
    "    worker_seed = RANDOM_STATE + worker_id\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "seed_everything(RANDOM_STATE)\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "FOLDS_JSON = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 64\n",
    "BASE_LR = 1e-3\n",
    "WARMUP_EPOCHS = 8\n",
    "PATIENCE = 12\n",
    "\n",
    "# Split de validación por fold (por sujetos)\n",
    "VAL_SUBJECT_FRAC = 0.18\n",
    "VAL_STRAT_SUBJECT = True\n",
    "\n",
    "# Prepro global\n",
    "RESAMPLE_HZ = None\n",
    "DO_NOTCH = True\n",
    "DO_BANDPASS = False\n",
    "BP_LO, BP_HI = 4.0, 38.0\n",
    "DO_CAR = False\n",
    "ZSCORE_PER_EPOCH = False\n",
    "\n",
    "# Modelo\n",
    "D_MODEL = 128\n",
    "N_HEADS = 4\n",
    "N_LAYERS = 2\n",
    "P_DROP = 0.2\n",
    "P_DROP_ENCODER = 0.3\n",
    "\n",
    "# Ventana temporal\n",
    "TMIN, TMAX = -1.0, 5.0\n",
    "\n",
    "# TTA / SUBWINDOW en TEST\n",
    "SW_MODE = 'tta'   # 'none'|'subwin'|'tta'\n",
    "SW_ENABLE = True\n",
    "TTA_SHIFTS_S = [-0.075, -0.05, -0.025, 0.0, 0.025, 0.05, 0.075]\n",
    "SW_LEN, SW_STRIDE = 4.5, 1.5\n",
    "COMBINE_TTA_AND_SUBWIN = False\n",
    "\n",
    "# Sampler balanceado\n",
    "USE_WEIGHTED_SAMPLER = True\n",
    "\n",
    "# EMA (solo GLOBAL). En FT se desactiva.\n",
    "USE_EMA = True\n",
    "EMA_DECAY = 0.9995\n",
    "\n",
    "# Fine-tuning (por sujeto) — sigue activo, pero ahora con n_cls=2\n",
    "FT_N_FOLDS = 4\n",
    "FT_FREEZE_EPOCHS = 8\n",
    "FT_UNFREEZE_EPOCHS = 8\n",
    "FT_PATIENCE = 6\n",
    "FT_BATCH = 64\n",
    "FT_LR_HEAD = 1e-3\n",
    "FT_LR_BACKBONE = 2e-4\n",
    "FT_WD = 1e-3\n",
    "FT_AUG = dict(p_jitter=0.25, p_noise=0.25, p_chdrop=0.10, max_jitter_frac=0.02, noise_std=0.02, max_chdrop=1)\n",
    "\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','FCz']\n",
    "CLASS_NAMES = ['left', 'right']  # <-- 2 clases\n",
    "\n",
    "# Solo runs de imaginación L/R\n",
    "IMAGERY_RUNS_LR = {4, 8, 12}\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Usando dispositivo: {DEVICE}\")\n",
    "print(\"🧠 INICIANDO EXPERIMENTO CON CNN+Transformer (K-Fold por sujeto)\")\n",
    "print(f\"🔧 Configuración: 2c (L/R), 8 canales, 6s | EPOCHS={EPOCHS}, BATCH={BATCH_SIZE}, LR={BASE_LR} | ZSCORE_PER_EPOCH={ZSCORE_PER_EPOCH}\")\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES I/O\n",
    "# =========================\n",
    "def normalize_ch_name(name: str) -> str:\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', name)\n",
    "    return s.upper()\n",
    "\n",
    "NORMALIZED_TARGETS = [normalize_ch_name(c) for c in EXPECTED_8]\n",
    "\n",
    "def pick_8_channels(raw: mne.io.BaseRaw) -> mne.io.BaseRaw:\n",
    "    chs = raw.info['ch_names']\n",
    "    norm_map = {normalize_ch_name(ch): ch for ch in chs}\n",
    "    picked = []\n",
    "    for target_norm, target_orig in zip(NORMALIZED_TARGETS, EXPECTED_8):\n",
    "        if target_norm in norm_map:\n",
    "            picked.append(norm_map[target_norm])\n",
    "        else:\n",
    "            raise RuntimeError(f\"Canal requerido '{target_orig}' no encontrado. Disponibles: {chs}\")\n",
    "    return raw.pick(picks=picked)\n",
    "\n",
    "def list_subject_imagery_edfs(subject_id: str) -> list:\n",
    "    subj_dir = DATA_RAW / subject_id\n",
    "    edfs = []\n",
    "    for r in [4, 6, 8, 10, 12, 14]:\n",
    "        edfs.extend(glob(str(subj_dir / f\"{subject_id}R{r:02d}.edf\")))\n",
    "    return sorted(edfs)\n",
    "\n",
    "def subject_id_to_int(s: str) -> int:\n",
    "    m = re.match(r'[Ss](\\d+)', s)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "def load_subject_epochs(subject_id: str, resample_hz: int, do_notch: bool, do_bandpass: bool,\n",
    "                        do_car: bool, bp_lo: float, bp_hi: float):\n",
    "    edfs = list_subject_imagery_edfs(subject_id)\n",
    "    if len(edfs) == 0:\n",
    "        return np.empty((0,8,1), dtype=np.float32), np.empty((0,), dtype=int), None\n",
    "\n",
    "    X_list, y_list, sfreq_list = [], [], []\n",
    "    for edf_path in edfs:\n",
    "        m = re.search(r\"R(\\d{2})\", Path(edf_path).name)\n",
    "        run = int(m.group(1)) if m else -1\n",
    "\n",
    "        # Usar SOLO runs L/R\n",
    "        if run not in IMAGERY_RUNS_LR:\n",
    "            continue\n",
    "\n",
    "        raw = mne.io.read_raw_edf(edf_path, preload=True, verbose='ERROR')\n",
    "        raw = pick_8_channels(raw)\n",
    "\n",
    "        if do_notch:\n",
    "            raw.notch_filter(freqs=[60.0], picks='all', verbose='ERROR')\n",
    "        if do_bandpass:\n",
    "            raw.filter(l_freq=bp_lo, h_freq=bp_hi, picks='all', verbose='ERROR')\n",
    "        if do_car:\n",
    "            raw.set_eeg_reference('average', projection=False, verbose='ERROR')\n",
    "        if resample_hz is not None and resample_hz > 0:\n",
    "            raw.resample(resample_hz)\n",
    "\n",
    "        sfreq = raw.info['sfreq']\n",
    "        events, event_id = mne.events_from_annotations(raw, verbose='ERROR')\n",
    "\n",
    "        # Mantener solo T1/T2 -> left/right\n",
    "        keep = {k: v for k, v in event_id.items() if k in {'T1', 'T2'}}\n",
    "        if len(keep) == 0:\n",
    "            continue\n",
    "\n",
    "        epochs = mne.Epochs(raw, events=events, event_id=keep, tmin=TMIN, tmax=TMAX,\n",
    "                            baseline=None, preload=True, verbose='ERROR')\n",
    "        X = epochs.get_data()\n",
    "\n",
    "        if ZSCORE_PER_EPOCH:\n",
    "            X = X.astype(np.float32)\n",
    "            eps = 1e-6\n",
    "            mu = X.mean(axis=2, keepdims=True)\n",
    "            sd = X.std(axis=2, keepdims=True) + eps\n",
    "            X = (X - mu) / sd\n",
    "\n",
    "        ev_codes = epochs.events[:, 2]\n",
    "        inv = {v: k for k, v in keep.items()}\n",
    "        # 0=left(T1), 1=right(T2)\n",
    "        y_run = np.array([0 if inv[c] == 'T1' else 1 for c in ev_codes], dtype=int)\n",
    "\n",
    "        X_list.append(X)\n",
    "        y_list.append(y_run)\n",
    "        sfreq_list.append(sfreq)\n",
    "\n",
    "    if len(X_list) == 0:\n",
    "        return np.empty((0,8,1), dtype=np.float32), np.empty((0,), dtype=int), None\n",
    "\n",
    "    X_all = np.concatenate(X_list, axis=0).astype(np.float32)\n",
    "    y_all = np.concatenate(y_list, axis=0).astype(int)\n",
    "\n",
    "    if len(set([int(round(s)) for s in sfreq_list])) != 1:\n",
    "        raise RuntimeError(f\"Sampling rates inconsistentes: {sfreq_list}\")\n",
    "\n",
    "    return X_all, y_all, sfreq_list[0]\n",
    "\n",
    "def load_fold_subjects(folds_json: Path, fold: int):\n",
    "    with open(folds_json, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    for item in data.get('folds', []):\n",
    "        if int(item.get('fold', -1)) == int(fold):\n",
    "            return list(item.get('train', [])), list(item.get('test', []))\n",
    "    raise ValueError(f\"Fold {fold} not found in {folds_json}\")\n",
    "\n",
    "def standardize_per_channel(train_X, other_X):\n",
    "    C = train_X.shape[1]\n",
    "    train_X = train_X.astype(np.float32)\n",
    "    other_X = other_X.astype(np.float32)\n",
    "    for c in range(C):\n",
    "        mu = train_X[:, c, :].mean()\n",
    "        sd = train_X[:, c, :].std()\n",
    "        sd = sd if sd > 1e-6 else 1.0\n",
    "        train_X[:, c, :] = (train_X[:, c, :] - mu) / sd\n",
    "        other_X[:, c, :] = (other_X[:, c, :] - mu) / sd\n",
    "    return train_X, other_X\n",
    "\n",
    "# =========================\n",
    "# MODELO (GroupNorm en conv)\n",
    "# =========================\n",
    "def make_gn(num_channels, num_groups=8):\n",
    "    g = min(num_groups, num_channels)\n",
    "    while num_channels % g != 0 and g > 1:\n",
    "        g -= 1\n",
    "    return nn.GroupNorm(g, num_channels)\n",
    "\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k, s=1, p=0, p_drop=0.2):\n",
    "        super().__init__()\n",
    "        self.dw = nn.Conv1d(in_ch, in_ch, kernel_size=k, stride=s, padding=p, groups=in_ch, bias=False)\n",
    "        self.pw = nn.Conv1d(in_ch, out_ch, kernel_size=1, bias=False)\n",
    "        self.norm = make_gn(out_ch)\n",
    "        self.act = nn.ELU()\n",
    "        self.dropout = nn.Dropout(p=p_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dw(x); x = self.pw(x); x = self.norm(x)\n",
    "        x = self.act(x); x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class EEGCNNTransformer(nn.Module):\n",
    "    def __init__(self, n_ch=8, n_cls=2, d_model=128, n_heads=4, n_layers=2,\n",
    "                 p_drop=0.2, p_drop_encoder=0.3):\n",
    "        super().__init__()\n",
    "        self.conv_t = nn.Sequential(\n",
    "            nn.Conv1d(n_ch, 32, kernel_size=129, stride=2, padding=64, bias=False),\n",
    "            make_gn(32),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=p_drop),\n",
    "            DepthwiseSeparableConv(32, 64, k=31, s=2, p=15, p_drop=p_drop),\n",
    "            DepthwiseSeparableConv(64, 128, k=15, s=2, p=7,  p_drop=p_drop),\n",
    "        )\n",
    "        self.proj = nn.Conv1d(128, d_model, kernel_size=1, bias=False)\n",
    "        self.dropout = nn.Dropout(p=p_drop_encoder)\n",
    "        self.pos_encoding = None\n",
    "        enc = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=2*d_model,\n",
    "            batch_first=True, activation='gelu', dropout=0.1, norm_first=False\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc, num_layers=n_layers)\n",
    "        self.cls = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        nn.init.normal_(self.cls, std=0.02)\n",
    "        self.head = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, n_cls))\n",
    "\n",
    "    def _positional_encoding(self, L, d):\n",
    "        pos = torch.arange(0, L, dtype=torch.float32).unsqueeze(1)\n",
    "        i   = torch.arange(0, d, dtype=torch.float32).unsqueeze(0)\n",
    "        angle = pos / torch.pow(10000, (2 * (i//2)) / d)\n",
    "        pe = torch.zeros(L, d, dtype=torch.float32)\n",
    "        pe[:, 0::2] = torch.sin(angle[:, 0::2])\n",
    "        pe[:, 1::2] = torch.cos(angle[:, 1::2])\n",
    "        return pe\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.conv_t(x)           # (B, 128, T')\n",
    "        z = self.proj(z)             # (B, d_model, T')\n",
    "        z = self.dropout(z)\n",
    "        z = z.transpose(1, 2)        # (B, T', d_model)\n",
    "        B, L, D = z.shape\n",
    "        if (self.pos_encoding is None) or (self.pos_encoding.shape[0] != L) or (self.pos_encoding.shape[1] != D):\n",
    "            self.pos_encoding = self._positional_encoding(L, D).to(z.device)\n",
    "        z = z + self.pos_encoding[None, :, :]\n",
    "        cls_tok = self.cls.expand(B, -1, -1)\n",
    "        z = torch.cat([cls_tok, z], dim=1)\n",
    "        z = self.encoder(z)\n",
    "        cls = z[:, 0, :]\n",
    "        return self.head(cls)\n",
    "\n",
    "# =========================\n",
    "# FOCAL LOSS (2 clases)\n",
    "# =========================\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha: torch.Tensor, gamma: float = 1.5, reduction: str = 'mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha / alpha.sum()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        logp = nn.functional.log_softmax(logits, dim=-1)      # (B,C)\n",
    "        p = logp.exp()\n",
    "        idx = torch.arange(target.shape[0], device=logits.device)\n",
    "        pt = p[idx, target]\n",
    "        logpt = logp[idx, target]\n",
    "        at = self.alpha[target]\n",
    "        loss = - at * ((1 - pt) ** self.gamma) * logpt\n",
    "        if self.reduction == 'mean': return loss.mean()\n",
    "        if self.reduction == 'sum':  return loss.sum()\n",
    "        return loss\n",
    "\n",
    "# =========================\n",
    "# AUGMENTS\n",
    "# =========================\n",
    "def augment_batch(\n",
    "    xb,\n",
    "    p_jitter=0.35, p_noise=0.35, p_chdrop=0.15,\n",
    "    max_jitter_frac=0.03, noise_std=0.03, max_chdrop=1\n",
    "):\n",
    "    B, C, T = xb.shape\n",
    "    if np.random.rand() < p_jitter:\n",
    "        max_shift = int(max(1, T*max_jitter_frac))\n",
    "        shifts = torch.randint(low=-max_shift, high=max_shift+1, size=(B,), device=xb.device)\n",
    "        for i in range(B):\n",
    "            xb[i] = torch.roll(xb[i], shifts=int(shifts[i].item()), dims=-1)\n",
    "    if np.random.rand() < p_noise:\n",
    "        xb = xb + noise_std*torch.randn_like(xb)\n",
    "    if np.random.rand() < p_chdrop and max_chdrop > 0:\n",
    "        k = min(max_chdrop, C)\n",
    "        for i in range(B):\n",
    "            idx = torch.randperm(C, device=xb.device)[:k]\n",
    "            xb[i, idx, :] = 0.0\n",
    "    return xb\n",
    "\n",
    "# Versión suave para FT\n",
    "def augment_batch_ft(xb):\n",
    "    return augment_batch(xb, **FT_AUG)\n",
    "\n",
    "# =========================\n",
    "# EMA de pesos (solo global)\n",
    "# =========================\n",
    "class ModelEMA:\n",
    "    def __init__(self, model: nn.Module, decay: float = 0.9995, device=None):\n",
    "        self.ema = self._clone(model).to(device if device is not None else next(model.parameters()).device)\n",
    "        self.decay = decay\n",
    "        self._updates = 0\n",
    "        self.update(model, force=True)\n",
    "\n",
    "    def _clone(self, model):\n",
    "        ema = type(model)()\n",
    "        ema.load_state_dict(model.state_dict())\n",
    "        for p in ema.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        return ema\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model: nn.Module, force: bool = False):\n",
    "        d = self.decay\n",
    "        if self._updates < 1000:\n",
    "            d = (self._updates / 1000.0) * self.decay\n",
    "        msd = model.state_dict()\n",
    "        esd = self.ema.state_dict()\n",
    "        for k in esd.keys():\n",
    "            if esd[k].dtype.is_floating_point:\n",
    "                esd[k].mul_(d).add_(msd[k].detach(), alpha=1.0 - d)\n",
    "            else:\n",
    "                esd[k] = msd[k]\n",
    "        self._updates += 1\n",
    "\n",
    "# =========================\n",
    "# INFERENCIA TTA / SUBWINDOW\n",
    "# =========================\n",
    "def subwindow_logits(model, X, sfreq, sw_len, sw_stride, device):\n",
    "    model.eval()\n",
    "    wl = int(round(sw_len * sfreq))\n",
    "    st = int(round(sw_stride * sfreq))\n",
    "    wl = max(1, min(wl, X.shape[-1])); st = max(1, st)\n",
    "    out = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(X.shape[0]):\n",
    "            x = X[i]; acc = []\n",
    "            for s in range(0, max(1, X.shape[-1]-wl+1), st):\n",
    "                seg = x[:, s:s+wl]\n",
    "                if seg.shape[-1] < wl:\n",
    "                    pad = wl - seg.shape[-1]\n",
    "                    seg = np.pad(seg, ((0,0),(0,pad)), mode='edge')\n",
    "                xb = torch.tensor(seg[None, ...], dtype=torch.float32, device=device)\n",
    "                logit = model(xb).detach().cpu().numpy()[0]\n",
    "                acc.append(logit)\n",
    "            acc = np.mean(np.stack(acc, axis=0), axis=0) if len(acc) else np.zeros(2, dtype=np.float32)\n",
    "            out.append(acc)\n",
    "    return np.stack(out, axis=0)\n",
    "\n",
    "def time_shift_tta_logits(model, X, sfreq, shifts_s, device):\n",
    "    model.eval()\n",
    "    T = X.shape[-1]; out = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(X.shape[0]):\n",
    "            x0 = X[i]; acc = []\n",
    "            for sh in shifts_s:\n",
    "                shift = int(round(sh * sfreq))\n",
    "                if shift == 0:\n",
    "                    x = x0\n",
    "                elif shift > 0:\n",
    "                    x = np.pad(x0[:, shift:], ((0,0),(0,shift)), mode='edge')[:, :T]\n",
    "                else:\n",
    "                    shift = -shift\n",
    "                    x = np.pad(x0[:, :-shift], ((0,0),(shift,0)), mode='edge')[:, :T]\n",
    "                xb = torch.tensor(x[None, ...], dtype=torch.float32, device=device)\n",
    "                logit = model(xb).detach().cpu().numpy()[0]\n",
    "                acc.append(logit)\n",
    "            out.append(np.mean(np.stack(acc, axis=0), axis=0))\n",
    "    return np.stack(out, axis=0)\n",
    "\n",
    "# =========================\n",
    "# Utilidades splits estratificados por sujeto\n",
    "# =========================\n",
    "def build_subject_label_map(subject_ids):\n",
    "    y_dom_list = []\n",
    "    for sid in subject_ids:\n",
    "        Xs, ys, _ = load_subject_epochs(sid, RESAMPLE_HZ, DO_NOTCH, DO_BANDPASS, DO_CAR, BP_LO, BP_HI)\n",
    "        if len(ys) == 0:\n",
    "            y_dom_list.append(-1)\n",
    "            continue\n",
    "        binc = np.bincount(ys, minlength=2)\n",
    "        y_dom = int(np.argmax(binc))\n",
    "        y_dom_list.append(y_dom)\n",
    "    return np.array(y_dom_list, dtype=int)\n",
    "\n",
    "# =========================\n",
    "# TRAIN/EVAL GLOBAL POR FOLD\n",
    "# =========================\n",
    "def train_one_fold(fold:int, device):\n",
    "    def load_fold_subjects_local(folds_json: Path, fold: int):\n",
    "        return load_fold_subjects(folds_json, fold)\n",
    "\n",
    "    train_sub, test_sub = load_fold_subjects_local(FOLDS_JSON, fold)\n",
    "    train_sub = [s for s in train_sub if subject_id_to_int(s) not in EXCLUDE_SUBJECTS]\n",
    "    test_sub  = [s for s in test_sub  if subject_id_to_int(s) not in EXCLUDE_SUBJECTS]\n",
    "\n",
    "    rng = np.random.RandomState(RANDOM_STATE + fold)\n",
    "    tr_subjects = sorted(train_sub)\n",
    "\n",
    "    if VAL_STRAT_SUBJECT and len(tr_subjects) > 1:\n",
    "        y_dom = build_subject_label_map(tr_subjects)\n",
    "        if np.any(y_dom < 0):\n",
    "            mask = y_dom >= 0\n",
    "            moda = int(np.bincount(y_dom[mask]).argmax()) if mask.sum() > 0 else 0\n",
    "            y_dom[~mask] = moda\n",
    "        n_val_subj = max(1, int(round(len(tr_subjects) * VAL_SUBJECT_FRAC)))\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=n_val_subj, random_state=RANDOM_STATE + fold)\n",
    "        idx = np.arange(len(tr_subjects))\n",
    "        _, val_idx = next(sss.split(idx, y_dom))\n",
    "        val_subjects = sorted([tr_subjects[i] for i in val_idx])\n",
    "        train_subjects = [s for s in tr_subjects if s not in val_subjects]\n",
    "    else:\n",
    "        tr_subjects_shuf = tr_subjects.copy()\n",
    "        rng.shuffle(tr_subjects_shuf)\n",
    "        n_val_subj = max(1, int(round(len(tr_subjects_shuf) * VAL_SUBJECT_FRAC)))\n",
    "        val_subjects = sorted(tr_subjects_shuf[:n_val_subj])\n",
    "        train_subjects = sorted(tr_subjects_shuf[n_val_subj:])\n",
    "\n",
    "    # Carga TRAIN/VAL/TEST\n",
    "    X_tr_list, y_tr_list, sub_tr_list = [], [], []\n",
    "    X_val_list, y_val_list, sub_val_list = [], [], []\n",
    "    X_te_list, y_te_list, sub_te_list = [], [], []\n",
    "    sfreq = None\n",
    "\n",
    "    for sid in tqdm(train_subjects, desc=f\"Cargando train fold{fold}\"):\n",
    "        Xs, ys, sf = load_subject_epochs(sid, RESAMPLE_HZ, DO_NOTCH, DO_BANDPASS, DO_CAR, BP_LO, BP_HI)\n",
    "        if len(ys) == 0: continue\n",
    "        X_tr_list.append(Xs); y_tr_list.append(ys)\n",
    "        sub_tr_list.append(np.full_like(ys, fill_value=subject_id_to_int(sid)))\n",
    "        sfreq = sf if sfreq is None else sfreq\n",
    "\n",
    "    for sid in tqdm(val_subjects, desc=f\"Cargando val fold{fold}\"):\n",
    "        Xs, ys, sf = load_subject_epochs(sid, RESAMPLE_HZ, DO_NOTCH, DO_BANDPASS, DO_CAR, BP_LO, BP_HI)\n",
    "        if len(ys) == 0: continue\n",
    "        X_val_list.append(Xs); y_val_list.append(ys)\n",
    "        sub_val_list.append(np.full_like(ys, fill_value=subject_id_to_int(sid)))\n",
    "        sfreq = sf if sfreq is None else sfreq\n",
    "\n",
    "    for sid in tqdm(test_sub, desc=f\"Cargando test fold{fold}\"):\n",
    "        Xs, ys, sf = load_subject_epochs(sid, RESAMPLE_HZ, DO_NOTCH, DO_BANDPASS, DO_CAR, BP_LO, BP_HI)\n",
    "        if len(ys) == 0: continue\n",
    "        X_te_list.append(Xs); y_te_list.append(ys)\n",
    "        sub_te_list.append(np.full_like(ys, fill_value=subject_id_to_int(sid)))\n",
    "        sfreq = sf if sfreq is None else sfreq\n",
    "\n",
    "    # Concatenar\n",
    "    X_tr = np.concatenate(X_tr_list, axis=0); y_tr = np.concatenate(y_tr_list, axis=0)\n",
    "    sub_tr = np.concatenate(sub_tr_list, axis=0)\n",
    "    X_val = np.concatenate(X_val_list, axis=0); y_val = np.concatenate(y_val_list, axis=0)\n",
    "    sub_val = np.concatenate(sub_val_list, axis=0)\n",
    "    X_te = np.concatenate(X_te_list, axis=0); y_te = np.concatenate(y_te_list, axis=0)\n",
    "    sub_te = np.concatenate(sub_te_list, axis=0)\n",
    "\n",
    "    print(f\"[Fold {fold}/5] Entrenando modelo global... (n_train={len(y_tr)} | n_val={len(y_val)} | n_test={len(y_te)})\")\n",
    "\n",
    "    # Normalización por canal (fit en TRAIN y aplicar a VAL/TEST)\n",
    "    if ZSCORE_PER_EPOCH:\n",
    "        X_tr_std, X_val_std, X_te_std = X_tr, X_val, X_te\n",
    "    else:\n",
    "        X_tr_std, X_val_std = standardize_per_channel(X_tr, X_val)\n",
    "        _,        X_te_std  = standardize_per_channel(X_tr, X_te)\n",
    "\n",
    "    # Datasets\n",
    "    tr_ds  = TensorDataset(torch.tensor(X_tr_std),  torch.tensor(y_tr).long(),  torch.tensor(sub_tr).long())\n",
    "    val_ds = TensorDataset(torch.tensor(X_val_std), torch.tensor(y_val).long(), torch.tensor(sub_val).long())\n",
    "    te_ds  = TensorDataset(torch.tensor(X_te_std),  torch.tensor(y_te).long(),  torch.tensor(sub_te).long())\n",
    "\n",
    "    # Weighted sampler templado: w = (1/ws)^a * (1/wk)^b\n",
    "    def make_weighted_sampler(dataset: TensorDataset):\n",
    "        _Xb, yb, sb = dataset.tensors\n",
    "        yb_np = yb.numpy()\n",
    "        sb_np = sb.numpy()\n",
    "        uniq_s, cnt_s = np.unique(sb_np, return_counts=True)\n",
    "        map_s = {s:c for s,c in zip(uniq_s, cnt_s)}\n",
    "        key = sb_np.astype(np.int64) * 10 + yb_np.astype(np.int64)\n",
    "        uniq_k, cnt_k = np.unique(key, return_counts=True)\n",
    "        map_k = {k:c for k,c in zip(uniq_k, cnt_k)}\n",
    "        a, b = 0.8, 1.0\n",
    "        w = []\n",
    "        for s, y in zip(sb_np, yb_np):\n",
    "            k = int(s)*10 + int(y)\n",
    "            ws = float(map_s[int(s)])\n",
    "            wk = float(map_k[k])\n",
    "            w.append((ws ** (-a)) * (wk ** (-b)))\n",
    "        w = np.array(w, dtype=np.float64)\n",
    "        w = w / (w.mean() + 1e-12)\n",
    "        sampler = WeightedRandomSampler(weights=torch.tensor(w, dtype=torch.double),\n",
    "                                        num_samples=len(yb_np), replacement=True)\n",
    "        return sampler\n",
    "\n",
    "    if USE_WEIGHTED_SAMPLER:\n",
    "        tr_sampler = make_weighted_sampler(tr_ds)\n",
    "        tr_ld  = DataLoader(tr_ds, batch_size=BATCH_SIZE, sampler=tr_sampler, drop_last=False, worker_init_fn=seed_worker)\n",
    "    else:\n",
    "        tr_ld  = DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True,  drop_last=False, worker_init_fn=seed_worker)\n",
    "\n",
    "    val_ld = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, worker_init_fn=seed_worker)\n",
    "    te_ld  = DataLoader(te_ds,  batch_size=BATCH_SIZE, shuffle=False, drop_last=False, worker_init_fn=seed_worker)\n",
    "\n",
    "    # Modelo (2 clases)\n",
    "    model = EEGCNNTransformer(n_ch=8, n_cls=2, d_model=D_MODEL, n_heads=N_HEADS,\n",
    "                              n_layers=N_LAYERS, p_drop=P_DROP, p_drop_encoder=P_DROP_ENCODER).to(device)\n",
    "\n",
    "    # Optimizador + Focal Loss (2 clases)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=1e-2)\n",
    "    class_counts = np.bincount(y_tr, minlength=2).astype(np.float32)\n",
    "    inv = class_counts.sum() / (2.0 * np.maximum(class_counts, 1.0))\n",
    "    alpha = torch.tensor(inv, dtype=torch.float32, device=device)\n",
    "    crit = FocalLoss(alpha=alpha, gamma=1.5, reduction='mean')\n",
    "\n",
    "    # LR scheduler Warmup+Cosine\n",
    "    from torch.optim.lr_scheduler import LambdaLR\n",
    "    total_epochs = EPOCHS\n",
    "    warmup_epochs = max(1, int(WARMUP_EPOCHS))\n",
    "    min_factor = 0.1\n",
    "    def lr_lambda(current_epoch):\n",
    "        if current_epoch < warmup_epochs:\n",
    "            return (current_epoch + 1) / warmup_epochs\n",
    "        progress = (current_epoch - warmup_epochs) / max(1, (total_epochs - warmup_epochs))\n",
    "        progress = min(1.0, max(0.0, progress))\n",
    "        return min_factor + 0.5 * (1.0 - min_factor) * (1.0 + np.cos(np.pi * progress))\n",
    "    scheduler = LambdaLR(opt, lr_lambda=lr_lambda)\n",
    "\n",
    "    # EMA (solo global)\n",
    "    if USE_EMA:\n",
    "        ema = ModelEMA(model, decay=EMA_DECAY, device=device)\n",
    "    else:\n",
    "        ema = None\n",
    "\n",
    "    # Entrenamiento global con early stopping por F1 macro\n",
    "    best_f1, best_state, wait = 0.0, None, 0\n",
    "    hist = {\"ep\": [], \"tr_loss\": [], \"tr_acc\": [], \"val_acc\": [], \"val_f1m\": [], \"lr\": []}\n",
    "\n",
    "    def evaluate_on(loader, use_ema=True):\n",
    "        mdl = ema.ema if (ema is not None and use_ema) else model\n",
    "        mdl.eval()\n",
    "        preds, gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _sb in loader:\n",
    "                xb = xb.to(device)\n",
    "                p = mdl(xb).argmax(dim=1).cpu().numpy()\n",
    "                preds.append(p); gts.append(yb.numpy())\n",
    "        preds = np.concatenate(preds); gts = np.concatenate(gts)\n",
    "        acc = accuracy_score(gts, preds)\n",
    "        f1m = f1_score(gts, preds, average='macro')\n",
    "        return acc, f1m\n",
    "\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        tr_loss, n_seen, tr_correct = 0.0, 0, 0\n",
    "        for xb, yb, _sb in tr_ld:\n",
    "            xb = xb.to(device); yb = yb.to(device)\n",
    "            xb = augment_batch(xb)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            opt.step()\n",
    "            if ema is not None:\n",
    "                ema.update(model)\n",
    "\n",
    "            tr_loss += loss.item() * len(yb)\n",
    "            n_seen += len(yb)\n",
    "            tr_correct += (logits.argmax(1) == yb).sum().item()\n",
    "        tr_loss /= max(1, n_seen)\n",
    "        tr_acc = tr_correct / max(1, n_seen)\n",
    "\n",
    "        acc, f1m = evaluate_on(val_ld, use_ema=True)\n",
    "\n",
    "        hist[\"ep\"].append(ep)\n",
    "        hist[\"tr_loss\"].append(tr_loss)\n",
    "        hist[\"tr_acc\"].append(tr_acc)\n",
    "        hist[\"val_acc\"].append(acc)\n",
    "        hist[\"val_f1m\"].append(f1m)\n",
    "        hist[\"lr\"].append(scheduler.get_last_lr()[0])\n",
    "\n",
    "        print(f\"  Época {ep:3d} | train_loss={tr_loss:.4f} | train_acc={tr_acc:.4f} | val_acc={acc:.4f} | val_f1m={f1m:.4f} | LR={scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        improved = f1m > best_f1 + 1e-4\n",
    "        if improved:\n",
    "            best_f1 = f1m\n",
    "            ref_model = ema.ema if ema is not None else model\n",
    "            best_state = {k: v.detach().cpu() for k, v in ref_model.state_dict().items()}\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "\n",
    "        scheduler.step()\n",
    "        if wait >= PATIENCE:\n",
    "            print(f\"  Early stopping en época {ep} (mejor val_f1m={best_f1:.4f})\")\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        (ema.ema if (ema is not None) else model).load_state_dict(best_state)\n",
    "\n",
    "    # ---- Guardar curva ----\n",
    "    fig = plt.figure(figsize=(8,4.5))\n",
    "    ax1 = plt.gca()\n",
    "    ax1.plot(hist[\"ep\"], hist[\"tr_loss\"], label=\"train_loss\")\n",
    "    ax1.plot(hist[\"ep\"], hist[\"tr_acc\"], label=\"tr_acc\")\n",
    "    ax1.plot(hist[\"ep\"], hist[\"val_acc\"], label=\"val_acc\")\n",
    "    ax1.set_xlabel(\"Época\"); ax1.set_title(f\"Fold {fold} — Curva de entrenamiento (2 clases)\")\n",
    "    ax1.legend(); ax1.grid(True, alpha=0.3)\n",
    "    out_png = f\"training_curve_fold{fold}.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=140)\n",
    "    plt.close(fig)\n",
    "    print(f\"↳ Curva de entrenamiento guardada: {out_png}\")\n",
    "\n",
    "    # ---- Evaluación final en TEST (global) ----\n",
    "    eval_model = ema.ema if (ema is not None) else model\n",
    "    eval_model.eval()\n",
    "\n",
    "    sfreq_used = RESAMPLE_HZ\n",
    "    if sfreq_used is None:\n",
    "        sfreq_used = int(round(X_te_std.shape[-1] / (TMAX - TMIN)))\n",
    "\n",
    "    if (not SW_ENABLE) or SW_MODE == 'none':\n",
    "        preds, gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _sb in te_ld:\n",
    "                xb = xb.to(device)\n",
    "                p = eval_model(xb).argmax(dim=1).cpu().numpy()\n",
    "                preds.append(p); gts.append(yb.numpy())\n",
    "        preds = np.concatenate(preds); gts = np.concatenate(gts)\n",
    "    elif SW_MODE in ('subwin', 'tta'):\n",
    "        logits_tta = None\n",
    "        logits_sw  = None\n",
    "        if SW_MODE == 'subwin':\n",
    "            logits_sw = subwindow_logits(eval_model, X_te_std, sfreq_used, SW_LEN, SW_STRIDE, device)\n",
    "        elif SW_MODE == 'tta':\n",
    "            logits_tta = time_shift_tta_logits(eval_model, X_te_std, sfreq_used, TTA_SHIFTS_S, device)\n",
    "\n",
    "        if COMBINE_TTA_AND_SUBWIN:\n",
    "            if logits_tta is None:\n",
    "                logits_tta = time_shift_tta_logits(eval_model, X_te_std, sfreq_used, TTA_SHIFTS_S, device)\n",
    "            if logits_sw is None:\n",
    "                logits_sw  = subwindow_logits(eval_model, X_te_std, sfreq_used, SW_LEN, SW_STRIDE, device)\n",
    "            logits = 0.5 * logits_tta + 0.5 * logits_sw\n",
    "        else:\n",
    "            logits = logits_tta if logits_tta is not None else logits_sw\n",
    "\n",
    "        preds = logits.argmax(axis=1); gts = y_te\n",
    "    else:\n",
    "        raise ValueError(f\"SW_MODE desconocido: {SW_MODE}\")\n",
    "\n",
    "    acc = accuracy_score(gts, preds)\n",
    "    f1m = f1_score(gts, preds, average='macro')\n",
    "    print(f\"[Fold {fold}/5] Global acc={acc:.4f}\\n\")\n",
    "    print(classification_report(gts, preds, target_names=[c.replace('_',' ') for c in CLASS_NAMES], digits=4))\n",
    "    print(\"Confusion matrix (rows=true, cols=pred):\")\n",
    "    cm = confusion_matrix(gts, preds, labels=[0,1])\n",
    "    print(cm)\n",
    "    fig = plt.figure(figsize=(4.8,4.2))\n",
    "    plt.imshow(cm, cmap='Blues'); plt.title(f\"Confusion — Fold {fold} Global (2 clases)\")\n",
    "    plt.xlabel(\"pred\"); plt.ylabel(\"true\")\n",
    "    plt.colorbar(); plt.tight_layout()\n",
    "    plt.savefig(f\"confusion_global_fold{fold}.png\", dpi=140); plt.close(fig)\n",
    "    print(f\"↳ Matriz de confusión guardada: confusion_global_fold{fold}.png\")\n",
    "\n",
    "    # =========================\n",
    "    # FINE-TUNING PROGRESIVO POR SUJETO (4-fold CV interno) — 2 clases\n",
    "    # =========================\n",
    "    subjects = np.unique(sub_te)\n",
    "    subj_to_idx = {s: np.where(sub_te == s)[0] for s in subjects}\n",
    "\n",
    "    def make_ft_optimizer(model):\n",
    "        head_params = list(model.head.parameters())\n",
    "        backbone_params = [p for n,p in model.named_parameters() if not n.startswith('head.')]\n",
    "        return torch.optim.AdamW([\n",
    "            {'params': backbone_params, 'lr': FT_LR_BACKBONE},\n",
    "            {'params': head_params,     'lr': FT_LR_HEAD}\n",
    "        ], weight_decay=FT_WD)\n",
    "\n",
    "    def freeze_backbone(model, freeze=True):\n",
    "        for n,p in model.named_parameters():\n",
    "            if not n.startswith('head.'):\n",
    "                p.requires_grad_(not freeze)\n",
    "\n",
    "    def ft_make_criterion_from_counts(counts):\n",
    "        inv = counts.sum() / (2.0 * np.maximum(counts.astype(np.float32), 1.0))\n",
    "        a = torch.tensor(inv, dtype=torch.float32, device=device)\n",
    "        return FocalLoss(alpha=a, gamma=1.5, reduction='mean')\n",
    "\n",
    "    def evaluate_tensor(model_t, X, y):\n",
    "        model_t.eval()\n",
    "        with torch.no_grad():\n",
    "            xb = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "            p = model_t(xb).argmax(1).cpu().numpy()\n",
    "        return accuracy_score(y, p), f1_score(y, p, average='macro'), p\n",
    "\n",
    "    all_true, all_pred = [], []\n",
    "\n",
    "    # Base: mejores pesos globales (EMA si existe)\n",
    "    base_state = (ema.ema if (ema is not None) else model).state_dict()\n",
    "\n",
    "    for s in subjects:\n",
    "        idx = subj_to_idx[s]\n",
    "        Xs, ys = X_te_std[idx], y_te[idx]\n",
    "\n",
    "        # Si no hay ambas clases o muy pocos ejemplos, solo eval directa\n",
    "        if len(np.unique(ys)) < 2 or len(ys) < 20:\n",
    "            eval_model = EEGCNNTransformer(n_ch=8, n_cls=2, d_model=D_MODEL, n_heads=N_HEADS,\n",
    "                                           n_layers=N_LAYERS, p_drop=P_DROP, p_drop_encoder=P_DROP_ENCODER).to(device)\n",
    "            eval_model.load_state_dict(base_state)\n",
    "            _, _, pred_s = evaluate_tensor(eval_model, Xs, ys)\n",
    "            all_true.append(ys); all_pred.append(pred_s)\n",
    "            continue\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=FT_N_FOLDS, shuffle=True, random_state=RANDOM_STATE + fold + int(s))\n",
    "        for tr_idx, te_idx in skf.split(Xs, ys):\n",
    "            Xtr, ytr = Xs[tr_idx].copy(), ys[tr_idx].copy()\n",
    "            Xte_s, yte_s = Xs[te_idx].copy(), ys[te_idx].copy()\n",
    "\n",
    "            # Estandarización por sujeto (fit en FT-train)\n",
    "            Xtr_std, Xte_std = standardize_per_channel(Xtr, Xte_s)\n",
    "\n",
    "            # Modelo fresh desde el global\n",
    "            ft_model = EEGCNNTransformer(n_ch=8, n_cls=2, d_model=D_MODEL, n_heads=N_HEADS,\n",
    "                                         n_layers=N_LAYERS, p_drop=P_DROP, p_drop_encoder=P_DROP_ENCODER).to(device)\n",
    "            ft_model.load_state_dict(base_state)\n",
    "\n",
    "            # optim + criterio (α de este sujeto/split)\n",
    "            opt_ft = make_ft_optimizer(ft_model)\n",
    "            alpha_counts = np.bincount(ytr, minlength=2)\n",
    "            ft_crit = ft_make_criterion_from_counts(alpha_counts)\n",
    "\n",
    "            # loaders FT\n",
    "            ds_tr = TensorDataset(torch.tensor(Xtr_std), torch.tensor(ytr).long())\n",
    "            ds_te = TensorDataset(torch.tensor(Xte_std), torch.tensor(yte_s).long())\n",
    "            ld_tr = DataLoader(ds_tr, batch_size=FT_BATCH, shuffle=True,  drop_last=False, worker_init_fn=seed_worker)\n",
    "            ld_te = DataLoader(ds_te, batch_size=FT_BATCH, shuffle=False, drop_last=False, worker_init_fn=seed_worker)\n",
    "\n",
    "            # ETAPA 1: freeze backbone\n",
    "            freeze_backbone(ft_model, True)\n",
    "            best_f1_s, best_state_s, wait_s = -1, None, 0\n",
    "            for ep in range(1, FT_FREEZE_EPOCHS+1):\n",
    "                ft_model.train()\n",
    "                for xb, yb in ld_tr:\n",
    "                    xb = xb.to(device); yb = yb.to(device)\n",
    "                    xb = augment_batch_ft(xb)\n",
    "                    opt_ft.zero_grad()\n",
    "                    logits = ft_model(xb)\n",
    "                    loss = ft_crit(logits, yb)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(ft_model.parameters(), 1.0)\n",
    "                    opt_ft.step()\n",
    "\n",
    "                acc_s, f1_s = evaluate_tensor(ft_model, Xte_std, yte_s)[:2]\n",
    "                improved = f1_s > best_f1_s + 1e-4\n",
    "                if improved:\n",
    "                    best_f1_s = f1_s\n",
    "                    best_state_s = {k: v.detach().cpu() for k,v in ft_model.state_dict().items()}\n",
    "                    wait_s = 0\n",
    "                else:\n",
    "                    wait_s += 1\n",
    "                if wait_s >= FT_PATIENCE:\n",
    "                    break\n",
    "\n",
    "            if best_state_s is not None:\n",
    "                ft_model.load_state_dict(best_state_s)\n",
    "\n",
    "            # ETAPA 2: unfreeze (todo entrenable)\n",
    "            freeze_backbone(ft_model, False)\n",
    "            best_f1_s2, best_state_s2, wait_s2 = -1, None, 0\n",
    "            for ep in range(1, FT_UNFREEZE_EPOCHS+1):\n",
    "                ft_model.train()\n",
    "                for xb, yb in ld_tr:\n",
    "                    xb = xb.to(device); yb = yb.to(device)\n",
    "                    xb = augment_batch_ft(xb)\n",
    "                    opt_ft.zero_grad()\n",
    "                    logits = ft_model(xb)\n",
    "                    loss = ft_crit(logits, yb)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(ft_model.parameters(), 1.0)\n",
    "                    opt_ft.step()\n",
    "\n",
    "                acc_s, f1_s = evaluate_tensor(ft_model, Xte_std, yte_s)[:2]\n",
    "                improved = f1_s > best_f1_s2 + 1e-4\n",
    "                if improved:\n",
    "                    best_f1_s2 = f1_s\n",
    "                    best_state_s2 = {k: v.detach().cpu() for k,v in ft_model.state_dict().items()}\n",
    "                    wait_s2 = 0\n",
    "                else:\n",
    "                    wait_s2 += 1\n",
    "                if wait_s2 >= FT_PATIENCE:\n",
    "                    break\n",
    "\n",
    "            if best_state_s2 is not None:\n",
    "                ft_model.load_state_dict(best_state_s2)\n",
    "\n",
    "            # métricas del fold del sujeto\n",
    "            _, _, pred_s = evaluate_tensor(ft_model, Xte_std, yte_s)\n",
    "            all_true.append(yte_s); all_pred.append(pred_s)\n",
    "\n",
    "    # resumen FT por sujeto\n",
    "    all_true = np.concatenate(all_true) if len(all_true) else np.array([], dtype=int)\n",
    "    all_pred = np.concatenate(all_pred) if len(all_pred) else np.array([], dtype=int)\n",
    "    if len(all_true) > 0:\n",
    "        ft_acc = accuracy_score(all_true, all_pred)\n",
    "        ft_f1m = f1_score(all_true, all_pred, average='macro')\n",
    "        print(f\"  Fine-tuning PROGRESIVO (por sujeto, {FT_N_FOLDS}-fold CV) acc={ft_acc:.4f}\")\n",
    "        print(f\"  Δ(FT-Global) = {ft_acc - acc:+.4f}\")\n",
    "        cm_ft = confusion_matrix(all_true, all_pred, labels=[0,1])\n",
    "        print(\"Confusion matrix FT (rows=true, cols=pred):\")\n",
    "        print(cm_ft)\n",
    "        fig = plt.figure(figsize=(4.8,4.2))\n",
    "        plt.imshow(cm_ft, cmap='Greens'); plt.title(f\"Confusion — Fold {fold} FT (2 clases)\")\n",
    "        plt.xlabel(\"pred\"); plt.ylabel(\"true\")\n",
    "        plt.colorbar(); plt.tight_layout()\n",
    "        plt.savefig(f\"confusion_ft_fold{fold}.png\", dpi=140); plt.close(fig)\n",
    "        print(f\"↳ Matriz de confusión FT guardada: confusion_ft_fold{fold}.png\")\n",
    "    else:\n",
    "        ft_acc = float('nan')\n",
    "        print(\"  Fine-tuning PROGRESIVO: no se generaron splits válidos (sujetos con una sola clase o muy pocos ensayos).\")\n",
    "\n",
    "    return acc, f1m, ft_acc\n",
    "\n",
    "# =========================\n",
    "# LOOP 5 FOLDS + RESUMEN\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    acc_folds, f1_folds, ft_acc_folds = [], [], []\n",
    "    for fold in range(1, 6):\n",
    "        acc, f1m, ft_acc = train_one_fold(fold, DEVICE)\n",
    "        acc_folds.append(f\"{acc:.4f}\")\n",
    "        f1_folds.append(f\"{f1m:.4f}\")\n",
    "        ft_acc_folds.append(\"nan\" if (ft_acc != ft_acc) else f\"{ft_acc:.4f}\")  # maneja NaN\n",
    "\n",
    "    acc_mean = float(np.mean([float(a) for a in acc_folds]))\n",
    "    f1_mean  = float(np.mean([float(f) for f in f1_folds]))\n",
    "    valid_ft = [float(a) for a in ft_acc_folds if a != \"nan\"]\n",
    "    ft_acc_mean = float(np.mean(valid_ft)) if len(valid_ft) else float('nan')\n",
    "    delta_mean = (ft_acc_mean - acc_mean) if (ft_acc_mean == ft_acc_mean) else float('nan')\n",
    "\n",
    "    print(\"\\n============================================================\")\n",
    "    print(\"RESULTADOS FINALES (2 clases: left/right)\")\n",
    "    print(\"============================================================\")\n",
    "    print(f\"Global folds (ACC): {acc_folds}\")\n",
    "    print(f\"Global mean ACC: {acc_mean:.4f}\")\n",
    "    print(f\"F1 folds (MACRO): {f1_folds}\")\n",
    "    print(f\"F1 mean (MACRO): {f1_mean:.4f}\")\n",
    "    print(f\"Fine-tune PROGRESIVO folds: {ft_acc_folds}\")\n",
    "    if ft_acc_mean == ft_acc_mean:\n",
    "        print(f\"Fine-tune PROGRESIVO mean: {ft_acc_mean:.4f}\")\n",
    "        print(f\"Δ(FT-Global) mean: {delta_mean:+.4f}\")\n",
    "    else:\n",
    "        print(\"Fine-tune PROGRESIVO mean: N/A (insuficientes sujetos/conjuntos válidos)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
