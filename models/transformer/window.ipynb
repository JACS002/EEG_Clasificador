{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "795e8a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Dispositivo: cuda\n",
      "🧠 MI-EEG — CNN+Transformer (21/cls/subj) + Sliding Window + FT progresivo\n",
      "🔧 Escenario: 4c | rango ensayo=(0.0, 4.0) s | subventana=2.0s | stride=0.5s | subwins/ensayo=5 | band-pass=OFF\n",
      "⚙️  Balance: 21 ensayos por clase y por sujeto (con reemplazo si falta)\n",
      "⚙️  FT (sin fuga): GroupKFold por ensayo + GroupShuffleSplit en validación interna\n",
      "Sujetos elegibles: 103 → [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recolectando y balanceando ensayos base por sujeto: 100%|██████████| 103/103 [00:36<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensayos base balanceados: 8652  (~ #sujetos_utiles * 84)\n",
      "Sujetos con ensayos balanceados: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold 1/5] Entrenando global (VAL por ensayo)...\n",
      "  Época   1 | train(ensayo)=0.3913 | val(ensayo)=0.3601\n",
      "  Época   5 | train(ensayo)=0.4621 | val(ensayo)=0.4256\n",
      "  Época  10 | train(ensayo)=0.5616 | val(ensayo)=0.4216\n",
      "  Época  15 | train(ensayo)=0.6284 | val(ensayo)=0.3998\n",
      "  Early stopping @ epoch 19 (best val ensayo=0.4415)\n",
      "↳ Curva: training_curve_cnnTrans_subjFolds_fold1.png\n",
      "[Fold 1] Ensayo acc=0.3532\n",
      "\n",
      "=== Reporte por ENSAYO (agregado) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.4006    0.2880    0.3351       441\n",
      "       Right     0.4130    0.2154    0.2832       441\n",
      "  Both Fists     0.3033    0.5488    0.3906       441\n",
      "   Both Feet     0.3795    0.3605    0.3698       441\n",
      "\n",
      "    accuracy                         0.3532      1764\n",
      "   macro avg     0.3741    0.3532    0.3447      1764\n",
      "weighted avg     0.3741    0.3532    0.3447      1764\n",
      "\n",
      "F1-macro: 0.3447 | Kappa: 0.1376\n",
      "  FT progresivo por ENSAYO acc=0.3203 | sujetos=84\n",
      "  Δ(FT-Ensayo - Global-Ensayo) = -0.0329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold 2/5] Entrenando global (VAL por ensayo)...\n",
      "  Época   1 | train(ensayo)=0.3315 | val(ensayo)=0.3304\n",
      "  Época   5 | train(ensayo)=0.4603 | val(ensayo)=0.3929\n",
      "  Época  10 | train(ensayo)=0.5660 | val(ensayo)=0.3780\n",
      "  Época  15 | train(ensayo)=0.6189 | val(ensayo)=0.3810\n",
      "  Early stopping @ epoch 17 (best val ensayo=0.4117)\n",
      "↳ Curva: training_curve_cnnTrans_subjFolds_fold2.png\n",
      "[Fold 2] Ensayo acc=0.4008\n",
      "\n",
      "=== Reporte por ENSAYO (agregado) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.4634    0.4739    0.4686       441\n",
      "       Right     0.4496    0.3537    0.3959       441\n",
      "  Both Fists     0.3135    0.3946    0.3494       441\n",
      "   Both Feet     0.4088    0.3810    0.3944       441\n",
      "\n",
      "    accuracy                         0.4008      1764\n",
      "   macro avg     0.4088    0.4008    0.4021      1764\n",
      "weighted avg     0.4088    0.4008    0.4021      1764\n",
      "\n",
      "F1-macro: 0.4021 | Kappa: 0.2011\n",
      "  FT progresivo por ENSAYO acc=0.3781 | sujetos=84\n",
      "  Δ(FT-Ensayo - Global-Ensayo) = -0.0227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold 3/5] Entrenando global (VAL por ensayo)...\n",
      "  Época   1 | train(ensayo)=0.3411 | val(ensayo)=0.3433\n",
      "  Época   5 | train(ensayo)=0.4635 | val(ensayo)=0.3542\n",
      "  Época  10 | train(ensayo)=0.5798 | val(ensayo)=0.3790\n",
      "  Época  15 | train(ensayo)=0.5889 | val(ensayo)=0.3403\n",
      "  Early stopping @ epoch 19 (best val ensayo=0.3790)\n",
      "↳ Curva: training_curve_cnnTrans_subjFolds_fold3.png\n",
      "[Fold 3] Ensayo acc=0.3730\n",
      "\n",
      "=== Reporte por ENSAYO (agregado) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.4642    0.2789    0.3484       441\n",
      "       Right     0.3594    0.6349    0.4590       441\n",
      "  Both Fists     0.3444    0.1882    0.2434       441\n",
      "   Both Feet     0.3591    0.3900    0.3739       441\n",
      "\n",
      "    accuracy                         0.3730      1764\n",
      "   macro avg     0.3818    0.3730    0.3562      1764\n",
      "weighted avg     0.3818    0.3730    0.3562      1764\n",
      "\n",
      "F1-macro: 0.3562 | Kappa: 0.1640\n",
      "  FT progresivo por ENSAYO acc=0.3617 | sujetos=84\n",
      "  Δ(FT-Ensayo - Global-Ensayo) = -0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold 4/5] Entrenando global (VAL por ensayo)...\n",
      "  Época   1 | train(ensayo)=0.3170 | val(ensayo)=0.2798\n",
      "  Época   5 | train(ensayo)=0.4740 | val(ensayo)=0.3413\n",
      "  Época  10 | train(ensayo)=0.5784 | val(ensayo)=0.3591\n",
      "  Época  15 | train(ensayo)=0.6337 | val(ensayo)=0.3294\n",
      "  Early stopping @ epoch 17 (best val ensayo=0.3611)\n",
      "↳ Curva: training_curve_cnnTrans_subjFolds_fold4.png\n",
      "[Fold 4] Ensayo acc=0.4101\n",
      "\n",
      "=== Reporte por ENSAYO (agregado) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.3899    0.6071    0.4749       420\n",
      "       Right     0.4573    0.3190    0.3759       420\n",
      "  Both Fists     0.3475    0.2333    0.2792       420\n",
      "   Both Feet     0.4479    0.4810    0.4638       420\n",
      "\n",
      "    accuracy                         0.4101      1680\n",
      "   macro avg     0.4107    0.4101    0.3984      1680\n",
      "weighted avg     0.4107    0.4101    0.3984      1680\n",
      "\n",
      "F1-macro: 0.3984 | Kappa: 0.2135\n",
      "  FT progresivo por ENSAYO acc=0.3935 | sujetos=80\n",
      "  Δ(FT-Ensayo - Global-Ensayo) = -0.0167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold 5/5] Entrenando global (VAL por ensayo)...\n",
      "  Época   1 | train(ensayo)=0.3077 | val(ensayo)=0.2966\n",
      "  Época   5 | train(ensayo)=0.4561 | val(ensayo)=0.3532\n",
      "  Época  10 | train(ensayo)=0.5395 | val(ensayo)=0.3393\n",
      "  Época  15 | train(ensayo)=0.6155 | val(ensayo)=0.3393\n",
      "  Early stopping @ epoch 15 (best val ensayo=0.3532)\n",
      "↳ Curva: training_curve_cnnTrans_subjFolds_fold5.png\n",
      "[Fold 5] Ensayo acc=0.4417\n",
      "\n",
      "=== Reporte por ENSAYO (agregado) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.4970    0.3929    0.4388       420\n",
      "       Right     0.4608    0.5881    0.5167       420\n",
      "  Both Fists     0.4035    0.3286    0.3622       420\n",
      "   Both Feet     0.4085    0.4571    0.4315       420\n",
      "\n",
      "    accuracy                         0.4417      1680\n",
      "   macro avg     0.4425    0.4417    0.4373      1680\n",
      "weighted avg     0.4425    0.4417    0.4373      1680\n",
      "\n",
      "F1-macro: 0.4373 | Kappa: 0.2556\n",
      "  FT progresivo por ENSAYO acc=0.4149 | sujetos=80\n",
      "  Δ(FT-Ensayo - Global-Ensayo) = -0.0268\n",
      "\n",
      "============================================================\n",
      "RESULTADOS FINALES — CNN+Transformer (21/cls/subj) + Sliding Window + Kfold SUBJECTS-only\n",
      "============================================================\n",
      "Ensayo folds: ['0.3532', '0.4008', '0.3730', '0.4101', '0.4417']\n",
      "Ensayo mean: 0.3958\n",
      "FT prog (ensayo) folds: ['0.3203', '0.3781', '0.3617', '0.3935', '0.4149']\n",
      "FT (ensayo) mean: 0.3737\n",
      "Δ(FT-Ensayo - Global-Ensayo) mean: -0.0221\n",
      "↳ Matriz de confusión (ensayo): confusion_cnnTrans_subjectFolds_trial_allfolds.png\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ===========================================\n",
    "# MI-EEG (PhysioNet eegmmidb) — CNN+Transformer (Balanced 21/cls/subj) + Sliding Window\n",
    "# Patch: usar Kfold5.json SOLO para sujetos por fold (ignorar tr_idx/te_idx)\n",
    "# Split por sujeto/ensayo -> luego windowing (sin fuga). Métrica por ENSAYO.\n",
    "# FT por sujeto (L2-SP) agrupando por ensayo (sin fuga).\n",
    "# ===========================================\n",
    "\n",
    "import os, re, math, json, copy, random, itertools, collections\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, Subset\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, cohen_kappa_score\n",
    "from sklearn.utils import check_random_state\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'           # .../S###/S###R##.edf\n",
    "FOLDS_JSON = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE); np.random.seed(RANDOM_STATE); random.seed(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "print(f\"🚀 Dispositivo: {DEVICE}\")\n",
    "\n",
    "FS = 160.0\n",
    "\n",
    "# Escenario y rango base por ensayo\n",
    "CLASS_SCENARIO = '4c'            # '2c','3c','4c'\n",
    "WINDOW_GLOBAL = (0.0, 4.0)       # segundos rel. T1/T2 (ensayo base)\n",
    "\n",
    "# Sliding window (mismo nº por ensayo)\n",
    "WIN_LEN_SEC = 2.0\n",
    "WIN_STRIDE_SEC = 0.5\n",
    "N_SUBWINS = int(math.floor((WINDOW_GLOBAL[1]-WINDOW_GLOBAL[0]-WIN_LEN_SEC)/WIN_STRIDE_SEC) + 1)\n",
    "\n",
    "# Balance 21 por clase/sujeto\n",
    "N_PER_CLASS_PER_SUBJECT = 21\n",
    "\n",
    "# CV\n",
    "N_FOLDS = 5\n",
    "GLOBAL_VAL_SPLIT = 0.15\n",
    "GLOBAL_PATIENCE  = 10\n",
    "\n",
    "# Entrenamiento\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS_GLOBAL = 80\n",
    "LR_INIT = 1e-3\n",
    "\n",
    "# FT por sujeto\n",
    "CALIB_CV_FOLDS = 4\n",
    "FT_EPOCHS = 30\n",
    "FT_BASE_LR = 5e-5\n",
    "FT_HEAD_LR = 1e-3\n",
    "FT_L2SP = 1e-4\n",
    "FT_PATIENCE = 5\n",
    "FT_VAL_RATIO = 0.2\n",
    "\n",
    "# Band-pass\n",
    "USE_BANDPASS = False\n",
    "BP_LO, BP_HI = 8.0, 30.0\n",
    "BP_METHOD, BP_PHASE = 'fir', 'zero'\n",
    "\n",
    "# Dataset\n",
    "EXCLUDE_SUBJECTS = {38,88,89,92,100,104}\n",
    "MI_RUNS_LR = [4,8,12]            # Left/Right\n",
    "MI_RUNS_OF = [6,10,14]           # Both Fists / Both Feet\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','FCz']\n",
    "\n",
    "CLASS_NAMES = ['Left', 'Right', 'Both Fists', 'Both Feet']\n",
    "\n",
    "# =========================\n",
    "# UTIL: canales/edf\n",
    "# =========================\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = re.sub(r'([A-Za-z])0([0-9])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])Z$', r'\\1z', s)\n",
    "    s = s.replace('fp', 'Fp').replace('FP', 'Fp')\n",
    "    s = ''.join(ch.upper() if ch != 'z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {}\n",
    "    for ch in raw.ch_names:\n",
    "        lab = normalize_label(ch)\n",
    "        lab = lab[:-1] + 'z' if lab.endswith('Z') else lab\n",
    "        lab = re.sub(r'([A-Z])Z$', r'\\1z', lab)\n",
    "        mapping[ch] = lab\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired_channels=EXPECTED_8):\n",
    "    missing = [ch for ch in desired_channels if ch not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"[WARN] faltan canales {missing} en {getattr(raw,'filenames',[''])[0]}\")\n",
    "        return None\n",
    "    raw.reorder_channels([ch for ch in raw.ch_names if ch in desired_channels] +\n",
    "                         [ch for ch in raw.ch_names if ch not in desired_channels])\n",
    "    raw.pick_channels(desired_channels, ordered=True)\n",
    "    return raw\n",
    "\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(run_id:int):\n",
    "    if run_id in MI_RUNS_LR: return 'LR'\n",
    "    if run_id in MI_RUNS_OF: return 'OF'\n",
    "    return None\n",
    "\n",
    "_SNR_TABLE = None\n",
    "def _load_snr_table():\n",
    "    global _SNR_TABLE\n",
    "    if _SNR_TABLE is not None: return _SNR_TABLE\n",
    "    csv_path = PROJ / 'reports' / 'psd_mains' / 'psd_mains_summary.csv'\n",
    "    if csv_path.exists():\n",
    "        try: _SNR_TABLE = pd.read_csv(csv_path)\n",
    "        except Exception as e:\n",
    "            print(f\"[SNR] No se pudo leer {csv_path}: {e}\")\n",
    "            _SNR_TABLE = None\n",
    "    return _SNR_TABLE\n",
    "\n",
    "def _decide_notch(subject, run, th_db=10.0):\n",
    "    df = _load_snr_table()\n",
    "    if df is None:  return 60.0\n",
    "    row = df[(df['subject']==subject) & (df['run']==run)]\n",
    "    if row.empty:   return 60.0\n",
    "    snr50 = float(row['snr50_db'].iloc[0]); snr60 = float(row['snr60_db'].iloc[0])\n",
    "    if snr60 >= th_db and snr60 >= snr50: return 60.0\n",
    "    if snr50 >= th_db and snr50 >  snr60: return 50.0\n",
    "    return None\n",
    "\n",
    "def read_raw_edf(path: Path):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        mont = mne.channels.make_standard_montage('standard_1020')\n",
    "        raw.set_montage(mont, on_missing='ignore')\n",
    "    except Exception:\n",
    "        pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad=\"auto\")\n",
    "    raw = ensure_channels_order(raw, EXPECTED_8)\n",
    "    if raw is None: return None\n",
    "\n",
    "    sid, rid = parse_subject_run(path)\n",
    "    notch = _decide_notch(sid, rid)\n",
    "    if notch is not None:\n",
    "        raw.notch_filter(freqs=[float(notch)], picks='eeg', method='spectrum_fit', phase='zero')\n",
    "\n",
    "    if USE_BANDPASS and (BP_LO is not None) and (BP_HI is not None):\n",
    "        raw.filter(l_freq=float(BP_LO), h_freq=float(BP_HI),\n",
    "                   picks='eeg', method=BP_METHOD, phase=BP_PHASE, verbose=False)\n",
    "    return raw\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    out = []\n",
    "    if raw.annotations is None or len(raw.annotations) == 0: return out\n",
    "    def _norm(s): return str(s).strip().upper().replace(' ','')\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = _norm(desc)\n",
    "        if tag in ('T1','T2'): out.append((float(onset), tag))\n",
    "    out.sort()\n",
    "    dedup = []; last_t1 = last_t2 = -1e9\n",
    "    for t, tag in out:\n",
    "        if tag == 'T1':\n",
    "            if (t - last_t1) >= 0.5: dedup.append((t, tag)); last_t1 = t\n",
    "        else:\n",
    "            if (t - last_t2) >= 0.5: dedup.append((t, tag)); last_t2 = t\n",
    "    return dedup\n",
    "\n",
    "# =========================\n",
    "# Dataset helpers\n",
    "# =========================\n",
    "def subjects_available():\n",
    "    subs = []\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try: sid = int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir / f\"S{sid:03d}R{r:02d}.edf\").exists() for r in (MI_RUNS_LR+MI_RUNS_OF))\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "def extract_base_trials_from_run(edf_path: Path, scenario: str, window_global=(0.0,4.0)):\n",
    "    subj, run = parse_subject_run(edf_path)\n",
    "    kind = run_kind(run)\n",
    "    if kind not in ('LR','OF'): return []\n",
    "\n",
    "    raw = read_raw_edf(edf_path)\n",
    "    if raw is None: return []\n",
    "    data = raw.get_data()\n",
    "    fs = raw.info['sfreq']\n",
    "    out = []\n",
    "    events = collect_events_T1T2(raw)\n",
    "    rel_start, rel_end = window_global\n",
    "    ev_idx = 0\n",
    "    for onset_sec, tag in events:\n",
    "        ev_idx += 1\n",
    "        if kind == 'LR':\n",
    "            label = 0 if tag=='T1' else 1\n",
    "        else:\n",
    "            label = 2 if tag=='T1' else 3\n",
    "        if   scenario == '2c' and label not in (0,1): continue\n",
    "        elif scenario == '3c' and label not in (0,1,2): continue\n",
    "        elif scenario == '4c' and label not in (0,1,2,3): continue\n",
    "\n",
    "        s0 = int(round((raw.first_time + onset_sec + rel_start) * fs))\n",
    "        e0 = int(round((raw.first_time + onset_sec + rel_end)   * fs))\n",
    "        if s0 < 0 or e0 > data.shape[1] or e0 <= s0: continue\n",
    "\n",
    "        seg = data[:, s0:e0].T.astype(np.float32)  # (T_base, C)\n",
    "        seg = (seg - seg.mean(axis=0, keepdims=True)) / (seg.std(axis=0, keepdims=True) + 1e-6)\n",
    "        trial_key = f\"S{subj:03d}_R{run:02d}_E{ev_idx:03d}\"\n",
    "        out.append((seg, label, subj, trial_key))\n",
    "    return out\n",
    "\n",
    "def build_balanced_trials(subjects, scenario='4c', window_global=(0.0,4.0), n_per_class=N_PER_CLASS_PER_SUBJECT):\n",
    "    trials_balanced = []\n",
    "    for s in tqdm(subjects, desc=\"Recolectando y balanceando ensayos base por sujeto\"):\n",
    "        sdir = DATA_RAW / f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "        bins = {0:[],1:[],2:[],3:[]}\n",
    "        for r in (MI_RUNS_LR + MI_RUNS_OF):\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            for t in extract_base_trials_from_run(p, scenario, window_global):\n",
    "                bins[t[1]].append(t)\n",
    "        # descarta sujeto si no tiene todas las clases\n",
    "        if any(len(bins[c]) == 0 for c in (0,1,2,3)): continue\n",
    "        rng = check_random_state(RANDOM_STATE + s)\n",
    "        for c in (0,1,2,3):\n",
    "            pool = bins[c]\n",
    "            if len(pool) >= n_per_class:\n",
    "                rng.shuffle(pool); take = pool[:n_per_class]\n",
    "            else:\n",
    "                idx = rng.choice(len(pool), size=n_per_class, replace=True)\n",
    "                take = [pool[i] for i in idx]\n",
    "            trials_balanced.extend(take)\n",
    "    print(f\"Ensayos base balanceados: {len(trials_balanced)}  (~ #sujetos_utiles * {n_per_class*4})\")\n",
    "    return trials_balanced\n",
    "\n",
    "def window_trials(trials, win_len_sec=2.0, win_stride_sec=0.5, fs=160.0):\n",
    "    \"\"\"Windowing para una lista de ensayos (de un split). Retorna X,y,subj,trial_ids.\"\"\"\n",
    "    X, y, g, tkeys = [], [], [], []\n",
    "    win_len = int(round(win_len_sec * fs))\n",
    "    stride  = int(round(win_stride_sec * fs))\n",
    "    for seg, lab, subj, tkey in trials:\n",
    "        T = seg.shape[0]\n",
    "        starts = [i for i in range(0, T - win_len + 1, stride)]\n",
    "        # asegura N_SUBWINS por ensayo\n",
    "        if len(starts) > N_SUBWINS: starts = starts[:N_SUBWINS]\n",
    "        while len(starts) < N_SUBWINS and (T - win_len) >= 0:\n",
    "            starts.append(T - win_len)\n",
    "        for s in starts:\n",
    "            e = s + win_len\n",
    "            sub = seg[s:e, :].astype(np.float32)\n",
    "            sub = (sub - sub.mean(axis=0, keepdims=True)) / (sub.std(axis=0, keepdims=True) + 1e-6)\n",
    "            X.append(sub); y.append(lab); g.append(subj); tkeys.append(tkey)\n",
    "    uniq = {k:i for i,k in enumerate(sorted(set(tkeys)))}\n",
    "    trial_ids = np.asarray([uniq[k] for k in tkeys], dtype=np.int64)\n",
    "    X = np.stack(X, axis=0).astype(np.float32)\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "    g = np.asarray(g, dtype=np.int64)\n",
    "    return X, y, g, trial_ids, uniq\n",
    "\n",
    "# =========================\n",
    "# Dataset torch\n",
    "# =========================\n",
    "class EEGTrials(Dataset):\n",
    "    def __init__(self, X, y, groups, trial_ids):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.g = groups.astype(np.int64)\n",
    "        self.t = trial_ids.astype(np.int64)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]; x = np.expand_dims(x, 0)  # (1,Tw,C)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx]), torch.tensor(self.t[idx])\n",
    "\n",
    "def build_weighted_sampler(y, groups):\n",
    "    y = np.asarray(y); groups = np.asarray(groups)\n",
    "    class_counts = np.bincount(y, minlength=len(np.unique(y))).astype(float)\n",
    "    class_w = 1.0 / class_counts[y]\n",
    "    subj_vals, subj_counts = np.unique(groups, return_counts=True)\n",
    "    subj_map = {s:c for s,c in zip(subj_vals, subj_counts)}\n",
    "    subj_w = np.array([1.0/subj_map[g] for g in groups], dtype=float)\n",
    "    w = class_w * subj_w; w = w / w.mean()\n",
    "    return WeightedRandomSampler(weights=torch.from_numpy(w).float(), num_samples=len(w), replacement=True)\n",
    "\n",
    "# =========================\n",
    "# Modelo: CNN + Transformer\n",
    "# =========================\n",
    "class ChannelDropout(nn.Module):\n",
    "    def __init__(self, p=0.1): super().__init__(); self.p = p\n",
    "    def forward(self, x):\n",
    "        if not self.training or self.p<=0: return x\n",
    "        B,_,T,C = x.shape\n",
    "        mask = (torch.rand(B,1,1,C, device=x.device) > self.p).float()\n",
    "        return x * mask\n",
    "\n",
    "@torch.no_grad()\n",
    "def apply_max_norm(model, max_value=2.0, p=2.0):\n",
    "    for m in model.modules():\n",
    "        if hasattr(m,'weight') and isinstance(m,(nn.Conv2d, nn.Linear)):\n",
    "            w = m.weight.data\n",
    "            norms = w.view(w.size(0), -1).norm(p=p, dim=1, keepdim=True)\n",
    "            desired = torch.clamp(norms, max=max_value)\n",
    "            w.view(w.size(0), -1).mul_(desired / (1e-8 + norms))\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 4000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0), persistent=False)\n",
    "    def forward(self, x):\n",
    "        T = x.size(1); return x + self.pe[:, :T, :]\n",
    "\n",
    "class CNNTokenizer(nn.Module):\n",
    "    def __init__(self, n_ch: int, d_feat: int = 128, k_temporal: int = 128, stride_t: int = 4, chdrop_p: float = 0.1, drop_p: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.chdrop = ChannelDropout(chdrop_p)\n",
    "        self.conv_t = nn.Conv2d(1, d_feat, kernel_size=(k_temporal,1), stride=(stride_t,1), padding=(k_temporal//2,0), bias=False)\n",
    "        self.bn_t   = nn.BatchNorm2d(d_feat, momentum=0.99, eps=1e-3)\n",
    "        self.act    = nn.ELU()\n",
    "        self.conv_sp_dw = nn.Conv2d(d_feat, d_feat, kernel_size=(1,n_ch), groups=d_feat, bias=False)\n",
    "        self.bn_sp      = nn.BatchNorm2d(d_feat, momentum=0.99, eps=1e-3)\n",
    "        self.conv_pw = nn.Conv2d(d_feat, d_feat, kernel_size=(1,1), bias=False)\n",
    "        self.bn_pw   = nn.BatchNorm2d(d_feat, momentum=0.99, eps=1e-3)\n",
    "        self.drop    = nn.Dropout(drop_p)\n",
    "    def forward(self, x):\n",
    "        z = self.chdrop(x)\n",
    "        z = self.conv_t(z); z = self.bn_t(z); z = self.act(z)         # (B,D,T',C)\n",
    "        z = self.conv_sp_dw(z); z = self.bn_sp(z); z = self.act(z)    # (B,D,T',1)\n",
    "        z = self.conv_pw(z); z = self.bn_pw(z); z = self.act(z)\n",
    "        z = self.drop(z)\n",
    "        z = z.squeeze(-1).transpose(1,2)  # (B,T',D)\n",
    "        return z\n",
    "\n",
    "def _enc_layer(d_model=128, nhead=4, dim_ff=256, drop=0.2):\n",
    "    return nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_ff,\n",
    "                                      dropout=drop, activation='gelu', batch_first=True, norm_first=True)\n",
    "\n",
    "class TemporalTransformer(nn.Module):\n",
    "    def __init__(self, d_model=128, nhead=4, dim_ff=256, depth=3, drop=0.2):\n",
    "        super().__init__()\n",
    "        self.pe = PositionalEncoding(d_model)\n",
    "        layer = _enc_layer(d_model, nhead, dim_ff, drop)\n",
    "        self.enc = nn.TransformerEncoder(layer, num_layers=depth)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    def forward(self, x):\n",
    "        x = self.pe(x)\n",
    "        x = self.enc(x)\n",
    "        return self.norm(x)\n",
    "\n",
    "class HybridCNNTransformer(nn.Module):\n",
    "    def __init__(self, n_ch: int, n_classes: int, seq_len: int,\n",
    "                 d_model: int = 128, nhead: int = 4, depth: int = 3,\n",
    "                 dim_ff: int = 256, k_temporal: int = 128, stride_t: int = 4,\n",
    "                 drop: float = 0.2, chdrop_p: float = 0.1, use_cls_token: bool = False):\n",
    "        super().__init__()\n",
    "        self.use_cls = use_cls_token\n",
    "        self.tokenizer = CNNTokenizer(n_ch, d_model, k_temporal, stride_t, chdrop_p, drop)\n",
    "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) if use_cls_token else None\n",
    "        self.transformer = TemporalTransformer(d_model, nhead, dim_ff, depth, drop)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(d_model, n_classes)\n",
    "        )\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tok = self.tokenizer(x)                   # (B,T',D)\n",
    "        if self.use_cls:\n",
    "            B = tok.size(0); cls_tok = self.cls.expand(B,-1,-1)\n",
    "            tok = torch.cat([cls_tok, tok], dim=1)\n",
    "        z = self.transformer(tok)\n",
    "        feat = z[:,0,:] if self.use_cls else z.mean(dim=1)\n",
    "        return self.head(feat)\n",
    "\n",
    "# =========================\n",
    "# Entrenamiento / Eval\n",
    "# =========================\n",
    "class SoftCE(nn.Module):\n",
    "    def __init__(self, n_classes: int, label_smoothing: float = 0.05, class_weights: torch.Tensor | None = None):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.ls = float(label_smoothing)\n",
    "        self.register_buffer('w', class_weights if class_weights is not None else None)\n",
    "    def forward(self, logits, targets_idx):\n",
    "        B, C = logits.size()\n",
    "        yt = F.one_hot(targets_idx, num_classes=C).float()\n",
    "        if self.ls > 0: yt = (1 - self.ls) * yt + self.ls * (1.0 / C)\n",
    "        logp = F.log_softmax(logits, dim=1)\n",
    "        loss = -(yt * logp)\n",
    "        if self.w is not None: loss = loss * self.w.unsqueeze(0)\n",
    "        return loss.sum(dim=1).mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def _predict_tta(model, xb, n=5, fs=160.0):\n",
    "    def time_jitter(x, max_ms=25):\n",
    "        max_shift = int(round(max_ms/1000.0 * fs))\n",
    "        if max_shift <= 0: return x\n",
    "        B,_,T,C = x.shape\n",
    "        shifts = torch.randint(low=-max_shift, high=max_shift+1, size=(B,), device=x.device)\n",
    "        out = torch.zeros_like(x)\n",
    "        for i,s in enumerate(shifts):\n",
    "            if s >= 0: out[i,:,s:,:] = x[i,:,:T-s,:]\n",
    "            else: s=-s; out[i,:,:T-s,:] = x[i,:,s:,:]\n",
    "        return out\n",
    "    outs = []\n",
    "    for _ in range(n):\n",
    "        outs.append(model(time_jitter(xb,25)))\n",
    "    return torch.stack(outs, dim=0).mean(dim=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_per_trial(model, loader, use_tta=True, tta_n=5, n_classes=4):\n",
    "    model.eval()\n",
    "    sums, counts, labels = {}, {}, {}\n",
    "    for xb, yb, _, tb in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = _predict_tta(model, xb, n=tta_n) if use_tta else model(xb)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        yb = yb.numpy(); tb = tb.numpy()\n",
    "        for i in range(xb.size(0)):\n",
    "            t = int(tb[i])\n",
    "            if t not in sums:\n",
    "                sums[t] = np.zeros((n_classes,), dtype=np.float64)\n",
    "                counts[t] = 0\n",
    "                labels[t] = int(yb[i])\n",
    "            sums[t] += logits[i]; counts[t] += 1\n",
    "    ids = sorted(sums.keys())\n",
    "    y_true_trial = np.asarray([labels[t] for t in ids], dtype=int)\n",
    "    y_pred_trial = np.asarray([int((sums[t]/max(1,counts[t])).argmax()) for t in ids], dtype=int)\n",
    "    acc = (y_true_trial == y_pred_trial).mean()\n",
    "    return y_true_trial, y_pred_trial, float(acc)\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes, title, fname):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.nan_to_num(cm_norm)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title); plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    ticks = np.arange(len(classes))\n",
    "    plt.xticks(ticks, classes, rotation=45, ha='right'); plt.yticks(ticks, classes)\n",
    "    fmt = '.2f'; thresh = cm_norm.max()/2.\n",
    "    for i,j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "        plt.text(j,i,format(cm_norm[i,j],fmt),ha=\"center\", color=\"white\" if cm_norm[i,j]>thresh else \"black\")\n",
    "    plt.ylabel('True'); plt.xlabel('Pred'); plt.tight_layout(); plt.savefig(fname, dpi=150); plt.close()\n",
    "\n",
    "def print_report(y_true, y_pred, classes, tag=\"\"):\n",
    "    print(f\"\\n=== Reporte {tag} ===\")\n",
    "    print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
    "    print(f\"F1-macro: {f1_score(y_true, y_pred, average='macro'):.4f} | Kappa: {cohen_kappa_score(y_true,y_pred):.4f}\")\n",
    "\n",
    "# =========================\n",
    "# FT (L2-SP), SIN fuga (agrupa por ensayo)\n",
    "# =========================\n",
    "def _freeze_for_mode(model: HybridCNNTransformer, mode: str):\n",
    "    for p in model.parameters(): p.requires_grad = False\n",
    "    if   mode=='out':\n",
    "        for p in model.head[-1].parameters(): p.requires_grad = True\n",
    "    elif mode=='head':\n",
    "        for p in model.head.parameters(): p.requires_grad = True\n",
    "    elif mode=='cnn+trans+head':\n",
    "        for p in model.tokenizer.parameters():   p.requires_grad = True\n",
    "        for p in model.transformer.parameters(): p.requires_grad = True\n",
    "        for p in model.head.parameters():        p.requires_grad = True\n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "\n",
    "def _param_groups(model: HybridCNNTransformer, mode: str):\n",
    "    if   mode=='out': return list(model.head[-1].parameters())\n",
    "    elif mode=='head': return list(model.head.parameters())\n",
    "    elif mode=='cnn+trans+head': \n",
    "        return list(model.tokenizer.parameters()) + list(model.transformer.parameters()) + list(model.head.parameters())\n",
    "\n",
    "def finetune_l2sp(model_global: HybridCNNTransformer, Xcal, ycal, trial_ids_cal,\n",
    "                  n_classes: int, mode='head', epochs=FT_EPOCHS, batch_size=32,\n",
    "                  base_lr=FT_BASE_LR, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                  val_ratio=FT_VAL_RATIO, seed=RANDOM_STATE, device=DEVICE):\n",
    "    # split interno sin fuga: por ensayo (GroupShuffleSplit)\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=val_ratio, random_state=seed)\n",
    "    (tr_idx, va_idx), = gss.split(Xcal, ycal, groups=trial_ids_cal)\n",
    "    Xtr, ytr = Xcal[tr_idx], ycal[tr_idx]\n",
    "    Xva, yva = Xcal[va_idx], ycal[va_idx]\n",
    "\n",
    "    ds_tr = torch.utils.data.TensorDataset(torch.from_numpy(Xtr).float().unsqueeze(1),\n",
    "                                           torch.from_numpy(ytr).long())\n",
    "    ds_va = torch.utils.data.TensorDataset(torch.from_numpy(Xva).float().unsqueeze(1),\n",
    "                                           torch.from_numpy(yva).long())\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    model = copy.deepcopy(model_global).to(device)\n",
    "    _freeze_for_mode(model, mode)\n",
    "    params = _param_groups(model, mode)\n",
    "\n",
    "    if mode=='cnn+trans+head':\n",
    "        tok_params = list(model.tokenizer.parameters()) + list(model.transformer.parameters())\n",
    "        head_params = list(model.head.parameters())\n",
    "        opt = torch.optim.Adam([\n",
    "            {'params': tok_params,  'lr': base_lr},\n",
    "            {'params': head_params, 'lr': head_lr},\n",
    "        ])\n",
    "        train_params = tok_params + head_params\n",
    "    else:\n",
    "        opt = torch.optim.Adam(params, lr=head_lr)\n",
    "        train_params = params\n",
    "\n",
    "    ref = [p.detach().clone().to(device) for p in train_params]   # L2-SP ref\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_state, best_val, bad = copy.deepcopy(model.state_dict()), float('inf'), 0\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            reg = 0.0\n",
    "            for p_cur, p_ref in zip(train_params, ref):\n",
    "                reg = reg + torch.sum((p_cur - p_ref)**2)\n",
    "            (loss + l2sp_lambda*reg).backward(); opt.step()\n",
    "            apply_max_norm(model, 2.0, p=2.0)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, nval = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                val_loss += F.cross_entropy(model(xb), yb).item() * xb.size(0)\n",
    "                nval += xb.size(0)\n",
    "        val_loss /= max(1,nval)\n",
    "        if val_loss + 1e-7 < best_val: best_val, bad, best_state = val_loss, 0, copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= FT_PATIENCE: break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# Folds JSON: SOLO sujetos (ignora índices)\n",
    "# =========================\n",
    "def read_folds_subjects_only(json_path, current_subjects):\n",
    "    \"\"\"\n",
    "    Lee Kfold5.json y devuelve lista de folds con train_subjects/test_subjects (enteros).\n",
    "    Ignora por completo tr_idx/te_idx si existen.\n",
    "    Valida que los sujetos existan en el dataset actual.\n",
    "    \"\"\"\n",
    "    p = Path(json_path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"No existe {p}. Debes proporcionar un JSON con sujetos por fold.\")\n",
    "    with open(p, 'r', encoding='utf-8') as f:\n",
    "        payload = json.load(f)\n",
    "    folds_raw = payload.get('folds', [])\n",
    "    if not folds_raw:\n",
    "        raise ValueError(\"JSON no contiene 'folds'.\")\n",
    "\n",
    "    cur_set = set(current_subjects)\n",
    "    folds = []\n",
    "    for fr in folds_raw:\n",
    "        train_s = fr.get('train_subjects', fr.get('train', []))\n",
    "        test_s  = fr.get('test_subjects',  fr.get('test',  []))\n",
    "        if not train_s or not test_s:\n",
    "            raise ValueError(\"Cada fold debe tener 'train_subjects' y 'test_subjects' (o 'train'/'test').\")\n",
    "\n",
    "        def to_int_list(lst):\n",
    "            out = []\n",
    "            for s in lst:\n",
    "                if isinstance(s, int): out.append(s)\n",
    "                elif isinstance(s, str) and s.upper().startswith('S') and s[1:].isdigit():\n",
    "                    out.append(int(s[1:]))\n",
    "                elif isinstance(s, str) and s.isdigit():\n",
    "                    out.append(int(s))\n",
    "                else:\n",
    "                    raise ValueError(f\"Sujeto inválido en JSON: {s}\")\n",
    "            return out\n",
    "\n",
    "        tr_int = to_int_list(train_s)\n",
    "        te_int = to_int_list(test_s)\n",
    "\n",
    "        # validación: sujetos del JSON deben existir\n",
    "        miss_tr = [s for s in tr_int if s not in cur_set]\n",
    "        miss_te = [s for s in te_int if s not in cur_set]\n",
    "        if miss_tr or miss_te:\n",
    "            raise ValueError(f\"Fold con sujetos no presentes en dataset actual. \"\n",
    "                             f\"Faltan en TRAIN: {miss_tr}, Faltan en TEST: {miss_te}\")\n",
    "\n",
    "        folds.append({'fold': fr.get('fold', len(folds)+1),\n",
    "                      'train_subjects': tr_int, 'test_subjects': te_int})\n",
    "    return folds\n",
    "\n",
    "# =========================\n",
    "# Experimento\n",
    "# =========================\n",
    "def plot_training_curves(history, fname):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(history['train_acc'], label='train_acc')\n",
    "    plt.plot(history['val_acc'], label='val_acc')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Training curve'); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(fname, dpi=150); plt.close()\n",
    "\n",
    "def run_experiment():\n",
    "    mne.set_log_level('WARNING')\n",
    "    subs = subjects_available()\n",
    "    print(f\"Sujetos elegibles: {len(subs)} → {subs[:10]}{'...' if len(subs)>10 else ''}\")\n",
    "\n",
    "    # 1) Ensayos balanceados (21/clase/sujeto) sobre TODOS los sujetos elegibles\n",
    "    trials_bal = build_balanced_trials(subs, scenario=CLASS_SCENARIO, window_global=WINDOW_GLOBAL,\n",
    "                                       n_per_class=N_PER_CLASS_PER_SUBJECT)\n",
    "\n",
    "    # Sujetos presentes tras balanceo\n",
    "    subj_in_trials = sorted({t[2] for t in trials_bal})\n",
    "    print(f\"Sujetos con ensayos balanceados: {len(subj_in_trials)}\")\n",
    "\n",
    "    # 2) Leer folds SOLO por sujetos\n",
    "    folds = read_folds_subjects_only(FOLDS_JSON, current_subjects=subj_in_trials)\n",
    "\n",
    "    # 3) Loop de folds\n",
    "    global_folds_trial, all_true_trial, all_pred_trial, ft_prog_folds = [], [], [], []\n",
    "\n",
    "    for f in folds:\n",
    "        fold = f['fold']\n",
    "        train_subs = set(f['train_subjects'])\n",
    "        test_subs  = set(f['test_subjects'])\n",
    "\n",
    "        # Split por sujeto (antes de windowing): seleccionamos ensayos (trial-level)\n",
    "        tr_trials = [t for t in trials_bal if t[2] in train_subs]\n",
    "        te_trials = [t for t in trials_bal if t[2] in test_subs]\n",
    "\n",
    "        # Sanity: cada clase presente en TEST por ensayo (antes de windowing)\n",
    "        lbl_te = [t[1] for t in te_trials]\n",
    "        present = {0,1,2,3}.intersection(set(lbl_te))\n",
    "        if present != {0,1,2,3}:\n",
    "            print(f\"[WARN] Fold {fold}: clases en TEST (trial-level) = {sorted(present)}; se salta este fold.\")\n",
    "            continue\n",
    "\n",
    "        # 3a) Split interno de VALID por sujeto dentro de TRAIN (sin fuga de sujetos)\n",
    "        train_subs_list = sorted(list(train_subs))\n",
    "        rng = np.random.RandomState(RANDOM_STATE + fold)\n",
    "        rng.shuffle(train_subs_list)\n",
    "        n_va = max(1, int(round(GLOBAL_VAL_SPLIT * len(train_subs_list))))\n",
    "        val_subs = set(train_subs_list[:n_va])\n",
    "        real_train_subs = set(train_subs_list[n_va:])\n",
    "\n",
    "        tr_trials_main = [t for t in tr_trials if t[2] in real_train_subs]\n",
    "        va_trials      = [t for t in tr_trials if t[2] in val_subs]\n",
    "\n",
    "        # 4) Windowing por split (sin fuga entre ensayos)\n",
    "        Xtr, ytr, gtr, ttr, _ = window_trials(trials=tr_trials_main, win_len_sec=WIN_LEN_SEC, win_stride_sec=WIN_STRIDE_SEC, fs=FS)\n",
    "        Xva, yva, gva, tva, _ = window_trials(trials=va_trials,      win_len_sec=WIN_LEN_SEC, win_stride_sec=WIN_STRIDE_SEC, fs=FS)\n",
    "        Xte, yte, gte, tte, _ = window_trials(trials=te_trials,      win_len_sec=WIN_LEN_SEC, win_stride_sec=WIN_STRIDE_SEC, fs=FS)\n",
    "\n",
    "        # DataLoaders\n",
    "        ds_tr = EEGTrials(Xtr, ytr, gtr, ttr)\n",
    "        ds_va = EEGTrials(Xva, yva, gva, tva)\n",
    "        ds_te = EEGTrials(Xte, yte, gte, tte)\n",
    "\n",
    "        tr_loader = DataLoader(ds_tr, batch_size=BATCH_SIZE,\n",
    "                               sampler=build_weighted_sampler(ytr, gtr), drop_last=False)\n",
    "        va_loader = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "        te_loader = DataLoader(ds_te, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "        # 5) Modelo\n",
    "        Tw, C = Xtr.shape[1], Xtr.shape[2]\n",
    "        n_classes = len(set(ytr.tolist()) | set(yva.tolist()) | set(yte.tolist()))\n",
    "        model = HybridCNNTransformer(\n",
    "            n_ch=C, n_classes=n_classes, seq_len=Tw,\n",
    "            d_model=128, nhead=4, depth=3, dim_ff=256,\n",
    "            k_temporal=128, stride_t=4, drop=0.2, chdrop_p=0.10, use_cls_token=False\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=LR_INIT)\n",
    "        criterion = SoftCE(n_classes=n_classes, label_smoothing=0.05, class_weights=None)\n",
    "\n",
    "        def _acc_trial(loader): return evaluate_per_trial(model, loader, use_tta=False, n_classes=n_classes)[2]\n",
    "\n",
    "        # 6) Entrenamiento (early stopping por ENSAYO)\n",
    "        history = {'train_acc': [], 'val_acc': []}\n",
    "        print(f\"\\n[Fold {fold}/{len(folds)}] Entrenando global (VAL por ensayo)...\")\n",
    "        best_state = copy.deepcopy(model.state_dict()); best_val = -1.0; bad = 0\n",
    "\n",
    "        for epoch in range(1, EPOCHS_GLOBAL+1):\n",
    "            model.train()\n",
    "            for xb, yb, _, _ in tr_loader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                loss = criterion(model(xb), yb)\n",
    "                opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "                apply_max_norm(model, 2.0, p=2.0)\n",
    "\n",
    "            tr_acc = _acc_trial(tr_loader)\n",
    "            va_acc = _acc_trial(va_loader)\n",
    "            history['train_acc'].append(tr_acc); history['val_acc'].append(va_acc)\n",
    "\n",
    "            if epoch % 5 == 0 or epoch in (1,10,20,50,80):\n",
    "                print(f\"  Época {epoch:3d} | train(ensayo)={tr_acc:.4f} | val(ensayo)={va_acc:.4f}\")\n",
    "\n",
    "            if va_acc > best_val + 1e-4:\n",
    "                best_val, bad = va_acc, 0\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "            else:\n",
    "                bad += 1\n",
    "                if bad >= GLOBAL_PATIENCE:\n",
    "                    print(f\"  Early stopping @ epoch {epoch} (best val ensayo={best_val:.4f})\")\n",
    "                    break\n",
    "\n",
    "        plot_training_curves(history, f\"training_curve_cnnTrans_subjFolds_fold{fold}.png\")\n",
    "        print(f\"↳ Curva: training_curve_cnnTrans_subjFolds_fold{fold}.png\")\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "        # 7) Test por ENSAYO (TTA)\n",
    "        y_true_tr, y_pred_tr, acc_tr = evaluate_per_trial(model, te_loader, use_tta=True, tta_n=5, n_classes=n_classes)\n",
    "        global_folds_trial.append(acc_tr)\n",
    "        all_true_trial.append(y_true_tr); all_pred_trial.append(y_pred_tr)\n",
    "        print(f\"[Fold {fold}] Ensayo acc={acc_tr:.4f}\")\n",
    "        print_report(y_true_tr, y_pred_tr, CLASS_NAMES, tag=\"por ENSAYO (agregado)\")\n",
    "\n",
    "        # 8) FT por sujeto (sin fuga, agrupando por ensayo)\n",
    "        #    Trabajamos dentro del set de TEST (por cada sujeto en test_subs)\n",
    "        y_true_ft_all, y_pred_ft_all, used_subjects = [], [], 0\n",
    "\n",
    "        # Para comodidad, indices por sujeto dentro de Xte\n",
    "        trial_to_label = {}\n",
    "        for idx in range(len(Xte)):\n",
    "            trial_to_label[int(tte[idx])] = int(yte[idx])\n",
    "\n",
    "        for sid in sorted(test_subs):\n",
    "            mask_sid = (gte == sid)\n",
    "            if mask_sid.sum() == 0: continue\n",
    "            Xs, ys, ts = Xte[mask_sid], yte[mask_sid], tte[mask_sid]\n",
    "\n",
    "            # GroupKFold por ensayo (sin fuga)\n",
    "            if len(np.unique(ts)) < CALIB_CV_FOLDS:\n",
    "                continue\n",
    "            gkf = GroupKFold(n_splits=CALIB_CV_FOLDS)\n",
    "            for tr_i, ho_i in gkf.split(Xs, ys, groups=ts):\n",
    "                Xcal, ycal, tcal = Xs[tr_i], ys[tr_i], ts[tr_i]\n",
    "                Xho,  yho,  tho  = Xs[ho_i], ys[ho_i], ts[ho_i]\n",
    "\n",
    "                m_head = finetune_l2sp(model, Xcal, ycal, tcal,\n",
    "                                       n_classes=n_classes, mode='head',\n",
    "                                       epochs=FT_EPOCHS, base_lr=FT_BASE_LR, head_lr=FT_HEAD_LR,\n",
    "                                       l2sp_lambda=FT_L2SP, val_ratio=FT_VAL_RATIO, device=DEVICE)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    xb = torch.from_numpy(Xho).float().unsqueeze(1).to(DEVICE)\n",
    "                    logits = m_head(xb).cpu().numpy()\n",
    "\n",
    "                sums = collections.defaultdict(lambda: np.zeros((n_classes,), dtype=np.float64))\n",
    "                counts = collections.defaultdict(int)\n",
    "                labels = {}\n",
    "                for i in range(len(Xho)):\n",
    "                    t = int(tho[i]); sums[t] += logits[i]; counts[t] += 1; labels[t] = int(yho[i])\n",
    "                for t in sums.keys():\n",
    "                    y_pred_ft_all.append(int((sums[t]/max(1,counts[t])).argmax()))\n",
    "                    y_true_ft_all.append(labels[t])\n",
    "                used_subjects += 1\n",
    "\n",
    "        if len(y_true_ft_all) > 0:\n",
    "            y_true_ft_all = np.asarray(y_true_ft_all, dtype=int)\n",
    "            y_pred_ft_all = np.asarray(y_pred_ft_all, dtype=int)\n",
    "            acc_ft = (y_true_ft_all == y_pred_ft_all).mean()\n",
    "            print(f\"  FT progresivo por ENSAYO acc={acc_ft:.4f} | sujetos={used_subjects}\")\n",
    "            print(f\"  Δ(FT-Ensayo - Global-Ensayo) = {acc_ft - acc_tr:+.4f}\")\n",
    "        else:\n",
    "            acc_ft = np.nan\n",
    "            print(\"  FT no ejecutado (fold sin sujetos aptos).\")\n",
    "        ft_prog_folds.append(acc_ft)\n",
    "\n",
    "    # ----- resultados globales -----\n",
    "    if len(all_true_trial)>0: all_true_trial = np.concatenate(all_true_trial); all_pred_trial = np.concatenate(all_pred_trial)\n",
    "    else: all_true_trial = np.array([],dtype=int); all_pred_trial = np.array([],dtype=int)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTADOS FINALES — CNN+Transformer (21/cls/subj) + Sliding Window + Kfold SUBJECTS-only\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Ensayo folds:\", [f\"{a:.4f}\" for a in global_folds_trial])\n",
    "    if len(global_folds_trial) > 0:\n",
    "        print(f\"Ensayo mean: {np.mean(global_folds_trial):.4f}\")\n",
    "    print(\"FT prog (ensayo) folds:\", [(\"nan\" if (a is None or np.isnan(a)) else f\"{a:.4f}\") for a in ft_prog_folds])\n",
    "    if len(ft_prog_folds) > 0:\n",
    "        print(f\"FT (ensayo) mean: {np.nanmean(ft_prog_folds):.4f}\")\n",
    "        print(f\"Δ(FT-Ensayo - Global-Ensayo) mean: {np.nanmean(ft_prog_folds) - np.mean(global_folds_trial):+.4f}\")\n",
    "\n",
    "    if all_true_trial.size > 0:\n",
    "        plot_confusion(all_true_trial, all_pred_trial, CLASS_NAMES,\n",
    "                       title=\"Confusion Matrix - Global Model (Per Trial, All Folds)\",\n",
    "                       fname=\"confusion_cnnTrans_subjectFolds_trial_allfolds.png\")\n",
    "        print(\"↳ Matriz de confusión (ensayo): confusion_cnnTrans_subjectFolds_trial_allfolds.png\")\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    bp_status = f\"ON [{BP_LO}-{BP_HI} Hz]\" if USE_BANDPASS and (BP_LO is not None) and (BP_HI is not None) else \"OFF\"\n",
    "    print(\"🧠 MI-EEG — CNN+Transformer (21/cls/subj) + Sliding Window + FT progresivo\")\n",
    "    print(f\"🔧 Escenario: {CLASS_SCENARIO} | rango ensayo={WINDOW_GLOBAL} s | subventana={WIN_LEN_SEC}s | stride={WIN_STRIDE_SEC}s | subwins/ensayo={N_SUBWINS} | band-pass={bp_status}\")\n",
    "    print(f\"⚙️  Balance: 21 ensayos por clase y por sujeto (con reemplazo si falta)\")\n",
    "    print(f\"⚙️  FT (sin fuga): GroupKFold por ensayo + GroupShuffleSplit en validación interna\")\n",
    "    run_experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "795128f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Dispositivo: cuda\n",
      "🧠 MI-EEG — CNN+Transformer (21/cls/subj) + Sliding Window + Cosine head + FT opcional\n",
      "🔧 Escenario: 4c | rango ensayo=(0.0, 4.0) s | subventana=2.5s | stride=0.5s | subwins/ensayo=4 | band-pass=OFF\n",
      "⚙️  Balance: 21 ensayos por clase y por sujeto (con reemplazo si falta)\n",
      "⚙️  FT strategy: none_bn_protos\n",
      "Sujetos elegibles: 103 → [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recolectando y balanceando ensayos base por sujeto: 100%|██████████| 103/103 [00:37<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensayos base balanceados: 8652  (~ #sujetos_utiles * 84)\n",
      "Sujetos con ensayos balanceados: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold 1/5] Entrenando global (VAL por ensayo)...\n",
      "  Época   1 | train(ensayo)=0.3952 | val(ensayo)=0.4147\n",
      "  Época   5 | train(ensayo)=0.4409 | val(ensayo)=0.4534\n",
      "  Época  10 | train(ensayo)=0.5419 | val(ensayo)=0.4425\n",
      "  Época  15 | train(ensayo)=0.6056 | val(ensayo)=0.4435\n",
      "  Early stopping @ epoch 19 (best val ensayo=0.4583)\n",
      "↳ Curva: training_curve_cnnTrans_cosine_fold1.png\n",
      "[Fold 1] Ensayo acc=0.3475\n",
      "\n",
      "=== Reporte por ENSAYO (agregado) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.4111    0.2676    0.3242       441\n",
      "       Right     0.4150    0.2766    0.3320       441\n",
      "  Both Fists     0.2922    0.6440    0.4020       441\n",
      "   Both Feet     0.4218    0.2018    0.2730       441\n",
      "\n",
      "    accuracy                         0.3475      1764\n",
      "   macro avg     0.3850    0.3475    0.3328      1764\n",
      "weighted avg     0.3850    0.3475    0.3328      1764\n",
      "\n",
      "F1-macro: 0.3328 | Kappa: 0.1300\n",
      "  FT (none_bn_protos) por ENSAYO acc=0.3832 | sujetos=84\n",
      "  Δ(FT - Global) = +0.0357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold 2/5] Entrenando global (VAL por ensayo)...\n",
      "  Época   1 | train(ensayo)=0.3844 | val(ensayo)=0.3690\n",
      "  Época   5 | train(ensayo)=0.4420 | val(ensayo)=0.3998\n",
      "  Época  10 | train(ensayo)=0.5009 | val(ensayo)=0.4315\n",
      "  Época  15 | train(ensayo)=0.5718 | val(ensayo)=0.4206\n",
      "  Early stopping @ epoch 16 (best val ensayo=0.4335)\n",
      "↳ Curva: training_curve_cnnTrans_cosine_fold2.png\n",
      "[Fold 2] Ensayo acc=0.4359\n",
      "\n",
      "=== Reporte por ENSAYO (agregado) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.4617    0.3832    0.4188       441\n",
      "       Right     0.4394    0.6327    0.5186       441\n",
      "  Both Fists     0.4519    0.2766    0.3432       441\n",
      "   Both Feet     0.4037    0.4512    0.4261       441\n",
      "\n",
      "    accuracy                         0.4359      1764\n",
      "   macro avg     0.4392    0.4359    0.4267      1764\n",
      "weighted avg     0.4392    0.4359    0.4267      1764\n",
      "\n",
      "F1-macro: 0.4267 | Kappa: 0.2479\n",
      "  FT (none_bn_protos) por ENSAYO acc=0.4348 | sujetos=84\n",
      "  Δ(FT - Global) = -0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold 3/5] Entrenando global (VAL por ensayo)...\n",
      "  Época   1 | train(ensayo)=0.3501 | val(ensayo)=0.3581\n",
      "  Época   5 | train(ensayo)=0.4347 | val(ensayo)=0.3800\n",
      "  Época  10 | train(ensayo)=0.5297 | val(ensayo)=0.4216\n",
      "  Época  15 | train(ensayo)=0.6162 | val(ensayo)=0.3998\n",
      "  Early stopping @ epoch 16 (best val ensayo=0.4306)\n",
      "↳ Curva: training_curve_cnnTrans_cosine_fold3.png\n",
      "[Fold 3] Ensayo acc=0.4019\n",
      "\n",
      "=== Reporte por ENSAYO (agregado) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.4363    0.4739    0.4543       441\n",
      "       Right     0.5306    0.2948    0.3790       441\n",
      "  Both Fists     0.3069    0.3855    0.3417       441\n",
      "   Both Feet     0.4115    0.4535    0.4315       441\n",
      "\n",
      "    accuracy                         0.4019      1764\n",
      "   macro avg     0.4213    0.4019    0.4016      1764\n",
      "weighted avg     0.4213    0.4019    0.4016      1764\n",
      "\n",
      "F1-macro: 0.4016 | Kappa: 0.2026\n",
      "  FT (none_bn_protos) por ENSAYO acc=0.4150 | sujetos=84\n",
      "  Δ(FT - Global) = +0.0130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold 4/5] Entrenando global (VAL por ensayo)...\n",
      "  Época   1 | train(ensayo)=0.3516 | val(ensayo)=0.2877\n",
      "  Época   5 | train(ensayo)=0.4616 | val(ensayo)=0.3938\n",
      "  Época  10 | train(ensayo)=0.5371 | val(ensayo)=0.3730\n",
      "  Época  15 | train(ensayo)=0.5627 | val(ensayo)=0.3522\n",
      "  Early stopping @ epoch 15 (best val ensayo=0.3938)\n",
      "↳ Curva: training_curve_cnnTrans_cosine_fold4.png\n",
      "[Fold 4] Ensayo acc=0.4286\n",
      "\n",
      "=== Reporte por ENSAYO (agregado) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.4252    0.6095    0.5010       420\n",
      "       Right     0.3945    0.5833    0.4707       420\n",
      "  Both Fists     0.3925    0.1000    0.1594       420\n",
      "   Both Feet     0.5057    0.4214    0.4597       420\n",
      "\n",
      "    accuracy                         0.4286      1680\n",
      "   macro avg     0.4295    0.4286    0.3977      1680\n",
      "weighted avg     0.4295    0.4286    0.3977      1680\n",
      "\n",
      "F1-macro: 0.3977 | Kappa: 0.2381\n",
      "  FT (none_bn_protos) por ENSAYO acc=0.4375 | sujetos=80\n",
      "  Δ(FT - Global) = +0.0089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold 5/5] Entrenando global (VAL por ensayo)...\n",
      "  Época   1 | train(ensayo)=0.3258 | val(ensayo)=0.2946\n",
      "  Época   5 | train(ensayo)=0.4639 | val(ensayo)=0.3879\n",
      "  Época  10 | train(ensayo)=0.5121 | val(ensayo)=0.3700\n",
      "  Early stopping @ epoch 13 (best val ensayo=0.4008)\n",
      "↳ Curva: training_curve_cnnTrans_cosine_fold5.png\n",
      "[Fold 5] Ensayo acc=0.4506\n",
      "\n",
      "=== Reporte por ENSAYO (agregado) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.4852    0.4690    0.4770       420\n",
      "       Right     0.4472    0.5548    0.4952       420\n",
      "  Both Fists     0.4080    0.3167    0.3566       420\n",
      "   Both Feet     0.4543    0.4619    0.4581       420\n",
      "\n",
      "    accuracy                         0.4506      1680\n",
      "   macro avg     0.4487    0.4506    0.4467      1680\n",
      "weighted avg     0.4487    0.4506    0.4467      1680\n",
      "\n",
      "F1-macro: 0.4467 | Kappa: 0.2675\n",
      "  FT (none_bn_protos) por ENSAYO acc=0.4804 | sujetos=80\n",
      "  Δ(FT - Global) = +0.0298\n",
      "\n",
      "============================================================\n",
      "RESULTADOS FINALES — CNN+Transformer + Cosine head + (none_bn_protos)\n",
      "============================================================\n",
      "Ensayo folds: ['0.3475', '0.4359', '0.4019', '0.4286', '0.4506']\n",
      "Ensayo mean: 0.4129\n",
      "FT prog (ensayo) folds: ['0.3832', '0.4348', '0.4150', '0.4375', '0.4804']\n",
      "Δ(FT - Global) mean: +0.0173\n",
      "↳ Matriz de confusión (ensayo): confusion_cnnTrans_cosine_trial_allfolds.png\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ===========================================\n",
    "# MI-EEG (PhysioNet eegmmidb) — CNN+Transformer (Balanced 21/cls/subj) + Sliding Window\n",
    "# Patch: usar Kfold5.json SOLO para sujetos por fold (ignora tr_idx/te_idx)\n",
    "# FT opcional:\n",
    "#   - \"none_bn_protos\": BN-Adapt + prototipos (sin entrenamiento)\n",
    "#   - \"head_l2sp\": fine-tuning de la cabeza con L2-SP\n",
    "# Cosine head con temperatura aprendible\n",
    "# Tokenización densa (k=64, stride=2) + CLS token\n",
    "# ===========================================\n",
    "\n",
    "import os, re, math, json, copy, random, itertools, collections\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, Subset\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, cohen_kappa_score\n",
    "from sklearn.utils import check_random_state\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'           # .../S###/S###R##.edf\n",
    "FOLDS_JSON = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE); np.random.seed(RANDOM_STATE); random.seed(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "print(f\"🚀 Dispositivo: {DEVICE}\")\n",
    "\n",
    "FS = 160.0\n",
    "\n",
    "# Escenario y rango base por ensayo\n",
    "CLASS_SCENARIO = '4c'            # '2c','3c','4c'\n",
    "WINDOW_GLOBAL = (0.0, 4.0)       # segundos rel. T1/T2 (ensayo base)\n",
    "\n",
    "# Sliding window (mismo nº por ensayo)\n",
    "WIN_LEN_SEC = 2.5                # recomendado 2.0–2.5; aquí 2.5 para +contexto\n",
    "WIN_STRIDE_SEC = 0.5\n",
    "N_SUBWINS = int(math.floor((WINDOW_GLOBAL[1]-WINDOW_GLOBAL[0]-WIN_LEN_SEC)/WIN_STRIDE_SEC) + 1)\n",
    "\n",
    "# Balance 21 por clase/sujeto\n",
    "N_PER_CLASS_PER_SUBJECT = 21\n",
    "\n",
    "# CV\n",
    "GLOBAL_VAL_SPLIT = 0.15\n",
    "GLOBAL_PATIENCE  = 10\n",
    "\n",
    "# Entrenamiento global\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS_GLOBAL = 80\n",
    "LR_INIT = 1e-3\n",
    "\n",
    "# FT por sujeto — estrategia\n",
    "# \"none_bn_protos\"  -> BN-Adapt + prototipos (sin entrenamiento)\n",
    "# \"head_l2sp\"       -> FT de la cabeza con L2-SP (supervisado)\n",
    "FT_STRATEGY = \"none_bn_protos\"\n",
    "\n",
    "# Hiper FT (solo si FT_STRATEGY == \"head_l2sp\")\n",
    "FT_EPOCHS = 12\n",
    "FT_BASE_LR = 5e-5        # no usado en \"head\" puro\n",
    "FT_HEAD_LR = 3e-4\n",
    "FT_L2SP = 5e-4\n",
    "FT_PATIENCE = 3\n",
    "FT_VAL_RATIO = 0.2\n",
    "CALIB_CV_FOLDS = 4\n",
    "\n",
    "# Band-pass\n",
    "USE_BANDPASS = False\n",
    "BP_LO, BP_HI = 8.0, 30.0\n",
    "BP_METHOD, BP_PHASE = 'fir', 'zero'\n",
    "\n",
    "# Dataset\n",
    "EXCLUDE_SUBJECTS = {38,88,89,92,100,104}\n",
    "MI_RUNS_LR = [4,8,12]            # Left/Right\n",
    "MI_RUNS_OF = [6,10,14]           # Both Fists / Both Feet\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','FCz']\n",
    "\n",
    "CLASS_NAMES = ['Left', 'Right', 'Both Fists', 'Both Feet']\n",
    "\n",
    "# =========================\n",
    "# UTIL: canales/edf\n",
    "# =========================\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = re.sub(r'([A-Za-z])0([0-9])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])Z$', r'\\1z', s)\n",
    "    s = s.replace('fp', 'Fp').replace('FP', 'Fp')\n",
    "    s = ''.join(ch.upper() if ch != 'z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {}\n",
    "    for ch in raw.ch_names:\n",
    "        lab = normalize_label(ch)\n",
    "        lab = lab[:-1] + 'z' if lab.endswith('Z') else lab\n",
    "        lab = re.sub(r'([A-Z])Z$', r'\\1z', lab)\n",
    "        mapping[ch] = lab\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired_channels=EXPECTED_8):\n",
    "    missing = [ch for ch in desired_channels if ch not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"[WARN] faltan canales {missing} en {getattr(raw,'filenames',[''])[0]}\")\n",
    "        return None\n",
    "    raw.reorder_channels([ch for ch in raw.ch_names if ch in desired_channels] +\n",
    "                         [ch for ch in raw.ch_names if ch not in desired_channels])\n",
    "    raw.pick_channels(desired_channels, ordered=True)\n",
    "    return raw\n",
    "\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(run_id:int):\n",
    "    if run_id in MI_RUNS_LR: return 'LR'\n",
    "    if run_id in MI_RUNS_OF: return 'OF'\n",
    "    return None\n",
    "\n",
    "_SNR_TABLE = None\n",
    "def _load_snr_table():\n",
    "    global _SNR_TABLE\n",
    "    if _SNR_TABLE is not None: return _SNR_TABLE\n",
    "    csv_path = PROJ / 'reports' / 'psd_mains' / 'psd_mains_summary.csv'\n",
    "    if csv_path.exists():\n",
    "        try: _SNR_TABLE = pd.read_csv(csv_path)\n",
    "        except Exception as e:\n",
    "            print(f\"[SNR] No se pudo leer {csv_path}: {e}\")\n",
    "            _SNR_TABLE = None\n",
    "    return _SNR_TABLE\n",
    "\n",
    "def _decide_notch(subject, run, th_db=10.0):\n",
    "    df = _load_snr_table()\n",
    "    if df is None:  return 60.0\n",
    "    row = df[(df['subject']==subject) & (df['run']==run)]\n",
    "    if row.empty:   return 60.0\n",
    "    snr50 = float(row['snr50_db'].iloc[0]); snr60 = float(row['snr60_db'].iloc[0])\n",
    "    if snr60 >= th_db and snr60 >= snr50: return 60.0\n",
    "    if snr50 >= th_db and snr50 >  snr60: return 50.0\n",
    "    return None\n",
    "\n",
    "def read_raw_edf(path: Path):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        mont = mne.channels.make_standard_montage('standard_1020')\n",
    "        raw.set_montage(mont, on_missing='ignore')\n",
    "    except Exception:\n",
    "        pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad=\"auto\")\n",
    "    raw = ensure_channels_order(raw, EXPECTED_8)\n",
    "    if raw is None: return None\n",
    "\n",
    "    sid, rid = parse_subject_run(path)\n",
    "    notch = _decide_notch(sid, rid)\n",
    "    if notch is not None:\n",
    "        raw.notch_filter(freqs=[float(notch)], picks='eeg', method='spectrum_fit', phase='zero')\n",
    "\n",
    "    if USE_BANDPASS and (BP_LO is not None) and (BP_HI is not None):\n",
    "        raw.filter(l_freq=float(BP_LO), h_freq=float(BP_HI),\n",
    "                   picks='eeg', method=BP_METHOD, phase=BP_PHASE, verbose=False)\n",
    "    return raw\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    out = []\n",
    "    if raw.annotations is None or len(raw.annotations) == 0: return out\n",
    "    def _norm(s): return str(s).strip().upper().replace(' ','')\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = _norm(desc)\n",
    "        if tag in ('T1','T2'): out.append((float(onset), tag))\n",
    "    out.sort()\n",
    "    dedup = []; last_t1 = last_t2 = -1e9\n",
    "    for t, tag in out:\n",
    "        if tag == 'T1':\n",
    "            if (t - last_t1) >= 0.5: dedup.append((t, tag)); last_t1 = t\n",
    "        else:\n",
    "            if (t - last_t2) >= 0.5: dedup.append((t, tag)); last_t2 = t\n",
    "    return dedup\n",
    "\n",
    "# =========================\n",
    "# Dataset helpers\n",
    "# =========================\n",
    "def subjects_available():\n",
    "    subs = []\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try: sid = int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir / f\"S{sid:03d}R{r:02d}.edf\").exists() for r in (MI_RUNS_LR+MI_RUNS_OF))\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "def extract_base_trials_from_run(edf_path: Path, scenario: str, window_global=(0.0,4.0)):\n",
    "    subj, run = parse_subject_run(edf_path)\n",
    "    kind = run_kind(run)\n",
    "    if kind not in ('LR','OF'): return []\n",
    "\n",
    "    raw = read_raw_edf(edf_path)\n",
    "    if raw is None: return []\n",
    "    data = raw.get_data()\n",
    "    fs = raw.info['sfreq']\n",
    "    out = []\n",
    "    events = collect_events_T1T2(raw)\n",
    "    rel_start, rel_end = window_global\n",
    "    ev_idx = 0\n",
    "    for onset_sec, tag in events:\n",
    "        ev_idx += 1\n",
    "        if kind == 'LR':\n",
    "            label = 0 if tag=='T1' else 1\n",
    "        else:\n",
    "            label = 2 if tag=='T1' else 3\n",
    "        if   scenario == '2c' and label not in (0,1): continue\n",
    "        elif scenario == '3c' and label not in (0,1,2): continue\n",
    "        elif scenario == '4c' and label not in (0,1,2,3): continue\n",
    "\n",
    "        s0 = int(round((raw.first_time + onset_sec + rel_start) * fs))\n",
    "        e0 = int(round((raw.first_time + onset_sec + rel_end)   * fs))\n",
    "        if s0 < 0 or e0 > data.shape[1] or e0 <= s0: continue\n",
    "\n",
    "        seg = data[:, s0:e0].T.astype(np.float32)  # (T_base, C)\n",
    "        seg = (seg - seg.mean(axis=0, keepdims=True)) / (seg.std(axis=0, keepdims=True) + 1e-6)\n",
    "        trial_key = f\"S{subj:03d}_R{run:02d}_E{ev_idx:03d}\"\n",
    "        out.append((seg, label, subj, trial_key))\n",
    "    return out\n",
    "\n",
    "def build_balanced_trials(subjects, scenario='4c', window_global=(0.0,4.0), n_per_class=N_PER_CLASS_PER_SUBJECT):\n",
    "    trials_balanced = []\n",
    "    for s in tqdm(subjects, desc=\"Recolectando y balanceando ensayos base por sujeto\"):\n",
    "        sdir = DATA_RAW / f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "        bins = {0:[],1:[],2:[],3:[]}\n",
    "        for r in (MI_RUNS_LR + MI_RUNS_OF):\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            for t in extract_base_trials_from_run(p, scenario, window_global):\n",
    "                bins[t[1]].append(t)\n",
    "        # descarta sujeto si no tiene todas las clases\n",
    "        if any(len(bins[c]) == 0 for c in (0,1,2,3)): continue\n",
    "        rng = check_random_state(RANDOM_STATE + s)\n",
    "        for c in (0,1,2,3):\n",
    "            pool = bins[c]\n",
    "            if len(pool) >= n_per_class:\n",
    "                rng.shuffle(pool); take = pool[:n_per_class]\n",
    "            else:\n",
    "                idx = rng.choice(len(pool), size=n_per_class, replace=True)\n",
    "                take = [pool[i] for i in idx]\n",
    "            trials_balanced.extend(take)\n",
    "    print(f\"Ensayos base balanceados: {len(trials_balanced)}  (~ #sujetos_utiles * {n_per_class*4})\")\n",
    "    return trials_balanced\n",
    "\n",
    "def window_trials(trials, win_len_sec=2.0, win_stride_sec=0.5, fs=160.0):\n",
    "    \"\"\"Windowing para una lista de ensayos (de un split). Retorna X,y,subj,trial_ids.\"\"\"\n",
    "    X, y, g, tkeys = [], [], [], []\n",
    "    win_len = int(round(win_len_sec * fs))\n",
    "    stride  = int(round(win_stride_sec * fs))\n",
    "    for seg, lab, subj, tkey in trials:\n",
    "        T = seg.shape[0]\n",
    "        starts = [i for i in range(0, T - win_len + 1, stride)]\n",
    "        # asegura N_SUBWINS por ensayo\n",
    "        if len(starts) > N_SUBWINS: starts = starts[:N_SUBWINS]\n",
    "        while len(starts) < N_SUBWINS and (T - win_len) >= 0:\n",
    "            starts.append(T - win_len)\n",
    "        for s in starts:\n",
    "            e = s + win_len\n",
    "            sub = seg[s:e, :].astype(np.float32)\n",
    "            sub = (sub - sub.mean(axis=0, keepdims=True)) / (sub.std(axis=0, keepdims=True) + 1e-6)\n",
    "            X.append(sub); y.append(lab); g.append(subj); tkeys.append(tkey)\n",
    "    uniq = {k:i for i,k in enumerate(sorted(set(tkeys)))}\n",
    "    trial_ids = np.asarray([uniq[k] for k in tkeys], dtype=np.int64)\n",
    "    X = np.stack(X, axis=0).astype(np.float32)\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "    g = np.asarray(g, dtype=np.int64)\n",
    "    return X, y, g, trial_ids, uniq\n",
    "\n",
    "# =========================\n",
    "# Dataset torch\n",
    "# =========================\n",
    "class EEGTrials(Dataset):\n",
    "    def __init__(self, X, y, groups, trial_ids):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.g = groups.astype(np.int64)\n",
    "        self.t = trial_ids.astype(np.int64)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]; x = np.expand_dims(x, 0)  # (1,Tw,C)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx]), torch.tensor(self.t[idx])\n",
    "\n",
    "def build_weighted_sampler(y, groups):\n",
    "    y = np.asarray(y); groups = np.asarray(groups)\n",
    "    class_counts = np.bincount(y, minlength=len(np.unique(y))).astype(float)\n",
    "    class_w = 1.0 / class_counts[y]\n",
    "    subj_vals, subj_counts = np.unique(groups, return_counts=True)\n",
    "    subj_map = {s:c for s,c in zip(subj_vals, subj_counts)}\n",
    "    subj_w = np.array([1.0/subj_map[g] for g in groups], dtype=float)\n",
    "    w = class_w * subj_w; w = w / w.mean()\n",
    "    return WeightedRandomSampler(weights=torch.from_numpy(w).float(), num_samples=len(w), replacement=True)\n",
    "\n",
    "# =========================\n",
    "# Modelo: CNN + Transformer + Cosine head\n",
    "# =========================\n",
    "class ChannelDropout(nn.Module):\n",
    "    def __init__(self, p=0.1): super().__init__(); self.p = p\n",
    "    def forward(self, x):\n",
    "        if not self.training or self.p<=0: return x\n",
    "        B,_,T,C = x.shape\n",
    "        mask = (torch.rand(B,1,1,C, device=x.device) > self.p).float()\n",
    "        return x * mask\n",
    "\n",
    "@torch.no_grad()\n",
    "def apply_max_norm(model, max_value=2.0, p=2.0):\n",
    "    for m in model.modules():\n",
    "        if hasattr(m,'weight') and isinstance(m,(nn.Conv2d, nn.Linear)):\n",
    "            w = m.weight.data\n",
    "            norms = w.view(w.size(0), -1).norm(p=p, dim=1, keepdim=True)\n",
    "            desired = torch.clamp(norms, max=max_value)\n",
    "            w.view(w.size(0), -1).mul_(desired / (1e-8 + norms))\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 4000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0), persistent=False)\n",
    "    def forward(self, x):\n",
    "        T = x.size(1); return x + self.pe[:, :T, :]\n",
    "\n",
    "class CNNTokenizer(nn.Module):\n",
    "    def __init__(self, n_ch: int, d_feat: int = 128, k_temporal: int = 64, stride_t: int = 2, chdrop_p: float = 0.1, drop_p: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.chdrop = ChannelDropout(chdrop_p)\n",
    "        self.conv_t = nn.Conv2d(1, d_feat, kernel_size=(k_temporal,1), stride=(stride_t,1), padding=(k_temporal//2,0), bias=False)\n",
    "        self.bn_t   = nn.BatchNorm2d(d_feat, momentum=0.99, eps=1e-3)\n",
    "        self.act    = nn.ELU()\n",
    "        self.conv_sp_dw = nn.Conv2d(d_feat, d_feat, kernel_size=(1,n_ch), groups=d_feat, bias=False)\n",
    "        self.bn_sp      = nn.BatchNorm2d(d_feat, momentum=0.99, eps=1e-3)\n",
    "        self.conv_pw = nn.Conv2d(d_feat, d_feat, kernel_size=(1,1), bias=False)\n",
    "        self.bn_pw   = nn.BatchNorm2d(d_feat, momentum=0.99, eps=1e-3)\n",
    "        self.drop    = nn.Dropout(drop_p)\n",
    "    def forward(self, x):\n",
    "        z = self.chdrop(x)\n",
    "        z = self.conv_t(z); z = self.bn_t(z); z = self.act(z)         # (B,D,T',C)\n",
    "        z = self.conv_sp_dw(z); z = self.bn_sp(z); z = self.act(z)    # (B,D,T',1)\n",
    "        z = self.conv_pw(z); z = self.bn_pw(z); z = self.act(z)\n",
    "        z = self.drop(z)\n",
    "        z = z.squeeze(-1).transpose(1,2)  # (B,T',D)\n",
    "        return z\n",
    "\n",
    "def _enc_layer(d_model=128, nhead=4, dim_ff=256, drop=0.2):\n",
    "    return nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_ff,\n",
    "                                      dropout=drop, activation='gelu', batch_first=True, norm_first=True)\n",
    "\n",
    "class TemporalTransformer(nn.Module):\n",
    "    def __init__(self, d_model=128, nhead=4, dim_ff=256, depth=3, drop=0.2):\n",
    "        super().__init__()\n",
    "        self.pe = PositionalEncoding(d_model)\n",
    "        layer = _enc_layer(d_model, nhead, dim_ff, drop)\n",
    "        self.enc = nn.TransformerEncoder(layer, num_layers=depth)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    def forward(self, x):\n",
    "        x = self.pe(x)\n",
    "        x = self.enc(x)\n",
    "        return self.norm(x)\n",
    "\n",
    "class CosineClassifier(nn.Module):\n",
    "    def __init__(self, d, n_classes, tau=10.0):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.randn(n_classes, d))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        self.log_tau = nn.Parameter(torch.log(torch.tensor(tau, dtype=torch.float32)))\n",
    "    def forward(self, f):  # f: (B,d)\n",
    "        f_n = F.normalize(f, dim=1)                  # (B,d)\n",
    "        W_n = F.normalize(self.W, dim=1)             # (C,d)\n",
    "        cos = torch.mm(f_n, W_n.t())                 # (B,C)\n",
    "        return torch.exp(self.log_tau) * cos\n",
    "\n",
    "class HybridCNNTransformer(nn.Module):\n",
    "    def __init__(self, n_ch: int, n_classes: int, seq_len: int,\n",
    "                 d_model: int = 128, nhead: int = 4, depth: int = 3,\n",
    "                 dim_ff: int = 256, k_temporal: int = 64, stride_t: int = 2,\n",
    "                 drop: float = 0.2, chdrop_p: float = 0.1, use_cls_token: bool = True):\n",
    "        super().__init__()\n",
    "        self.use_cls = use_cls_token\n",
    "        self.tokenizer = CNNTokenizer(n_ch, d_model, k_temporal, stride_t, chdrop_p, drop)\n",
    "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) if use_cls_token else None\n",
    "        self.transformer = TemporalTransformer(d_model, nhead, dim_ff, depth, drop)\n",
    "        # Cosine head\n",
    "        self.norm_head = nn.LayerNorm(d_model)\n",
    "        self.head = CosineClassifier(d_model, n_classes, tau=10.0)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tok = self.tokenizer(x)                   # (B,T',D)\n",
    "        if self.use_cls:\n",
    "            B = tok.size(0); cls_tok = self.cls.expand(B,-1,-1)\n",
    "            tok = torch.cat([cls_tok, tok], dim=1)\n",
    "        z = self.transformer(tok)\n",
    "        feat = z[:,0,:] if self.use_cls else z.mean(dim=1)\n",
    "        feat = self.norm_head(feat)\n",
    "        return self.head(feat)                    # (B,C)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def extract_feats(self, x):\n",
    "        tok = self.tokenizer(x)\n",
    "        if self.use_cls:\n",
    "            B = tok.size(0); cls_tok = self.cls.expand(B,-1,-1)\n",
    "            tok = torch.cat([cls_tok, tok], dim=1)\n",
    "        z = self.transformer(tok)\n",
    "        feat = z[:,0,:] if self.use_cls else z.mean(dim=1)\n",
    "        return self.norm_head(feat)\n",
    "\n",
    "# =========================\n",
    "# Entrenamiento / Eval\n",
    "# =========================\n",
    "class SoftCE(nn.Module):\n",
    "    def __init__(self, n_classes: int, label_smoothing: float = 0.05, class_weights: torch.Tensor | None = None):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.ls = float(label_smoothing)\n",
    "        self.register_buffer('w', class_weights if class_weights is not None else None)\n",
    "    def forward(self, logits, targets_idx):\n",
    "        B, C = logits.size()\n",
    "        yt = F.one_hot(targets_idx, num_classes=C).float()\n",
    "        if self.ls > 0: yt = (1 - self.ls) * yt + self.ls * (1.0 / C)\n",
    "        logp = F.log_softmax(logits, dim=1)\n",
    "        loss = -(yt * logp)\n",
    "        if self.w is not None: loss = loss * self.w.unsqueeze(0)\n",
    "        return loss.sum(dim=1).mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def _predict_tta(model, xb, n=5, fs=160.0):\n",
    "    def time_jitter(x, max_ms=25):\n",
    "        max_shift = int(round(max_ms/1000.0 * fs))\n",
    "        if max_shift <= 0: return x\n",
    "        B,_,T,C = x.shape\n",
    "        shifts = torch.randint(low=-max_shift, high=max_shift+1, size=(B,), device=x.device)\n",
    "        out = torch.zeros_like(x)\n",
    "        for i,s in enumerate(shifts):\n",
    "            if s >= 0: out[i,:,s:,:] = x[i,:,:T-s,:]\n",
    "            else: s=-s; out[i,:,:T-s,:] = x[i,:,s:,:]\n",
    "        return out\n",
    "    outs = []\n",
    "    for _ in range(n):\n",
    "        outs.append(model(time_jitter(xb,25)))\n",
    "    return torch.stack(outs, dim=0).mean(dim=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_per_trial(model, loader, use_tta=True, tta_n=5, n_classes=4):\n",
    "    model.eval()\n",
    "    sums, counts, labels = {}, {}, {}\n",
    "    for xb, yb, _, tb in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = _predict_tta(model, xb, n=tta_n) if use_tta else model(xb)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        yb = yb.numpy(); tb = tb.numpy()\n",
    "        for i in range(xb.size(0)):\n",
    "            t = int(tb[i])\n",
    "            if t not in sums:\n",
    "                sums[t] = np.zeros((n_classes,), dtype=np.float64)\n",
    "                counts[t] = 0\n",
    "                labels[t] = int(yb[i])\n",
    "            sums[t] += logits[i]; counts[t] += 1\n",
    "    ids = sorted(sums.keys())\n",
    "    y_true_trial = np.asarray([labels[t] for t in ids], dtype=int)\n",
    "    y_pred_trial = np.asarray([int((sums[t]/max(1,counts[t])).argmax()) for t in ids], dtype=int)\n",
    "    acc = (y_true_trial == y_pred_trial).mean()\n",
    "    return y_true_trial, y_pred_trial, float(acc)\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes, title, fname):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.nan_to_num(cm_norm)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title); plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    ticks = np.arange(len(classes))\n",
    "    plt.xticks(ticks, classes, rotation=45, ha='right'); plt.yticks(ticks, classes)\n",
    "    fmt = '.2f'; thresh = cm_norm.max()/2.\n",
    "    for i,j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "        plt.text(j,i,format(cm_norm[i,j],fmt),ha=\"center\", color=\"white\" if cm_norm[i,j]>thresh else \"black\")\n",
    "    plt.ylabel('True'); plt.xlabel('Pred'); plt.tight_layout(); plt.savefig(fname, dpi=150); plt.close()\n",
    "\n",
    "def print_report(y_true, y_pred, classes, tag=\"\"):\n",
    "    print(f\"\\n=== Reporte {tag} ===\")\n",
    "    print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
    "    print(f\"F1-macro: {f1_score(y_true, y_pred, average='macro'):.4f} | Kappa: {cohen_kappa_score(y_true,y_pred):.4f}\")\n",
    "\n",
    "# =========================\n",
    "# BN-Adapt + Prototipos (FT sin entrenamiento)\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def bn_adapt(model, X_cal, batch=128):\n",
    "    \"\"\"Actualiza running stats de BatchNorm SOLO con datos del sujeto (sin gradientes).\"\"\"\n",
    "    was_training = model.training\n",
    "    model.train()\n",
    "    for i in range(0, len(X_cal), batch):\n",
    "        xb = torch.from_numpy(X_cal[i:i+batch]).float().unsqueeze(1).to(DEVICE)\n",
    "        _ = model(xb)\n",
    "    model.eval()\n",
    "    if was_training: model.train()\n",
    "\n",
    "@torch.no_grad()\n",
    "def set_prototypes(model: HybridCNNTransformer, X_cal, y_cal, n_classes, batch=128):\n",
    "    \"\"\"Calcula prototipos por clase (features medios) y actualiza pesos del Cosine head.\"\"\"\n",
    "    feats = []\n",
    "    labs = []\n",
    "    model.eval()\n",
    "    for i in range(0, len(X_cal), batch):\n",
    "        xb = torch.from_numpy(X_cal[i:i+batch]).float().unsqueeze(1).to(DEVICE)\n",
    "        f = model.extract_feats(xb).cpu()\n",
    "        feats.append(f); labs.append(torch.from_numpy(y_cal[i:i+batch]))\n",
    "    F_all = torch.cat(feats,0); Y_all = torch.cat(labs,0)\n",
    "    protos = []\n",
    "    for c in range(n_classes):\n",
    "        m = (Y_all==c)\n",
    "        if m.any():\n",
    "            proto = F.normalize(F_all[m].mean(0, keepdim=True), dim=1)\n",
    "        else:\n",
    "            proto = F.normalize(F_all.mean(0, keepdim=True), dim=1)\n",
    "        protos.append(proto)\n",
    "    W = torch.cat(protos,0)  # (C,d)\n",
    "    model.head.W.data.copy_(W)\n",
    "\n",
    "# =========================\n",
    "# FT con L2-SP (solo cabeza)\n",
    "# =========================\n",
    "def _freeze_for_mode(model: HybridCNNTransformer, mode: str):\n",
    "    for p in model.parameters(): p.requires_grad = False\n",
    "    if   mode=='head':\n",
    "        for p in model.norm_head.parameters(): p.requires_grad = True\n",
    "        for p in model.head.parameters():      p.requires_grad = True\n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "\n",
    "def _param_groups(model: HybridCNNTransformer, mode: str):\n",
    "    if   mode=='head': \n",
    "        return list(model.norm_head.parameters()) + list(model.head.parameters())\n",
    "\n",
    "def finetune_head_l2sp(model_global: HybridCNNTransformer, Xcal, ycal, trial_ids_cal,\n",
    "                       n_classes: int, epochs=FT_EPOCHS, batch_size=32,\n",
    "                       head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                       val_ratio=FT_VAL_RATIO, seed=RANDOM_STATE, device=DEVICE):\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=val_ratio, random_state=seed)\n",
    "    (tr_idx, va_idx), = gss.split(Xcal, ycal, groups=trial_ids_cal)\n",
    "    Xtr, ytr = Xcal[tr_idx], ycal[tr_idx]\n",
    "    Xva, yva = Xcal[va_idx], ycal[va_idx]\n",
    "\n",
    "    ds_tr = torch.utils.data.TensorDataset(torch.from_numpy(Xtr).float().unsqueeze(1),\n",
    "                                           torch.from_numpy(ytr).long())\n",
    "    ds_va = torch.utils.data.TensorDataset(torch.from_numpy(Xva).float().unsqueeze(1),\n",
    "                                           torch.from_numpy(yva).long())\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    model = copy.deepcopy(model_global).to(device)\n",
    "    _freeze_for_mode(model, 'head')\n",
    "    params = _param_groups(model, 'head')\n",
    "    opt = torch.optim.Adam(params, lr=head_lr)\n",
    "    ref = [p.detach().clone().to(device) for p in params]\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_state, best_val, bad = copy.deepcopy(model.state_dict()), float('inf'), 0\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            reg = 0.0\n",
    "            for p_cur, p_ref in zip(params, ref):\n",
    "                reg = reg + torch.sum((p_cur - p_ref)**2)\n",
    "            (loss + l2sp_lambda*reg).backward(); opt.step()\n",
    "            apply_max_norm(model, 2.0, p=2.0)\n",
    "\n",
    "        # valid\n",
    "        model.eval()\n",
    "        val_loss, nval = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                val_loss += F.cross_entropy(model(xb), yb).item() * xb.size(0)\n",
    "                nval += xb.size(0)\n",
    "        val_loss /= max(1,nval)\n",
    "        if val_loss + 1e-7 < best_val: best_val, bad, best_state = val_loss, 0, copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= FT_PATIENCE: break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# Folds JSON: SOLO sujetos (ignora índices)\n",
    "# =========================\n",
    "def read_folds_subjects_only(json_path, current_subjects):\n",
    "    \"\"\"\n",
    "    Lee Kfold5.json y devuelve lista de folds con train_subjects/test_subjects (enteros).\n",
    "    Ignora por completo tr_idx/te_idx si existen.\n",
    "    Valida que los sujetos existan en el dataset actual.\n",
    "    \"\"\"\n",
    "    p = Path(json_path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"No existe {p}. Debes proporcionar un JSON con sujetos por fold.\")\n",
    "    with open(p, 'r', encoding='utf-8') as f:\n",
    "        payload = json.load(f)\n",
    "    folds_raw = payload.get('folds', [])\n",
    "    if not folds_raw:\n",
    "        raise ValueError(\"JSON no contiene 'folds'.\")\n",
    "\n",
    "    cur_set = set(current_subjects)\n",
    "    folds = []\n",
    "    for fr in folds_raw:\n",
    "        train_s = fr.get('train_subjects', fr.get('train', []))\n",
    "        test_s  = fr.get('test_subjects',  fr.get('test',  []))\n",
    "        if not train_s or not test_s:\n",
    "            raise ValueError(\"Cada fold debe tener 'train_subjects' y 'test_subjects' (o 'train'/'test').\")\n",
    "\n",
    "        def to_int_list(lst):\n",
    "            out = []\n",
    "            for s in lst:\n",
    "                if isinstance(s, int): out.append(s)\n",
    "                elif isinstance(s, str) and s.upper().startswith('S') and s[1:].isdigit():\n",
    "                    out.append(int(s[1:]))\n",
    "                elif isinstance(s, str) and s.isdigit():\n",
    "                    out.append(int(s))\n",
    "                else:\n",
    "                    raise ValueError(f\"Sujeto inválido en JSON: {s}\")\n",
    "            return out\n",
    "\n",
    "        tr_int = to_int_list(train_s)\n",
    "        te_int = to_int_list(test_s)\n",
    "\n",
    "        miss_tr = [s for s in tr_int if s not in cur_set]\n",
    "        miss_te = [s for s in te_int if s not in cur_set]\n",
    "        if miss_tr or miss_te:\n",
    "            raise ValueError(f\"Fold con sujetos no presentes en dataset actual. \"\n",
    "                             f\"Faltan en TRAIN: {miss_tr}, Faltan en TEST: {miss_te}\")\n",
    "\n",
    "        folds.append({'fold': fr.get('fold', len(folds)+1),\n",
    "                      'train_subjects': tr_int, 'test_subjects': te_int})\n",
    "    return folds\n",
    "\n",
    "# =========================\n",
    "# Experimento\n",
    "# =========================\n",
    "def plot_training_curves(history, fname):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(history['train_acc'], label='train_acc')\n",
    "    plt.plot(history['val_acc'], label='val_acc')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Training curve'); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(fname, dpi=150); plt.close()\n",
    "\n",
    "def run_experiment():\n",
    "    mne.set_log_level('WARNING')\n",
    "    subs = subjects_available()\n",
    "    print(f\"Sujetos elegibles: {len(subs)} → {subs[:10]}{'...' if len(subs)>10 else ''}\")\n",
    "\n",
    "    # Ensayos balanceados (21/clase/sujeto)\n",
    "    trials_bal = build_balanced_trials(subs, scenario=CLASS_SCENARIO, window_global=WINDOW_GLOBAL,\n",
    "                                       n_per_class=N_PER_CLASS_PER_SUBJECT)\n",
    "\n",
    "    # Sujetos presentes tras balanceo\n",
    "    subj_in_trials = sorted({t[2] for t in trials_bal})\n",
    "    print(f\"Sujetos con ensayos balanceados: {len(subj_in_trials)}\")\n",
    "\n",
    "    # Leer folds SOLO por sujetos\n",
    "    folds = read_folds_subjects_only(FOLDS_JSON, current_subjects=subj_in_trials)\n",
    "\n",
    "    global_folds_trial, all_true_trial, all_pred_trial, ft_prog_folds = [], [], [], []\n",
    "\n",
    "    for f in folds:\n",
    "        fold = f['fold']\n",
    "        train_subs = set(f['train_subjects'])\n",
    "        test_subs  = set(f['test_subjects'])\n",
    "\n",
    "        # Split por sujeto (antes de windowing)\n",
    "        tr_trials = [t for t in trials_bal if t[2] in train_subs]\n",
    "        te_trials = [t for t in trials_bal if t[2] in test_subs]\n",
    "\n",
    "        # Sanity: todas las clases presentes en TEST por ensayo\n",
    "        lbl_te = [t[1] for t in te_trials]\n",
    "        present = {0,1,2,3}.intersection(set(lbl_te))\n",
    "        if present != {0,1,2,3}:\n",
    "            print(f\"[WARN] Fold {fold}: clases en TEST (trial-level) = {sorted(present)}; se salta este fold.\")\n",
    "            continue\n",
    "\n",
    "        # VALID por sujeto dentro de TRAIN\n",
    "        train_subs_list = sorted(list(train_subs))\n",
    "        rng = np.random.RandomState(RANDOM_STATE + fold)\n",
    "        rng.shuffle(train_subs_list)\n",
    "        n_va = max(1, int(round(GLOBAL_VAL_SPLIT * len(train_subs_list))))\n",
    "        val_subs = set(train_subs_list[:n_va])\n",
    "        real_train_subs = set(train_subs_list[n_va:])\n",
    "\n",
    "        tr_trials_main = [t for t in tr_trials if t[2] in real_train_subs]\n",
    "        va_trials      = [t for t in tr_trials if t[2] in val_subs]\n",
    "\n",
    "        # Windowing por split (sin fuga)\n",
    "        Xtr, ytr, gtr, ttr, _ = window_trials(trials=tr_trials_main, win_len_sec=WIN_LEN_SEC, win_stride_sec=WIN_STRIDE_SEC, fs=FS)\n",
    "        Xva, yva, gva, tva, _ = window_trials(trials=va_trials,      win_len_sec=WIN_LEN_SEC, win_stride_sec=WIN_STRIDE_SEC, fs=FS)\n",
    "        Xte, yte, gte, tte, _ = window_trials(trials=te_trials,      win_len_sec=WIN_LEN_SEC, win_stride_sec=WIN_STRIDE_SEC, fs=FS)\n",
    "\n",
    "        # DataLoaders\n",
    "        ds_tr = EEGTrials(Xtr, ytr, gtr, ttr)\n",
    "        ds_va = EEGTrials(Xva, yva, gva, tva)\n",
    "        ds_te = EEGTrials(Xte, yte, gte, tte)\n",
    "\n",
    "        tr_loader = DataLoader(ds_tr, batch_size=BATCH_SIZE,\n",
    "                               sampler=build_weighted_sampler(ytr, gtr), drop_last=False)\n",
    "        va_loader = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "        te_loader = DataLoader(ds_te, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "        # Modelo\n",
    "        Tw, C = Xtr.shape[1], Xtr.shape[2]\n",
    "        n_classes = len(set(ytr.tolist()) | set(yva.tolist()) | set(yte.tolist()))\n",
    "        model = HybridCNNTransformer(\n",
    "            n_ch=C, n_classes=n_classes, seq_len=Tw,\n",
    "            d_model=128, nhead=4, depth=3, dim_ff=256,\n",
    "            k_temporal=64, stride_t=2, drop=0.2, chdrop_p=0.10, use_cls_token=True\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=LR_INIT)\n",
    "        criterion = SoftCE(n_classes=n_classes, label_smoothing=0.05, class_weights=None)\n",
    "\n",
    "        def _acc_trial(loader): return evaluate_per_trial(model, loader, use_tta=False, n_classes=n_classes)[2]\n",
    "\n",
    "        # Entrenamiento global (early stopping por ENSAYO)\n",
    "        history = {'train_acc': [], 'val_acc': []}\n",
    "        print(f\"\\n[Fold {fold}/{len(folds)}] Entrenando global (VAL por ensayo)...\")\n",
    "        best_state = copy.deepcopy(model.state_dict()); best_val = -1.0; bad = 0\n",
    "\n",
    "        for epoch in range(1, EPOCHS_GLOBAL+1):\n",
    "            model.train()\n",
    "            for xb, yb, _, _ in tr_loader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                loss = criterion(model(xb), yb)\n",
    "                opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "                apply_max_norm(model, 2.0, p=2.0)\n",
    "\n",
    "            tr_acc = _acc_trial(tr_loader)\n",
    "            va_acc = _acc_trial(va_loader)\n",
    "            history['train_acc'].append(tr_acc); history['val_acc'].append(va_acc)\n",
    "\n",
    "            if epoch % 5 == 0 or epoch in (1,10,20,50,80):\n",
    "                print(f\"  Época {epoch:3d} | train(ensayo)={tr_acc:.4f} | val(ensayo)={va_acc:.4f}\")\n",
    "\n",
    "            if va_acc > best_val + 1e-4:\n",
    "                best_val, bad = va_acc, 0\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "            else:\n",
    "                bad += 1\n",
    "                if bad >= GLOBAL_PATIENCE:\n",
    "                    print(f\"  Early stopping @ epoch {epoch} (best val ensayo={best_val:.4f})\")\n",
    "                    break\n",
    "\n",
    "        plot_training_curves(history, f\"training_curve_cnnTrans_cosine_fold{fold}.png\")\n",
    "        print(f\"↳ Curva: training_curve_cnnTrans_cosine_fold{fold}.png\")\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "        # Test por ENSAYO (TTA)\n",
    "        y_true_tr, y_pred_tr, acc_tr = evaluate_per_trial(model, te_loader, use_tta=True, tta_n=5, n_classes=n_classes)\n",
    "        print(f\"[Fold {fold}] Ensayo acc={acc_tr:.4f}\")\n",
    "        print_report(y_true_tr, y_pred_tr, CLASS_NAMES, tag=\"por ENSAYO (agregado)\")\n",
    "        global_folds_trial.append(acc_tr)\n",
    "        all_true_trial.append(y_true_tr); all_pred_trial.append(y_pred_tr)\n",
    "\n",
    "        # ====== FT por sujeto ======\n",
    "        y_true_ft_all, y_pred_ft_all, used_subjects = [], [], 0\n",
    "\n",
    "        # Índices por sujeto en Xte\n",
    "        for sid in sorted(test_subs):\n",
    "            mask_sid = (gte == sid)\n",
    "            if mask_sid.sum() == 0: continue\n",
    "            Xs, ys, ts = Xte[mask_sid], yte[mask_sid], tte[mask_sid]\n",
    "\n",
    "            # GroupKFold por ensayo (sin fuga)\n",
    "            if len(np.unique(ts)) < CALIB_CV_FOLDS:\n",
    "                continue\n",
    "            gkf = GroupKFold(n_splits=CALIB_CV_FOLDS)\n",
    "            for tr_i, ho_i in gkf.split(Xs, ys, groups=ts):\n",
    "                Xcal, ycal, tcal = Xs[tr_i], ys[tr_i], ts[tr_i]\n",
    "                Xho,  yho,  tho  = Xs[ho_i], ys[ho_i], ts[ho_i]\n",
    "\n",
    "                if FT_STRATEGY == \"none_bn_protos\":\n",
    "                    # 1) BN-Adapt (sin etiquetas)\n",
    "                    bn_adapt(model, Xcal)\n",
    "                    # 2) Prototipos (opcional; usa etiquetas de calibración)\n",
    "                    set_prototypes(model, Xcal, ycal, n_classes)\n",
    "                    m_ft = model  # usamos el propio modelo adaptado (sin copiar)\n",
    "\n",
    "                elif FT_STRATEGY == \"head_l2sp\":\n",
    "                    m_ft = finetune_head_l2sp(model, Xcal, ycal, tcal,\n",
    "                                              n_classes=n_classes, epochs=FT_EPOCHS,\n",
    "                                              head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                                              val_ratio=FT_VAL_RATIO, device=DEVICE)\n",
    "                else:\n",
    "                    raise ValueError(f\"Estrategia FT desconocida: {FT_STRATEGY}\")\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    xb = torch.from_numpy(Xho).float().unsqueeze(1).to(DEVICE)\n",
    "                    logits = m_ft(xb).cpu().numpy()\n",
    "\n",
    "                # Agregar por ensayo\n",
    "                sums = collections.defaultdict(lambda: np.zeros((n_classes,), dtype=np.float64))\n",
    "                counts = collections.defaultdict(int)\n",
    "                labels = {}\n",
    "                for i in range(len(Xho)):\n",
    "                    t = int(tho[i]); sums[t] += logits[i]; counts[t] += 1; labels[t] = int(yho[i])\n",
    "                for t in sums.keys():\n",
    "                    y_pred_ft_all.append(int((sums[t]/max(1,counts[t])).argmax()))\n",
    "                    y_true_ft_all.append(labels[t])\n",
    "                used_subjects += 1\n",
    "\n",
    "        if len(y_true_ft_all) > 0:\n",
    "            y_true_ft_all = np.asarray(y_true_ft_all, dtype=int)\n",
    "            y_pred_ft_all = np.asarray(y_pred_ft_all, dtype=int)\n",
    "            acc_ft = (y_true_ft_all == y_pred_ft_all).mean()\n",
    "            print(f\"  FT ({FT_STRATEGY}) por ENSAYO acc={acc_ft:.4f} | sujetos={used_subjects}\")\n",
    "            print(f\"  Δ(FT - Global) = {acc_ft - acc_tr:+.4f}\")\n",
    "        else:\n",
    "            acc_ft = np.nan\n",
    "            print(\"  FT no ejecutado (fold sin sujetos aptos).\")\n",
    "\n",
    "        ft_prog_folds.append(acc_ft)\n",
    "\n",
    "    # ----- resultados globales -----\n",
    "    if len(all_true_trial)>0: all_true_trial = np.concatenate(all_true_trial); all_pred_trial = np.concatenate(all_pred_trial)\n",
    "    else: all_true_trial = np.array([],dtype=int); all_pred_trial = np.array([],dtype=int)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"RESULTADOS FINALES — CNN+Transformer + Cosine head + ({FT_STRATEGY})\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Ensayo folds:\", [f\"{a:.4f}\" for a in global_folds_trial])\n",
    "    if len(global_folds_trial) > 0:\n",
    "        print(f\"Ensayo mean: {np.mean(global_folds_trial):.4f}\")\n",
    "    print(\"FT prog (ensayo) folds:\", [(\"nan\" if (a is None or np.isnan(a)) else f\"{a:.4f}\") for a in ft_prog_folds])\n",
    "    if len(ft_prog_folds) > 0 and len(global_folds_trial) > 0:\n",
    "        print(f\"Δ(FT - Global) mean: {np.nanmean(ft_prog_folds) - np.mean(global_folds_trial):+.4f}\")\n",
    "\n",
    "    if all_true_trial.size > 0:\n",
    "        plot_confusion(all_true_trial, all_pred_trial, CLASS_NAMES,\n",
    "                       title=\"Confusion Matrix - Global Model (Per Trial, All Folds)\",\n",
    "                       fname=\"confusion_cnnTrans_cosine_trial_allfolds.png\")\n",
    "        print(\"↳ Matriz de confusión (ensayo): confusion_cnnTrans_cosine_trial_allfolds.png\")\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    bp_status = f\"ON [{BP_LO}-{BP_HI} Hz]\" if USE_BANDPASS and (BP_LO is not None) and (BP_HI is not None) else \"OFF\"\n",
    "    print(\"🧠 MI-EEG — CNN+Transformer (21/cls/subj) + Sliding Window + Cosine head + FT opcional\")\n",
    "    print(f\"🔧 Escenario: {CLASS_SCENARIO} | rango ensayo={WINDOW_GLOBAL} s | subventana={WIN_LEN_SEC}s | stride={WIN_STRIDE_SEC}s | subwins/ensayo={N_SUBWINS} | band-pass={bp_status}\")\n",
    "    print(f\"⚙️  Balance: 21 ensayos por clase y por sujeto (con reemplazo si falta)\")\n",
    "    print(f\"⚙️  FT strategy: {FT_STRATEGY}\")\n",
    "    run_experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dc03bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Dispositivo: cuda\n",
      "🧠 MI-EEG — CNN+Transformer (21/cls/subj) + Sliding Window + Cosine head\n",
      "🔧 Escenario: 4c | rango ensayo=(0.0, 4.0) s | subventana=2.5s | stride=0.5s | subwins/ensayo=4 | band-pass=OFF\n",
      "⚙️  EA: True | MMD_LAMBDA=0.05 | TTA-BN test: True\n",
      "Sujetos elegibles: 103 → [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recolectando y balanceando ensayos base por sujeto:   0%|          | 0/103 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recolectando y balanceando ensayos base por sujeto: 100%|██████████| 103/103 [00:37<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensayos base balanceados: 8652  (~ #sujetos_utiles * 84)\n",
      "Sujetos con ensayos balanceados: 103\n",
      "🔧 EA: calculando whiteners por sujeto...\n",
      "EA aplicado a 103 sujetos.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold 1/5] Entrenando global (VAL por ensayo)...\n",
      "  Época   1 | train(ensayo)=0.3594 | val(ensayo)=0.3919\n",
      "  Época   5 | train(ensayo)=0.4083 | val(ensayo)=0.3899\n",
      "  Época  10 | train(ensayo)=0.4701 | val(ensayo)=0.5020\n",
      "  Época  15 | train(ensayo)=0.4811 | val(ensayo)=0.4474\n",
      "  Época  20 | train(ensayo)=0.4964 | val(ensayo)=0.4345\n",
      "  Época  25 | train(ensayo)=0.5145 | val(ensayo)=0.4544\n",
      "  Early stopping @ epoch 28 (best val ensayo=0.5119)\n",
      "↳ Curva: training_curve_cnnTrans_ea_mmd_fold1.png\n",
      "🔧 TTA-BN: adaptando BN por sujeto en TEST...\n",
      "[Fold 1] Ensayo acc=0.2500\n",
      "Distribución real por clase en test (por ensayo): [441 441 441 441]\n",
      "\n",
      "=== Reporte por ENSAYO (agregado) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.2500    1.0000    0.4000       441\n",
      "       Right     0.0000    0.0000    0.0000       441\n",
      "  Both Fists     0.0000    0.0000    0.0000       441\n",
      "   Both Feet     0.0000    0.0000    0.0000       441\n",
      "\n",
      "    accuracy                         0.2500      1764\n",
      "   macro avg     0.0625    0.2500    0.1000      1764\n",
      "weighted avg     0.0625    0.2500    0.1000      1764\n",
      "\n",
      "F1-macro: 0.1000 | Kappa: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold 2/5] Entrenando global (VAL por ensayo)...\n",
      "  Época   1 | train(ensayo)=0.3441 | val(ensayo)=0.3552\n",
      "  Época   5 | train(ensayo)=0.3860 | val(ensayo)=0.3889\n",
      "  Época  10 | train(ensayo)=0.4271 | val(ensayo)=0.4187\n",
      "  Época  15 | train(ensayo)=0.4353 | val(ensayo)=0.4286\n",
      "  Época  20 | train(ensayo)=0.4554 | val(ensayo)=0.4127\n",
      "  Época  25 | train(ensayo)=0.4230 | val(ensayo)=0.3810\n",
      "  Época  30 | train(ensayo)=0.4722 | val(ensayo)=0.3968\n",
      "  Early stopping @ epoch 31 (best val ensayo=0.4554)\n",
      "↳ Curva: training_curve_cnnTrans_ea_mmd_fold2.png\n",
      "🔧 TTA-BN: adaptando BN por sujeto en TEST...\n",
      "[Fold 2] Ensayo acc=0.2426\n",
      "Distribución real por clase en test (por ensayo): [441 441 441 441]\n",
      "\n",
      "=== Reporte por ENSAYO (agregado) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.0000    0.0000    0.0000       441\n",
      "       Right     0.1993    0.1224    0.1517       441\n",
      "  Both Fists     0.0000    0.0000    0.0000       441\n",
      "   Both Feet     0.2505    0.8481    0.3868       441\n",
      "\n",
      "    accuracy                         0.2426      1764\n",
      "   macro avg     0.1124    0.2426    0.1346      1764\n",
      "weighted avg     0.1124    0.2426    0.1346      1764\n",
      "\n",
      "F1-macro: 0.1346 | Kappa: -0.0098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold 3/5] Entrenando global (VAL por ensayo)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a375879b1383>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    891\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"🔧 Escenario: {CLASS_SCENARIO} | rango ensayo={WINDOW_GLOBAL} s | subventana={WIN_LEN_SEC}s | stride={WIN_STRIDE_SEC}s | subwins/ensayo={N_SUBWINS} | band-pass={bp_status}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"⚙️  EA: {USE_EA} | MMD_LAMBDA={MMD_LAMBDA} | TTA-BN test: {TTA_BN_ON_TEST}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m     \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-a375879b1383>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m    780\u001b[0m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMMD_LAMBDA\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmmd2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m                 \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m                 \u001b[0mapply_max_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ===========================================\n",
    "# MI-EEG (PhysioNet eegmmidb) — CNN+Transformer\n",
    "# Balanced 21/cls/subj + Sliding Window + Cosine head\n",
    "# + Euclidean Alignment (EA) + (MMD) + TTA-BN opcional\n",
    "# ✅ Fix: ClassBalancedFocal ahora sincroniza el tensor de pesos (cb_w) con el device de los logits.\n",
    "# ===========================================\n",
    "\n",
    "import os, re, math, json, copy, random, itertools\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, cohen_kappa_score\n",
    "from sklearn.utils import check_random_state\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'           # .../S###/S###R##.edf\n",
    "FOLDS_JSON = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE); np.random.seed(RANDOM_STATE); random.seed(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "print(f\"🚀 Dispositivo: {DEVICE}\")\n",
    "\n",
    "FS = 160.0\n",
    "\n",
    "CLASS_SCENARIO = '4c'\n",
    "WINDOW_GLOBAL = (0.0, 4.0)\n",
    "\n",
    "WIN_LEN_SEC = 2.5\n",
    "WIN_STRIDE_SEC = 0.5\n",
    "def _num_subwins(win_len_s, stride_s, rng):\n",
    "    return int(math.floor((rng[1]-rng[0]-win_len_s)/stride_s) + 1)\n",
    "N_SUBWINS = _num_subwins(WIN_LEN_SEC, WIN_STRIDE_SEC, WINDOW_GLOBAL)\n",
    "\n",
    "N_PER_CLASS_PER_SUBJECT = 21\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS_GLOBAL = 80\n",
    "LR_INIT = 1e-3\n",
    "GLOBAL_VAL_SPLIT_SUBJ = 0.15\n",
    "GLOBAL_PATIENCE  = 10\n",
    "\n",
    "USE_SWA = True\n",
    "SWA_START_FRAC = 0.6\n",
    "SWA_LR_FACTOR = 0.5\n",
    "\n",
    "TTA_BN_ON_TEST = True\n",
    "\n",
    "MMD_LAMBDA = 0.05\n",
    "MMD_MAX_SAMPLES_PER_GROUP = 64\n",
    "\n",
    "USE_EA = True\n",
    "EA_EPS = 1e-6\n",
    "\n",
    "USE_BANDPASS = False\n",
    "BP_LO, BP_HI = 8.0, 30.0\n",
    "BP_METHOD, BP_PHASE = 'fir', 'zero'\n",
    "\n",
    "EXCLUDE_SUBJECTS = {38,88,89,92,100,104}\n",
    "MI_RUNS_LR = [4,8,12]\n",
    "MI_RUNS_OF = [6,10,14]\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','FCz']\n",
    "CLASS_NAMES = ['Left', 'Right', 'Both Fists', 'Both Feet']\n",
    "\n",
    "PER_CLASS_TAU = np.array([1.00, 1.00, 1.15, 1.10], dtype=np.float32)\n",
    "\n",
    "# =========================\n",
    "# UTIL: canales/edf\n",
    "# =========================\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = s.replace('fp', 'Fp').replace('FP', 'Fp')\n",
    "    s = ''.join(ch.upper() if ch != 'z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {}\n",
    "    for ch in raw.ch_names:\n",
    "        lab = normalize_label(ch)\n",
    "        lab = lab[:-1] + 'z' if lab.endswith('Z') else lab\n",
    "        lab = re.sub(r'([A-Z])Z$', r'\\1z', lab)\n",
    "        mapping[ch] = lab\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired_channels=EXPECTED_8):\n",
    "    missing = [ch for ch in desired_channels if ch not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"[WARN] faltan canales {missing} en {getattr(raw,'filenames',[''])[0]}\")\n",
    "        return None\n",
    "    raw.reorder_channels([ch for ch in raw.ch_names if ch in desired_channels] +\n",
    "                         [ch for ch in raw.ch_names if ch not in desired_channels])\n",
    "    raw.pick_channels(desired_channels, ordered=True)\n",
    "    return raw\n",
    "\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(run_id:int):\n",
    "    if run_id in MI_RUNS_LR: return 'LR'\n",
    "    if run_id in MI_RUNS_OF: return 'OF'\n",
    "    return None\n",
    "\n",
    "_SNR_TABLE = None\n",
    "def _load_snr_table():\n",
    "    global _SNR_TABLE\n",
    "    if _SNR_TABLE is not None: return _SNR_TABLE\n",
    "    csv_path = PROJ / 'reports' / 'psd_mains' / 'psd_mains_summary.csv'\n",
    "    if csv_path.exists():\n",
    "        try: _SNR_TABLE = pd.read_csv(csv_path)\n",
    "        except Exception:\n",
    "            _SNR_TABLE = None\n",
    "    return _SNR_TABLE\n",
    "\n",
    "def _decide_notch(subject, run, th_db=10.0):\n",
    "    df = _load_snr_table()\n",
    "    if df is None:  return 60.0\n",
    "    row = df[(df['subject']==subject) & (df['run']==run)]\n",
    "    if row.empty:   return 60.0\n",
    "    snr50 = float(row['snr50_db'].iloc[0]); snr60 = float(row['snr60_db'].iloc[0])\n",
    "    if snr60 >= th_db and snr60 >= snr50: return 60.0\n",
    "    if snr50 >= th_db and snr50 >  snr60: return 50.0\n",
    "    return None\n",
    "\n",
    "def read_raw_edf(path: Path):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        mont = mne.channels.make_standard_montage('standard_1020')\n",
    "        raw.set_montage(mont, on_missing='ignore')\n",
    "    except Exception:\n",
    "        pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad=\"auto\")\n",
    "    raw = ensure_channels_order(raw, EXPECTED_8)\n",
    "    if raw is None: return None\n",
    "\n",
    "    sid, rid = parse_subject_run(path)\n",
    "    notch = _decide_notch(sid, rid)\n",
    "    if notch is not None:\n",
    "        raw.notch_filter(freqs=[float(notch)], picks='eeg', method='spectrum_fit', phase='zero')\n",
    "\n",
    "    if USE_BANDPASS and (BP_LO is not None) and (BP_HI is not None):\n",
    "        raw.filter(l_freq=float(BP_LO), h_freq=float(BP_HI),\n",
    "                   picks='eeg', method=BP_METHOD, phase=BP_PHASE, verbose=False)\n",
    "    return raw\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    out = []\n",
    "    if raw.annotations is None or len(raw.annotations) == 0: return out\n",
    "    def _norm(s): return str(s).strip().upper().replace(' ','')\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = _norm(desc)\n",
    "        if tag in ('T1','T2'): out.append((float(onset), tag))\n",
    "    out.sort()\n",
    "    dedup = []; last_t1 = last_t2 = -1e9\n",
    "    for t, tag in out:\n",
    "        if tag == 'T1':\n",
    "            if (t - last_t1) >= 0.5: dedup.append((t, tag)); last_t1 = t\n",
    "        else:\n",
    "            if (t - last_t2) >= 0.5: dedup.append((t, tag)); last_t2 = t\n",
    "    return dedup\n",
    "\n",
    "# =========================\n",
    "# Dataset helpers\n",
    "# =========================\n",
    "def subjects_available():\n",
    "    subs = []\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try: sid = int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir / f\"S{sid:03d}R{r:02d}.edf\").exists() for r in (MI_RUNS_LR+MI_RUNS_OF))\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "def extract_base_trials_from_run(edf_path: Path, scenario: str, window_global=(0.0,4.0)):\n",
    "    subj, run = parse_subject_run(edf_path)\n",
    "    kind = run_kind(run)\n",
    "    if kind not in ('LR','OF'): return []\n",
    "\n",
    "    raw = read_raw_edf(edf_path)\n",
    "    if raw is None: return []\n",
    "    data = raw.get_data()\n",
    "    fs = raw.info['sfreq']\n",
    "    out = []\n",
    "    events = collect_events_T1T2(raw)\n",
    "    rel_start, rel_end = window_global\n",
    "    ev_idx = 0\n",
    "    for onset_sec, tag in events:\n",
    "        ev_idx += 1\n",
    "        if kind == 'LR':\n",
    "            label = 0 if tag=='T1' else 1\n",
    "        else:\n",
    "            label = 2 if tag=='T1' else 3\n",
    "        if   scenario == '2c' and label not in (0,1): continue\n",
    "        elif scenario == '3c' and label not in (0,1,2): continue\n",
    "        elif scenario == '4c' and label not in (0,1,2,3): continue\n",
    "\n",
    "        s0 = int(round((raw.first_time + onset_sec + rel_start) * fs))\n",
    "        e0 = int(round((raw.first_time + onset_sec + rel_end)   * fs))\n",
    "        if s0 < 0 or e0 > data.shape[1] or e0 <= s0: continue\n",
    "\n",
    "        seg = data[:, s0:e0].T.astype(np.float32)  # (T_base, C)\n",
    "        seg = (seg - seg.mean(axis=0, keepdims=True)) / (seg.std(axis=0, keepdims=True) + 1e-6)\n",
    "        trial_key = f\"S{subj:03d}_R{run:02d}_E{ev_idx:03d}\"\n",
    "        out.append((seg, label, subj, trial_key))\n",
    "    return out\n",
    "\n",
    "def build_balanced_trials(subjects, scenario='4c', window_global=(0.0,4.0), n_per_class=N_PER_CLASS_PER_SUBJECT):\n",
    "    trials_balanced = []\n",
    "    for s in tqdm(subjects, desc=\"Recolectando y balanceando ensayos base por sujeto\"):\n",
    "        sdir = DATA_RAW / f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "        bins = {0:[],1:[],2:[],3:[]}\n",
    "        for r in (MI_RUNS_LR + MI_RUNS_OF):\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            for t in extract_base_trials_from_run(p, scenario, window_global):\n",
    "                bins[t[1]].append(t)\n",
    "        if any(len(bins[c]) == 0 for c in (0,1,2,3)): continue\n",
    "        rng = check_random_state(RANDOM_STATE + s)\n",
    "        for c in (0,1,2,3):\n",
    "            pool = bins[c]\n",
    "            if len(pool) >= n_per_class:\n",
    "                rng.shuffle(pool); take = pool[:n_per_class]\n",
    "            else:\n",
    "                idx = rng.choice(len(pool), size=n_per_class, replace=True)\n",
    "                take = [pool[i] for i in idx]\n",
    "            trials_balanced.extend(take)\n",
    "    print(f\"Ensayos base balanceados: {len(trials_balanced)}  (~ #sujetos_utiles * {n_per_class*4})\")\n",
    "    return trials_balanced\n",
    "\n",
    "# ---------- EA ----------\n",
    "def _sym_invsqrt(mat, eps=1e-6):\n",
    "    w, v = np.linalg.eigh(mat)\n",
    "    w = np.clip(w, eps, None)\n",
    "    invsqrt = (v * (w**-0.5))[..., :] @ v.T\n",
    "    return invsqrt\n",
    "\n",
    "def compute_ea_whiteners_per_subject(trials_balanced, eps=EA_EPS):\n",
    "    by_subj = {}\n",
    "    for seg, _, subj, _ in trials_balanced:\n",
    "        by_subj.setdefault(subj, []).append(seg)\n",
    "    whiten = {}\n",
    "    for subj, segs in by_subj.items():\n",
    "        covs = []\n",
    "        for X in segs:\n",
    "            C = (X.T @ X) / float(max(1, X.shape[0]))\n",
    "            covs.append(C)\n",
    "        Cmean = np.mean(covs, axis=0)\n",
    "        W = _sym_invsqrt(Cmean, eps=eps)  # (C,C)\n",
    "        whiten[subj] = W.astype(np.float32)\n",
    "    return whiten\n",
    "\n",
    "def apply_ea_to_trials(trials_balanced, whiteners):\n",
    "    out = []\n",
    "    for seg, lab, subj, tkey in trials_balanced:\n",
    "        W = whiteners.get(subj, None)\n",
    "        if W is not None:\n",
    "            seg = seg @ W.T\n",
    "        out.append((seg, lab, subj, tkey))\n",
    "    return out\n",
    "\n",
    "# ---------- Windowing ----------\n",
    "def window_trials(trials, win_len_sec=2.5, win_stride_sec=0.5, fs=160.0, n_subwins=N_SUBWINS):\n",
    "    X, y, g, tkeys = [], [], [], []\n",
    "    win_len = int(round(win_len_sec * fs))\n",
    "    stride  = int(round(win_stride_sec * fs))\n",
    "    for seg, lab, subj, tkey in trials:\n",
    "        T = seg.shape[0]\n",
    "        starts = [i for i in range(0, T - win_len + 1, stride)]\n",
    "        if len(starts) > n_subwins: starts = starts[:n_subwins]\n",
    "        while len(starts) < n_subwins and (T - win_len) >= 0:\n",
    "            starts.append(T - win_len)\n",
    "        for s in starts:\n",
    "            e = s + win_len\n",
    "            sub = seg[s:e, :].astype(np.float32)\n",
    "            sub = (sub - sub.mean(axis=0, keepdims=True)) / (sub.std(axis=0, keepdims=True) + 1e-6)\n",
    "            X.append(sub); y.append(lab); g.append(subj); tkeys.append(tkey)\n",
    "    uniq = {k:i for i,k in enumerate(sorted(set(tkeys)))}\n",
    "    trial_ids = np.asarray([uniq[k] for k in tkeys], dtype=np.int64)\n",
    "    X = np.stack(X, axis=0).astype(np.float32)\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "    g = np.asarray(g, dtype=np.int64)\n",
    "    return X, y, g, trial_ids, uniq\n",
    "\n",
    "# =========================\n",
    "# Dataset torch\n",
    "# =========================\n",
    "class EEGTrials(Dataset):\n",
    "    def __init__(self, X, y, groups, trial_ids, subjs_for_mmd=None):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.g = groups.astype(np.int64)\n",
    "        self.t = trial_ids.astype(np.int64)\n",
    "        self.s = self.g if subjs_for_mmd is None else subjs_for_mmd.astype(np.int64)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]; x = np.expand_dims(x, 0)  # (1,Tw,C)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx]), torch.tensor(self.t[idx]), torch.tensor(self.s[idx])\n",
    "\n",
    "def build_weighted_sampler(y, groups):\n",
    "    y = np.asarray(y); groups = np.asarray(groups)\n",
    "    class_counts = np.bincount(y, minlength=len(np.unique(y))).astype(float)\n",
    "    class_w = 1.0 / class_counts[y]\n",
    "    subj_vals, subj_counts = np.unique(groups, return_counts=True)\n",
    "    subj_map = {s:c for s,c in zip(subj_vals, subj_counts)}\n",
    "    subj_w = np.array([1.0/subj_map[g] for g in groups], dtype=float)\n",
    "    w = class_w * subj_w; w = w / w.mean()\n",
    "    return WeightedRandomSampler(weights=torch.from_numpy(w).float(), num_samples=len(w), replacement=True)\n",
    "\n",
    "# =========================\n",
    "# Modelo: CNN + Transformer + Cosine head\n",
    "# =========================\n",
    "class ChannelDropout(nn.Module):\n",
    "    def __init__(self, p=0.1): super().__init__(); self.p = p\n",
    "    def forward(self, x):\n",
    "        if not self.training or self.p<=0: return x\n",
    "        B,_,T,C = x.shape\n",
    "        mask = (torch.rand(B,1,1,C, device=x.device) > self.p).float()\n",
    "        return x * mask\n",
    "\n",
    "@torch.no_grad()\n",
    "def apply_max_norm(model, max_value=2.0, p=2.0):\n",
    "    for m in model.modules():\n",
    "        if hasattr(m,'weight') and isinstance(m,(nn.Conv2d, nn.Linear)):\n",
    "            w = m.weight.data\n",
    "            norms = w.view(w.size(0), -1).norm(p=p, dim=1, keepdim=True)\n",
    "            desired = torch.clamp(norms, max=max_value)\n",
    "            w.view(w.size(0), -1).mul_(desired / (1e-8 + norms))\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 4000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0), persistent=False)\n",
    "    def forward(self, x):\n",
    "        T = x.size(1); return x + self.pe[:, :T, :]\n",
    "\n",
    "class CNNTokenizer(nn.Module):\n",
    "    def __init__(self, n_ch: int, d_feat: int = 128, k_temporal: int = 64, stride_t: int = 2, chdrop_p: float = 0.1, drop_p: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.chdrop = ChannelDropout(chdrop_p)\n",
    "        self.conv_t = nn.Conv2d(1, d_feat, kernel_size=(k_temporal,1), stride=(stride_t,1), padding=(k_temporal//2,0), bias=False)\n",
    "        self.bn_t   = nn.BatchNorm2d(d_feat, momentum=0.99, eps=1e-3)\n",
    "        self.act    = nn.ELU()\n",
    "        self.conv_sp_dw = nn.Conv2d(d_feat, d_feat, kernel_size=(1,n_ch), groups=d_feat, bias=False)\n",
    "        self.bn_sp      = nn.BatchNorm2d(d_feat, momentum=0.99, eps=1e-3)\n",
    "        self.conv_pw = nn.Conv2d(d_feat, d_feat, kernel_size=(1,1), bias=False)\n",
    "        self.bn_pw   = nn.BatchNorm2d(d_feat, momentum=0.99, eps=1e-3)\n",
    "        self.drop    = nn.Dropout(drop_p)\n",
    "    def forward(self, x):\n",
    "        z = self.chdrop(x)\n",
    "        z = self.conv_t(z); z = self.bn_t(z); z = self.act(z)\n",
    "        z = self.conv_sp_dw(z); z = self.bn_sp(z); z = self.act(z)\n",
    "        z = self.conv_pw(z); z = self.bn_pw(z); z = self.act(z)\n",
    "        z = self.drop(z)\n",
    "        z = z.squeeze(-1).transpose(1,2)  # (B,T',D)\n",
    "        return z\n",
    "\n",
    "def _enc_layer(d_model=128, nhead=4, dim_ff=256, drop=0.2):\n",
    "    return nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_ff,\n",
    "                                      dropout=drop, activation='gelu', batch_first=True, norm_first=True)\n",
    "\n",
    "class TemporalTransformer(nn.Module):\n",
    "    def __init__(self, d_model=128, nhead=4, dim_ff=256, depth=3, drop=0.2):\n",
    "        super().__init__()\n",
    "        self.pe = PositionalEncoding(d_model)\n",
    "        layer = _enc_layer(d_model, nhead, dim_ff, drop)\n",
    "        self.enc = nn.TransformerEncoder(layer, num_layers=depth)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    def forward(self, x):\n",
    "        x = self.pe(x)\n",
    "        x = self.enc(x)\n",
    "        return self.norm(x)\n",
    "\n",
    "class CosineClassifier(nn.Module):\n",
    "    def __init__(self, d, n_classes, tau=10.0):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.randn(n_classes, d))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        self.log_tau = nn.Parameter(torch.log(torch.tensor(tau, dtype=torch.float32)))\n",
    "    def forward(self, f):  # f: (B,d)\n",
    "        f_n = F.normalize(f, dim=1)\n",
    "        W_n = F.normalize(self.W, dim=1)\n",
    "        cos = torch.mm(f_n, W_n.t())\n",
    "        return torch.exp(self.log_tau) * cos\n",
    "\n",
    "class HybridCNNTransformer(nn.Module):\n",
    "    def __init__(self, n_ch: int, n_classes: int, seq_len: int,\n",
    "                 d_model: int = 128, nhead: int = 4, depth: int = 3,\n",
    "                 dim_ff: int = 256, k_temporal: int = 64, stride_t: int = 2,\n",
    "                 drop: float = 0.2, chdrop_p: float = 0.1, use_cls_token: bool = True):\n",
    "        super().__init__()\n",
    "        self.use_cls = use_cls_token\n",
    "        self.tokenizer = CNNTokenizer(n_ch, d_model, k_temporal, stride_t, chdrop_p, drop)\n",
    "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) if use_cls_token else None\n",
    "        self.transformer = TemporalTransformer(d_model, nhead, dim_ff, depth, drop)\n",
    "        self.norm_head = nn.LayerNorm(d_model)\n",
    "        self.head = CosineClassifier(d_model, n_classes, tau=10.0)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "\n",
    "    def tokenize(self, x):\n",
    "        tok = self.tokenizer(x)                 # (B,T',D)\n",
    "        if self.use_cls:\n",
    "            B = tok.size(0); cls_tok = self.cls.expand(B,-1,-1)\n",
    "            tok = torch.cat([cls_tok, tok], dim=1)\n",
    "        return tok\n",
    "\n",
    "    def forward_from_tokens(self, tok):\n",
    "        z = self.transformer(tok)\n",
    "        feat = z[:,0,:] if self.use_cls else z.mean(dim=1)\n",
    "        feat = self.norm_head(feat)\n",
    "        return self.head(feat)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tok = self.tokenize(x)\n",
    "        return self.forward_from_tokens(tok)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def extract_feats(self, x):\n",
    "        tok = self.tokenize(x)\n",
    "        z = self.transformer(tok)\n",
    "        feat = z[:,0,:] if self.use_cls else z.mean(dim=1)\n",
    "        return self.norm_head(feat)\n",
    "\n",
    "# =========================\n",
    "# Losses / Métricas\n",
    "# =========================\n",
    "class ClassBalancedFocal(nn.Module):\n",
    "    \"\"\"Class-Balanced Focal Loss con label smoothing.\n",
    "       ✅ FIX DEVICE: cb_w siempre se usa en el mismo device que los logits.\"\"\"\n",
    "    def __init__(self, n_classes: int, beta: float = 0.999, gamma: float = 2.0, label_smoothing: float = 0.05):\n",
    "        super().__init__()\n",
    "        self.n = n_classes\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.ls = label_smoothing\n",
    "        self.register_buffer('cb_w', torch.ones(n_classes))  # buffer en CPU por defecto\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_weights(self, counts: np.ndarray):\n",
    "        eff_num = (1.0 - np.power(self.beta, counts)) / (1.0 - self.beta + 1e-8)\n",
    "        w = (eff_num.sum() / (eff_num + 1e-8))\n",
    "        w = w / w.mean()\n",
    "        # mantener device del buffer\n",
    "        self.cb_w = torch.tensor(w, dtype=torch.float32, device=self.cb_w.device)\n",
    "\n",
    "    def forward(self, logits, y):\n",
    "        B, C = logits.size()\n",
    "        device = logits.device\n",
    "        with torch.no_grad():\n",
    "            y_one = F.one_hot(y, C).float()\n",
    "            if self.ls > 0:\n",
    "                y_one = (1 - self.ls) * y_one + self.ls * (1.0 / C)\n",
    "        logp = F.log_softmax(logits, dim=1)\n",
    "        p = logp.exp()\n",
    "        focal = ((1 - p) ** self.gamma)\n",
    "        loss = -(y_one * focal * logp)\n",
    "        # mover cb_w al device de logits si hace falta\n",
    "        cw = self.cb_w if self.cb_w.device == device else self.cb_w.to(device)\n",
    "        loss = loss * cw.unsqueeze(0)\n",
    "        return loss.sum(dim=1).mean()\n",
    "\n",
    "def print_report(y_true, y_pred, classes, tag=\"\"):\n",
    "    print(f\"\\n=== Reporte {tag} ===\")\n",
    "    print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
    "    print(f\"F1-macro: {f1_score(y_true, y_pred, average='macro'):.4f} | Kappa: {cohen_kappa_score(y_true,y_pred):.4f}\")\n",
    "\n",
    "# ---------- MMD ----------\n",
    "def _rbf_mmd2(x, y, gamma=None):\n",
    "    if gamma is None:\n",
    "        with torch.no_grad():\n",
    "            z = torch.cat([x, y], dim=0)\n",
    "            d2 = torch.cdist(z, z, p=2.0).pow(2)\n",
    "            med2 = torch.median(d2[d2>0]).clamp(min=1e-8)\n",
    "            gamma = 1.0 / (2.0 * med2)\n",
    "    Kxx = torch.exp(-gamma * torch.cdist(x, x, p=2.0).pow(2))\n",
    "    Kyy = torch.exp(-gamma * torch.cdist(y, y, p=2.0).pow(2))\n",
    "    Kxy = torch.exp(-gamma * torch.cdist(x, y, p=2.0).pow(2))\n",
    "    n = x.size(0); m = y.size(0)\n",
    "    mmd2 = (Kxx.sum() - torch.diag(Kxx).sum()) / (n*(n-1) + 1e-8) \\\n",
    "         + (Kyy.sum() - torch.diag(Kyy).sum()) / (m*(m-1) + 1e-8) \\\n",
    "         - 2.0 * Kxy.mean()\n",
    "    return mmd2\n",
    "\n",
    "def mmd_between_subject_groups(model: HybridCNNTransformer, xb, subj_ids, max_samples=64):\n",
    "    subj_ids = subj_ids.detach().cpu().numpy().tolist()\n",
    "    uniq = sorted(set(subj_ids))\n",
    "    if len(uniq) < 2: return None\n",
    "    counts = {s: subj_ids.count(s) for s in uniq}\n",
    "    top = sorted(counts.items(), key=lambda p: p[1], reverse=True)[:2]\n",
    "    sA, sB = top[0][0], top[1][0]\n",
    "    maskA = torch.tensor([si==sA for si in subj_ids], device=xb.device)\n",
    "    maskB = torch.tensor([si==sB for si in subj_ids], device=xb.device)\n",
    "    if maskA.sum()<2 or maskB.sum()<2: return None\n",
    "    idxA = torch.where(maskA)[0]\n",
    "    idxB = torch.where(maskB)[0]\n",
    "    nA = min(max_samples, idxA.numel())\n",
    "    nB = min(max_samples, idxB.numel())\n",
    "    idxA = idxA[torch.randperm(idxA.numel(), device=xb.device)[:nA]]\n",
    "    idxB = idxB[torch.randperm(idxB.numel(), device=xb.device)[:nB]]\n",
    "    fA = model.extract_feats(xb[idxA]).detach()\n",
    "    fB = model.extract_feats(xb[idxB]).detach()\n",
    "    mmd2 = _rbf_mmd2(fA, fB)\n",
    "    return mmd2\n",
    "\n",
    "# ---------- Feature Mixup en tokens ----------\n",
    "def feature_mixup(tok, alpha=0.2):\n",
    "    if alpha <= 0:\n",
    "        return tok, None\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    perm = torch.randperm(tok.size(0), device=tok.device)\n",
    "    tok_mix = lam * tok + (1 - lam) * tok[perm]\n",
    "    return tok_mix, perm\n",
    "\n",
    "# =========================\n",
    "# Inferencia por ensayo (τ per-class)\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_per_trial(model, loader, use_tta=True, tta_n=5, n_classes=4, per_class_tau=PER_CLASS_TAU):\n",
    "    model.eval()\n",
    "    sums, counts, labels = {}, {}, {}\n",
    "    for xb, yb, _, tb, _ in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        if use_tta:\n",
    "            def time_jitter(x, max_ms=25, fs=160.0):\n",
    "                max_shift = int(round(max_ms/1000.0 * fs))\n",
    "                if max_shift <= 0: return x\n",
    "                B,_,T,C = x.shape\n",
    "                shifts = torch.randint(low=-max_shift, high=max_shift+1, size=(B,), device=x.device)\n",
    "                out = torch.zeros_like(x)\n",
    "                for i,s in enumerate(shifts):\n",
    "                    if s >= 0: out[i,:,s:,:] = x[i,:,:T-s,:]\n",
    "                    else: s=-s; out[i,:,:T-s,:] = x[i,:,s:,:]\n",
    "                return out\n",
    "            outs = []\n",
    "            for _ in range(tta_n):\n",
    "                outs.append(model(time_jitter(xb,25)))\n",
    "            logits = torch.stack(outs, dim=0).mean(dim=0)\n",
    "        else:\n",
    "            logits = model(xb)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        logits = logits / per_class_tau.reshape(1, -1)  # calibración por-clase\n",
    "\n",
    "        yb = yb.numpy(); tb = tb.numpy()\n",
    "        for i in range(xb.size(0)):\n",
    "            t = int(tb[i])\n",
    "            if t not in sums:\n",
    "                sums[t] = np.zeros((n_classes,), dtype=np.float64)\n",
    "                counts[t] = 0\n",
    "                labels[t] = int(yb[i])\n",
    "            sums[t] += logits[i]; counts[t] += 1\n",
    "    ids = sorted(sums.keys())\n",
    "    y_true_trial = np.asarray([labels[t] for t in ids], dtype=int)\n",
    "    y_pred_trial = np.asarray([int((sums[t]/max(1,counts[t])).argmax()) for t in ids], dtype=int)\n",
    "    acc = (y_true_trial == y_pred_trial).mean()\n",
    "    return y_true_trial, y_pred_trial, float(acc)\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes, title, fname):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.nan_to_num(cm_norm)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title); plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    ticks = np.arange(len(classes))\n",
    "    plt.xticks(ticks, classes, rotation=45, ha='right'); plt.yticks(ticks, classes)\n",
    "    fmt = '.2f'; thresh = cm_norm.max()/2.\n",
    "    for i,j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "        plt.text(j,i,format(cm_norm[i,j],fmt),ha=\"center\", color=\"white\" if cm_norm[i,j]>thresh else \"black\")\n",
    "    plt.ylabel('True'); plt.xlabel('Pred'); plt.tight_layout(); plt.savefig(fname, dpi=150); plt.close()\n",
    "\n",
    "# =========================\n",
    "# TTA BN-only por sujeto (smoothing)\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def bn_adapt_subject_smooth(model, X_subj, batch=256, momentum=0.2):\n",
    "    saved = []\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            saved.append((m, m.running_mean.clone(), m.running_var.clone()))\n",
    "    was_training = model.training\n",
    "    model.train()\n",
    "    for i in range(0, len(X_subj), batch):\n",
    "        xb = torch.from_numpy(X_subj[i:i+batch]).float().unsqueeze(1).to(DEVICE)\n",
    "        _ = model(xb)\n",
    "    model.eval()\n",
    "    for m, old_mean, old_var in saved:\n",
    "        m.running_mean = (1 - momentum) * old_mean + momentum * m.running_mean\n",
    "        m.running_var  = (1 - momentum) * old_var  + momentum * m.running_var\n",
    "    if was_training: model.train()\n",
    "\n",
    "# =========================\n",
    "# Folds (subjects-only)\n",
    "# =========================\n",
    "def read_folds_subjects_only(json_path, current_subjects):\n",
    "    p = Path(json_path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"No existe {p}. Provee Kfold5.json con sujetos por fold.\")\n",
    "    with open(p, 'r', encoding='utf-8') as f:\n",
    "        payload = json.load(f)\n",
    "    folds_raw = payload.get('folds', [])\n",
    "    if not folds_raw:\n",
    "        raise ValueError(\"JSON no contiene 'folds'.\")\n",
    "\n",
    "    cur_set = set(current_subjects)\n",
    "    folds = []\n",
    "    for fr in folds_raw:\n",
    "        train_s = fr.get('train_subjects', fr.get('train', []))\n",
    "        test_s  = fr.get('test_subjects',  fr.get('test',  []))\n",
    "        if not train_s or not test_s:\n",
    "            raise ValueError(\"Cada fold debe tener 'train_subjects' y 'test_subjects' (o 'train'/'test').\")\n",
    "\n",
    "        def to_int_list(lst):\n",
    "            out = []\n",
    "            for s in lst:\n",
    "                if isinstance(s, int): out.append(s)\n",
    "                elif isinstance(s, str) and s.upper().startswith('S') and s[1:].isdigit():\n",
    "                    out.append(int(s[1:]))\n",
    "                elif isinstance(s, str) and s.isdigit():\n",
    "                    out.append(int(s))\n",
    "                else:\n",
    "                    raise ValueError(f\"Sujeto inválido en JSON: {s}\")\n",
    "            return out\n",
    "\n",
    "        tr_int = to_int_list(train_s)\n",
    "        te_int = to_int_list(test_s)\n",
    "\n",
    "        miss_tr = [s for s in tr_int if s not in cur_set]\n",
    "        miss_te = [s for s in te_int if s not in cur_set]\n",
    "        if miss_tr or miss_te:\n",
    "            raise ValueError(f\"Fold con sujetos fuera del dataset. Falta TRAIN: {miss_tr}, TEST: {miss_te}\")\n",
    "\n",
    "        folds.append({'fold': fr.get('fold', len(folds)+1),\n",
    "                      'train_subjects': tr_int, 'test_subjects': te_int})\n",
    "    return folds\n",
    "\n",
    "# =========================\n",
    "# Experimento\n",
    "# =========================\n",
    "def plot_training_curves(history, fname):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(history['train_acc'], label='train_acc')\n",
    "    plt.plot(history['val_acc'], label='val_acc')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Training curve'); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(fname, dpi=150); plt.close()\n",
    "\n",
    "def run_experiment():\n",
    "    mne.set_log_level('WARNING')\n",
    "    subs = subjects_available()\n",
    "    print(f\"Sujetos elegibles: {len(subs)} → {subs[:10]}{'...' if len(subs)>10 else ''}\")\n",
    "\n",
    "    trials_bal = build_balanced_trials(subs, scenario=CLASS_SCENARIO, window_global=WINDOW_GLOBAL,\n",
    "                                       n_per_class=N_PER_CLASS_PER_SUBJECT)\n",
    "    subj_in_trials = sorted({t[2] for t in trials_bal})\n",
    "    print(f\"Sujetos con ensayos balanceados: {len(subj_in_trials)}\")\n",
    "\n",
    "    if USE_EA:\n",
    "        print(\"🔧 EA: calculando whiteners por sujeto...\")\n",
    "        Wsubj = compute_ea_whiteners_per_subject(trials_bal)\n",
    "        trials_bal = apply_ea_to_trials(trials_bal, Wsubj)\n",
    "        print(f\"EA aplicado a {len(Wsubj)} sujetos.\")\n",
    "\n",
    "    folds = read_folds_subjects_only(FOLDS_JSON, current_subjects=subj_in_trials)\n",
    "\n",
    "    global_folds_trial, all_true_trial, all_pred_trial = [], [], []\n",
    "\n",
    "    for f in folds:\n",
    "        fold = f['fold']\n",
    "        train_subs = set(f['train_subjects'])\n",
    "        test_subs  = set(f['test_subjects'])\n",
    "\n",
    "        tr_trials = [t for t in trials_bal if t[2] in train_subs]\n",
    "        te_trials = [t for t in trials_bal if t[2] in test_subs]\n",
    "\n",
    "        train_subs_list = sorted(list(train_subs))\n",
    "        rng = np.random.RandomState(RANDOM_STATE + fold)\n",
    "        rng.shuffle(train_subs_list)\n",
    "        n_va = max(1, int(round(GLOBAL_VAL_SPLIT_SUBJ * len(train_subs_list))))\n",
    "        val_subs = set(train_subs_list[:n_va])\n",
    "        real_train_subs = set(train_subs_list[n_va:])\n",
    "\n",
    "        tr_trials_main = [t for t in tr_trials if t[2] in real_train_subs]\n",
    "        va_trials      = [t for t in tr_trials if t[2] in val_subs]\n",
    "\n",
    "        Xtr, ytr, gtr, ttr, _ = window_trials(trials=tr_trials_main, win_len_sec=WIN_LEN_SEC, win_stride_sec=WIN_STRIDE_SEC, fs=FS, n_subwins=N_SUBWINS)\n",
    "        Xva, yva, gva, tva, _ = window_trials(trials=va_trials,      win_len_sec=WIN_LEN_SEC, win_stride_sec=WIN_STRIDE_SEC, fs=FS, n_subwins=N_SUBWINS)\n",
    "        Xte, yte, gte, tte, _ = window_trials(trials=te_trials,      win_len_sec=WIN_LEN_SEC, win_stride_sec=WIN_STRIDE_SEC, fs=FS, n_subwins=N_SUBWINS)\n",
    "\n",
    "        ds_tr = EEGTrials(Xtr, ytr, gtr, ttr)\n",
    "        ds_va = EEGTrials(Xva, yva, gva, tva)\n",
    "        ds_te = EEGTrials(Xte, yte, gte, tte)\n",
    "\n",
    "        tr_loader = DataLoader(ds_tr, batch_size=BATCH_SIZE,\n",
    "                               sampler=build_weighted_sampler(ytr, gtr), drop_last=False)\n",
    "        va_loader = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "        te_loader = DataLoader(ds_te, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "        Tw, C = Xtr.shape[1], Xtr.shape[2]\n",
    "        n_classes = 4\n",
    "        model = HybridCNNTransformer(\n",
    "            n_ch=C, n_classes=n_classes, seq_len=Tw,\n",
    "            d_model=128, nhead=4, depth=3, dim_ff=256,\n",
    "            k_temporal=64, stride_t=2, drop=0.2, chdrop_p=0.10, use_cls_token=True\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=LR_INIT)\n",
    "\n",
    "        criterion = ClassBalancedFocal(n_classes=n_classes, beta=0.999, gamma=2.0, label_smoothing=0.05)\n",
    "        class_counts = np.bincount(ytr, minlength=n_classes)\n",
    "        criterion.update_weights(class_counts)\n",
    "\n",
    "        if USE_SWA:\n",
    "            swa_start_epoch = max(5, int(SWA_START_FRAC * EPOCHS_GLOBAL))\n",
    "            swa_model = AveragedModel(model)\n",
    "            swa_scheduler = SWALR(opt, anneal_strategy=\"cos\", anneal_epochs=5, swa_lr=LR_INIT * SWA_LR_FACTOR)\n",
    "\n",
    "        def _acc_trial(loader):\n",
    "            y_true_tr, y_pred_tr, acc_tr = evaluate_per_trial(model, loader, use_tta=False, n_classes=n_classes, per_class_tau=PER_CLASS_TAU)\n",
    "            return acc_tr\n",
    "\n",
    "        history = {'train_acc': [], 'val_acc': []}\n",
    "        print(f\"\\n[Fold {fold}/{len(folds)}] Entrenando global (VAL por ensayo)...\")\n",
    "        best_state = copy.deepcopy(model.state_dict()); best_val = -1.0; bad = 0\n",
    "\n",
    "        for epoch in range(1, EPOCHS_GLOBAL+1):\n",
    "            model.train()\n",
    "            for xb, yb, gb, tb, sb in tr_loader:\n",
    "                xb, yb, sb = xb.to(DEVICE), yb.to(DEVICE), sb.to(DEVICE)\n",
    "\n",
    "                tok = model.tokenize(xb)\n",
    "                tok, _ = feature_mixup(tok, alpha=0.2)\n",
    "\n",
    "                logits = model.forward_from_tokens(tok)\n",
    "                loss = criterion(logits, yb)\n",
    "\n",
    "                if MMD_LAMBDA > 0.0:\n",
    "                    mmd2 = mmd_between_subject_groups(model, xb, sb, max_samples=MMD_MAX_SAMPLES_PER_GROUP)\n",
    "                    if mmd2 is not None:\n",
    "                        loss = loss + MMD_LAMBDA * mmd2\n",
    "\n",
    "                opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "                apply_max_norm(model, 2.0, p=2.0)\n",
    "\n",
    "            if USE_SWA and epoch >= swa_start_epoch:\n",
    "                swa_model.update_parameters(model)\n",
    "                swa_scheduler.step()\n",
    "\n",
    "            tr_acc = _acc_trial(tr_loader)\n",
    "            va_acc = _acc_trial(va_loader)\n",
    "            history['train_acc'].append(tr_acc); history['val_acc'].append(va_acc)\n",
    "\n",
    "            if epoch % 5 == 0 or epoch in (1,10,20,50,80):\n",
    "                print(f\"  Época {epoch:3d} | train(ensayo)={tr_acc:.4f} | val(ensayo)={va_acc:.4f}\")\n",
    "\n",
    "            if va_acc > best_val + 1e-4:\n",
    "                best_val, bad = va_acc, 0\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "            else:\n",
    "                bad += 1\n",
    "                if bad >= GLOBAL_PATIENCE:\n",
    "                    print(f\"  Early stopping @ epoch {epoch} (best val ensayo={best_val:.4f})\")\n",
    "                    break\n",
    "\n",
    "        if USE_SWA:\n",
    "            torch.optim.swa_utils.update_bn(tr_loader, swa_model, device=DEVICE)\n",
    "            model.load_state_dict(swa_model.module.state_dict())\n",
    "        else:\n",
    "            model.load_state_dict(best_state)\n",
    "\n",
    "        plot_training_curves(history, f\"training_curve_cnnTrans_ea_mmd_fold{fold}.png\")\n",
    "        print(f\"↳ Curva: training_curve_cnnTrans_ea_mmd_fold{fold}.png\")\n",
    "\n",
    "        if TTA_BN_ON_TEST:\n",
    "            print(\"🔧 TTA-BN: adaptando BN por sujeto en TEST...\")\n",
    "            subj2idx = {}\n",
    "            for i, sid in enumerate(gte):\n",
    "                subj2idx.setdefault(int(sid), []).append(i)\n",
    "\n",
    "            sums_all, counts_all, labels_all = {}, {}, {}\n",
    "            model.eval()\n",
    "\n",
    "            for sid, idxs in subj2idx.items():\n",
    "                Xs = Xte[idxs]\n",
    "                bn_adapt_subject_smooth(model, Xs, batch=256, momentum=0.2)\n",
    "\n",
    "                for i0 in range(0, len(idxs), BATCH_SIZE):\n",
    "                    sel = idxs[i0:i0+BATCH_SIZE]\n",
    "                    xb = torch.from_numpy(Xte[sel]).float().unsqueeze(1).to(DEVICE)\n",
    "\n",
    "                    outs = []\n",
    "                    with torch.no_grad():\n",
    "                        for _ in range(5):\n",
    "                            def time_jitter(x, max_ms=25, fs=160.0):\n",
    "                                max_shift = int(round(max_ms/1000.0 * fs))\n",
    "                                if max_shift <= 0: return x\n",
    "                                B,_,T,C = x.shape\n",
    "                                shifts = torch.randint(low=-max_shift, high=max_shift+1, size=(B,), device=x.device)\n",
    "                                out = torch.zeros_like(x)\n",
    "                                for ii,s in enumerate(shifts):\n",
    "                                    if s >= 0: out[ii,:,s:,:] = x[ii,:,:T-s,:]\n",
    "                                    else: s = -s; out[ii,:,:T-s,:] = x[ii,:,s:,:]\n",
    "                                return out\n",
    "                            outs.append(model(time_jitter(xb,25)))\n",
    "                    logits = torch.stack(outs, 0).mean(0).cpu().numpy()\n",
    "                    logits = logits / PER_CLASS_TAU.reshape(1, -1)\n",
    "\n",
    "                    for k, j in enumerate(sel):\n",
    "                        trial_id = int(tte[j])\n",
    "                        ytrue    = int(yte[j])\n",
    "                        if trial_id not in sums_all:\n",
    "                            sums_all[trial_id] = np.zeros((n_classes,), dtype=np.float64)\n",
    "                            counts_all[trial_id] = 0\n",
    "                            labels_all[trial_id] = ytrue\n",
    "                        sums_all[trial_id] += logits[k]\n",
    "                        counts_all[trial_id] += 1\n",
    "\n",
    "            ids = sorted(sums_all.keys())\n",
    "            y_true_tr = np.asarray([labels_all[t] for t in ids], dtype=int)\n",
    "            y_pred_tr = np.asarray([int((sums_all[t]/max(1,counts_all[t])).argmax()) for t in ids], dtype=int)\n",
    "            acc_tr = (y_true_tr == y_pred_tr).mean()\n",
    "        else:\n",
    "            y_true_tr, y_pred_tr, acc_tr = evaluate_per_trial(model, te_loader, use_tta=True, n_classes=n_classes, per_class_tau=PER_CLASS_TAU)\n",
    "\n",
    "        print(f\"[Fold {fold}] Ensayo acc={acc_tr:.4f}\")\n",
    "        print(\"Distribución real por clase en test (por ensayo):\", np.bincount(y_true_tr, minlength=4))\n",
    "        print_report(y_true_tr, y_pred_tr, CLASS_NAMES, tag=\"por ENSAYO (agregado)\")\n",
    "        global_folds_trial.append(acc_tr)\n",
    "        all_true_trial.append(y_true_tr); all_pred_trial.append(y_pred_tr)\n",
    "\n",
    "    if len(all_true_trial)>0: all_true_trial = np.concatenate(all_true_trial); all_pred_trial = np.concatenate(all_pred_trial)\n",
    "    else: all_true_trial = np.array([],dtype=int); all_pred_trial = np.array([],dtype=int)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"RESULTADOS FINALES — CNN+Transformer + Cosine head + EA={USE_EA} + MMD={MMD_LAMBDA} + TTA-BN={TTA_BN_ON_TEST}\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Ensayo folds:\", [f\"{a:.4f}\" for a in global_folds_trial])\n",
    "    if len(global_folds_trial) > 0:\n",
    "        print(f\"Ensayo mean: {np.mean(global_folds_trial):.4f}\")\n",
    "\n",
    "    if all_true_trial.size > 0:\n",
    "        plot_confusion(all_true_trial, all_pred_trial, CLASS_NAMES,\n",
    "                       title=\"Confusion Matrix - Global Model (Per Trial, All Folds)\",\n",
    "                       fname=\"confusion_cnnTrans_ea_mmd_ttabn_trial_allfolds.png\")\n",
    "        print(\"↳ Matriz de confusión (ensayo): confusion_cnnTrans_ea_mmd_ttabn_trial_allfolds.png\")\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    bp_status = f\"ON [{BP_LO}-{BP_HI} Hz]\" if USE_BANDPASS and (BP_LO is not None) and (BP_HI is not None) else \"OFF\"\n",
    "    print(\"🧠 MI-EEG — CNN+Transformer (21/cls/subj) + Sliding Window + Cosine head\")\n",
    "    print(f\"🔧 Escenario: {CLASS_SCENARIO} | rango ensayo={WINDOW_GLOBAL} s | subventana={WIN_LEN_SEC}s | stride={WIN_STRIDE_SEC}s | subwins/ensayo={N_SUBWINS} | band-pass={bp_status}\")\n",
    "    print(f\"⚙️  EA: {USE_EA} | MMD_LAMBDA={MMD_LAMBDA} | TTA-BN test: {TTA_BN_ON_TEST}\")\n",
    "    run_experiment()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
