{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "424c22f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Usando dispositivo: cuda\n",
      "🧠 INICIANDO EXPERIMENTO Conformer-MI v2 — balance-first (4 clases, 8 canales)\n",
      "⚙️  GLOBAL: epochs=100, lr=0.001, warmup=5, batch=32\n",
      "🔧 Augments: bandpass=ON, amp_scale=(0.8, 1.2), ch_drop=0.1\n",
      "🔧 FT por sujeto (adaptativo): epochs=15, head_lr=0.001, base_lr=5e-05, L2SP=0.0001\n",
      "Sujetos elegibles: 103 → [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Escaneando eventos (sin crops):   0%|          | 0/103 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Escaneando eventos (sin crops): 100%|██████████| 103/103 [00:39<00:00,  2.64it/s]\n",
      "Generando segmentos (con crops):   1%|          | 1/103 [00:00<00:14,  6.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] S001 R08 C0 onset=120.400 win=(0.00,3.00) s=19264 e=19744 T=480 C=8\n",
      "[OK] S001 R08 C0 onset=20.800 win=(0.00,3.00) s=3328 e=3808 T=480 C=8\n",
      "[OK] S001 R04 C0 onset=12.500 win=(0.00,3.00) s=2000 e=2480 T=480 C=8\n",
      "[OK] S001 R08 C0 onset=4.200 win=(0.00,3.00) s=672 e=1152 T=480 C=8\n",
      "[OK] S001 R12 C0 onset=29.100 win=(0.00,3.00) s=4656 e=5136 T=480 C=8\n",
      "[OK] S001 R08 C0 onset=78.900 win=(0.00,3.00) s=12624 e=13104 T=480 C=8\n",
      "[OK] S001 R04 C0 onset=20.800 win=(0.00,3.00) s=3328 e=3808 T=480 C=8\n",
      "[OK] S001 R08 C0 onset=87.200 win=(0.00,3.00) s=13952 e=14432 T=480 C=8\n",
      "[OK] S001 R04 C0 onset=87.200 win=(0.00,3.00) s=13952 e=14432 T=480 C=8\n",
      "[OK] S001 R04 C0 onset=45.700 win=(0.00,3.00) s=7312 e=7792 T=480 C=8\n",
      "[OK] S001 R08 C0 onset=54.000 win=(0.00,3.00) s=8640 e=9120 T=480 C=8\n",
      "[OK] S001 R12 C0 onset=70.600 win=(0.00,3.00) s=11296 e=11776 T=480 C=8\n",
      "[OK] S001 R04 C0 onset=62.300 win=(0.00,3.00) s=9968 e=10448 T=480 C=8\n",
      "[OK] S001 R04 C0 onset=78.900 win=(0.00,3.00) s=12624 e=13104 T=480 C=8\n",
      "[OK] S001 R12 C0 onset=37.400 win=(0.00,3.00) s=5984 e=6464 T=480 C=8\n",
      "[OK] S001 R12 C0 onset=12.500 win=(0.00,3.00) s=2000 e=2480 T=480 C=8\n",
      "[OK] S001 R12 C0 onset=95.500 win=(0.00,3.00) s=15280 e=15760 T=480 C=8\n",
      "[OK] S001 R12 C0 onset=112.100 win=(0.00,3.00) s=17936 e=18416 T=480 C=8\n",
      "[OK] S001 R04 C0 onset=120.400 win=(0.00,3.00) s=19264 e=19744 T=480 C=8\n",
      "[OK] S001 R08 C0 onset=37.400 win=(0.00,3.00) s=5984 e=6464 T=480 C=8\n",
      "[OK] S001 R08 C0 onset=112.100 win=(0.00,3.00) s=17936 e=18416 T=480 C=8\n",
      "[OK] S001 R12 C1 onset=78.900 win=(0.00,3.00) s=12624 e=13104 T=480 C=8\n",
      "[OK] S001 R04 C1 onset=29.100 win=(0.00,3.00) s=4656 e=5136 T=480 C=8\n",
      "[OK] S001 R04 C1 onset=95.500 win=(0.00,3.00) s=15280 e=15760 T=480 C=8\n",
      "[OK] S001 R04 C1 onset=4.200 win=(0.00,3.00) s=672 e=1152 T=480 C=8\n",
      "[OK] S001 R08 C1 onset=62.300 win=(0.00,3.00) s=9968 e=10448 T=480 C=8\n",
      "[OK] S001 R08 C1 onset=12.500 win=(0.00,3.00) s=2000 e=2480 T=480 C=8\n",
      "[OK] S001 R04 C1 onset=70.600 win=(0.00,3.00) s=11296 e=11776 T=480 C=8\n",
      "[OK] S001 R08 C1 onset=95.500 win=(0.00,3.00) s=15280 e=15760 T=480 C=8\n",
      "[OK] S001 R08 C1 onset=29.100 win=(0.00,3.00) s=4656 e=5136 T=480 C=8\n",
      "[OK] S001 R04 C1 onset=54.000 win=(0.00,3.00) s=8640 e=9120 T=480 C=8\n",
      "[OK] S001 R04 C1 onset=112.100 win=(0.00,3.00) s=17936 e=18416 T=480 C=8\n",
      "[OK] S001 R12 C1 onset=103.800 win=(0.00,3.00) s=16608 e=17088 T=480 C=8\n",
      "[OK] S001 R04 C1 onset=37.400 win=(0.00,3.00) s=5984 e=6464 T=480 C=8\n",
      "[OK] S001 R12 C1 onset=87.200 win=(0.00,3.00) s=13952 e=14432 T=480 C=8\n",
      "[OK] S001 R12 C1 onset=54.000 win=(0.00,3.00) s=8640 e=9120 T=480 C=8\n",
      "[OK] S001 R08 C1 onset=103.800 win=(0.00,3.00) s=16608 e=17088 T=480 C=8\n",
      "[OK] S001 R12 C1 onset=4.200 win=(0.00,3.00) s=672 e=1152 T=480 C=8\n",
      "[OK] S001 R12 C1 onset=20.800 win=(0.00,3.00) s=3328 e=3808 T=480 C=8\n",
      "[OK] S001 R08 C1 onset=45.700 win=(0.00,3.00) s=7312 e=7792 T=480 C=8\n",
      "[OK] S001 R12 C1 onset=45.700 win=(0.00,3.00) s=7312 e=7792 T=480 C=8\n",
      "[OK] S001 R12 C1 onset=120.400 win=(0.00,3.00) s=19264 e=19744 T=480 C=8\n",
      "[OK] S001 R06 C2 onset=12.500 win=(0.00,3.00) s=2000 e=2480 T=480 C=8\n",
      "[OK] S001 R06 C2 onset=20.800 win=(0.00,3.00) s=3328 e=3808 T=480 C=8\n",
      "[OK] S001 R06 C2 onset=37.400 win=(0.00,3.00) s=5984 e=6464 T=480 C=8\n",
      "[OK] S001 R06 C2 onset=62.300 win=(0.00,3.00) s=9968 e=10448 T=480 C=8\n",
      "[OK] S001 R06 C2 onset=70.600 win=(0.00,3.00) s=11296 e=11776 T=480 C=8\n",
      "[OK] S001 R06 C2 onset=95.500 win=(0.00,3.00) s=15280 e=15760 T=480 C=8\n",
      "[OK] S001 R06 C2 onset=103.800 win=(0.00,3.00) s=16608 e=17088 T=480 C=8\n",
      "[OK] S001 R10 C2 onset=4.200 win=(0.00,3.00) s=672 e=1152 T=480 C=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando segmentos (con crops): 100%|██████████| 103/103 [00:39<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUG WINDOWS SUMMARY ===\n",
      "Eventos brutos: 8652\n",
      "Crops OK:      8652\n",
      "Skips OOB:     0\n",
      "Skips LEN:     0\n",
      "Skips NaN:     0\n",
      "Dataset construido: N=8652 | T=480 | C=8 | sujetos únicos=103\n",
      "Listo para entrenar: N=8652 | T=480 | C=8 | clases=4 | sujetos=103\n",
      "\n",
      "[Fold 1/5] Entrenando ConformerMI v2 (balance-first)... (train=5796 | val=1092 | test=1764)\n",
      "  Ep   1 | train_acc=0.3092 | val_acc=0.3159 | LR=0.00020\n",
      "  Ep   5 | train_acc=0.4382 | val_acc=0.4103 | LR=0.00100\n",
      "  Ep  10 | train_acc=0.4750 | val_acc=0.4167 | LR=0.00100\n",
      "  Ep  15 | train_acc=0.4762 | val_acc=0.4167 | LR=0.00098\n",
      "  Ep  20 | train_acc=0.5300 | val_acc=0.4460 | LR=0.00095\n",
      "  Ep  25 | train_acc=0.5064 | val_acc=0.4332 | LR=0.00091\n",
      "  Ep  30 | train_acc=0.5342 | val_acc=0.4451 | LR=0.00085\n",
      "  Ep  35 | train_acc=0.5437 | val_acc=0.4341 | LR=0.00079\n",
      "  Early stopping en época 36 (mejor val_acc=0.4597)\n",
      "[Fold 1/5] Global acc=0.4461\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.4753    0.5669    0.5171       441\n",
      "       Right     0.4780    0.5170    0.4967       441\n",
      "  Both Fists     0.3729    0.3061    0.3362       441\n",
      "   Both Feet     0.4361    0.3946    0.4143       441\n",
      "\n",
      "    accuracy                         0.4461      1764\n",
      "   macro avg     0.4406    0.4461    0.4411      1764\n",
      "weighted avg     0.4406    0.4461    0.4411      1764\n",
      "\n",
      "  Fine-tuning (por sujeto, CV adaptativo) acc=0.4643 | sujetos=21\n",
      "  Δ(FT-Global) = +0.0181\n",
      "\n",
      "[Fold 2/5] Entrenando ConformerMI v2 (balance-first)... (train=5796 | val=1092 | test=1764)\n",
      "  Ep   1 | train_acc=0.2852 | val_acc=0.2985 | LR=0.00020\n",
      "  Ep   5 | train_acc=0.4144 | val_acc=0.3956 | LR=0.00100\n",
      "  Ep  10 | train_acc=0.4517 | val_acc=0.4332 | LR=0.00100\n",
      "  Ep  15 | train_acc=0.4650 | val_acc=0.4267 | LR=0.00098\n",
      "  Ep  20 | train_acc=0.4808 | val_acc=0.4423 | LR=0.00095\n",
      "  Early stopping en época 21 (mejor val_acc=0.4588)\n",
      "[Fold 2/5] Global acc=0.4938\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5061    0.6621    0.5737       441\n",
      "       Right     0.5904    0.4739    0.5258       441\n",
      "  Both Fists     0.4500    0.4286    0.4390       441\n",
      "   Both Feet     0.4383    0.4104    0.4239       441\n",
      "\n",
      "    accuracy                         0.4938      1764\n",
      "   macro avg     0.4962    0.4938    0.4906      1764\n",
      "weighted avg     0.4962    0.4938    0.4906      1764\n",
      "\n",
      "  Fine-tuning (por sujeto, CV adaptativo) acc=0.5096 | sujetos=21\n",
      "  Δ(FT-Global) = +0.0159\n",
      "\n",
      "[Fold 3/5] Entrenando ConformerMI v2 (balance-first)... (train=5796 | val=1092 | test=1764)\n",
      "  Ep   1 | train_acc=0.3485 | val_acc=0.3315 | LR=0.00020\n",
      "  Ep   5 | train_acc=0.4375 | val_acc=0.4515 | LR=0.00100\n",
      "  Ep  10 | train_acc=0.4796 | val_acc=0.4634 | LR=0.00100\n",
      "  Ep  15 | train_acc=0.4976 | val_acc=0.4771 | LR=0.00098\n",
      "  Ep  20 | train_acc=0.5016 | val_acc=0.4762 | LR=0.00095\n",
      "  Ep  25 | train_acc=0.5159 | val_acc=0.4808 | LR=0.00091\n",
      "  Ep  30 | train_acc=0.5295 | val_acc=0.4679 | LR=0.00085\n",
      "  Early stopping en época 33 (mejor val_acc=0.4963)\n",
      "[Fold 3/5] Global acc=0.4393\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.4915    0.4580    0.4742       441\n",
      "       Right     0.4988    0.4739    0.4860       441\n",
      "  Both Fists     0.3729    0.3991    0.3855       441\n",
      "   Both Feet     0.4069    0.4263    0.4164       441\n",
      "\n",
      "    accuracy                         0.4393      1764\n",
      "   macro avg     0.4425    0.4393    0.4405      1764\n",
      "weighted avg     0.4425    0.4393    0.4405      1764\n",
      "\n",
      "  Fine-tuning (por sujeto, CV adaptativo) acc=0.4569 | sujetos=21\n",
      "  Δ(FT-Global) = +0.0176\n",
      "\n",
      "[Fold 4/5] Entrenando ConformerMI v2 (balance-first)... (train=5880 | val=1092 | test=1680)\n",
      "  Ep   1 | train_acc=0.3452 | val_acc=0.3617 | LR=0.00020\n",
      "  Ep   5 | train_acc=0.4301 | val_acc=0.4441 | LR=0.00100\n",
      "  Ep  10 | train_acc=0.4587 | val_acc=0.4386 | LR=0.00100\n",
      "  Ep  15 | train_acc=0.4828 | val_acc=0.4652 | LR=0.00098\n",
      "  Ep  20 | train_acc=0.4939 | val_acc=0.4515 | LR=0.00095\n",
      "  Ep  25 | train_acc=0.5068 | val_acc=0.4625 | LR=0.00091\n",
      "  Early stopping en época 25 (mejor val_acc=0.4652)\n",
      "[Fold 4/5] Global acc=0.4738\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5640    0.5452    0.5545       420\n",
      "       Right     0.5872    0.4810    0.5288       420\n",
      "  Both Fists     0.4402    0.3595    0.3958       420\n",
      "   Both Feet     0.3646    0.5095    0.4250       420\n",
      "\n",
      "    accuracy                         0.4738      1680\n",
      "   macro avg     0.4890    0.4738    0.4760      1680\n",
      "weighted avg     0.4890    0.4738    0.4760      1680\n",
      "\n",
      "  Fine-tuning (por sujeto, CV adaptativo) acc=0.4863 | sujetos=20\n",
      "  Δ(FT-Global) = +0.0125\n",
      "\n",
      "[Fold 5/5] Entrenando ConformerMI v2 (balance-first)... (train=5880 | val=1092 | test=1680)\n",
      "  Ep   1 | train_acc=0.2952 | val_acc=0.3269 | LR=0.00020\n",
      "  Ep   5 | train_acc=0.4323 | val_acc=0.4386 | LR=0.00100\n",
      "  Ep  10 | train_acc=0.4701 | val_acc=0.4533 | LR=0.00100\n",
      "  Ep  15 | train_acc=0.4986 | val_acc=0.4597 | LR=0.00098\n",
      "  Ep  20 | train_acc=0.4964 | val_acc=0.4588 | LR=0.00095\n",
      "  Ep  25 | train_acc=0.5070 | val_acc=0.4579 | LR=0.00091\n",
      "  Early stopping en época 28 (mejor val_acc=0.4762)\n",
      "[Fold 5/5] Global acc=0.4589\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5336    0.5667    0.5497       420\n",
      "       Right     0.4706    0.5143    0.4915       420\n",
      "  Both Fists     0.3755    0.4310    0.4013       420\n",
      "   Both Feet     0.4642    0.3238    0.3815       420\n",
      "\n",
      "    accuracy                         0.4589      1680\n",
      "   macro avg     0.4610    0.4589    0.4560      1680\n",
      "weighted avg     0.4610    0.4589    0.4560      1680\n",
      "\n",
      "  Fine-tuning (por sujeto, CV adaptativo) acc=0.4679 | sujetos=20\n",
      "  Δ(FT-Global) = +0.0089\n",
      "\n",
      "============================================================\n",
      "RESULTADOS FINALES\n",
      "============================================================\n",
      "Global folds: ['0.4461', '0.4938', '0.4393', '0.4738', '0.4589']\n",
      "Global mean: 0.4624\n",
      "Fine-tune folds: ['0.4643', '0.5096', '0.4569', '0.4863', '0.4679']\n",
      "Fine-tune mean: 0.4770\n",
      "Δ(FT-Global) mean: +0.0146\n",
      "↳ Matriz de confusión guardada: confusion_conformer_v2_balancefirst_global_allfolds.png\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Conformer-MI (v2) — Balanceo previo (21 eventos/clase/sujeto) ANTES de crops\n",
    "# Cambios:\n",
    "# - Balanceo (down/up) por sujeto/clase a 21 EVENTOS en bruto (no croppeados)\n",
    "# - Luego se generan los CROPS a partir de esos eventos balanceados\n",
    "# - Ventanas: (0.5–3.5), (1.0–4.0), (1.5–4.5) → ~3 s @160 Hz\n",
    "# - Regularización ↑: dropout=0.3, sd_prob=0.2, weight_decay=1e-4, GLOBAL_PATIENCE=10\n",
    "# - TTA en test\n",
    "# - Augments train: time jitter + amplitude scaling + channel dropout\n",
    "# - Band-pass augment opcional (apagado por defecto), notch adaptativo como antes\n",
    "\n",
    "import os, re, math, json, copy, random, itertools\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
    "\n",
    "from sklearn.model_selection import (GroupKFold, GroupShuffleSplit, StratifiedKFold,\n",
    "                                     StratifiedShuffleSplit)\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# RUTAS (del usuario)\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "FOLDS_DIR = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# CONFIG GLOBAL\n",
    "# =========================\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE); np.random.seed(RANDOM_STATE); random.seed(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "FS = 160.0\n",
    "N_FOLDS = 5\n",
    "CLASS_NAMES_4C = ['Left', 'Right', 'Both Fists', 'Both Feet']\n",
    "N_CLASSES = 4\n",
    "\n",
    "# === CROPS (DESPUÉS del balanceo de eventos)\n",
    "CROPS = [(0.0, 3.0)]  # ~3.0 s cada uno\n",
    "\n",
    "# === Augments (train)\n",
    "USE_BANDPASS_AUG = True      # <- por defecto SIN band-pass; ponlo True si quieres probar\n",
    "BANDPASS_PROB = 0.25          # prob. de band-pass cuando USE_BANDPASS_AUG=True\n",
    "AMP_SCALE_RANGE = (0.8, 1.2)  # escalado por canal\n",
    "CHANNEL_DROP_PROB = 0.1       # prob. de \"apagar\" un canal completo en train\n",
    "\n",
    "# Entrenamiento global\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS_GLOBAL = 100\n",
    "LR_INIT = 1e-3\n",
    "WARMUP_EPOCHS = 5\n",
    "GLOBAL_VAL_SPLIT = 0.15\n",
    "GLOBAL_PATIENCE = 10          # más estricto para evitar overfitting\n",
    "LOG_EVERY = 5\n",
    "\n",
    "# Fine-tuning por sujeto\n",
    "DO_SUBJECT_FT = True\n",
    "CALIB_CV_FOLDS = 4\n",
    "FT_EPOCHS = 15\n",
    "FT_HEAD_LR = 1e-3\n",
    "FT_BASE_LR = 5e-5\n",
    "FT_L2SP = 1e-4\n",
    "FT_PATIENCE = 5\n",
    "FT_VAL_RATIO = 0.2\n",
    "\n",
    "# Sujetos/runs\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "MI_RUNS_LR = [4, 8, 12]     # Left/Right\n",
    "MI_RUNS_OF = [6, 10, 14]    # Both Fists / Both Feet\n",
    "BASELINE_RUNS_EO = [1]\n",
    "\n",
    "# Canales esperados\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','FCz']\n",
    "\n",
    "DEBUG_WINDOWS = True           # <--- pon a False cuando termines de depurar\n",
    "DEBUG_MAX_PRINTS = 50   \n",
    "# =========================\n",
    "# UTILIDADES CANALES / EDF\n",
    "# =========================\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = re.sub(r'([A-Za-z])0([0-9])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])Z$', r'\\1z', s)\n",
    "    s = s.replace('fp', 'Fp').replace('FP', 'Fp')\n",
    "    s = ''.join(ch.upper() if ch != 'z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {}\n",
    "    for ch in raw.ch_names:\n",
    "        lab = normalize_label(ch)\n",
    "        lab = lab[:-1] + 'z' if lab.endswith('Z') else lab\n",
    "        lab = re.sub(r'([A-Z])Z$', r'\\1z', lab)\n",
    "        mapping[ch] = lab\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired_channels=EXPECTED_8):\n",
    "    missing = [ch for ch in desired_channels if ch not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"[WARN] faltan canales {missing} en archivo {getattr(raw,'filenames', [''])[0]}\")\n",
    "        return None\n",
    "    raw.reorder_channels([ch for ch in raw.ch_names if ch in desired_channels] +\n",
    "                         [ch for ch in raw.ch_names if ch not in desired_channels])\n",
    "    raw.pick_channels(desired_channels, ordered=True)\n",
    "    return raw\n",
    "\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(run_id:int):\n",
    "    if run_id in MI_RUNS_LR: return 'LR'\n",
    "    if run_id in MI_RUNS_OF: return 'OF'\n",
    "    if run_id in BASELINE_RUNS_EO: return 'EO'\n",
    "    return None\n",
    "\n",
    "# --- Notch según tabla SNR opcional ---\n",
    "_SNR_TABLE = None\n",
    "def _load_snr_table():\n",
    "    global _SNR_TABLE\n",
    "    if _SNR_TABLE is not None:\n",
    "        return _SNR_TABLE\n",
    "    csv_path = PROJ / 'reports' / 'psd_mains' / 'psd_mains_summary.csv'\n",
    "    if csv_path.exists():\n",
    "        try:\n",
    "            _SNR_TABLE = pd.read_csv(csv_path)\n",
    "        except:\n",
    "            _SNR_TABLE = None\n",
    "    return _SNR_TABLE\n",
    "\n",
    "def _decide_notch(subject, run, th_db=10.0):\n",
    "    df = _load_snr_table()\n",
    "    if df is None:\n",
    "        return 60.0\n",
    "    row = df[(df['subject']==subject) & (df['run']==run)]\n",
    "    if row.empty:\n",
    "        return 60.0\n",
    "    snr50 = float(row['snr50_db'].iloc[0]); snr60 = float(row['snr60_db'].iloc[0])\n",
    "    if snr60 >= th_db and snr60 >= snr50: return 60.0\n",
    "    if snr50 >= th_db and snr50 >  snr60: return 50.0\n",
    "    return None\n",
    "\n",
    "def read_raw_edf(path: Path):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        mont = mne.channels.make_standard_montage('standard_1020')\n",
    "        raw.set_montage(mont, on_missing='ignore')\n",
    "    except Exception:\n",
    "        pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad=\"auto\")\n",
    "    raw = ensure_channels_order(raw, EXPECTED_8)\n",
    "    if raw is None:\n",
    "        return None\n",
    "    sid, rid = parse_subject_run(path)\n",
    "    notch = _decide_notch(sid, rid)\n",
    "    if notch is not None:\n",
    "        raw.notch_filter(freqs=[float(notch)], picks='eeg', method='spectrum_fit', phase='zero')\n",
    "    return raw\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    \"\"\"Extrae eventos T1/T2 y aplica deduplicación mínima (>=0.5s).\"\"\"\n",
    "    if raw.annotations is None or len(raw.annotations) == 0:\n",
    "        return []\n",
    "    def _norm(s): return str(s).strip().upper().replace(' ', '')\n",
    "\n",
    "    res = []\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = _norm(desc)\n",
    "        if tag in ('T1','T2'):\n",
    "            res.append((float(onset), tag))\n",
    "    res.sort()\n",
    "\n",
    "    dedup = []\n",
    "    last_t1 = last_t2 = -1e9\n",
    "    for t, tag in res:\n",
    "        if tag == 'T1':\n",
    "            if (t - last_t1) >= 0.5:\n",
    "                dedup.append((t, tag)); last_t1 = t\n",
    "        else:\n",
    "            if (t - last_t2) >= 0.5:\n",
    "                dedup.append((t, tag)); last_t2 = t\n",
    "    return dedup\n",
    "\n",
    "# =========================\n",
    "# DISCOVERY DE SUJETOS\n",
    "# =========================\n",
    "def subjects_available():\n",
    "    subs = []\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try: sid = int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir / f\"S{sid:03d}R{r:02d}.edf\").exists() for r in (MI_RUNS_LR + MI_RUNS_OF))\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "# =========================\n",
    "# BALANCEO POR EVENTOS (ANTES de CROPS)\n",
    "# =========================\n",
    "def _label_from_tag(kind: str, tag: str):\n",
    "    if kind == 'LR':\n",
    "        return 0 if tag == 'T1' else 1 if tag == 'T2' else None\n",
    "    elif kind == 'OF':\n",
    "        return 2 if tag == 'T1' else 3 if tag == 'T2' else None\n",
    "    return None\n",
    "\n",
    "def gather_all_events(subjects):\n",
    "    \"\"\"Junta TODOS los eventos T1/T2 por sujeto, SIN crops.\"\"\"\n",
    "    per_subj = {}  # subj -> {0:[...],1:[...],2:[...],3:[...]}\n",
    "    for s in tqdm(subjects, desc=\"Escaneando eventos (sin crops)\"):\n",
    "        sdir = DATA_RAW / f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "        per_subj[s] = {0:[], 1:[], 2:[], 3:[]}\n",
    "        for r in (MI_RUNS_LR + MI_RUNS_OF + BASELINE_RUNS_EO):\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            kind = run_kind(r)\n",
    "            if kind not in ('LR','OF'): \n",
    "                continue\n",
    "            raw = read_raw_edf(p)\n",
    "            if raw is None: \n",
    "                continue\n",
    "            events = collect_events_T1T2(raw)\n",
    "            for onset_sec, tag in events:\n",
    "                lab = _label_from_tag(kind, tag)\n",
    "                if lab is None: \n",
    "                    continue\n",
    "                per_subj[s][lab].append({\n",
    "                    \"path\": p,\n",
    "                    \"onset\": float(onset_sec),\n",
    "                    \"label\": int(lab),\n",
    "                    \"subject\": int(s),\n",
    "                    \"run\": int(r)\n",
    "                })\n",
    "    return per_subj\n",
    "\n",
    "def balance_events_per_subject(per_subj, need_per_class=21, rng_seed=RANDOM_STATE):\n",
    "    \"\"\"Para cada sujeto y clase, deja EXACTAMENTE 'need_per_class' eventos (down/up).\"\"\"\n",
    "    rng = check_random_state(rng_seed)\n",
    "    kept = {}\n",
    "    for s, cls_dict in per_subj.items():\n",
    "        kept[s] = {}\n",
    "        skip_subject = False\n",
    "        for c in range(4):\n",
    "            events = cls_dict.get(c, [])\n",
    "            n0 = len(events)\n",
    "            if n0 == 0:\n",
    "                skip_subject = True\n",
    "                break\n",
    "            if n0 == need_per_class:\n",
    "                sel = events\n",
    "            elif n0 > need_per_class:\n",
    "                idx = rng.choice(n0, size=need_per_class, replace=False)\n",
    "                sel = [events[i] for i in idx]\n",
    "            else:\n",
    "                idx = rng.choice(n0, size=need_per_class, replace=True)\n",
    "                sel = [events[i] for i in idx]\n",
    "            kept[s][c] = sel\n",
    "        if skip_subject:\n",
    "            kept[s] = None\n",
    "            print(f\"[SKIP] S{s:03d} (alguna clase sin eventos)\")\n",
    "    kept = {s:cls for s,cls in kept.items() if cls is not None}\n",
    "    return kept\n",
    "\n",
    "# =========================\n",
    "# APLICAR CROPS a los eventos balanceados\n",
    "# =========================\n",
    "# === debug global ===\n",
    "DEBUG_WINDOWS = True           # <--- pon a False cuando termines de depurar\n",
    "DEBUG_MAX_PRINTS = 50          # límite para no spamear\n",
    "\n",
    "def segments_from_balanced_events(kept_balanced, crops=CROPS, fs=FS):\n",
    "    \"\"\"Genera segmentos (T,C) a partir de los eventos ya balanceados (lee EDF una vez por archivo).\"\"\"\n",
    "    X, y, groups = [], [], []\n",
    "    ch_template = None\n",
    "    cache_raw = {}  # path -> raw\n",
    "\n",
    "    # contadores de diagnóstico\n",
    "    dbg_total_events = 0\n",
    "    dbg_total_crops_ok = 0\n",
    "    dbg_skips_oob = 0       # fuera de rango (out-of-bounds)\n",
    "    dbg_skips_short = 0     # longitud inesperada (< 1 muestra)\n",
    "    dbg_skips_nan = 0       # datos NaN o inf\n",
    "    dbg_prints = 0\n",
    "\n",
    "    try:\n",
    "        for s, cls_dict in tqdm(kept_balanced.items(), desc=\"Generando segmentos (con crops)\"):\n",
    "            for c in range(4):\n",
    "                ev_list = cls_dict[c]\n",
    "                for ev in ev_list:\n",
    "                    dbg_total_events += 1\n",
    "                    p = ev[\"path\"]; onset = ev[\"onset\"]; sid = ev[\"subject\"]; rid = ev[\"run\"]\n",
    "                    if p not in cache_raw:\n",
    "                        raw = read_raw_edf(p)\n",
    "                        if raw is None:\n",
    "                            if DEBUG_WINDOWS and dbg_prints < DEBUG_MAX_PRINTS:\n",
    "                                print(f\"[DEBUG] No se pudo leer RAW: {p}\")\n",
    "                                dbg_prints += 1\n",
    "                            continue\n",
    "                        cache_raw[p] = raw\n",
    "                    raw = cache_raw[p]\n",
    "                    data = raw.get_data()  # (C, T_total)\n",
    "                    T_total = data.shape[1]\n",
    "                    if ch_template is None:\n",
    "                        ch_template = raw.ch_names\n",
    "\n",
    "                    for rel_start, rel_end in crops:\n",
    "                        s_samp = int(round((raw.first_time + onset + rel_start) * fs))\n",
    "                        e_samp = int(round((raw.first_time + onset + rel_end)   * fs))\n",
    "                        exp_len = int(round((rel_end - rel_start) * fs))\n",
    "\n",
    "                        # chequeo de rango\n",
    "                        if s_samp < 0 or e_samp > T_total:\n",
    "                            dbg_skips_oob += 1\n",
    "                            if DEBUG_WINDOWS and dbg_prints < DEBUG_MAX_PRINTS:\n",
    "                                print(f\"[SKIP OOB] S{sid:03d} R{rid:02d} C{c} \"\n",
    "                                      f\"onset={onset:.3f}s win=({rel_start:.2f},{rel_end:.2f}) \"\n",
    "                                      f\"s={s_samp} e={e_samp} T_total={T_total}\")\n",
    "                                dbg_prints += 1\n",
    "                            continue\n",
    "\n",
    "                        # extrae segmento\n",
    "                        seg = data[:, s_samp:e_samp].T.astype(np.float32)  # (T, C)\n",
    "\n",
    "                        # chequeo de longitud\n",
    "                        if seg.shape[0] != exp_len or seg.shape[0] <= 0:\n",
    "                            dbg_skips_short += 1\n",
    "                            if DEBUG_WINDOWS and dbg_prints < DEBUG_MAX_PRINTS:\n",
    "                                print(f\"[SKIP LEN] S{sid:03d} R{rid:02d} C{c} \"\n",
    "                                      f\"esperado_T={exp_len} got_T={seg.shape[0]} \"\n",
    "                                      f\"s={s_samp} e={e_samp}\")\n",
    "                                dbg_prints += 1\n",
    "                            continue\n",
    "\n",
    "                        # chequeo NaNs/infs\n",
    "                        if not np.isfinite(seg).all():\n",
    "                            dbg_skips_nan += 1\n",
    "                            if DEBUG_WINDOWS and dbg_prints < DEBUG_MAX_PRINTS:\n",
    "                                print(f\"[SKIP NaN] S{sid:03d} R{rid:02d} C{c} \"\n",
    "                                      f\"s={s_samp} e={e_samp}\")\n",
    "                                dbg_prints += 1\n",
    "                            continue\n",
    "\n",
    "                        # OK: añadimos\n",
    "                        X.append(seg); y.append(c); groups.append(sid)\n",
    "                        dbg_total_crops_ok += 1\n",
    "\n",
    "                        if DEBUG_WINDOWS and dbg_prints < DEBUG_MAX_PRINTS:\n",
    "                            print(f\"[OK] S{sid:03d} R{rid:02d} C{c} onset={onset:.3f} \"\n",
    "                                  f\"win=({rel_start:.2f},{rel_end:.2f}) s={s_samp} e={e_samp} \"\n",
    "                                  f\"T={seg.shape[0]} C={seg.shape[1]}\")\n",
    "                            dbg_prints += 1\n",
    "    finally:\n",
    "        cache_raw.clear()\n",
    "\n",
    "    if DEBUG_WINDOWS:\n",
    "        print(\"=== DEBUG WINDOWS SUMMARY ===\")\n",
    "        print(f\"Eventos brutos: {dbg_total_events}\")\n",
    "        print(f\"Crops OK:      {dbg_total_crops_ok}\")\n",
    "        print(f\"Skips OOB:     {dbg_skips_oob}\")\n",
    "        print(f\"Skips LEN:     {dbg_skips_short}\")\n",
    "        print(f\"Skips NaN:     {dbg_skips_nan}\")\n",
    "\n",
    "    X = np.stack(X, axis=0) if len(X)>0 else np.zeros((0,1,1), np.float32)\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "    groups = np.asarray(groups, dtype=np.int64)\n",
    "    n, T, C = X.shape\n",
    "    print(f\"Dataset construido: N={n} | T={T} | C={C} | sujetos únicos={len(np.unique(groups))}\")\n",
    "    return X, y, groups, ch_template\n",
    "\n",
    "# =========================\n",
    "# NORMALIZACIÓN POR FOLD\n",
    "# =========================\n",
    "def compute_fold_norm_stats(X_tr):\n",
    "    mu = X_tr.mean(axis=(0,1), keepdims=True)\n",
    "    sigma = X_tr.std(axis=(0,1), keepdims=True) + 1e-6\n",
    "    return mu, sigma\n",
    "\n",
    "def apply_norm_stats(X, mu, sigma):\n",
    "    return (X - mu) / sigma\n",
    "\n",
    "# =========================\n",
    "# AUGMENTS (utils)\n",
    "# =========================\n",
    "def bandpass_augment(x_np, fs=160.0, p=BANDPASS_PROB):\n",
    "    if not USE_BANDPASS_AUG or random.random() > p:\n",
    "        return x_np\n",
    "    x = x_np.copy()\n",
    "    try:\n",
    "        x_mne = mne.filter.filter_data(x.T, sfreq=fs, l_freq=8., h_freq=30., verbose='ERROR').T\n",
    "        return x_mne.astype(np.float32)\n",
    "    except Exception:\n",
    "        return x_np\n",
    "\n",
    "def amplitude_scale_per_channel(x_np, low=AMP_SCALE_RANGE[0], high=AMP_SCALE_RANGE[1]):\n",
    "    scales = np.random.uniform(low, high, size=(x_np.shape[1],)).astype(np.float32)  # C\n",
    "    return (x_np * scales[np.newaxis, :]).astype(np.float32)\n",
    "\n",
    "def channel_dropout(x_np, p=CHANNEL_DROP_PROB):\n",
    "    if p <= 0: return x_np\n",
    "    x = x_np.copy()\n",
    "    C = x.shape[1]\n",
    "    mask = (np.random.rand(C) > p).astype(np.float32)  # 0 -> canal apagado\n",
    "    # evitar caso todos-cero\n",
    "    if mask.sum() == 0:\n",
    "        mask[np.random.randint(0, C)] = 1.0\n",
    "    return (x * mask[np.newaxis, :]).astype(np.float32)\n",
    "\n",
    "def do_time_jitter(xb, max_ms=30, fs=160.0):\n",
    "    max_shift = int(round(max_ms/1000.0 * fs))\n",
    "    if max_shift <= 0: return xb\n",
    "    B,_,T,C = xb.shape\n",
    "    shifts = torch.randint(low=-max_shift, high=max_shift+1, size=(B,), device=xb.device)\n",
    "    out = torch.empty_like(xb)\n",
    "    for i,s in enumerate(shifts):\n",
    "        if s==0: out[i] = xb[i]; continue\n",
    "        if s>0:\n",
    "            out[i,:,s:,:] = xb[i,:,:T-s,:]\n",
    "            out[i,:,:s,:] = 0\n",
    "        else:\n",
    "            s = -s\n",
    "            out[i,:,:T-s,:] = xb[i,:,s:,:]\n",
    "            out[i,:,T-s:,:] = 0\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# DATASET TORCH\n",
    "# =========================\n",
    "class EEGTrials(Dataset):\n",
    "    def __init__(self, X, y, groups, fs=160.0, train_mode=False):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.g = groups.astype(np.int64)\n",
    "        self.fs = fs\n",
    "        self.train_mode = train_mode\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]   # (T,C)\n",
    "        if self.train_mode:\n",
    "            # augment opcional band-pass (por defecto off)\n",
    "            x = bandpass_augment(x, fs=self.fs)\n",
    "            # robustez inter-sujeto\n",
    "            x = amplitude_scale_per_channel(x, *AMP_SCALE_RANGE)\n",
    "            x = channel_dropout(x, p=CHANNEL_DROP_PROB)\n",
    "        x = np.expand_dims(x, 0)  # (1,T,C)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx])\n",
    "\n",
    "# =========================\n",
    "# MODELO: Conformer-MI\n",
    "# =========================\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=4000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        L = x.size(1)\n",
    "        return x + self.pe[:L].unsqueeze(0)\n",
    "\n",
    "class ConvModule(nn.Module):\n",
    "    def __init__(self, d_model, dw_kernel=15):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.pw1 = nn.Conv1d(d_model, 2*d_model, kernel_size=1)\n",
    "        self.dw = nn.Conv1d(d_model, d_model, kernel_size=dw_kernel, padding=dw_kernel//2, groups=d_model)\n",
    "        self.bn = nn.BatchNorm1d(d_model)\n",
    "        self.act = nn.SiLU()\n",
    "        self.pw2 = nn.Conv1d(d_model, d_model, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        y = self.ln(x)\n",
    "        y = y.transpose(1,2)\n",
    "        y = self.pw1(y)\n",
    "        a, b = y.chunk(2, dim=1)\n",
    "        y = a * torch.sigmoid(b)\n",
    "        y = self.dw(y)\n",
    "        y = self.bn(y)\n",
    "        y = self.act(y)\n",
    "        y = self.pw2(y)\n",
    "        y = y.transpose(1,2)\n",
    "        return y\n",
    "\n",
    "class ConformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads=2, dropout=0.3, dw_kernel=15, ffn_mult=2.0, sd_prob=0.2):\n",
    "        super().__init__()\n",
    "        self.sd_prob = sd_prob\n",
    "        self.ln_attn = nn.LayerNorm(d_model)\n",
    "        self.mha = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.conv = ConvModule(d_model, dw_kernel=dw_kernel)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        self.ln_ffn = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, int(ffn_mult*d_model)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(ffn_mult*d_model), d_model),\n",
    "        )\n",
    "        self.drop3 = nn.Dropout(dropout)\n",
    "\n",
    "    def _stochastic_residual(self, x, y, training):\n",
    "        if not training or self.sd_prob <= 0.0:\n",
    "            return x + y\n",
    "        keep = torch.rand(x.size(0), device=x.device) > self.sd_prob\n",
    "        keep = keep.float().view(-1,1,1)\n",
    "        return x + y * keep\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.ln_attn(x)\n",
    "        y, _ = self.mha(y, y, y, need_weights=False)\n",
    "        x = self._stochastic_residual(x, self.drop1(y), self.training)\n",
    "        y = self.conv(x)\n",
    "        x = self._stochastic_residual(x, self.drop2(y), self.training)\n",
    "        y = self.ffn(self.ln_ffn(x))\n",
    "        x = self._stochastic_residual(x, self.drop3(y), self.training)\n",
    "        return x\n",
    "\n",
    "class ConformerMI(nn.Module):\n",
    "    def __init__(self, n_ch=8, n_classes=4, d_model=64, n_heads=2, n_blocks=3,\n",
    "                 patch_len=6, dropout=0.3, sd_prob=0.2, dw_kernel=15):\n",
    "        super().__init__()\n",
    "        self.n_ch = n_ch\n",
    "        self.n_classes = n_classes\n",
    "        self.d_model = d_model\n",
    "        self.patch_len = patch_len\n",
    "\n",
    "        # Stem: proyección espacial a d_model\n",
    "        self.spatial_pw = nn.Conv1d(n_ch, d_model, kernel_size=1, bias=False)\n",
    "        self.spatial_bn = nn.BatchNorm1d(d_model)\n",
    "        self.spatial_act = nn.GELU()\n",
    "\n",
    "        # Patchify temporal\n",
    "        self.patchify = nn.Conv1d(d_model, d_model, kernel_size=patch_len, stride=patch_len, bias=False)\n",
    "\n",
    "        self.pos = SinusoidalPositionalEncoding(d_model, max_len=4000)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ConformerBlock(d_model, n_heads=n_heads, dropout=dropout, dw_kernel=dw_kernel,\n",
    "                           ffn_mult=2.0, sd_prob=sd_prob*(i+1)/n_blocks)\n",
    "            for i in range(n_blocks)\n",
    "        ])\n",
    "        self.ln_out = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B,_,T,C = x.shape\n",
    "        x = x.squeeze(1).permute(0,2,1)      # (B,C,T)\n",
    "        x = self.spatial_pw(x)               # (B,D,T)\n",
    "        x = self.spatial_bn(x); x = self.spatial_act(x)\n",
    "        x = self.patchify(x)                 # (B,D,L)\n",
    "        x = x.transpose(1,2)                 # (B,L,D)\n",
    "        x = self.pos(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.ln_out(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = x.mean(dim=1)\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "# =========================\n",
    "# ENTRENAMIENTO/EVAL\n",
    "# =========================\n",
    "def build_weighted_sampler(y, groups):\n",
    "    y = np.asarray(y); groups = np.asarray(groups)\n",
    "    class_counts = np.bincount(y, minlength=len(np.unique(y))).astype(float)\n",
    "    class_w = 1.0 / class_counts[y]\n",
    "    subj_vals, subj_counts = np.unique(groups, return_counts=True)\n",
    "    subj_map = {s:c for s,c in zip(subj_vals, subj_counts)}\n",
    "    subj_w = np.array([1.0/subj_map[g] for g in groups], dtype=float)\n",
    "    w = class_w * subj_w\n",
    "    w = w / w.mean()\n",
    "    return WeightedRandomSampler(weights=torch.from_numpy(w).float(), num_samples=len(w), replacement=True)\n",
    "\n",
    "def make_class_weight_tensor(y_indices, n_classes):\n",
    "    counts = np.bincount(y_indices, minlength=n_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    w = counts.sum() / counts\n",
    "    w = w / w.mean()\n",
    "    return torch.tensor(w, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, use_tta=False):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for xb, yb, _ in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        if use_tta:\n",
    "            outs = []\n",
    "            for _ in range(3):\n",
    "                outs.append(model(do_time_jitter(xb, max_ms=20, fs=FS)))\n",
    "            logits = torch.stack(outs, dim=0).mean(dim=0)\n",
    "        else:\n",
    "            logits = model(xb)\n",
    "        pred = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "        y_pred.extend(pred); y_true.extend(yb.numpy().tolist())\n",
    "    y_true = np.asarray(y_true, dtype=int); y_pred = np.asarray(y_pred, dtype=int)\n",
    "    return y_true, y_pred, float((y_true==y_pred).mean())\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes, title, fname):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.nan_to_num(cm_norm)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title); plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    ticks = np.arange(len(classes))\n",
    "    plt.xticks(ticks, classes, rotation=45, ha='right'); plt.yticks(ticks, classes)\n",
    "    fmt = '.2f'; thresh = cm_norm.max()/2.\n",
    "    for i,j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "        plt.text(j, i, format(cm_norm[i,j], fmt), ha=\"center\",\n",
    "                 color=\"white\" if cm_norm[i,j] > thresh else \"black\")\n",
    "    plt.ylabel('True'); plt.xlabel('Pred')\n",
    "    plt.tight_layout(); plt.savefig(fname, dpi=150, bbox_inches='tight'); plt.close()\n",
    "\n",
    "def cosine_with_warmup(epoch, total_epochs, base_lr=LR_INIT, warmup=WARMUP_EPOCHS, min_lr=1e-5):\n",
    "    if epoch < warmup:\n",
    "        return base_lr * (epoch+1) / warmup\n",
    "    progress = (epoch - warmup) / max(1, (total_epochs - warmup))\n",
    "    return min_lr + (base_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, epoch, total_epochs):\n",
    "    model.train()\n",
    "    for xb, yb, _ in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        xb = do_time_jitter(xb, max_ms=30, fs=FS)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def print_report(y_true, y_pred, classes):\n",
    "    print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
    "\n",
    "# =========================\n",
    "# FINE-TUNING POR SUJETO (CV adaptativo)\n",
    "# =========================\n",
    "def _freeze_all(m): \n",
    "    for p in m.parameters(): p.requires_grad = False\n",
    "\n",
    "def _unfreeze_head(m):\n",
    "    for p in m.head.parameters(): p.requires_grad = True\n",
    "\n",
    "def _unfreeze_lastblock_head(m):\n",
    "    _unfreeze_head(m)\n",
    "    for p in m.blocks[-1].parameters(): p.requires_grad = True\n",
    "    for p in m.ln_out.parameters(): p.requires_grad = True\n",
    "\n",
    "def l2sp_reg(train_params, ref_params):\n",
    "    reg = 0.0\n",
    "    for p_cur, p_ref in zip(train_params, ref_params):\n",
    "        reg = reg + torch.sum((p_cur - p_ref)**2)\n",
    "    return reg\n",
    "\n",
    "def _adaptive_cv_splits(Xs, ys, base_splits=CALIB_CV_FOLDS):\n",
    "    ys_np = np.asarray(ys)\n",
    "    counts = np.bincount(ys_np, minlength=N_CLASSES)\n",
    "    min_class = counts[counts>0].min() if counts.sum() > 0 else 0\n",
    "    splits = min(base_splits, max(2, int(min_class)))\n",
    "    if splits < 2:\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=RANDOM_STATE)\n",
    "        return sss.split(Xs, ys_np)\n",
    "    try:\n",
    "        skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "        list(skf.split(Xs, ys_np))\n",
    "        return skf.split(Xs, ys_np)\n",
    "    except Exception:\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=RANDOM_STATE)\n",
    "        return sss.split(Xs, ys_np)\n",
    "\n",
    "def subject_cv_finetune_predict_progressive(model_global, Xs, ys, n_classes=4):\n",
    "    y_true_full = np.empty_like(ys); y_pred_full = np.empty_like(ys)\n",
    "    for tr_idx, te_idx in _adaptive_cv_splits(Xs, ys, base_splits=CALIB_CV_FOLDS):\n",
    "        Xcal, ycal = Xs[tr_idx], ys[tr_idx]\n",
    "        Xho,  yho  = Xs[te_idx], ys[te_idx]\n",
    "\n",
    "        ds_te = EEGTrials(Xho,  yho,  np.zeros_like(yho),  train_mode=False)\n",
    "        dl_te = DataLoader(ds_te, batch_size=64, shuffle=False)\n",
    "\n",
    "        if len(np.unique(ycal)) >= 2 and len(ycal) >= 5:\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=FT_VAL_RATIO, random_state=RANDOM_STATE)\n",
    "            (tr2, va2), = sss.split(Xcal, ycal)\n",
    "        else:\n",
    "            idx = np.arange(len(ycal))\n",
    "            np.random.shuffle(idx)\n",
    "            cut = max(1, int(0.85*len(idx)))\n",
    "            tr2, va2 = idx[:cut], idx[cut:]\n",
    "\n",
    "        ds_tr2 = EEGTrials(Xcal[tr2], ycal[tr2], np.zeros_like(ycal[tr2]), train_mode=True)\n",
    "        ds_va2 = EEGTrials(Xcal[va2], ycal[va2], np.zeros_like(ycal[va2]), train_mode=False)\n",
    "        dl_tr2 = DataLoader(ds_tr2, batch_size=32, shuffle=True)\n",
    "        dl_va2 = DataLoader(ds_va2, batch_size=64, shuffle=False)\n",
    "\n",
    "        # HEAD only\n",
    "        m = copy.deepcopy(model_global).to(DEVICE)\n",
    "        _freeze_all(m); _unfreeze_head(m)\n",
    "        opt = optim.Adam(m.head.parameters(), lr=FT_HEAD_LR)\n",
    "        crit = nn.CrossEntropyLoss()\n",
    "        best = copy.deepcopy(m.state_dict()); best_va = 1e9; bad=0\n",
    "        for _ in range(FT_EPOCHS):\n",
    "            m.train()\n",
    "            for xb, yb, _ in dl_tr2:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                loss = crit(m(xb), yb)\n",
    "                loss.backward(); opt.step()\n",
    "            # val\n",
    "            m.eval(); val_loss=0.0; n=0\n",
    "            with torch.no_grad():\n",
    "                for xb, yb, _ in dl_va2:\n",
    "                    xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                    val_loss += crit(m(xb), yb).item()*xb.size(0); n+=xb.size(0)\n",
    "            val_loss/=max(1,n)\n",
    "            if val_loss < best_va-1e-7:\n",
    "                best_va = val_loss; best=copy.deepcopy(m.state_dict()); bad=0\n",
    "            else:\n",
    "                bad+=1\n",
    "                if bad>=FT_PATIENCE: break\n",
    "        m.load_state_dict(best)\n",
    "\n",
    "        # LAST BLOCK + HEAD con L2-SP\n",
    "        _freeze_all(m); _unfreeze_lastblock_head(m)\n",
    "        train_params = list(m.blocks[-1].parameters()) + list(m.ln_out.parameters()) + list(m.head.parameters())\n",
    "        ref = [p.detach().clone() for p in train_params]\n",
    "        opt = optim.Adam([\n",
    "            {\"params\": list(m.blocks[-1].parameters()) + list(m.ln_out.parameters()), \"lr\": FT_BASE_LR},\n",
    "            {\"params\": m.head.parameters(), \"lr\": FT_HEAD_LR},\n",
    "        ])\n",
    "        crit = nn.CrossEntropyLoss()\n",
    "        best = copy.deepcopy(m.state_dict()); best_va = 1e9; bad=0\n",
    "        for _ in range(FT_EPOCHS):\n",
    "            m.train()\n",
    "            for xb, yb, _ in dl_tr2:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                loss = crit(m(xb), yb) + FT_L2SP * l2sp_reg(train_params, ref)\n",
    "                loss.backward(); opt.step()\n",
    "            m.eval(); val_loss=0.0; n=0\n",
    "            with torch.no_grad():\n",
    "                for xb, yb, _ in dl_va2:\n",
    "                    xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                    val_loss += crit(m(xb), yb).item()*xb.size(0); n+=xb.size(0)\n",
    "            val_loss/=max(1,n)\n",
    "            if val_loss < best_va-1e-7:\n",
    "                best_va = val_loss; best=copy.deepcopy(m.state_dict()); bad=0\n",
    "            else:\n",
    "                bad+=1\n",
    "                if bad>=FT_PATIENCE: break\n",
    "        m.load_state_dict(best)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            m.eval()\n",
    "            preds=[]\n",
    "            for xb, _, _ in dl_te:\n",
    "                xb = xb.to(DEVICE)\n",
    "                logits = m(xb)\n",
    "                preds.extend(logits.argmax(1).cpu().numpy().tolist())\n",
    "        y_pred_full[te_idx] = np.asarray(preds); y_true_full[te_idx] = yho\n",
    "    return y_true_full, y_pred_full\n",
    "\n",
    "# =========================\n",
    "# FOLDS JSON helpers\n",
    "# =========================\n",
    "def save_group_folds_json_with_indices(subject_ids_str, groups_array, n_splits, out_json_path,\n",
    "                                       created_by=\"ConformerMI_v2_balFirst\", description=None):\n",
    "    out_json_path = Path(out_json_path)\n",
    "    unique_subjects_int = sorted(np.unique(groups_array).tolist())\n",
    "    subject_ids = [f\"S{sid:03d}\" for sid in unique_subjects_int]\n",
    "    if len(subject_ids) < n_splits:\n",
    "        raise ValueError(f\"n_splits={n_splits} mayor que número de sujetos={len(subject_ids)}\")\n",
    "    groups = np.arange(len(subject_ids))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    folds = []\n",
    "    fold_i = 0\n",
    "    for train_idx_grp, test_idx_grp in gkf.split(groups, groups, groups):\n",
    "        fold_i += 1\n",
    "        train_sids = [subject_ids[int(i)] for i in train_idx_grp]\n",
    "        test_sids  = [subject_ids[int(i)] for i in test_idx_grp]\n",
    "        train_sids_int = [int(s[1:]) for s in train_sids]\n",
    "        test_sids_int  = [int(s[1:]) for s in test_sids]\n",
    "        tr_idx = np.where(np.isin(groups_array, train_sids_int))[0].tolist()\n",
    "        te_idx = np.where(np.isin(groups_array, test_sids_int))[0].tolist()\n",
    "        folds.append({\n",
    "            \"fold\": int(fold_i),\n",
    "            \"train\": train_sids,\n",
    "            \"test\": test_sids,\n",
    "            \"tr_idx\": tr_idx,\n",
    "            \"te_idx\": te_idx\n",
    "        })\n",
    "    payload = {\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"created_by\": created_by,\n",
    "        \"description\": description if description is not None else \"\",\n",
    "        \"n_splits\": int(n_splits),\n",
    "        \"n_subjects\": len(subject_ids),\n",
    "        \"subject_ids\": subject_ids,\n",
    "        \"folds\": folds\n",
    "    }\n",
    "    out_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Folds JSON con índices guardado → {out_json_path}\")\n",
    "    return out_json_path\n",
    "\n",
    "def load_group_folds_json(path_json, expected_subject_ids=None, strict_check=True):\n",
    "    path_json = Path(path_json)\n",
    "    if not path_json.exists():\n",
    "        raise FileNotFoundError(f\"No existe {path_json}\")\n",
    "    with open(path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        payload = json.load(f)\n",
    "    subj_json = payload.get(\"subject_ids\", [])\n",
    "    if expected_subject_ids is not None:\n",
    "        expected = sorted(list(expected_subject_ids))\n",
    "        if subj_json != expected:\n",
    "            msg = (\"Los subject_ids del JSON no coinciden con expected_subject_ids.\\n\"\n",
    "                   f\"JSON has {len(subj_json)} subjects, expected {len(expected)}.\\n\"\n",
    "                   f\"First 10 JSON: {subj_json[:10]}\\nFirst 10 expected: {expected[:10]}\")\n",
    "            if strict_check:\n",
    "                raise ValueError(msg)\n",
    "            else:\n",
    "                print(\"WARNING: \" + msg)\n",
    "    return payload\n",
    "\n",
    "# =========================\n",
    "# EXPERIMENTO\n",
    "# =========================\n",
    "def run_experiment(save_folds_json=True, folds_json_path=FOLDS_DIR,\n",
    "                   folds_json_description=\"GroupKFold folds for ConformerMI v2 (balance-first)\"):\n",
    "    mne.set_log_level('WARNING')\n",
    "\n",
    "    subs = subjects_available()\n",
    "    print(f\"Sujetos elegibles: {len(subs)} → {subs[:10]}{'...' if len(subs)>10 else ''}\")\n",
    "\n",
    "    # 1) Recolectar eventos brutos T1/T2 (sin crops)\n",
    "    per_subj = gather_all_events(subs)\n",
    "\n",
    "    # 2) Balancear a 21 eventos por clase/sujeto\n",
    "    kept_bal = balance_events_per_subject(per_subj, need_per_class=21, rng_seed=RANDOM_STATE)\n",
    "\n",
    "    # 3) Generar segmentos aplicando CROPS SOLO sobre los eventos balanceados\n",
    "    X, y, groups, chs = segments_from_balanced_events(kept_bal, crops=CROPS, fs=FS)\n",
    "    N, T, C = X.shape\n",
    "    print(f\"Listo para entrenar: N={N} | T={T} | C={C} | clases={len(np.unique(y))} | sujetos={len(np.unique(groups))}\")\n",
    "\n",
    "    # preparar JSON folds\n",
    "    if folds_json_path is None:\n",
    "        folds_json_path = Path(\"folds\") / f\"group_folds_{N_FOLDS}splits.json\"\n",
    "    else:\n",
    "        folds_json_path = Path(folds_json_path)\n",
    "    folds_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    unique_subs = sorted(np.unique(groups).tolist())\n",
    "    subject_ids_str = [f\"S{s:03d}\" for s in unique_subs]\n",
    "\n",
    "    if not folds_json_path.exists():\n",
    "        if not save_folds_json:\n",
    "            raise FileNotFoundError(f\"Folds JSON no encontrado en {folds_json_path} y save_folds_json=False.\")\n",
    "        save_group_folds_json_with_indices(subject_ids_str, groups, n_splits=N_FOLDS,\n",
    "                                           out_json_path=folds_json_path,\n",
    "                                           created_by=\"ConformerMI_v2_balFirst\",\n",
    "                                           description=folds_json_description)\n",
    "    payload = load_group_folds_json(folds_json_path, expected_subject_ids=subject_ids_str, strict_check=False)\n",
    "    folds = payload[\"folds\"]\n",
    "\n",
    "    global_folds = []\n",
    "    ft_prog_folds = []\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "\n",
    "    for f in folds:\n",
    "        fold = f[\"fold\"]\n",
    "        tr_idx = np.asarray(f.get(\"tr_idx\", []), dtype=int)\n",
    "        te_idx = np.asarray(f.get(\"te_idx\", []), dtype=int)\n",
    "        if tr_idx.size == 0 or te_idx.size == 0:\n",
    "            print(f\"[WARN] fold {fold} sin índices tr/te válidos. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        # split de validación por sujetos\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=GLOBAL_VAL_SPLIT, random_state=RANDOM_STATE)\n",
    "        tr_subj_idx, va_subj_idx = next(gss.split(tr_idx, groups[tr_idx], groups[tr_idx]))\n",
    "        tr_sub_idx = tr_idx[tr_subj_idx]\n",
    "        va_idx     = tr_idx[va_subj_idx]\n",
    "\n",
    "        # Normalización por fold (usar sólo train)\n",
    "        mu, sigma = compute_fold_norm_stats(X[tr_sub_idx])\n",
    "        Xn = apply_norm_stats(X, mu, sigma)\n",
    "\n",
    "        ds_tr = EEGTrials(Xn[tr_sub_idx], y[tr_sub_idx], groups[tr_sub_idx], fs=FS, train_mode=True)\n",
    "        ds_va = EEGTrials(Xn[va_idx],     y[va_idx],     groups[va_idx],     fs=FS, train_mode=False)\n",
    "        ds_te = EEGTrials(Xn[te_idx],     y[te_idx],     groups[te_idx],     fs=FS, train_mode=False)\n",
    "\n",
    "        sampler = build_weighted_sampler(y[tr_sub_idx], groups[tr_sub_idx])\n",
    "        dl_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, sampler=sampler, drop_last=False)\n",
    "        dl_va = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "        dl_te = DataLoader(ds_te, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "        # modelo\n",
    "        model = ConformerMI(n_ch=C, n_classes=N_CLASSES,\n",
    "                            d_model=64, n_heads=2, n_blocks=3,\n",
    "                            patch_len=6, dropout=0.3, sd_prob=0.2, dw_kernel=15).to(DEVICE)\n",
    "\n",
    "        # CE ponderada (además del sampler)\n",
    "        class_weights = make_class_weight_tensor(y[tr_sub_idx], n_classes=N_CLASSES)\n",
    "        crit = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_INIT, weight_decay=1e-4)\n",
    "\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        best_val = -1.0; bad = 0\n",
    "\n",
    "        print(f\"\\n[Fold {fold}/{N_FOLDS}] Entrenando ConformerMI v2 (balance-first)...\"\n",
    "              f\" (train={len(tr_sub_idx)} | val={len(va_idx)} | test={len(te_idx)})\")\n",
    "\n",
    "        for epoch in range(EPOCHS_GLOBAL):\n",
    "            for pg in opt.param_groups:\n",
    "                pg['lr'] = cosine_with_warmup(epoch, EPOCHS_GLOBAL, base_lr=LR_INIT, warmup=WARMUP_EPOCHS)\n",
    "            train_one_epoch(model, dl_tr, opt, crit, epoch, EPOCHS_GLOBAL)\n",
    "\n",
    "            _, _, tr_acc = evaluate(model, dl_tr, use_tta=False)\n",
    "            _, _, va_acc = evaluate(model, dl_va, use_tta=False)\n",
    "\n",
    "            if (epoch+1) % LOG_EVERY == 0 or epoch in (0, 4, 9, 19, 49, 79):\n",
    "                cur_lr = opt.param_groups[0]['lr']\n",
    "                print(f\"  Ep {epoch+1:3d} | train_acc={tr_acc:.4f} | val_acc={va_acc:.4f} | LR={cur_lr:.5f}\")\n",
    "\n",
    "            if va_acc > best_val + 1e-4:\n",
    "                best_val = va_acc; best_state = copy.deepcopy(model.state_dict()); bad = 0\n",
    "            else:\n",
    "                bad += 1\n",
    "                if bad >= GLOBAL_PATIENCE:\n",
    "                    print(f\"  Early stopping en época {epoch+1} (mejor val_acc={best_val:.4f})\")\n",
    "                    break\n",
    "\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "        # evaluación global (con TTA activado)\n",
    "        y_true, y_pred, acc_global = evaluate(model, dl_te, use_tta=True)\n",
    "        global_folds.append(acc_global); all_true.append(y_true); all_pred.append(y_pred)\n",
    "        print(f\"[Fold {fold}/{N_FOLDS}] Global acc={acc_global:.4f}\")\n",
    "        print_report(y_true, y_pred, CLASS_NAMES_4C)\n",
    "\n",
    "        # ---------- Fine-tuning por sujeto (CV adaptativo) ----------\n",
    "        acc_ft = np.nan\n",
    "        if DO_SUBJECT_FT:\n",
    "            X_te, y_te, g_te = Xn[te_idx], y[te_idx], groups[te_idx]\n",
    "            y_true_ft_all, y_pred_ft_all = [], []\n",
    "            used_subjects = 0\n",
    "            for sid in np.unique(g_te):\n",
    "                idx = np.where(g_te == sid)[0]\n",
    "                Xs, ys = X_te[idx], y_te[idx]\n",
    "                if len(np.unique(ys)) < 2:\n",
    "                    continue\n",
    "                y_true_subj, y_pred_subj = subject_cv_finetune_predict_progressive(model, Xs, ys, n_classes=N_CLASSES)\n",
    "                y_true_ft_all.append(y_true_subj); y_pred_ft_all.append(y_pred_subj); used_subjects += 1\n",
    "            if len(y_true_ft_all) > 0:\n",
    "                y_true_ft_all = np.concatenate(y_true_ft_all)\n",
    "                y_pred_ft_all = np.concatenate(y_pred_ft_all)\n",
    "                acc_ft = (y_true_ft_all == y_pred_ft_all).mean()\n",
    "                print(f\"  Fine-tuning (por sujeto, CV adaptativo) acc={acc_ft:.4f} | sujetos={used_subjects}\")\n",
    "                print(f\"  Δ(FT-Global) = {acc_ft - acc_global:+.4f}\")\n",
    "            else:\n",
    "                print(\"  Fine-tuning no ejecutado (sujeto(s) con muestras/clases insuficientes).\")\n",
    "        ft_prog_folds.append(acc_ft)\n",
    "\n",
    "    # ---------- resultados finales ----------\n",
    "    if len(all_true) > 0:\n",
    "        all_true = np.concatenate(all_true)\n",
    "        all_pred = np.concatenate(all_pred)\n",
    "    else:\n",
    "        all_true = np.array([], dtype=int)\n",
    "        all_pred = np.array([], dtype=int)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTADOS FINALES\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Global folds:\", [f\"{a:.4f}\" for a in global_folds])\n",
    "    if len(global_folds) > 0:\n",
    "        print(f\"Global mean: {np.mean(global_folds):.4f}\")\n",
    "\n",
    "    print(\"Fine-tune folds:\", [(\"nan\" if (a is None or np.isnan(a)) else f\"{a:.4f}\") for a in ft_prog_folds])\n",
    "    if len(ft_prog_folds) > 0:\n",
    "        print(f\"Fine-tune mean: {np.nanmean(ft_prog_folds):.4f}\")\n",
    "        print(f\"Δ(FT-Global) mean: {np.nanmean(ft_prog_folds) - np.mean(global_folds):+.4f}\")\n",
    "\n",
    "    if all_true.size > 0:\n",
    "        plot_confusion(all_true, all_pred, CLASS_NAMES_4C,\n",
    "                       title=\"Confusion Matrix - ConformerMI v2 (Global, all folds, balance-first)\",\n",
    "                       fname=\"confusion_conformer_v2_balancefirst_global_allfolds.png\")\n",
    "        print(\"↳ Matriz de confusión guardada: confusion_conformer_v2_balancefirst_global_allfolds.png\")\n",
    "\n",
    "    return {\n",
    "        \"global_folds\": global_folds,\n",
    "        \"ft_prog_folds\": ft_prog_folds,\n",
    "        \"all_true\": all_true,\n",
    "        \"all_pred\": all_pred,\n",
    "        \"folds_json_path\": str(folds_json_path)\n",
    "    }\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🧠 INICIANDO EXPERIMENTO Conformer-MI v2 — balance-first (4 clases, 8 canales)\")\n",
    "    print(f\"⚙️  GLOBAL: epochs={EPOCHS_GLOBAL}, lr={LR_INIT}, warmup={WARMUP_EPOCHS}, batch={BATCH_SIZE}\")\n",
    "    print(f\"🔧 Augments: bandpass={'ON' if USE_BANDPASS_AUG else 'OFF'}, amp_scale={AMP_SCALE_RANGE}, ch_drop={CHANNEL_DROP_PROB}\")\n",
    "    if DO_SUBJECT_FT:\n",
    "        print(f\"🔧 FT por sujeto (adaptativo): epochs={FT_EPOCHS}, head_lr={FT_HEAD_LR}, base_lr={FT_BASE_LR}, L2SP={FT_L2SP}\")\n",
    "    run_experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b00ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# CNN + Transformer (Conformer-lite) para MI\n",
    "# - MISMO pipeline que tu EEGNet (6 s ventana, augments, mixup, cutout OF, SGDR, TTA=5, FT progresivo L2-SP)\n",
    "# - Solo cambia el modelo\n",
    "\n",
    "import os, re, math, random, json, itertools, copy\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold, StratifiedShuffleSplit, GroupShuffleSplit\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# CONFIGURACIÓN GENERAL\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "FOLDS_DIR = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dispositivo y semilla\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE); np.random.seed(RANDOM_STATE); random.seed(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "# Escenario y ventana\n",
    "CLASS_SCENARIO = '4c'\n",
    "WINDOW_MODE = '6s'   # 6 s → ~960 muestras @160 Hz\n",
    "FS = 160.0\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Entrenamiento global\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS_GLOBAL = 100\n",
    "LR_INIT = 1e-2\n",
    "SGDR_T0 = 6\n",
    "SGDR_Tmult = 2\n",
    "\n",
    "# Validación/ES global\n",
    "GLOBAL_VAL_SPLIT = 0.15\n",
    "GLOBAL_PATIENCE  = 10\n",
    "LOG_EVERY        = 5\n",
    "\n",
    "# Fine-tuning por sujeto (protocolo robusto)\n",
    "CALIB_CV_FOLDS = 4\n",
    "FT_EPOCHS = 30\n",
    "FT_BASE_LR = 5e-5\n",
    "FT_HEAD_LR = 1e-3\n",
    "FT_L2SP = 1e-4\n",
    "FT_PATIENCE = 5\n",
    "FT_VAL_RATIO = 0.2\n",
    "\n",
    "# Normalización por época canal-a-canal (z-score)\n",
    "NORM_EPOCH_ZSCORE = True\n",
    "\n",
    "# Sujetos excluidos\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "\n",
    "# Runs\n",
    "MI_RUNS_LR = [4, 8, 12]\n",
    "MI_RUNS_OF = [6, 10, 14]\n",
    "BASELINE_RUNS_EO = [1]\n",
    "\n",
    "# Canales (8 con FCz)\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','FCz']\n",
    "\n",
    "CLASS_NAMES_4C = ['Left', 'Right', 'Both Fists', 'Both Feet']\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES DE CANALES / EDF\n",
    "# =========================\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = re.sub(r'([A-Za-z])0([0-9])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])Z$', r'\\1z', s)\n",
    "    s = s.replace('fp', 'Fp').replace('FP', 'Fp')\n",
    "    s = ''.join(ch.upper() if ch != 'z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {}\n",
    "    for ch in raw.ch_names:\n",
    "        lab = normalize_label(ch)\n",
    "        lab = lab[:-1] + 'z' if lab.endswith('Z') else lab\n",
    "        lab = re.sub(r'([A-Z])Z$', r'\\1z', lab)\n",
    "        mapping[ch] = lab\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired_channels=EXPECTED_8):\n",
    "    missing = [ch for ch in desired_channels if ch not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"Warning: faltan canales {missing} en archivo {getattr(raw,'filenames', [''])[0]}\")\n",
    "        return None\n",
    "    raw.reorder_channels([ch for ch in raw.ch_names if ch in desired_channels] +\n",
    "                         [ch for ch in raw.ch_names if ch not in desired_channels])\n",
    "    raw.pick_channels(desired_channels, ordered=True)\n",
    "    return raw\n",
    "\n",
    "# =========================\n",
    "# LECTURA DE EDF y EVENTOS\n",
    "# =========================\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(run_id:int):\n",
    "    if run_id in MI_RUNS_LR: return 'LR'\n",
    "    if run_id in MI_RUNS_OF: return 'OF'\n",
    "    if run_id in BASELINE_RUNS_EO: return 'EO'\n",
    "    return None\n",
    "\n",
    "# --- Notch adaptativo: lee CSV si existe (subject, run, snr50_db, snr60_db)\n",
    "_SNR_TABLE = None\n",
    "def _load_snr_table():\n",
    "    global _SNR_TABLE\n",
    "    if _SNR_TABLE is not None:\n",
    "        return _SNR_TABLE\n",
    "    csv_path = PROJ / 'reports' / 'psd_mains' / 'psd_mains_summary.csv'\n",
    "    if csv_path.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            _SNR_TABLE = df\n",
    "        except Exception as e:\n",
    "            print(f\"[SNR] No se pudo leer {csv_path}: {e}\")\n",
    "            _SNR_TABLE = None\n",
    "    return _SNR_TABLE\n",
    "\n",
    "def _decide_notch(subject, run, th_db=10.0):\n",
    "    df = _load_snr_table()\n",
    "    if df is None:  # default\n",
    "        return 60.0\n",
    "    row = df[(df['subject']==subject) & (df['run']==run)]\n",
    "    if row.empty:\n",
    "        return 60.0\n",
    "    snr50 = float(row['snr50_db'].iloc[0]); snr60 = float(row['snr60_db'].iloc[0])\n",
    "    if snr60 >= th_db and snr60 >= snr50: return 60.0\n",
    "    if snr50 >= th_db and snr50 >  snr60: return 50.0\n",
    "    return None  # no notch si no sobresale\n",
    "\n",
    "def read_raw_edf(path: Path):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        mont = mne.channels.make_standard_montage('standard_1020')\n",
    "        raw.set_montage(mont, on_missing='ignore')\n",
    "    except Exception:\n",
    "        pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad=\"auto\")\n",
    "    raw = ensure_channels_order(raw, EXPECTED_8)\n",
    "    if raw is None:\n",
    "        return None\n",
    "\n",
    "    # Notch adaptativo (por SNR); si no hay CSV, por defecto 60 Hz\n",
    "    sid, rid = parse_subject_run(path)\n",
    "    notch = _decide_notch(sid, rid)\n",
    "    if notch is not None:\n",
    "        raw.notch_filter(freqs=[float(notch)], picks='eeg', method='spectrum_fit', phase='zero')\n",
    "    return raw\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    if raw.annotations is None or len(raw.annotations) == 0:\n",
    "        return []\n",
    "    def _norm(s): return str(s).strip().upper().replace(' ', '')\n",
    "    res = []\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = _norm(desc)\n",
    "        if tag in ('T1','T2'):\n",
    "            res.append((float(onset), tag))\n",
    "    res.sort()\n",
    "    dedup = []\n",
    "    last_t1 = last_t2 = -1e9\n",
    "    for t, tag in res:\n",
    "        if tag == 'T1':\n",
    "            if (t - last_t1) >= 0.5: dedup.append((t, tag)); last_t1 = t\n",
    "        else:\n",
    "            if (t - last_t2) >= 0.5: dedup.append((t, tag)); last_t2 = t\n",
    "    return dedup\n",
    "\n",
    "# =========================\n",
    "# CONSTRUCCIÓN DE DATASETS (ventana 6s)\n",
    "# =========================\n",
    "def subjects_available():\n",
    "    subs = []\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try: sid = int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir / f\"S{sid:03d}R{r:02d}.edf\").exists() for r in (MI_RUNS_LR + MI_RUNS_OF))\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "def extract_trials_from_run(edf_path: Path, scenario: str, window_mode: str):\n",
    "    subj, run = parse_subject_run(edf_path)\n",
    "    kind = run_kind(run)\n",
    "    if kind not in ('LR','OF','EO'):\n",
    "        return ([], [])\n",
    "\n",
    "    raw = read_raw_edf(edf_path)\n",
    "    if raw is None:\n",
    "        return ([], [])\n",
    "\n",
    "    data = raw.get_data()\n",
    "    fs = raw.info['sfreq']\n",
    "    assert abs(fs - FS) < 1e-6\n",
    "\n",
    "    out = []\n",
    "    if kind in ('LR','OF'):\n",
    "        events = collect_events_T1T2(raw)\n",
    "        # 6 s: de -1 a +5 s respecto al onset → T=960\n",
    "        if window_mode == '3s':\n",
    "            rel_start, rel_end = 0.0, 3.0\n",
    "        else:\n",
    "            rel_start, rel_end = -1.0, 5.0\n",
    "\n",
    "        for onset_sec, tag in events:\n",
    "            if kind == 'LR':\n",
    "                if tag == 'T1': label = 'L'\n",
    "                elif tag == 'T2': label = 'R'\n",
    "                else: continue\n",
    "            else:\n",
    "                if tag == 'T1': label = 'BFISTS'\n",
    "                elif tag == 'T2': label = 'BFEET'\n",
    "                else: continue\n",
    "\n",
    "            if scenario == '2c' and label not in ('L','R'): continue\n",
    "            if scenario == '3c' and label not in ('L','R','BFISTS'): continue\n",
    "            if scenario == '4c' and label not in ('L','R','BFISTS','BFEET'): continue\n",
    "\n",
    "            s = int(round((raw.first_time + onset_sec + rel_start) * fs))\n",
    "            e = int(round((raw.first_time + onset_sec + rel_end) * fs))\n",
    "            if s < 0 or e > data.shape[1]:\n",
    "                continue\n",
    "\n",
    "            seg = data[:, s:e].T.astype(np.float32)\n",
    "            if NORM_EPOCH_ZSCORE:\n",
    "                seg = (seg - seg.mean(axis=0, keepdims=True)) / (seg.std(axis=0, keepdims=True) + 1e-6)\n",
    "\n",
    "            if label == 'L':       y = 0\n",
    "            elif label == 'R':     y = 1\n",
    "            elif label == 'BFISTS':y = 2\n",
    "            elif label == 'BFEET': y = 3\n",
    "            else: continue\n",
    "\n",
    "            out.append((seg, y, subj))\n",
    "\n",
    "    elif kind == 'EO':\n",
    "        return ([], raw.ch_names)\n",
    "\n",
    "    return out, raw.ch_names\n",
    "\n",
    "def build_dataset_all(subjects, scenario='4c', window_mode='6s'):\n",
    "    X, y, groups = [], [], []\n",
    "    ch_template = None\n",
    "\n",
    "    for s in tqdm(subjects, desc=\"Construyendo dataset (RAW)\"):\n",
    "        sdir = DATA_RAW / f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "\n",
    "        trials_L, trials_R, trials_FISTS, trials_FEET = [], [], [], []\n",
    "\n",
    "        for r in MI_RUNS_LR:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 0: trials_L.append(seg)\n",
    "                elif lab == 1: trials_R.append(seg)\n",
    "\n",
    "        for r in MI_RUNS_OF:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 2: trials_FISTS.append(seg)\n",
    "                elif lab == 3: trials_FEET.append(seg)\n",
    "\n",
    "        need_per_class = 21\n",
    "        def pick(trials, n, rng):\n",
    "            if len(trials) < n:\n",
    "                idx = rng.choice(len(trials), size=n, replace=True)\n",
    "                return [trials[i] for i in idx]\n",
    "            rng.shuffle(trials)\n",
    "            return trials[:n]\n",
    "\n",
    "        rng = check_random_state(RANDOM_STATE + s)\n",
    "        if len(trials_L)==0 or len(trials_R)==0 or len(trials_FISTS)==0 or len(trials_FEET)==0:\n",
    "            continue\n",
    "\n",
    "        Lp  = pick(trials_L,     need_per_class, rng)\n",
    "        Rp  = pick(trials_R,     need_per_class, rng)\n",
    "        FIp = pick(trials_FISTS, need_per_class, rng)\n",
    "        FEp = pick(trials_FEET,  need_per_class, rng)\n",
    "\n",
    "        pack = [(Lp, 0), (Rp, 1), (FIp, 2), (FEp, 3)]\n",
    "        for segs, lab in pack:\n",
    "            for seg in segs:\n",
    "                X.append(seg); y.append(lab); groups.append(s)\n",
    "\n",
    "    X = np.stack(X, axis=0)\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "    groups = np.asarray(groups, dtype=np.int64)\n",
    "\n",
    "    n, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Dataset construido: N={n} | T={T} | C={C} | clases={n_classes} | sujetos únicos={len(np.unique(groups))}\")\n",
    "    return X, y, groups, ch_template\n",
    "\n",
    "# =========================\n",
    "# AUGMENTS (solo train)\n",
    "# =========================\n",
    "def do_time_jitter(x, max_ms=50, fs=160.0):\n",
    "    # x: (B,1,T,C) torch.float\n",
    "    max_shift = int(round(max_ms/1000.0 * fs))\n",
    "    if max_shift <= 0: return x\n",
    "    B,_,T,C = x.shape\n",
    "    shifts = torch.randint(low=-max_shift, high=max_shift+1, size=(B,), device=x.device)\n",
    "    out = torch.empty_like(x)\n",
    "    for i,s in enumerate(shifts):\n",
    "        if s==0: out[i] = x[i]; continue\n",
    "        if s>0:\n",
    "            out[i,:,s:,:] = x[i,:,:T-s,:]\n",
    "            out[i,:,:s,:] = 0\n",
    "        else:\n",
    "            s = -s\n",
    "            out[i,:,:T-s,:] = x[i,:,s:,:]\n",
    "            out[i,:,T-s:,:] = 0\n",
    "    return out\n",
    "\n",
    "def do_gaussian_noise(x, sigma=0.01):\n",
    "    if sigma<=0: return x\n",
    "    return x + sigma*torch.randn_like(x)\n",
    "\n",
    "def do_temporal_cutout_masked(x, y, classes_mask={2,3}, min_ms=30, max_ms=60, fs=160.0):\n",
    "    # Aplica cutout solo si y pertenece a classes_mask (OF)\n",
    "    B,_,T,C = x.shape\n",
    "    if B == 0: return x\n",
    "    Lmin = int(round(min_ms/1000.0*fs))\n",
    "    Lmax = int(round(max_ms/1000.0*fs))\n",
    "    if Lmin<=0 or Lmax<=0 or Lmin>Lmax: return x\n",
    "    out = x.clone()\n",
    "    y_np = y.detach().cpu().numpy()\n",
    "    for i in range(B):\n",
    "        if int(y_np[i]) not in classes_mask: \n",
    "            continue\n",
    "        L = random.randint(Lmin, Lmax)\n",
    "        if L>=T: continue\n",
    "        s = random.randint(0, T-L)\n",
    "        out[i,:,s:s+L,:] = 0\n",
    "    return out\n",
    "\n",
    "def mixup_batch(x, y, n_classes, alpha=0.2):\n",
    "    if alpha<=0:\n",
    "        y_onehot = torch.nn.functional.one_hot(y, num_classes=n_classes).float()\n",
    "        return x, y_onehot, 1.0\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    perm = torch.randperm(x.size(0), device=x.device)\n",
    "    x_mix = lam*x + (1-lam)*x[perm]\n",
    "    y_a = torch.nn.functional.one_hot(y, num_classes=n_classes).float()\n",
    "    y_b = y_a[perm]\n",
    "    y_mix = lam*y_a + (1-lam)*y_b\n",
    "    return x_mix, y_mix, lam\n",
    "\n",
    "class WeightedSoftCrossEntropy(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-entropy suave (targets en probas, p.ej. mixup) con pesos por clase.\n",
    "    \"\"\"\n",
    "    def __init__(self, class_weights=None, label_smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.register_buffer('w', None if class_weights is None else class_weights.clone().float())\n",
    "        self.ls = float(label_smoothing)\n",
    "    def forward(self, logits, target_probs):\n",
    "        if self.ls > 0:\n",
    "            K = logits.size(1)\n",
    "            target_probs = (1-self.ls)*target_probs + self.ls*(1.0/K)\n",
    "        logp = torch.log_softmax(logits, dim=1)  # (B,C)\n",
    "        loss_per_class = -(target_probs * logp)  # (B,C)\n",
    "        if self.w is not None:\n",
    "            loss_per_class = loss_per_class * self.w.unsqueeze(0)\n",
    "        loss = loss_per_class.sum(dim=1).mean()\n",
    "        return loss\n",
    "\n",
    "# =========================\n",
    "# MODELO: CNN + Transformer (Conformer-lite)\n",
    "# =========================\n",
    "class SinPosEnc(nn.Module):\n",
    "    def __init__(self, d_model, max_len=4000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):  # x: (B,L,D)\n",
    "        L = x.size(1)\n",
    "        return x + self.pe[:L].unsqueeze(0)\n",
    "\n",
    "class ConvModule1D(nn.Module):\n",
    "    def __init__(self, d_model, dw_kernel=15, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.pw1 = nn.Conv1d(d_model, 2*d_model, kernel_size=1)\n",
    "        self.dw  = nn.Conv1d(d_model, d_model, kernel_size=dw_kernel, padding=dw_kernel//2, groups=d_model)\n",
    "        self.bn  = nn.BatchNorm1d(d_model)\n",
    "        self.act = nn.SiLU()\n",
    "        self.pw2 = nn.Conv1d(d_model, d_model, kernel_size=1)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):  # (B,L,D)\n",
    "        y = self.ln(x).transpose(1,2)          # (B,D,L)\n",
    "        y = self.pw1(y)\n",
    "        a,b = y.chunk(2, dim=1)\n",
    "        y = a * torch.sigmoid(b)\n",
    "        y = self.dw(y)\n",
    "        y = self.bn(y); y = self.act(y)\n",
    "        y = self.pw2(y)\n",
    "        y = self.drop(y.transpose(1,2))        # (B,L,D)\n",
    "        return y\n",
    "\n",
    "class ConformerLiteBlock(nn.Module):\n",
    "    def __init__(self, d_model=48, n_heads=2, dropout=0.3, dw_kernel=15, ffn_mult=2.0, sd_prob=0.2):\n",
    "        super().__init__()\n",
    "        self.sd_prob = sd_prob\n",
    "        self.ln_attn = nn.LayerNorm(d_model)\n",
    "        self.mha = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.conv = ConvModule1D(d_model, dw_kernel=dw_kernel, dropout=dropout)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        self.ln_ffn = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, int(ffn_mult*d_model)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(ffn_mult*d_model), d_model),\n",
    "        )\n",
    "        self.drop3 = nn.Dropout(dropout)\n",
    "\n",
    "    def _res(self, x, y, training):\n",
    "        if not training or self.sd_prob <= 0.0:\n",
    "            return x + y\n",
    "        keep = (torch.rand(x.size(0), device=x.device) > self.sd_prob).float().view(-1,1,1)\n",
    "        return x + y * keep\n",
    "\n",
    "    def forward(self, x):  # (B,L,D)\n",
    "        y = self.ln_attn(x)\n",
    "        y, _ = self.mha(y, y, y, need_weights=False)\n",
    "        x = self._res(x, self.drop1(y), self.training)\n",
    "        y = self.conv(x)\n",
    "        x = self._res(x, self.drop2(y), self.training)\n",
    "        y = self.ffn(self.ln_ffn(x))\n",
    "        x = self._res(x, self.drop3(y), self.training)\n",
    "        return x\n",
    "\n",
    "class CNNTransMI(nn.Module):\n",
    "    \"\"\"\n",
    "    Entrada: (B, 1, T, C) con T=960,C=8 (6s @160Hz)\n",
    "    Pipeline:\n",
    "      - Stem temporal (CNN) ligero\n",
    "      - Proyección espacial 1x1 (C->D)\n",
    "      - Patchify temporal (stride=patch_len)\n",
    "      - N bloques Conformer-lite\n",
    "      - AvgPool temporal + MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, n_ch=8, n_classes=4,\n",
    "                 d_model=48, n_heads=2, n_blocks=3,\n",
    "                 patch_len=8, dropout=0.3, sd_prob=0.2, dw_kernel=15):\n",
    "        super().__init__()\n",
    "        self.n_ch = n_ch\n",
    "        self.d_model = d_model\n",
    "        self.patch_len = patch_len\n",
    "\n",
    "        # Stem temporal ligero (sobre T; trabaja canal-independiente)\n",
    "        self.temporal_stem = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=(15,1), padding=(7,0), bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(8, 8, kernel_size=(15,1), padding=(7,0), bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        # Reordenar y proyección espacial a D\n",
    "        self.spatial_pw = nn.Conv1d(n_ch*8, d_model, kernel_size=1, bias=False)\n",
    "        self.spatial_bn = nn.BatchNorm1d(d_model)\n",
    "        self.spatial_act = nn.GELU()\n",
    "\n",
    "        # Patchify temporal (stride=patch_len)\n",
    "        self.patchify = nn.Conv1d(d_model, d_model, kernel_size=patch_len, stride=patch_len, bias=False)\n",
    "\n",
    "        self.pos = SinPosEnc(d_model, max_len=4000)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ConformerLiteBlock(d_model, n_heads=n_heads, dropout=dropout,\n",
    "                               dw_kernel=dw_kernel, ffn_mult=2.0, sd_prob=sd_prob*(i+1)/n_blocks)\n",
    "            for i in range(n_blocks)\n",
    "        ])\n",
    "        self.ln_out = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x: (B,1,T,C)\n",
    "        B,_,T,C = x.shape\n",
    "        z = self.temporal_stem(x)                 # (B,8,T,C)\n",
    "        z = z.permute(0,3,2,1).contiguous()       # (B,C,T,8)\n",
    "        z = z.view(B, C*8, T)                     # (B,C*8,T)\n",
    "        z = self.spatial_pw(z)                    # (B,D,T)\n",
    "        z = self.spatial_bn(z); z = self.spatial_act(z)\n",
    "        z = self.patchify(z)                      # (B,D,L)\n",
    "        z = z.transpose(1,2)                      # (B,L,D)\n",
    "        z = self.pos(z)\n",
    "        for blk in self.blocks:\n",
    "            z = blk(z)\n",
    "        z = self.ln_out(z)\n",
    "        z = z.mean(dim=1)                         # (B,D)\n",
    "        return self.head(z)\n",
    "\n",
    "# =========================\n",
    "# TORCH DATASET\n",
    "# =========================\n",
    "class EEGTrials(Dataset):\n",
    "    def __init__(self, X, y, groups):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.g = groups.astype(np.int64)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        x = np.expand_dims(x, 0)                 # (1, T, C)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx])\n",
    "\n",
    "# =========================\n",
    "# UTIL: MAX-NORM (para estabilizar FT)\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def apply_max_norm(model, max_value=2.0, p=2.0):\n",
    "    layers = []\n",
    "    # aplicarlo a capas densas finales; opcional a convs\n",
    "    if hasattr(model, 'head'):\n",
    "        for m in model.head:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                layers.append(m)\n",
    "    for layer in layers:\n",
    "        if hasattr(layer, 'weight') and layer.weight is not None:\n",
    "            w = layer.weight.data\n",
    "            norms = w.view(w.size(0), -1).norm(p=p, dim=1, keepdim=True)\n",
    "            desired = torch.clamp(norms, max=max_value)\n",
    "            w.view(w.size(0), -1).mul_(desired / (1e-8 + norms))\n",
    "\n",
    "# =========================\n",
    "# ENTRENAMIENTO / EVALUACIÓN\n",
    "# =========================\n",
    "def build_weighted_sampler(y, groups):\n",
    "    # pesos inversos por clase y por sujeto → promueve balance en cada batch\n",
    "    y = np.asarray(y); groups = np.asarray(groups)\n",
    "    class_counts = np.bincount(y, minlength=len(np.unique(y))).astype(float)\n",
    "    class_w = 1.0 / class_counts[y]\n",
    "    subj_vals, subj_counts = np.unique(groups, return_counts=True)\n",
    "    subj_map = {s:c for s,c in zip(subj_vals, subj_counts)}\n",
    "    subj_w = np.array([1.0/subj_map[g] for g in groups], dtype=float)\n",
    "    w = class_w * subj_w\n",
    "    w = w / w.mean()\n",
    "    w_t = torch.from_numpy(w).float()\n",
    "    sampler = WeightedRandomSampler(weights=w_t, num_samples=len(w_t), replacement=True)\n",
    "    return sampler\n",
    "\n",
    "def make_class_weight_tensor(y_indices, n_classes, boost_bfists=1.20):\n",
    "    # pesos inversos por clase, normalizados; clase 2 (*Both Fists*) con boost\n",
    "    counts = np.bincount(y_indices, minlength=n_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    w = counts.sum() / counts\n",
    "    w = w / w.mean()\n",
    "    w[2] *= boost_bfists\n",
    "    w = w / w.mean()\n",
    "    return torch.tensor(w, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "def train_epoch(model, loader, opt, criterion_soft, n_classes,\n",
    "                do_aug=True, fs=160.0, maxnorm=None):\n",
    "    model.train()\n",
    "    for xb, yb, _ in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "\n",
    "        # ---- AUGMENTS (orden: jitter → noise → cutout focalizado → mixup) ----\n",
    "        if do_aug:\n",
    "            xb = do_time_jitter(xb, max_ms=50, fs=fs)\n",
    "            xb = do_gaussian_noise(xb, sigma=0.01)\n",
    "            xb = do_temporal_cutout_masked(xb, yb, classes_mask={2,3}, min_ms=30, max_ms=60, fs=fs)\n",
    "            xb, yt, _ = mixup_batch(xb, yb, n_classes=n_classes, alpha=0.2)\n",
    "            logits = model(xb)\n",
    "            loss = criterion_soft(logits, yt)\n",
    "        else:\n",
    "            logits = model(xb)\n",
    "            yt = torch.nn.functional.one_hot(yb, num_classes=n_classes).float()\n",
    "            loss = criterion_soft(logits, yt)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if maxnorm is not None:\n",
    "            apply_max_norm(model, max_value=maxnorm, p=2.0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _predict_tta(model, xb, n=5, fs=160.0):\n",
    "    outs = []\n",
    "    for _ in range(n):\n",
    "        xj = do_time_jitter(xb, max_ms=25, fs=fs)\n",
    "        outs.append(model(xj))\n",
    "    return torch.stack(outs, dim=0).mean(dim=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_with_preds(model, loader, use_tta=True, tta_n=5):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for xb, yb, _ in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        if use_tta:\n",
    "            logits = _predict_tta(model, xb, n=tta_n, fs=FS)\n",
    "        else:\n",
    "            logits = model(xb)\n",
    "        pred = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "        y_pred.extend(pred)\n",
    "        y_true.extend(yb.numpy().tolist())\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    return y_true, y_pred, float(acc)\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes, title, fname):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.nan_to_num(cm_norm)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm_norm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "        plt.text(j, i, format(cm_norm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm_norm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label'); plt.xlabel('Predicted label')\n",
    "    plt.tight_layout(); plt.savefig(fname, dpi=150, bbox_inches='tight'); plt.close()\n",
    "\n",
    "def plot_training_curves(history, fname):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(history['train_acc'], label='train_acc')\n",
    "    plt.plot(history['val_acc'], label='val_acc')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
    "    plt.title('Training curve'); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(fname, dpi=150); plt.close()\n",
    "\n",
    "def print_report(y_true, y_pred, classes):\n",
    "    print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
    "\n",
    "# =========================\n",
    "# FINE-TUNING PROGRESIVO por sujeto (con validación interna)\n",
    "# =========================\n",
    "def _freeze_for_mode_cnntrans(model, mode):\n",
    "    # Congela todo y libera según el modo\n",
    "    for p in model.parameters(): p.requires_grad = False\n",
    "\n",
    "    if mode == 'out':\n",
    "        # Solo la última capa\n",
    "        for p in model.head[-1].parameters(): p.requires_grad = True\n",
    "    elif mode == 'head':\n",
    "        for m in model.head:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                for p in m.parameters(): p.requires_grad = True\n",
    "    elif mode == 'spatial+head':\n",
    "        # Último bloque + ln_out + head\n",
    "        for p in model.blocks[-1].parameters(): p.requires_grad = True\n",
    "        for p in model.ln_out.parameters(): p.requires_grad = True\n",
    "        for m in model.head:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                for p in m.parameters(): p.requires_grad = True\n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "\n",
    "def _param_groups_cnntrans(model, mode):\n",
    "    if mode == 'out':\n",
    "        return list(model.head[-1].parameters())\n",
    "    elif mode == 'head':\n",
    "        params = []\n",
    "        for m in model.head:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                params += list(m.parameters())\n",
    "        return params\n",
    "    elif mode == 'spatial+head':\n",
    "        base = list(model.blocks[-1].parameters()) + list(model.ln_out.parameters())\n",
    "        head = []\n",
    "        for m in model.head:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                head += list(m.parameters())\n",
    "        return base + head\n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "\n",
    "def _class_weights(y_np, n_classes):\n",
    "    counts = np.bincount(y_np, minlength=n_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    weights = counts.sum() / counts\n",
    "    weights = weights / weights.mean()\n",
    "    return torch.tensor(weights, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "def _train_one_mode(model, X_cal, y_cal, n_classes, mode,\n",
    "                    epochs=FT_EPOCHS, batch_size=32,\n",
    "                    head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                    l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=RANDOM_STATE)\n",
    "    (tr_idx, va_idx), = sss.split(X_cal, y_cal)\n",
    "    Xtr, ytr = X_cal[tr_idx], y_cal[tr_idx]\n",
    "    Xva, yva = X_cal[va_idx], y_cal[va_idx]\n",
    "\n",
    "    ds_tr = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xtr).float().unsqueeze(1),\n",
    "        torch.from_numpy(ytr).long()\n",
    "    )\n",
    "    ds_va = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xva).float().unsqueeze(1),\n",
    "        torch.from_numpy(yva).long()\n",
    "    )\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    _freeze_for_mode_cnntrans(model, mode)\n",
    "\n",
    "    if mode == 'spatial+head':\n",
    "        base_params = list(model.blocks[-1].parameters()) + list(model.ln_out.parameters())\n",
    "        head_params = []\n",
    "        for m in model.head:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                head_params += list(m.parameters())\n",
    "        train_params = base_params + head_params\n",
    "        opt = optim.Adam([\n",
    "            {\"params\": base_params, \"lr\": base_lr},\n",
    "            {\"params\": head_params, \"lr\": head_lr},\n",
    "        ])\n",
    "    else:\n",
    "        train_params = _param_groups_cnntrans(model, mode)\n",
    "        opt = optim.Adam(train_params, lr=head_lr)\n",
    "\n",
    "    ref = [p.detach().clone().to(p.device) for p in train_params]\n",
    "    class_w = _class_weights(ytr, n_classes)\n",
    "    crit = nn.CrossEntropyLoss(weight=class_w)\n",
    "\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_val = float('inf'); bad = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            reg = 0.0\n",
    "            for p_cur, p_ref in zip(train_params, ref):\n",
    "                reg = reg + torch.sum((p_cur - p_ref)**2)\n",
    "            loss = loss + l2sp_lambda * reg\n",
    "            loss.backward(); opt.step()\n",
    "            apply_max_norm(model, max_value=2.0, p=2.0)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0; nval = 0\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                loss = crit(logits, yb)\n",
    "                val_loss += loss.item() * xb.size(0); nval += xb.size(0)\n",
    "            val_loss /= max(1, nval)\n",
    "\n",
    "        if val_loss + 1e-7 < best_val:\n",
    "            best_val = val_loss; bad = 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience: break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_numpy(model, X_np, device):\n",
    "    model.eval()\n",
    "    xb = torch.from_numpy(X_np).float().unsqueeze(1).to(device)\n",
    "    logits = model(xb)\n",
    "    return logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "def subject_cv_finetune_predict_progressive(model_global, Xs, ys, device,\n",
    "                                            n_splits=CALIB_CV_FOLDS, n_classes=4):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    y_true_full = np.empty_like(ys); y_pred_full = np.empty_like(ys)\n",
    "\n",
    "    for tr_idx, te_idx in skf.split(Xs, ys):\n",
    "        Xcal, ycal = Xs[tr_idx], ys[tr_idx]\n",
    "        Xho,  yho  = Xs[te_idx], ys[te_idx]\n",
    "\n",
    "        m_out = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_out, Xcal, ycal, n_classes, mode='out',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_out = predict_numpy(m_out, Xho, device); acc_out = (yhat_out == yho).mean()\n",
    "\n",
    "        m_head = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_head, Xcal, ycal, n_classes, mode='head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_head = predict_numpy(m_head, Xho, device); acc_head = (yhat_head == yho).mean()\n",
    "\n",
    "        m_sp = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_sp, Xcal, ycal, n_classes, mode='spatial+head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                        l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_sp = predict_numpy(m_sp, Xho, device); acc_sp = (yhat_sp == yho).mean()\n",
    "\n",
    "        best_idx = np.argmax([acc_out, acc_head, acc_sp])\n",
    "        yhat_best = [yhat_out, yhat_head, yhat_sp][best_idx]\n",
    "\n",
    "        y_true_full[te_idx] = yho; y_pred_full[te_idx] = yhat_best\n",
    "\n",
    "    return y_true_full, y_pred_full\n",
    "\n",
    "# =========================\n",
    "# FOLDS JSON helpers\n",
    "# =========================\n",
    "def save_group_folds_json_with_indices(subject_ids_str, groups_array, n_splits, out_json_path,\n",
    "                                       created_by=\"cnntrans_experiment\", description=None):\n",
    "    out_json_path = Path(out_json_path)\n",
    "    unique_subjects_int = sorted(np.unique(groups_array).tolist())\n",
    "    subject_ids = [f\"S{sid:03d}\" for sid in unique_subjects_int]\n",
    "\n",
    "    if len(subject_ids) < n_splits:\n",
    "        raise ValueError(f\"n_splits={n_splits} mayor que número de sujetos={len(subject_ids)}\")\n",
    "\n",
    "    groups = np.arange(len(subject_ids))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    folds = []\n",
    "    fold_i = 0\n",
    "    for train_idx_grp, test_idx_grp in gkf.split(groups, groups, groups):\n",
    "        fold_i += 1\n",
    "        train_sids = [subject_ids[int(i)] for i in train_idx_grp]\n",
    "        test_sids  = [subject_ids[int(i)] for i in test_idx_grp]\n",
    "\n",
    "        train_sids_int = [int(s[1:]) for s in train_sids]\n",
    "        test_sids_int  = [int(s[1:]) for s in test_sids]\n",
    "\n",
    "        tr_idx = np.where(np.isin(groups_array, train_sids_int))[0].tolist()\n",
    "        te_idx = np.where(np.isin(groups_array, test_sids_int))[0].tolist()\n",
    "\n",
    "        folds.append({\n",
    "            \"fold\": int(fold_i),\n",
    "            \"train\": train_sids,\n",
    "            \"test\": test_sids,\n",
    "            \"tr_idx\": tr_idx,\n",
    "            \"te_idx\": te_idx\n",
    "        })\n",
    "\n",
    "    payload = {\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"created_by\": created_by,\n",
    "        \"description\": description if description is not None else \"\",\n",
    "        \"n_splits\": int(n_splits),\n",
    "        \"n_subjects\": len(subject_ids),\n",
    "        \"subject_ids\": subject_ids,\n",
    "        \"folds\": folds\n",
    "    }\n",
    "\n",
    "    out_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Folds JSON con índices guardado → {out_json_path}\")\n",
    "    return out_json_path\n",
    "\n",
    "def load_group_folds_json(path_json, expected_subject_ids=None, strict_check=True):\n",
    "    path_json = Path(path_json)\n",
    "    if not path_json.exists():\n",
    "        raise FileNotFoundError(f\"No existe {path_json}\")\n",
    "    with open(path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        payload = json.load(f)\n",
    "\n",
    "    subj_json = payload.get(\"subject_ids\", [])\n",
    "    if expected_subject_ids is not None:\n",
    "        expected = sorted(list(expected_subject_ids))\n",
    "        if subj_json != expected:\n",
    "            msg = (\"Los subject_ids del JSON no coinciden con expected_subject_ids.\\n\"\n",
    "                   f\"JSON has {len(subj_json)} subjects, expected {len(expected)}.\\n\"\n",
    "                   f\"First 10 JSON: {subj_json[:10]}\\nFirst 10 expected: {expected[:10]}\")\n",
    "            if strict_check:\n",
    "                raise ValueError(msg)\n",
    "            else:\n",
    "                print(\"WARNING: \" + msg)\n",
    "    return payload\n",
    "\n",
    "# =========================\n",
    "# EXPERIMENTO\n",
    "# =========================\n",
    "def run_experiment(save_folds_json=True, folds_json_path=FOLDS_DIR, folds_json_description=\"GroupKFold folds for CNN+Transformer\"):\n",
    "    mne.set_log_level('WARNING')\n",
    "\n",
    "    subs = subjects_available()\n",
    "    print(f\"Sujetos elegibles: {len(subs)} → {subs[:10]}{'...' if len(subs)>10 else ''}\")\n",
    "\n",
    "    X, y, groups, chs = build_dataset_all(subs, scenario=CLASS_SCENARIO, window_mode=WINDOW_MODE)\n",
    "    N, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Listo para entrenar: N={N} | T={T} | C={C} | clases={n_classes} | sujetos={len(np.unique(groups))}\")\n",
    "\n",
    "    ds = EEGTrials(X, y, groups)\n",
    "\n",
    "    # preparar JSON folds\n",
    "    if folds_json_path is None:\n",
    "        folds_json_path = Path(\"folds\") / f\"group_folds_{N_FOLDS}splits.json\"\n",
    "    else:\n",
    "        folds_json_path = Path(folds_json_path)\n",
    "    folds_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    unique_subs = sorted(np.unique(groups).tolist())\n",
    "    subject_ids_str = [f\"S{s:03d}\" for s in unique_subs]\n",
    "\n",
    "    if not folds_json_path.exists():\n",
    "        if not save_folds_json:\n",
    "            raise FileNotFoundError(f\"Folds JSON no encontrado en {folds_json_path} y save_folds_json=False.\")\n",
    "        save_group_folds_json_with_indices(subject_ids_str, groups, n_splits=N_FOLDS,\n",
    "                                           out_json_path=folds_json_path,\n",
    "                                           created_by=\"CNNTransMI\",\n",
    "                                           description=folds_json_description)\n",
    "\n",
    "    payload = load_group_folds_json(folds_json_path, expected_subject_ids=subject_ids_str, strict_check=False)\n",
    "    folds = payload[\"folds\"]\n",
    "\n",
    "    # bucle por folds\n",
    "    global_folds = []\n",
    "    ft_prog_folds = []\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "\n",
    "    for f in folds:\n",
    "        fold = f[\"fold\"]\n",
    "        tr_idx = np.asarray(f.get(\"tr_idx\", []), dtype=int)\n",
    "        te_idx = np.asarray(f.get(\"te_idx\", []), dtype=int)\n",
    "\n",
    "        if tr_idx.size == 0 or te_idx.size == 0:\n",
    "            print(f\"Advertencia: fold {fold} sin índices tr/te válidos. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        # ===== Split de validación por sujetos =====\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=GLOBAL_VAL_SPLIT, random_state=RANDOM_STATE)\n",
    "        tr_subj_idx, va_subj_idx = next(gss.split(tr_idx, groups[tr_idx], groups[tr_idx]))\n",
    "        tr_sub_idx = tr_idx[tr_subj_idx]\n",
    "        va_idx     = tr_idx[va_subj_idx]\n",
    "\n",
    "        # Sampler balanceado para train\n",
    "        sampler = build_weighted_sampler(y[tr_sub_idx], groups[tr_sub_idx])\n",
    "\n",
    "        tr_loader = DataLoader(Subset(ds, tr_sub_idx), batch_size=BATCH_SIZE, sampler=sampler, drop_last=False)\n",
    "        va_loader = DataLoader(Subset(ds, va_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "        te_loader = DataLoader(Subset(ds, te_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "        # ===== CNN + Transformer =====\n",
    "        model = CNNTransMI(\n",
    "            n_ch=C, n_classes=n_classes,\n",
    "            d_model=48, n_heads=2, n_blocks=3,\n",
    "            patch_len=8, dropout=0.3, sd_prob=0.2, dw_kernel=15\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # Opt y scheduler SGDR\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_INIT)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=SGDR_T0, T_mult=SGDR_Tmult)\n",
    "\n",
    "        # Métrica rápida\n",
    "        def _acc(loader):\n",
    "            return evaluate_with_preds(model, loader, use_tta=True, tta_n=5)[2]\n",
    "\n",
    "        # Historia para curvas\n",
    "        history = {'train_acc': [], 'val_acc': []}\n",
    "\n",
    "        # ---- Criterio suave con ponderación por clase (+20% BFISTS) ----\n",
    "        class_weights = make_class_weight_tensor(y[tr_sub_idx], n_classes, boost_bfists=1.20)\n",
    "        criterion_soft = WeightedSoftCrossEntropy(class_weights, label_smoothing=0.05)\n",
    "\n",
    "        # ===== Entrenamiento global =====\n",
    "        print(f\"\\n[Fold {fold}/{N_FOLDS}] Entrenando modelo global...\"\n",
    "              f\" (n_train={len(tr_sub_idx)} | n_val={len(va_idx)} | n_test={len(te_idx)})\")\n",
    "\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        best_val = -1.0\n",
    "        bad = 0\n",
    "\n",
    "        for epoch in range(1, EPOCHS_GLOBAL + 1):\n",
    "            train_epoch(model, tr_loader, opt, criterion_soft, n_classes=n_classes,\n",
    "                        do_aug=True, fs=FS, maxnorm=2.0)\n",
    "            scheduler.step(epoch-1 + 1e-8)  # tick suave\n",
    "\n",
    "            # eval\n",
    "            tr_acc = _acc(tr_loader)\n",
    "            va_acc = _acc(va_loader)\n",
    "            history['train_acc'].append(tr_acc)\n",
    "            history['val_acc'].append(va_acc)\n",
    "\n",
    "            if (epoch % LOG_EVERY == 0) or epoch in (1, 10, 20, 50, 100):\n",
    "                cur_lr = opt.param_groups[0]['lr']\n",
    "                print(f\"  Época {epoch:3d} | train_acc={tr_acc:.4f} | val_acc={va_acc:.4f} | LR={cur_lr:.5f}\")\n",
    "\n",
    "            if va_acc > best_val + 1e-4:\n",
    "                best_val = va_acc\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "                bad = 0\n",
    "            else:\n",
    "                bad += 1\n",
    "                if bad >= GLOBAL_PATIENCE:\n",
    "                    print(f\"  Early stopping en época {epoch} (mejor val_acc={best_val:.4f})\")\n",
    "                    break\n",
    "\n",
    "        # guardar curva de entrenamiento\n",
    "        curve_path = f\"training_curve_fold{fold}.png\"\n",
    "        plot_training_curves(history, curve_path)\n",
    "        print(f\"↳ Curva de entrenamiento guardada: {curve_path}\")\n",
    "\n",
    "        # cargar mejor estado antes de evaluar en test\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "        # ===== Evaluación global (inter-sujeto puro) =====\n",
    "        y_true, y_pred, acc_global = evaluate_with_preds(model, te_loader, use_tta=True, tta_n=5)\n",
    "        global_folds.append(acc_global)\n",
    "        all_true.append(y_true); all_pred.append(y_pred)\n",
    "\n",
    "        print(f\"[Fold {fold}/{N_FOLDS}] Global acc={acc_global:.4f}\")\n",
    "        print_report(y_true, y_pred, CLASS_NAMES_4C)\n",
    "\n",
    "        # ---------- Fine-tuning PROGRESIVO por sujeto con 4-fold CV ----------\n",
    "        X_te, y_te, g_te = X[te_idx], y[te_idx], groups[te_idx]\n",
    "\n",
    "        y_true_ft_all, y_pred_ft_all = [], []\n",
    "        used_subjects = 0\n",
    "        for sid in np.unique(g_te):\n",
    "            idx = np.where(g_te == sid)[0]\n",
    "            Xs, ys = X_te[idx], y_te[idx]\n",
    "\n",
    "            if len(ys) < CALIB_CV_FOLDS or len(np.unique(ys)) < 2:\n",
    "                continue\n",
    "\n",
    "            y_true_subj, y_pred_subj = subject_cv_finetune_predict_progressive(\n",
    "                model, Xs, ys, DEVICE, n_splits=CALIB_CV_FOLDS, n_classes=n_classes\n",
    "            )\n",
    "            y_true_ft_all.append(y_true_subj)\n",
    "            y_pred_ft_all.append(y_pred_subj)\n",
    "            used_subjects += 1\n",
    "\n",
    "        if len(y_true_ft_all) > 0:\n",
    "            y_true_ft_all = np.concatenate(y_true_ft_all)\n",
    "            y_pred_ft_all = np.concatenate(y_pred_ft_all)\n",
    "            acc_ft = (y_true_ft_all == y_pred_ft_all).mean()\n",
    "            print(f\"  Fine-tuning PROGRESIVO (por sujeto, {CALIB_CV_FOLDS}-fold CV) acc={acc_ft:.4f} | sujetos={used_subjects}\")\n",
    "            print(f\"  Δ(FT-Global) = {acc_ft - acc_global:+.4f}\")\n",
    "        else:\n",
    "            acc_ft = np.nan\n",
    "            print(\"  Fine-tuning PROGRESIVO no ejecutado (sujeto(s) con muestras insuficientes).\")\n",
    "\n",
    "        ft_prog_folds.append(acc_ft)\n",
    "\n",
    "    # ---------- resultados finales ----------\n",
    "    if len(all_true) > 0:\n",
    "        all_true = np.concatenate(all_true)\n",
    "        all_pred = np.concatenate(all_pred)\n",
    "    else:\n",
    "        all_true = np.array([], dtype=int)\n",
    "        all_pred = np.array([], dtype=int)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTADOS FINALES\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Global folds:\", [f\"{a:.4f}\" for a in global_folds])\n",
    "    if len(global_folds) > 0:\n",
    "        print(f\"Global mean: {np.mean(global_folds):.4f}\")\n",
    "\n",
    "    print(\"Fine-tune PROGRESIVO folds:\", [(\"nan\" if (a is None or np.isnan(a)) else f\"{a:.4f}\") for a in ft_prog_folds])\n",
    "    if len(ft_prog_folds) > 0:\n",
    "        print(f\"Fine-tune PROGRESIVO mean: {np.nanmean(ft_prog_folds):.4f}\")\n",
    "        print(f\"Δ(FT-Global) mean: {np.nanmean(ft_prog_folds) - np.mean(global_folds):+.4f}\")\n",
    "\n",
    "    if all_true.size > 0:\n",
    "        plot_confusion(all_true, all_pred, CLASS_NAMES_4C,\n",
    "                       title=\"Confusion Matrix - Global Model (All Folds)\",\n",
    "                       fname=\"confusion_global_allfolds.png\")\n",
    "        print(\"\\n↳ Matriz de confusión guardada: confusion_global_allfolds.png\")\n",
    "\n",
    "    return {\n",
    "        \"global_folds\": global_folds,\n",
    "        \"ft_prog_folds\": ft_prog_folds,\n",
    "        \"all_true\": all_true,\n",
    "        \"all_pred\": all_pred,\n",
    "        \"folds_json_path\": str(folds_json_path)\n",
    "    }\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🧠 INICIANDO EXPERIMENTO CNN + Transformer (Conformer-lite) + FINE-TUNING PROGRESIVO\")\n",
    "    print(f\"🔧 Configuración: {CLASS_SCENARIO}, {len(EXPECTED_8)} canales, {WINDOW_MODE}\")\n",
    "    print(f\"⚙️  FT: epochs={FT_EPOCHS}, base_lr={FT_BASE_LR}, head_lr={FT_HEAD_LR}, L2SP={FT_L2SP}, patience={FT_PATIENCE}, CV={CALIB_CV_FOLDS}\")\n",
    "    run_experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d82774e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Usando dispositivo: cuda\n",
      "🧠 INICIANDO EXPERIMENTO CNN+Transformer v2 (6s, 8 canales)\n",
      "🔧 Configuración: 4c, 8 canales, 6s\n",
      "⚙️  FT: epochs=30, base_lr=5e-05, head_lr=0.001, L2SP=0.0001, patience=5, CV=4\n",
      "Sujetos elegibles: 103 → [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Construyendo dataset (RAW): 100%|██████████| 103/103 [00:48<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset construido: N=8652 | T=960 | C=8 | clases=4 | sujetos únicos=103\n",
      "Listo para entrenar: N=8652 | T=960 | C=8 | clases=4 | sujetos=103\n",
      "\n",
      "[Fold 1/5] Entrenando modelo global... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  Época   1 | train_acc=0.2692 | val_acc=0.2564 | LR=0.00300\n",
      "  Época   5 | train_acc=0.2985 | val_acc=0.2830 | LR=0.00075\n",
      "  Época  10 | train_acc=0.4665 | val_acc=0.4249 | LR=0.00256\n",
      "  Época  15 | train_acc=0.5204 | val_acc=0.4368 | LR=0.00075\n",
      "  Época  20 | train_acc=0.4971 | val_acc=0.4377 | LR=0.00299\n",
      "  Época  25 | train_acc=0.5311 | val_acc=0.4670 | LR=0.00256\n",
      "  Época  30 | train_acc=0.5711 | val_acc=0.4808 | LR=0.00170\n",
      "  Época  35 | train_acc=0.5863 | val_acc=0.4670 | LR=0.00075\n",
      "  Época  40 | train_acc=0.5908 | val_acc=0.4799 | LR=0.00011\n",
      "  Early stopping en época 40 (mejor val_acc=0.4808)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold1.png\n",
      "[Fold 1/5] Global acc=0.4751\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.6065    0.5102    0.5542       441\n",
      "       Right     0.5336    0.5397    0.5366       441\n",
      "  Both Fists     0.3623    0.4444    0.3992       441\n",
      "   Both Feet     0.4409    0.4059    0.4227       441\n",
      "\n",
      "    accuracy                         0.4751      1764\n",
      "   macro avg     0.4858    0.4751    0.4782      1764\n",
      "weighted avg     0.4858    0.4751    0.4782      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5091 | sujetos=21\n",
      "  Δ(FT-Global) = +0.0340\n",
      "\n",
      "[Fold 2/5] Entrenando modelo global... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  Época   1 | train_acc=0.2514 | val_acc=0.2482 | LR=0.00300\n",
      "  Época   5 | train_acc=0.4610 | val_acc=0.4267 | LR=0.00075\n",
      "  Época  10 | train_acc=0.4881 | val_acc=0.4240 | LR=0.00256\n",
      "  Época  15 | train_acc=0.5126 | val_acc=0.4615 | LR=0.00075\n",
      "  Época  20 | train_acc=0.4808 | val_acc=0.4570 | LR=0.00299\n",
      "  Época  25 | train_acc=0.5217 | val_acc=0.4734 | LR=0.00256\n",
      "  Época  30 | train_acc=0.5537 | val_acc=0.4789 | LR=0.00170\n",
      "  Época  35 | train_acc=0.5766 | val_acc=0.4918 | LR=0.00075\n",
      "  Época  40 | train_acc=0.5875 | val_acc=0.4789 | LR=0.00011\n",
      "  Época  45 | train_acc=0.5487 | val_acc=0.4744 | LR=0.00299\n",
      "  Early stopping en época 45 (mejor val_acc=0.4918)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold2.png\n",
      "[Fold 2/5] Global acc=0.5402\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5674    0.6871    0.6215       441\n",
      "       Right     0.6268    0.5941    0.6100       441\n",
      "  Both Fists     0.4357    0.4921    0.4622       441\n",
      "   Both Feet     0.5446    0.3878    0.4530       441\n",
      "\n",
      "    accuracy                         0.5402      1764\n",
      "   macro avg     0.5436    0.5402    0.5367      1764\n",
      "weighted avg     0.5436    0.5402    0.5367      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5862 | sujetos=21\n",
      "  Δ(FT-Global) = +0.0459\n",
      "\n",
      "[Fold 3/5] Entrenando modelo global... (n_train=5796 | n_val=1092 | n_test=1764)\n",
      "  Época   1 | train_acc=0.2617 | val_acc=0.2500 | LR=0.00300\n",
      "  Época   5 | train_acc=0.4526 | val_acc=0.4606 | LR=0.00075\n",
      "  Época  10 | train_acc=0.4793 | val_acc=0.4789 | LR=0.00256\n",
      "  Época  15 | train_acc=0.5371 | val_acc=0.5302 | LR=0.00075\n",
      "  Época  20 | train_acc=0.4896 | val_acc=0.5101 | LR=0.00299\n",
      "  Época  25 | train_acc=0.5335 | val_acc=0.4927 | LR=0.00256\n",
      "  Época  30 | train_acc=0.5688 | val_acc=0.5549 | LR=0.00170\n",
      "  Época  35 | train_acc=0.5747 | val_acc=0.5577 | LR=0.00075\n",
      "  Época  40 | train_acc=0.6014 | val_acc=0.5742 | LR=0.00011\n",
      "  Época  45 | train_acc=0.5773 | val_acc=0.5476 | LR=0.00299\n",
      "  Época  50 | train_acc=0.5821 | val_acc=0.5458 | LR=0.00285\n",
      "  Early stopping en época 50 (mejor val_acc=0.5742)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold3.png\n",
      "[Fold 3/5] Global acc=0.4972\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5642    0.5283    0.5457       441\n",
      "       Right     0.5810    0.5692    0.5750       441\n",
      "  Both Fists     0.3950    0.4989    0.4409       441\n",
      "   Both Feet     0.4779    0.3923    0.4309       441\n",
      "\n",
      "    accuracy                         0.4972      1764\n",
      "   macro avg     0.5045    0.4972    0.4981      1764\n",
      "weighted avg     0.5045    0.4972    0.4981      1764\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5300 | sujetos=21\n",
      "  Δ(FT-Global) = +0.0329\n",
      "\n",
      "[Fold 4/5] Entrenando modelo global... (n_train=5880 | n_val=1092 | n_test=1680)\n",
      "  Época   1 | train_acc=0.2573 | val_acc=0.2372 | LR=0.00300\n",
      "  Época   5 | train_acc=0.4393 | val_acc=0.4103 | LR=0.00075\n",
      "  Época  10 | train_acc=0.4810 | val_acc=0.4505 | LR=0.00256\n",
      "  Época  15 | train_acc=0.5143 | val_acc=0.4780 | LR=0.00075\n",
      "  Época  20 | train_acc=0.5051 | val_acc=0.4570 | LR=0.00299\n",
      "  Época  25 | train_acc=0.5293 | val_acc=0.4817 | LR=0.00256\n",
      "  Época  30 | train_acc=0.5202 | val_acc=0.4771 | LR=0.00170\n",
      "  Época  35 | train_acc=0.5583 | val_acc=0.4670 | LR=0.00075\n",
      "  Época  40 | train_acc=0.5803 | val_acc=0.4799 | LR=0.00011\n",
      "  Early stopping en época 41 (mejor val_acc=0.4954)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold4.png\n",
      "[Fold 4/5] Global acc=0.5042\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5951    0.5810    0.5880       420\n",
      "       Right     0.5836    0.5071    0.5427       420\n",
      "  Both Fists     0.3921    0.5929    0.4720       420\n",
      "   Both Feet     0.5222    0.3357    0.4087       420\n",
      "\n",
      "    accuracy                         0.5042      1680\n",
      "   macro avg     0.5233    0.5042    0.5028      1680\n",
      "weighted avg     0.5233    0.5042    0.5028      1680\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5185 | sujetos=20\n",
      "  Δ(FT-Global) = +0.0143\n",
      "\n",
      "[Fold 5/5] Entrenando modelo global... (n_train=5880 | n_val=1092 | n_test=1680)\n",
      "  Época   1 | train_acc=0.2543 | val_acc=0.2353 | LR=0.00300\n",
      "  Época   5 | train_acc=0.4119 | val_acc=0.3965 | LR=0.00075\n",
      "  Época  10 | train_acc=0.4469 | val_acc=0.4011 | LR=0.00256\n",
      "  Época  15 | train_acc=0.5175 | val_acc=0.4899 | LR=0.00075\n",
      "  Época  20 | train_acc=0.5037 | val_acc=0.4881 | LR=0.00299\n",
      "  Early stopping en época 23 (mejor val_acc=0.5037)\n",
      "↳ Curva de entrenamiento guardada: training_curve_fold5.png\n",
      "[Fold 5/5] Global acc=0.5054\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.6636    0.5071    0.5749       420\n",
      "       Right     0.5042    0.5690    0.5347       420\n",
      "  Both Fists     0.4117    0.5548    0.4726       420\n",
      "   Both Feet     0.5141    0.3905    0.4438       420\n",
      "\n",
      "    accuracy                         0.5054      1680\n",
      "   macro avg     0.5234    0.5054    0.5065      1680\n",
      "weighted avg     0.5234    0.5054    0.5065      1680\n",
      "\n",
      "  Fine-tuning PROGRESIVO (por sujeto, 4-fold CV) acc=0.5446 | sujetos=20\n",
      "  Δ(FT-Global) = +0.0393\n",
      "\n",
      "============================================================\n",
      "RESULTADOS FINALES\n",
      "============================================================\n",
      "Global folds: ['0.4751', '0.5402', '0.4972', '0.5042', '0.5054']\n",
      "Global mean: 0.5044\n",
      "Fine-tune PROGRESIVO folds: ['0.5091', '0.5862', '0.5300', '0.5185', '0.5446']\n",
      "Fine-tune PROGRESIVO mean: 0.5377\n",
      "Δ(FT-Global) mean: +0.0333\n",
      "\n",
      "↳ Matriz de confusión guardada: confusion_cnntransmi_v2_global_allfolds.png\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# CNN+Transformer v2 — mismo pipeline que tu EEGNet (6s), con mejoras:\n",
    "# - Optimizer: AdamW (lr=3e-3, weight_decay=1e-3) + SGDR\n",
    "# - Modelo: stem temporal ligero + depthwise espacial (estilo EEGNet) + PW→D\n",
    "# - Patchify: patch_len=12 (menos tokens), n_blocks=2\n",
    "# - Regularización: dropout=0.4, sd_prob=0.30, label_smoothing=0.10, ChannelDropout\n",
    "# - CLS token + attention pooling\n",
    "# - Mantiene: mixup, cutout OF, TTA, sampler por sujeto/clase, FT progresivo por sujeto\n",
    "\n",
    "import os, re, math, random, json, itertools, copy\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold, StratifiedShuffleSplit, GroupShuffleSplit\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# CONFIGURACIÓN GENERAL\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "FOLDS_DIR = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dispositivo y semilla\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE); np.random.seed(RANDOM_STATE); random.seed(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "# Escenario y ventana\n",
    "CLASS_SCENARIO = '4c'\n",
    "WINDOW_MODE = '6s'   # 6 segundos (-1 a 5) como en tu EEGNet\n",
    "FS = 160.0\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Entrenamiento global\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS_GLOBAL = 100\n",
    "LR_INIT = 3e-3            # <<< cambio\n",
    "SGDR_T0 = 6\n",
    "SGDR_Tmult = 2\n",
    "WEIGHT_DECAY = 1e-3       # <<< cambio\n",
    "\n",
    "# Validación/ES global\n",
    "GLOBAL_VAL_SPLIT = 0.15\n",
    "GLOBAL_PATIENCE  = 10\n",
    "LOG_EVERY        = 5\n",
    "\n",
    "# Fine-tuning por sujeto (protocolo robusto)\n",
    "CALIB_CV_FOLDS = 4\n",
    "FT_EPOCHS = 30\n",
    "FT_BASE_LR = 5e-5\n",
    "FT_HEAD_LR = 1e-3\n",
    "FT_L2SP = 1e-4\n",
    "FT_PATIENCE = 5\n",
    "FT_VAL_RATIO = 0.2\n",
    "\n",
    "# Normalización por época canal-a-canal (z-score)\n",
    "NORM_EPOCH_ZSCORE = True\n",
    "\n",
    "# Sujetos excluidos\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "\n",
    "# Runs\n",
    "MI_RUNS_LR = [4, 8, 12]\n",
    "MI_RUNS_OF = [6, 10, 14]\n",
    "BASELINE_RUNS_EO = [1]\n",
    "\n",
    "# Canales (8 con FCz)\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','FCz']\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES DE CANALES\n",
    "# =========================\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = re.sub(r'([A-Za-z])0([0-9])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])Z$', r'\\1z', s)\n",
    "    s = s.replace('fp', 'Fp').replace('FP', 'Fp')\n",
    "    s = ''.join(ch.upper() if ch != 'z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {}\n",
    "    for ch in raw.ch_names:\n",
    "        lab = normalize_label(ch)\n",
    "        lab = lab[:-1] + 'z' if lab.endswith('Z') else lab\n",
    "        lab = re.sub(r'([A-Z])Z$', r'\\1z', lab)\n",
    "        mapping[ch] = lab\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired_channels=EXPECTED_8):\n",
    "    missing = [ch for ch in desired_channels if ch not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"Warning: faltan canales {missing} en archivo {getattr(raw,'filenames', [''])[0]}\")\n",
    "        return None\n",
    "    raw.reorder_channels([ch for ch in raw.ch_names if ch in desired_channels] +\n",
    "                         [ch for ch in raw.ch_names if ch not in desired_channels])\n",
    "    raw.pick_channels(desired_channels, ordered=True)\n",
    "    return raw\n",
    "\n",
    "# =========================\n",
    "# LECTURA DE EDF y EVENTOS\n",
    "# =========================\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(run_id:int):\n",
    "    if run_id in MI_RUNS_LR: return 'LR'\n",
    "    if run_id in MI_RUNS_OF: return 'OF'\n",
    "    if run_id in BASELINE_RUNS_EO: return 'EO'\n",
    "    return None\n",
    "\n",
    "# --- Notch adaptativo: lee CSV si existe (subject, run, snr50_db, snr60_db)\n",
    "_SNR_TABLE = None\n",
    "def _load_snr_table():\n",
    "    global _SNR_TABLE\n",
    "    if _SNR_TABLE is not None:\n",
    "        return _SNR_TABLE\n",
    "    csv_path = PROJ / 'reports' / 'psd_mains' / 'psd_mains_summary.csv'\n",
    "    if csv_path.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            _SNR_TABLE = df\n",
    "        except Exception as e:\n",
    "            print(f\"[SNR] No se pudo leer {csv_path}: {e}\")\n",
    "            _SNR_TABLE = None\n",
    "    return _SNR_TABLE\n",
    "\n",
    "def _decide_notch(subject, run, th_db=10.0):\n",
    "    df = _load_snr_table()\n",
    "    if df is None:  # default\n",
    "        return 60.0\n",
    "    row = df[(df['subject']==subject) & (df['run']==run)]\n",
    "    if row.empty:\n",
    "        return 60.0\n",
    "    snr50 = float(row['snr50_db'].iloc[0]); snr60 = float(row['snr60_db'].iloc[0])\n",
    "    if snr60 >= th_db and snr60 >= snr50: return 60.0\n",
    "    if snr50 >= th_db and snr50 >  snr60: return 50.0\n",
    "    return None  # no notch si no sobresale\n",
    "\n",
    "def read_raw_edf(path: Path):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        mont = mne.channels.make_standard_montage('standard_1020')\n",
    "        raw.set_montage(mont, on_missing='ignore')\n",
    "    except Exception:\n",
    "        pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad=\"auto\")\n",
    "    raw = ensure_channels_order(raw, EXPECTED_8)\n",
    "    if raw is None:\n",
    "        return None\n",
    "\n",
    "    sid, rid = parse_subject_run(path)\n",
    "    notch = _decide_notch(sid, rid)\n",
    "    if notch is not None:\n",
    "        raw.notch_filter(freqs=[float(notch)], picks='eeg', method='spectrum_fit', phase='zero')\n",
    "\n",
    "    return raw\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    if raw.annotations is None or len(raw.annotations) == 0:\n",
    "        return []\n",
    "    def _norm(s): return str(s).strip().upper().replace(' ', '')\n",
    "    res = []\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = _norm(desc)\n",
    "        if tag in ('T1','T2'):\n",
    "            res.append((float(onset), tag))\n",
    "    res.sort()\n",
    "    dedup = []\n",
    "    last_t1 = last_t2 = -1e9\n",
    "    for t, tag in res:\n",
    "        if tag == 'T1':\n",
    "            if (t - last_t1) >= 0.5: dedup.append((t, tag)); last_t1 = t\n",
    "        else:\n",
    "            if (t - last_t2) >= 0.5: dedup.append((t, tag)); last_t2 = t\n",
    "    return dedup\n",
    "\n",
    "# =========================\n",
    "# CONSTRUCCIÓN DE DATASETS\n",
    "# =========================\n",
    "def subjects_available():\n",
    "    subs = []\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try: sid = int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir / f\"S{sid:03d}R{r:02d}.edf\").exists() for r in (MI_RUNS_LR + MI_RUNS_OF))\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "def extract_trials_from_run(edf_path: Path, scenario: str, window_mode: str):\n",
    "    subj, run = parse_subject_run(edf_path)\n",
    "    kind = run_kind(run)\n",
    "    if kind not in ('LR','OF','EO'):\n",
    "        return ([], [])\n",
    "\n",
    "    raw = read_raw_edf(edf_path)\n",
    "    if raw is None:\n",
    "        return ([], [])\n",
    "\n",
    "    data = raw.get_data()\n",
    "    fs = raw.info['sfreq']\n",
    "    assert abs(fs - FS) < 1e-6\n",
    "\n",
    "    out = []\n",
    "\n",
    "    if kind in ('LR','OF'):\n",
    "        events = collect_events_T1T2(raw)\n",
    "        if window_mode == '3s':\n",
    "            rel_start, rel_end = 0.0, 3.0\n",
    "        else:\n",
    "            rel_start, rel_end = -1.0, 5.0   # 6s\n",
    "\n",
    "        for onset_sec, tag in events:\n",
    "            if kind == 'LR':\n",
    "                if tag == 'T1': label = 'L'\n",
    "                elif tag == 'T2': label = 'R'\n",
    "                else: continue\n",
    "            else:\n",
    "                if tag == 'T1': label = 'BFISTS'\n",
    "                elif tag == 'T2': label = 'BFEET'\n",
    "                else: continue\n",
    "\n",
    "            if scenario == '2c' and label not in ('L','R'): continue\n",
    "            if scenario == '3c' and label not in ('L','R','BFISTS'): continue\n",
    "            if scenario == '4c' and label not in ('L','R','BFISTS','BFEET'): continue\n",
    "\n",
    "            s = int(round((raw.first_time + onset_sec + rel_start) * fs))\n",
    "            e = int(round((raw.first_time + onset_sec + rel_end) * fs))\n",
    "            if s < 0 or e > data.shape[1]:\n",
    "                continue\n",
    "\n",
    "            seg = data[:, s:e].T.astype(np.float32)\n",
    "            if NORM_EPOCH_ZSCORE:\n",
    "                seg = (seg - seg.mean(axis=0, keepdims=True)) / (seg.std(axis=0, keepdims=True) + 1e-6)\n",
    "\n",
    "            if label == 'L':       y = 0\n",
    "            elif label == 'R':     y = 1\n",
    "            elif label == 'BFISTS':y = 2\n",
    "            elif label == 'BFEET': y = 3\n",
    "            else: continue\n",
    "\n",
    "            out.append((seg, y, subj))\n",
    "\n",
    "    elif kind == 'EO':\n",
    "        return ([], raw.ch_names)\n",
    "\n",
    "    return out, raw.ch_names\n",
    "\n",
    "def build_dataset_all(subjects, scenario='4c', window_mode='6s'):\n",
    "    X, y, groups = [], [], []\n",
    "    ch_template = None\n",
    "\n",
    "    for s in tqdm(subjects, desc=\"Construyendo dataset (RAW)\"):\n",
    "        sdir = DATA_RAW / f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "\n",
    "        trials_L, trials_R, trials_FISTS, trials_FEET = [], [], [], []\n",
    "\n",
    "        for r in MI_RUNS_LR:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 0: trials_L.append(seg)\n",
    "                elif lab == 1: trials_R.append(seg)\n",
    "\n",
    "        for r in MI_RUNS_OF:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            outs, chs = extract_trials_from_run(p, scenario, window_mode)\n",
    "            if ch_template is None and chs: ch_template = chs\n",
    "            for seg, lab, _ in outs:\n",
    "                if lab == 2: trials_FISTS.append(seg)\n",
    "                elif lab == 3: trials_FEET.append(seg)\n",
    "\n",
    "        need_per_class = 21\n",
    "        def pick(trials, n, rng):\n",
    "            if len(trials) < n:\n",
    "                idx = rng.choice(len(trials), size=n, replace=True)\n",
    "                return [trials[i] for i in idx]\n",
    "            rng.shuffle(trials)\n",
    "            return trials[:n]\n",
    "\n",
    "        rng = check_random_state(RANDOM_STATE + s)\n",
    "        if len(trials_L)==0 or len(trials_R)==0 or len(trials_FISTS)==0 or len(trials_FEET)==0:\n",
    "            continue\n",
    "\n",
    "        Lp  = pick(trials_L,     need_per_class, rng)\n",
    "        Rp  = pick(trials_R,     need_per_class, rng)\n",
    "        FIp = pick(trials_FISTS, need_per_class, rng)\n",
    "        FEp = pick(trials_FEET,  need_per_class, rng)\n",
    "\n",
    "        pack = [(Lp, 0), (Rp, 1), (FIp, 2), (FEp, 3)]\n",
    "        for segs, lab in pack:\n",
    "            for seg in segs:\n",
    "                X.append(seg); y.append(lab); groups.append(s)\n",
    "\n",
    "    X = np.stack(X, axis=0)\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "    groups = np.asarray(groups, dtype=np.int64)\n",
    "\n",
    "    n, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Dataset construido: N={n} | T={T} | C={C} | clases={n_classes} | sujetos únicos={len(np.unique(groups))}\")\n",
    "    return X, y, groups, ch_template\n",
    "\n",
    "# =========================\n",
    "# AUGMENTS (solo train)\n",
    "# =========================\n",
    "def do_time_jitter(x, max_ms=50, fs=160.0):\n",
    "    B,_,T,C = x.shape\n",
    "    max_shift = int(round(max_ms/1000.0 * fs))\n",
    "    if max_shift <= 0: return x\n",
    "    shifts = torch.randint(low=-max_shift, high=max_shift+1, size=(B,), device=x.device)\n",
    "    out = torch.empty_like(x)\n",
    "    for i,s in enumerate(shifts):\n",
    "        if s==0: out[i] = x[i]; continue\n",
    "        if s>0:\n",
    "            out[i,:,s:,:] = x[i,:,:T-s,:]\n",
    "            out[i,:,:s,:] = 0\n",
    "        else:\n",
    "            s = -s\n",
    "            out[i,:,:T-s,:] = x[i,:,s:,:]\n",
    "            out[i,:,T-s:,:] = 0\n",
    "    return out\n",
    "\n",
    "def do_gaussian_noise(x, sigma=0.01):\n",
    "    if sigma<=0: return x\n",
    "    return x + sigma*torch.randn_like(x)\n",
    "\n",
    "def do_temporal_cutout_masked(x, y, classes_mask={2,3}, min_ms=30, max_ms=60, fs=160.0):\n",
    "    B,_,T,C = x.shape\n",
    "    Lmin = int(round(min_ms/1000.0*fs))\n",
    "    Lmax = int(round(max_ms/1000.0*fs))\n",
    "    if Lmin<=0 or Lmax<=0 or Lmin>Lmax: return x\n",
    "    out = x.clone()\n",
    "    y_np = y.detach().cpu().numpy()\n",
    "    for i in range(B):\n",
    "        if int(y_np[i]) not in classes_mask: \n",
    "            continue\n",
    "        L = random.randint(Lmin, Lmax)\n",
    "        if L>=T: continue\n",
    "        s = random.randint(0, T-L)\n",
    "        out[i,:,s:s+L,:] = 0\n",
    "    return out\n",
    "\n",
    "def mixup_batch(x, y, n_classes, alpha=0.2):\n",
    "    if alpha<=0:\n",
    "        y_onehot = torch.nn.functional.one_hot(y, num_classes=n_classes).float()\n",
    "        return x, y_onehot, 1.0\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    perm = torch.randperm(x.size(0), device=x.device)\n",
    "    x_mix = lam*x + (1-lam)*x[perm]\n",
    "    y_a = torch.nn.functional.one_hot(y, num_classes=n_classes).float()\n",
    "    y_b = y_a[perm]\n",
    "    y_mix = lam*y_a + (1-lam)*y_b\n",
    "    return x_mix, y_mix, lam\n",
    "\n",
    "class WeightedSoftCrossEntropy(nn.Module):\n",
    "    def __init__(self, class_weights=None, label_smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.register_buffer('w', None if class_weights is None else class_weights.clone().float())\n",
    "        self.ls = float(label_smoothing)\n",
    "    def forward(self, logits, target_probs):\n",
    "        if self.ls > 0:\n",
    "            K = logits.size(1)\n",
    "            target_probs = (1-self.ls)*target_probs + self.ls*(1.0/K)\n",
    "        logp = torch.log_softmax(logits, dim=1)\n",
    "        loss_per_class = -(target_probs * logp)\n",
    "        if self.w is not None:\n",
    "            loss_per_class = loss_per_class * self.w.unsqueeze(0)\n",
    "        loss = loss_per_class.sum(dim=1).mean()\n",
    "        return loss\n",
    "\n",
    "# =========================\n",
    "# DATASET TORCH\n",
    "# =========================\n",
    "class EEGTrials(Dataset):\n",
    "    def __init__(self, X, y, groups):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.g = groups.astype(np.int64)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        x = np.expand_dims(x, 0)     # (1, T, C)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx])\n",
    "\n",
    "CLASS_NAMES_4C = ['Left', 'Right', 'Both Fists', 'Both Feet']\n",
    "\n",
    "# =========================\n",
    "# UTIL: MAX-NORM (opcional tras step)\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def apply_max_norm(model, max_value=2.0, p=2.0):\n",
    "    layers = []\n",
    "    if hasattr(model, 'head') and isinstance(model.head, nn.Sequential):\n",
    "        for mod in model.head:\n",
    "            if hasattr(mod, 'weight'): layers.append(mod)\n",
    "    for layer in layers:\n",
    "        if hasattr(layer, 'weight') and layer.weight is not None:\n",
    "            w = layer.weight.data\n",
    "            norms = w.view(w.size(0), -1).norm(p=p, dim=1, keepdim=True)\n",
    "            desired = torch.clamp(norms, max=max_value)\n",
    "            w.view(w.size(0), -1).mul_(desired / (1e-8 + norms))\n",
    "\n",
    "# =========================\n",
    "# MODELO: CNN + Transformer v2\n",
    "# =========================\n",
    "class ChannelDropout(nn.Module):\n",
    "    def __init__(self, p=0.1): super().__init__(); self.p = p\n",
    "    def forward(self, x):  # (B,1,T,C)\n",
    "        if not self.training or self.p <= 0: return x\n",
    "        B,_,T,C = x.shape\n",
    "        mask = (torch.rand(B,1,1,C, device=x.device) > self.p).float()\n",
    "        return x * mask\n",
    "\n",
    "class SinPosEnc(nn.Module):\n",
    "    def __init__(self, d_model, max_len=4000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div); pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):  # (B,L,D)\n",
    "        L = x.size(1); return x + self.pe[:L].unsqueeze(0)\n",
    "\n",
    "class ConvModule1D(nn.Module):\n",
    "    def __init__(self, d_model, dw_kernel=15, dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.ln  = nn.LayerNorm(d_model)\n",
    "        self.pw1 = nn.Conv1d(d_model, 2*d_model, kernel_size=1)\n",
    "        self.dw  = nn.Conv1d(d_model, d_model, kernel_size=dw_kernel, padding=dw_kernel//2, groups=d_model)\n",
    "        self.bn  = nn.BatchNorm1d(d_model)\n",
    "        self.act = nn.SiLU()\n",
    "        self.pw2 = nn.Conv1d(d_model, d_model, kernel_size=1)\n",
    "        self.drop= nn.Dropout(dropout)\n",
    "    def forward(self, x):  # (B,L,D)\n",
    "        y = self.ln(x).transpose(1,2)         # (B,D,L)\n",
    "        y = self.pw1(y); a,b = y.chunk(2, dim=1); y = a * torch.sigmoid(b)\n",
    "        y = self.dw(y); y = self.bn(y); y = self.act(y); y = self.pw2(y)\n",
    "        return self.drop(y.transpose(1,2))\n",
    "\n",
    "class ConformerLiteBlock(nn.Module):\n",
    "    def __init__(self, d_model=48, n_heads=2, dropout=0.4, dw_kernel=15, ffn_mult=2.0, sd_prob=0.30):\n",
    "        super().__init__()\n",
    "        self.sd_prob = sd_prob\n",
    "        self.ln_attn = nn.LayerNorm(d_model)\n",
    "        self.mha     = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.drop1   = nn.Dropout(dropout)\n",
    "        self.conv    = ConvModule1D(d_model, dw_kernel=dw_kernel, dropout=dropout)\n",
    "        self.drop2   = nn.Dropout(dropout)\n",
    "        self.ln_ffn  = nn.LayerNorm(d_model)\n",
    "        self.ffn     = nn.Sequential(\n",
    "            nn.Linear(d_model, int(ffn_mult*d_model)),\n",
    "            nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(int(ffn_mult*d_model), d_model),\n",
    "        )\n",
    "        self.drop3   = nn.Dropout(dropout)\n",
    "\n",
    "    def _res(self, x, y, training):\n",
    "        if not training or self.sd_prob <= 0.0: return x + y\n",
    "        keep = (torch.rand(x.size(0), device=x.device) > self.sd_prob).float().view(-1,1,1)\n",
    "        return x + y * keep\n",
    "\n",
    "    def forward(self, x):            # (B,L,D)\n",
    "        q = self.ln_attn(x)\n",
    "        y, _ = self.mha(q, q, q, need_weights=False)\n",
    "        x = self._res(x, self.drop1(y), self.training)\n",
    "        y = self.conv(x)\n",
    "        x = self._res(x, self.drop2(y), self.training)\n",
    "        y = self.ffn(self.ln_ffn(x))\n",
    "        x = self._res(x, self.drop3(y), self.training)\n",
    "        return x\n",
    "\n",
    "class CNNTransMIv2(nn.Module):\n",
    "    def __init__(self, n_ch=8, n_classes=4,\n",
    "                 d_model=48, n_heads=2, n_blocks=2,\n",
    "                 patch_len=12, dropout=0.4, sd_prob=0.30, dw_kernel=15):\n",
    "        super().__init__()\n",
    "        self.n_ch = n_ch\n",
    "        self.d_model = d_model\n",
    "        self.patch_len = patch_len\n",
    "        self.chdrop = ChannelDropout(p=0.1)\n",
    "\n",
    "        # Stem temporal ligero (conv solo en T)\n",
    "        self.temporal_stem = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=(15,1), padding=(7,0), bias=False),\n",
    "            nn.BatchNorm2d(8), nn.GELU(),\n",
    "            nn.Conv2d(8, 8, kernel_size=(15,1), padding=(7,0), bias=False),\n",
    "            nn.BatchNorm2d(8), nn.GELU()\n",
    "        )\n",
    "\n",
    "        # Spatial estilo EEGNet: depthwise (1×C) por mapa\n",
    "        self.spatial_dw = nn.Conv2d(8, 8, kernel_size=(1, n_ch), groups=8, bias=False)\n",
    "        self.spatial_bn = nn.BatchNorm2d(8)\n",
    "        self.spatial_act= nn.GELU()\n",
    "\n",
    "        # Proyección a D con 1x1 sobre canales\n",
    "        self.proj_pw = nn.Conv1d(8, d_model, kernel_size=1, bias=False)\n",
    "        self.proj_bn = nn.BatchNorm1d(d_model)\n",
    "        self.proj_act= nn.GELU()\n",
    "\n",
    "        # Patchify temporal\n",
    "        self.patchify = nn.Conv1d(d_model, d_model, kernel_size=patch_len, stride=patch_len, bias=False)\n",
    "\n",
    "        # CLS token\n",
    "        self.cls = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        nn.init.trunc_normal_(self.cls, std=0.02)\n",
    "\n",
    "        self.pos = SinPosEnc(d_model, max_len=4000)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ConformerLiteBlock(d_model, n_heads=n_heads, dropout=dropout,\n",
    "                               dw_kernel=dw_kernel, ffn_mult=2.0, sd_prob=sd_prob*(i+1)/max(1,n_blocks))\n",
    "            for i in range(n_blocks)\n",
    "        ])\n",
    "        self.ln_out = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Attention pooling\n",
    "        self.attn_pool = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(d_model, 1)\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, 128), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # (B,1,T,C)\n",
    "        B,_,T,C = x.shape\n",
    "        x = self.chdrop(x)\n",
    "\n",
    "        z = self.temporal_stem(x)               # (B,8,T,C)\n",
    "        z = self.spatial_dw(z)                  # (B,8,T,1)\n",
    "        z = self.spatial_bn(z); z = self.spatial_act(z)\n",
    "        z = z.squeeze(3).transpose(1,2)         # (B,T,8)\n",
    "        z = z.transpose(1,2)                    # (B,8,T)\n",
    "        z = self.proj_pw(z)                     # (B,D,T)\n",
    "        z = self.proj_bn(z); z = self.proj_act(z)\n",
    "\n",
    "        z = self.patchify(z)                    # (B,D,L)\n",
    "        z = z.transpose(1,2)                    # (B,L,D)\n",
    "\n",
    "        cls = self.cls.expand(B, 1, -1)         # (B,1,D)\n",
    "        z = torch.cat([cls, z], dim=1)          # (B,1+L,D)\n",
    "        z = self.pos(z)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            z = blk(z)\n",
    "\n",
    "        z = self.ln_out(z)                      # (B,1+L,D)\n",
    "\n",
    "        att = self.attn_pool(z)                 # (B,1+L,1)\n",
    "        att = torch.softmax(att, dim=1)\n",
    "        z = (z * att).sum(dim=1)                # (B,D)\n",
    "\n",
    "        return self.head(z)\n",
    "\n",
    "# =========================\n",
    "# ENTRENAMIENTO / EVALUACIÓN\n",
    "# =========================\n",
    "def build_weighted_sampler(y, groups):\n",
    "    y = np.asarray(y); groups = np.asarray(groups)\n",
    "    class_counts = np.bincount(y, minlength=len(np.unique(y))).astype(float)\n",
    "    class_w = 1.0 / class_counts[y]\n",
    "    subj_vals, subj_counts = np.unique(groups, return_counts=True)\n",
    "    subj_map = {s:c for s,c in zip(subj_vals, subj_counts)}\n",
    "    subj_w = np.array([1.0/subj_map[g] for g in groups], dtype=float)\n",
    "    w = class_w * subj_w\n",
    "    w = w / w.mean()\n",
    "    w_t = torch.from_numpy(w).float()\n",
    "    sampler = WeightedRandomSampler(weights=w_t, num_samples=len(w_t), replacement=True)\n",
    "    return sampler\n",
    "\n",
    "def make_class_weight_tensor(y_indices, n_classes, boost_bfists=1.20):\n",
    "    counts = np.bincount(y_indices, minlength=n_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    w = counts.sum() / counts\n",
    "    w = w / w.mean()\n",
    "    w[2] *= boost_bfists  # Both Fists\n",
    "    w = w / w.mean()\n",
    "    return torch.tensor(w, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "def train_epoch(model, loader, opt, criterion_soft, n_classes,\n",
    "                do_aug=True, fs=160.0, maxnorm=None):\n",
    "    model.train()\n",
    "    for xb, yb, _ in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "\n",
    "        if do_aug:\n",
    "            xb = do_time_jitter(xb, max_ms=50, fs=fs)\n",
    "            xb = do_gaussian_noise(xb, sigma=0.01)\n",
    "            xb = do_temporal_cutout_masked(xb, yb, classes_mask={2,3}, min_ms=30, max_ms=60, fs=fs)\n",
    "            xb, yt, _ = mixup_batch(xb, yb, n_classes=n_classes, alpha=0.2)\n",
    "            logits = model(xb)\n",
    "            loss = criterion_soft(logits, yt)\n",
    "        else:\n",
    "            logits = model(xb)\n",
    "            yt = torch.nn.functional.one_hot(yb, num_classes=n_classes).float()\n",
    "            loss = criterion_soft(logits, yt)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if maxnorm is not None:\n",
    "            apply_max_norm(model, max_value=maxnorm, p=2.0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _predict_tta(model, xb, n=5, fs=160.0):\n",
    "    outs = []\n",
    "    for _ in range(n):\n",
    "        xj = do_time_jitter(xb, max_ms=25, fs=fs)\n",
    "        outs.append(model(xj))\n",
    "    return torch.stack(outs, dim=0).mean(dim=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_with_preds(model, loader, use_tta=True, tta_n=5):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for xb, yb, _ in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = _predict_tta(model, xb, n=tta_n, fs=FS) if use_tta else model(xb)\n",
    "        pred = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "        y_pred.extend(pred); y_true.extend(yb.numpy().tolist())\n",
    "    y_true = np.asarray(y_true, dtype=int); y_pred = np.asarray(y_pred, dtype=int)\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    return y_true, y_pred, float(acc)\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes, title, fname):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.nan_to_num(cm_norm)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title); plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    ticks = np.arange(len(classes))\n",
    "    plt.xticks(ticks, classes, rotation=45, ha='right'); plt.yticks(ticks, classes)\n",
    "    fmt = '.2f'; thresh = cm_norm.max()/2.\n",
    "    for i,j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "        plt.text(j, i, format(cm_norm[i,j], fmt), ha=\"center\",\n",
    "                 color=\"white\" if cm_norm[i,j] > thresh else \"black\")\n",
    "    plt.ylabel('True'); plt.xlabel('Pred')\n",
    "    plt.tight_layout(); plt.savefig(fname, dpi=150, bbox_inches='tight'); plt.close()\n",
    "\n",
    "def plot_training_curves(history, fname):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(history['train_acc'], label='train_acc')\n",
    "    plt.plot(history['val_acc'], label='val_acc')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
    "    plt.title('Training curve'); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(fname, dpi=150); plt.close()\n",
    "\n",
    "def print_report(y_true, y_pred, classes):\n",
    "    print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
    "\n",
    "# =========================\n",
    "# FINE-TUNING PROGRESIVO por sujeto\n",
    "# =========================\n",
    "def _param_groups(model, mode):\n",
    "    if mode == 'out':\n",
    "        train = list(model.head.parameters())\n",
    "    elif mode == 'head':\n",
    "        train = list(model.head.parameters())\n",
    "    elif mode == 'spatial+head':\n",
    "        train = (list(model.temporal_stem.parameters()) +\n",
    "                 list(model.spatial_dw.parameters()) +\n",
    "                 list(model.spatial_bn.parameters()) +\n",
    "                 list(model.proj_pw.parameters()) +\n",
    "                 list(model.proj_bn.parameters()) +\n",
    "                 list(model.head.parameters()))\n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "    return train\n",
    "\n",
    "def _freeze_for_mode(model, mode):\n",
    "    for p in model.parameters(): p.requires_grad = False\n",
    "    if mode == 'out' or mode == 'head':\n",
    "        for p in model.head.parameters(): p.requires_grad = True\n",
    "    elif mode == 'spatial+head':\n",
    "        for mod in [model.temporal_stem, model.spatial_dw, model.spatial_bn,\n",
    "                    model.proj_pw, model.proj_bn, model.head]:\n",
    "            for p in mod.parameters(): p.requires_grad = True\n",
    "\n",
    "def _class_weights(y_np, n_classes):\n",
    "    counts = np.bincount(y_np, minlength=n_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    weights = counts.sum() / counts\n",
    "    weights = weights / weights.mean()\n",
    "    return torch.tensor(weights, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "def _train_one_mode(model, X_cal, y_cal, n_classes, mode,\n",
    "                    epochs=FT_EPOCHS, batch_size=32,\n",
    "                    head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                    l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=RANDOM_STATE)\n",
    "    (tr_idx, va_idx), = sss.split(X_cal, y_cal)\n",
    "    Xtr, ytr = X_cal[tr_idx], y_cal[tr_idx]\n",
    "    Xva, yva = X_cal[va_idx], y_cal[va_idx]\n",
    "\n",
    "    ds_tr = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xtr).float().unsqueeze(1),\n",
    "        torch.from_numpy(ytr).long()\n",
    "    )\n",
    "    ds_va = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(Xva).float().unsqueeze(1),\n",
    "        torch.from_numpy(yva).long()\n",
    "    )\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    _freeze_for_mode(model, mode)\n",
    "\n",
    "    if mode == 'spatial+head':\n",
    "        base_params = (list(model.temporal_stem.parameters()) +\n",
    "                       list(model.spatial_dw.parameters()) +\n",
    "                       list(model.spatial_bn.parameters()) +\n",
    "                       list(model.proj_pw.parameters()) +\n",
    "                       list(model.proj_bn.parameters()))\n",
    "        head_params = list(model.head.parameters())\n",
    "        train_params = base_params + head_params\n",
    "        opt = optim.AdamW([\n",
    "            {\"params\": base_params, \"lr\": base_lr, \"weight_decay\": WEIGHT_DECAY},\n",
    "            {\"params\": head_params, \"lr\": head_lr, \"weight_decay\": WEIGHT_DECAY},\n",
    "        ])\n",
    "    else:\n",
    "        train_params = _param_groups(model, mode)\n",
    "        opt = optim.AdamW(train_params, lr=head_lr, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    ref = [p.detach().clone().to(p.device) for p in train_params]\n",
    "    class_w = _class_weights(ytr, n_classes)\n",
    "    crit = nn.CrossEntropyLoss(weight=class_w)\n",
    "\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_val = float('inf'); bad = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            reg = 0.0\n",
    "            for p_cur, p_ref in zip(train_params, ref):\n",
    "                reg = reg + torch.sum((p_cur - p_ref)**2)\n",
    "            loss = loss + l2sp_lambda * reg\n",
    "            loss.backward(); opt.step()\n",
    "            apply_max_norm(model, max_value=2.0, p=2.0)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0; nval = 0\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                loss = crit(logits, yb)\n",
    "                val_loss += loss.item() * xb.size(0); nval += xb.size(0)\n",
    "            val_loss /= max(1, nval)\n",
    "\n",
    "        if val_loss + 1e-7 < best_val:\n",
    "            best_val = val_loss; bad = 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience: break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_numpy(model, X_np, device):\n",
    "    model.eval()\n",
    "    xb = torch.from_numpy(X_np).float().unsqueeze(1).to(device)\n",
    "    logits = model(xb)\n",
    "    return logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "def subject_cv_finetune_predict_progressive(model_global, Xs, ys, device,\n",
    "                                            n_splits=CALIB_CV_FOLDS, n_classes=4):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    y_true_full = np.empty_like(ys); y_pred_full = np.empty_like(ys)\n",
    "\n",
    "    for tr_idx, te_idx in skf.split(Xs, ys):\n",
    "        Xcal, ycal = Xs[tr_idx], ys[tr_idx]\n",
    "        Xho,  yho  = Xs[te_idx], ys[te_idx]\n",
    "\n",
    "        m_out = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_out, Xcal, ycal, n_classes, mode='out',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_out = predict_numpy(m_out, Xho, device)\n",
    "\n",
    "        m_head = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_head, Xcal, ycal, n_classes, mode='head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, l2sp_lambda=FT_L2SP,\n",
    "                        patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_head = predict_numpy(m_head, Xho, device)\n",
    "\n",
    "        m_sp = copy.deepcopy(model_global)\n",
    "        _train_one_mode(m_sp, Xcal, ycal, n_classes, mode='spatial+head',\n",
    "                        epochs=FT_EPOCHS, head_lr=FT_HEAD_LR, base_lr=FT_BASE_LR,\n",
    "                        l2sp_lambda=FT_L2SP, patience=FT_PATIENCE, val_ratio=FT_VAL_RATIO)\n",
    "        yhat_sp = predict_numpy(m_sp, Xho, device)\n",
    "\n",
    "        scores = [ (yhat_out==yho).mean(), (yhat_head==yho).mean(), (yhat_sp==yho).mean() ]\n",
    "        best_idx = int(np.argmax(scores))\n",
    "        yhat_best = [yhat_out, yhat_head, yhat_sp][best_idx]\n",
    "\n",
    "        y_true_full[te_idx] = yho; y_pred_full[te_idx] = yhat_best\n",
    "\n",
    "    return y_true_full, y_pred_full\n",
    "\n",
    "# =========================\n",
    "# FOLDS JSON helpers\n",
    "# =========================\n",
    "def save_group_folds_json_with_indices(subject_ids_str, groups_array, n_splits, out_json_path,\n",
    "                                       created_by=\"CNNTransMIv2\", description=None):\n",
    "    out_json_path = Path(out_json_path)\n",
    "    unique_subjects_int = sorted(np.unique(groups_array).tolist())\n",
    "    subject_ids = [f\"S{sid:03d}\" for sid in unique_subjects_int]\n",
    "\n",
    "    if len(subject_ids) < n_splits:\n",
    "        raise ValueError(f\"n_splits={n_splits} mayor que número de sujetos={len(subject_ids)}\")\n",
    "\n",
    "    groups = np.arange(len(subject_ids))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    folds = []\n",
    "    fold_i = 0\n",
    "    for train_idx_grp, test_idx_grp in gkf.split(groups, groups, groups):\n",
    "        fold_i += 1\n",
    "        train_sids = [subject_ids[int(i)] for i in train_idx_grp]\n",
    "        test_sids  = [subject_ids[int(i)] for i in test_idx_grp]\n",
    "\n",
    "        train_sids_int = [int(s[1:]) for s in train_sids]\n",
    "        test_sids_int  = [int(s[1:]) for s in test_sids]\n",
    "\n",
    "        tr_idx = np.where(np.isin(groups_array, train_sids_int))[0].tolist()\n",
    "        te_idx = np.where(np.isin(groups_array, test_sids_int))[0].tolist()\n",
    "\n",
    "        folds.append({\n",
    "            \"fold\": int(fold_i),\n",
    "            \"train\": train_sids,\n",
    "            \"test\": test_sids,\n",
    "            \"tr_idx\": tr_idx,\n",
    "            \"te_idx\": te_idx\n",
    "        })\n",
    "\n",
    "    payload = {\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"created_by\": created_by,\n",
    "        \"description\": description if description is not None else \"\",\n",
    "        \"n_splits\": int(n_splits),\n",
    "        \"n_subjects\": len(subject_ids),\n",
    "        \"subject_ids\": subject_ids,\n",
    "        \"folds\": folds\n",
    "    }\n",
    "\n",
    "    out_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Folds JSON con índices guardado → {out_json_path}\")\n",
    "    return out_json_path\n",
    "\n",
    "def load_group_folds_json(path_json, expected_subject_ids=None, strict_check=True):\n",
    "    path_json = Path(path_json)\n",
    "    if not path_json.exists():\n",
    "        raise FileNotFoundError(f\"No existe {path_json}\")\n",
    "    with open(path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        payload = json.load(f)\n",
    "\n",
    "    subj_json = payload.get(\"subject_ids\", [])\n",
    "    if expected_subject_ids is not None:\n",
    "        expected = sorted(list(expected_subject_ids))\n",
    "        if subj_json != expected:\n",
    "            msg = (\"Los subject_ids del JSON no coinciden con expected_subject_ids.\\n\"\n",
    "                   f\"JSON has {len(subj_json)} subjects, expected {len(expected)}.\\n\"\n",
    "                   f\"First 10 JSON: {subj_json[:10]}\\nFirst 10 expected: {expected[:10]}\")\n",
    "            if strict_check:\n",
    "                raise ValueError(msg)\n",
    "            else:\n",
    "                print(\"WARNING: \" + msg)\n",
    "    return payload\n",
    "\n",
    "# =========================\n",
    "# EXPERIMENTO\n",
    "# =========================\n",
    "def run_experiment(save_folds_json=True, folds_json_path=FOLDS_DIR, folds_json_description=\"GroupKFold folds for CNNTransMIv2\"):\n",
    "    mne.set_log_level('WARNING')\n",
    "\n",
    "    subs = subjects_available()\n",
    "    print(f\"Sujetos elegibles: {len(subs)} → {subs[:10]}{'...' if len(subs)>10 else ''}\")\n",
    "\n",
    "    X, y, groups, chs = build_dataset_all(subs, scenario=CLASS_SCENARIO, window_mode=WINDOW_MODE)\n",
    "    N, T, C = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f\"Listo para entrenar: N={N} | T={T} | C={C} | clases={n_classes} | sujetos={len(np.unique(groups))}\")\n",
    "\n",
    "    ds = EEGTrials(X, y, groups)\n",
    "\n",
    "    # preparar JSON folds\n",
    "    if folds_json_path is None:\n",
    "        folds_json_path = Path(\"folds\") / f\"group_folds_{N_FOLDS}splits.json\"\n",
    "    else:\n",
    "        folds_json_path = Path(folds_json_path)\n",
    "    folds_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    unique_subs = sorted(np.unique(groups).tolist())\n",
    "    subject_ids_str = [f\"S{s:03d}\" for s in unique_subs]\n",
    "\n",
    "    if not folds_json_path.exists():\n",
    "        if not save_folds_json:\n",
    "            raise FileNotFoundError(f\"Folds JSON no encontrado en {folds_json_path} y save_folds_json=False.\")\n",
    "        save_group_folds_json_with_indices(subject_ids_str, groups, n_splits=N_FOLDS,\n",
    "                                           out_json_path=folds_json_path,\n",
    "                                           created_by=\"CNNTransMIv2\",\n",
    "                                           description=folds_json_description)\n",
    "\n",
    "    payload = load_group_folds_json(folds_json_path, expected_subject_ids=subject_ids_str, strict_check=False)\n",
    "    folds = payload[\"folds\"]\n",
    "\n",
    "    # bucle por folds\n",
    "    global_folds = []\n",
    "    ft_prog_folds = []\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "\n",
    "    for f in folds:\n",
    "        fold = f[\"fold\"]\n",
    "        tr_idx = np.asarray(f.get(\"tr_idx\", []), dtype=int)\n",
    "        te_idx = np.asarray(f.get(\"te_idx\", []), dtype=int)\n",
    "\n",
    "        if tr_idx.size == 0 or te_idx.size == 0:\n",
    "            print(f\"Advertencia: fold {fold} sin índices tr/te válidos. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        # ===== Split de validación por sujetos =====\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=GLOBAL_VAL_SPLIT, random_state=RANDOM_STATE)\n",
    "        tr_subj_idx, va_subj_idx = next(gss.split(tr_idx, groups[tr_idx], groups[tr_idx]))\n",
    "        tr_sub_idx = tr_idx[tr_subj_idx]\n",
    "        va_idx     = tr_idx[va_subj_idx]\n",
    "\n",
    "        # Sampler balanceado para train\n",
    "        sampler = build_weighted_sampler(y[tr_sub_idx], groups[tr_sub_idx])\n",
    "\n",
    "        tr_loader = DataLoader(Subset(ds, tr_sub_idx), batch_size=BATCH_SIZE, sampler=sampler, drop_last=False)\n",
    "        va_loader = DataLoader(Subset(ds, va_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "        te_loader = DataLoader(Subset(ds, te_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "        # ===== CNN + Transformer v2 =====\n",
    "        model = CNNTransMIv2(\n",
    "            n_ch=C, n_classes=n_classes,\n",
    "            d_model=48, n_heads=2, n_blocks=2,\n",
    "            patch_len=12, dropout=0.4, sd_prob=0.30, dw_kernel=15\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # Opt y scheduler SGDR (AdamW + WD)\n",
    "        opt = optim.AdamW(model.parameters(), lr=LR_INIT, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=SGDR_T0, T_mult=SGDR_Tmult)\n",
    "\n",
    "        # Métrica rápida\n",
    "        def _acc(loader):\n",
    "            return evaluate_with_preds(model, loader, use_tta=True, tta_n=5)[2]\n",
    "\n",
    "        # Historia para curvas\n",
    "        history = {'train_acc': [], 'val_acc': []}\n",
    "\n",
    "        # Criterio suave con ponderación por clase (+20% BFISTS) y LS=0.10\n",
    "        class_weights = make_class_weight_tensor(y[tr_sub_idx], n_classes, boost_bfists=1.20)\n",
    "        criterion_soft = WeightedSoftCrossEntropy(class_weights, label_smoothing=0.10)\n",
    "\n",
    "        # ===== Entrenamiento global =====\n",
    "        print(f\"\\n[Fold {fold}/{N_FOLDS}] Entrenando modelo global...\"\n",
    "              f\" (n_train={len(tr_sub_idx)} | n_val={len(va_idx)} | n_test={len(te_idx)})\")\n",
    "\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        best_val = -1.0\n",
    "        bad = 0\n",
    "\n",
    "        for epoch in range(1, EPOCHS_GLOBAL + 1):\n",
    "            train_epoch(model, tr_loader, opt, criterion_soft, n_classes=n_classes,\n",
    "                        do_aug=True, fs=FS, maxnorm=2.0)\n",
    "            scheduler.step(epoch-1 + 1e-8)\n",
    "\n",
    "            tr_acc = _acc(tr_loader)\n",
    "            va_acc = _acc(va_loader)\n",
    "            history['train_acc'].append(tr_acc)\n",
    "            history['val_acc'].append(va_acc)\n",
    "\n",
    "            if (epoch % LOG_EVERY == 0) or epoch in (1, 10, 20, 50, 100):\n",
    "                cur_lr = opt.param_groups[0]['lr']\n",
    "                print(f\"  Época {epoch:3d} | train_acc={tr_acc:.4f} | val_acc={va_acc:.4f} | LR={cur_lr:.5f}\")\n",
    "\n",
    "            if va_acc > best_val + 1e-4:\n",
    "                best_val = va_acc; best_state = copy.deepcopy(model.state_dict()); bad = 0\n",
    "            else:\n",
    "                bad += 1\n",
    "                if bad >= GLOBAL_PATIENCE:\n",
    "                    print(f\"  Early stopping en época {epoch} (mejor val_acc={best_val:.4f})\")\n",
    "                    break\n",
    "\n",
    "        curve_path = f\"training_curve_fold{fold}.png\"\n",
    "        plot_training_curves(history, curve_path)\n",
    "        print(f\"↳ Curva de entrenamiento guardada: {curve_path}\")\n",
    "\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "        # ===== Evaluación global (inter-sujeto puro) =====\n",
    "        y_true, y_pred, acc_global = evaluate_with_preds(model, te_loader, use_tta=True, tta_n=5)\n",
    "        global_folds.append(acc_global)\n",
    "        all_true.append(y_true); all_pred.append(y_pred)\n",
    "\n",
    "        print(f\"[Fold {fold}/{N_FOLDS}] Global acc={acc_global:.4f}\")\n",
    "        print_report(y_true, y_pred, CLASS_NAMES_4C)\n",
    "\n",
    "        # ---------- Fine-tuning PROGRESIVO por sujeto con 4-fold CV ----------\n",
    "        X_te, y_te, g_te = X[te_idx], y[te_idx], groups[te_idx]\n",
    "\n",
    "        y_true_ft_all, y_pred_ft_all = [], []\n",
    "        used_subjects = 0\n",
    "        for sid in np.unique(g_te):\n",
    "            idx = np.where(g_te == sid)[0]\n",
    "            Xs, ys = X_te[idx], y_te[idx]\n",
    "\n",
    "            if len(ys) < CALIB_CV_FOLDS or len(np.unique(ys)) < 2:\n",
    "                continue\n",
    "\n",
    "            y_true_subj, y_pred_subj = subject_cv_finetune_predict_progressive(\n",
    "                model, Xs, ys, DEVICE, n_splits=CALIB_CV_FOLDS, n_classes=n_classes\n",
    "            )\n",
    "            y_true_ft_all.append(y_true_subj)\n",
    "            y_pred_ft_all.append(y_pred_subj)\n",
    "            used_subjects += 1\n",
    "\n",
    "        if len(y_true_ft_all) > 0:\n",
    "            y_true_ft_all = np.concatenate(y_true_ft_all)\n",
    "            y_pred_ft_all = np.concatenate(y_pred_ft_all)\n",
    "            acc_ft = (y_true_ft_all == y_pred_ft_all).mean()\n",
    "            print(f\"  Fine-tuning PROGRESIVO (por sujeto, {CALIB_CV_FOLDS}-fold CV) acc={acc_ft:.4f} | sujetos={used_subjects}\")\n",
    "            print(f\"  Δ(FT-Global) = {acc_ft - acc_global:+.4f}\")\n",
    "        else:\n",
    "            acc_ft = np.nan\n",
    "            print(\"  Fine-tuning PROGRESIVO no ejecutado (sujeto(s) con muestras insuficientes).\")\n",
    "\n",
    "        ft_prog_folds.append(acc_ft)\n",
    "\n",
    "    # ---------- resultados finales ----------\n",
    "    if len(all_true) > 0:\n",
    "        all_true = np.concatenate(all_true)\n",
    "        all_pred = np.concatenate(all_pred)\n",
    "    else:\n",
    "        all_true = np.array([], dtype=int)\n",
    "        all_pred = np.array([], dtype=int)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTADOS FINALES\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Global folds:\", [f\"{a:.4f}\" for a in global_folds])\n",
    "    if len(global_folds) > 0:\n",
    "        print(f\"Global mean: {np.mean(global_folds):.4f}\")\n",
    "\n",
    "    print(\"Fine-tune PROGRESIVO folds:\", [(\"nan\" if (a is None or np.isnan(a)) else f\"{a:.4f}\") for a in ft_prog_folds])\n",
    "    if len(ft_prog_folds) > 0:\n",
    "        print(f\"Fine-tune PROGRESIVO mean: {np.nanmean(ft_prog_folds):.4f}\")\n",
    "        print(f\"Δ(FT-Global) mean: {np.nanmean(ft_prog_folds) - np.mean(global_folds):+.4f}\")\n",
    "\n",
    "    if all_true.size > 0:\n",
    "        plot_confusion(all_true, all_pred, CLASS_NAMES_4C,\n",
    "                       title=\"Confusion Matrix - CNNTransMIv2 (Global, all folds)\",\n",
    "                       fname=\"confusion_cnntransmi_v2_global_allfolds.png\")\n",
    "        print(\"\\n↳ Matriz de confusión guardada: confusion_cnntransmi_v2_global_allfolds.png\")\n",
    "\n",
    "    return {\n",
    "        \"global_folds\": global_folds,\n",
    "        \"ft_prog_folds\": ft_prog_folds,\n",
    "        \"all_true\": all_true,\n",
    "        \"all_pred\": all_pred,\n",
    "        \"folds_json_path\": str(folds_json_path)\n",
    "    }\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🧠 INICIANDO EXPERIMENTO CNN+Transformer v2 (6s, 8 canales)\")\n",
    "    print(f\"🔧 Configuración: {CLASS_SCENARIO}, {len(EXPECTED_8)} canales, {WINDOW_MODE}\")\n",
    "    print(f\"⚙️  FT: epochs={FT_EPOCHS}, base_lr={FT_BASE_LR}, head_lr={FT_HEAD_LR}, L2SP={FT_L2SP}, patience={FT_PATIENCE}, CV={CALIB_CV_FOLDS}\")\n",
    "    run_experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a5b971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Usando dispositivo: cuda\n",
      "🎯 Sliding windows → TRAIN=[(0.0, 3.0), (0.5, 3.5), (1.0, 4.0)] | EVAL=[(0.0, 3.0), (0.5, 3.5), (1.0, 4.0)]\n",
      "🧠 INICIANDO EEGNet + Sliding Windows + Event-level Eval\n",
      "🔧 WINDOW_SET=3s_base | POLICY=standard\n",
      "⚙️  GLOBAL: epochs=100, lr=0.01, batch=64, SGDR(T0=6,Tmult=2)\n",
      "🧪 FT por sujeto: ON (epochs=25, base_lr=5e-05, head_lr=0.001, L2SP=0.0001)\n",
      "Sujetos elegibles: 103 → [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Escaneando eventos (sin crops):   1%|          | 1/103 [00:00<00:14,  7.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Escaneando eventos (sin crops): 100%|██████████| 103/103 [00:37<00:00,  2.72it/s]\n",
      "Generando segmentos (con crops + event_ids): 100%|██████████| 103/103 [00:43<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUG WINDOWS SUMMARY ===\n",
      "Eventos brutos: 8652\n",
      "Crops OK:      25956\n",
      "Skips OOB:     0\n",
      "Skips LEN:     0\n",
      "Skips NaN:     0\n",
      "Dataset construido: N=25956 | T=480 | C=8 | sujetos únicos=103 | eventos únicos=8652\n",
      "Listo para entrenar: N=25956 | T=480 | C=8 | clases=4 | sujetos=103\n",
      "\n",
      "[Fold 1/5] Entrenando (train=5712 | val=1176 | test=1764)\n",
      "  Época   1 | val_acc(ev)=0.3571 | LR=0.01000\n",
      "  Época   5 | val_acc(ev)=0.3929 | LR=0.00250\n",
      "  Época  10 | val_acc(ev)=0.3801 | LR=0.00854\n",
      "  Época  15 | val_acc(ev)=0.3878 | LR=0.00250\n",
      "  Época  20 | val_acc(ev)=0.3903 | LR=0.00996\n",
      "  Early stopping @ 23 (mejor val_acc(ev)=0.4082)\n",
      "[Fold 1/5] Global acc (por evento) = 0.3214\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.4898    0.1633    0.2449       147\n",
      "       Right     0.5135    0.1293    0.2065       147\n",
      "  Both Fists     0.3022    0.7483    0.4305       147\n",
      "   Both Feet     0.2609    0.2449    0.2526       147\n",
      "\n",
      "    accuracy                         0.3214       588\n",
      "   macro avg     0.3916    0.3214    0.2836       588\n",
      "weighted avg     0.3916    0.3214    0.2836       588\n",
      "\n",
      "  Fine-tuning (por sujeto, agrupado ev) acc=0.6352 | Δ=+0.3138 | sujetos=21\n",
      "\n",
      "[Fold 2/5] Entrenando (train=5544 | val=1344 | test=1764)\n",
      "  Época   1 | val_acc(ev)=0.3527 | LR=0.01000\n",
      "  Época   5 | val_acc(ev)=0.3259 | LR=0.00250\n",
      "  Época  10 | val_acc(ev)=0.3259 | LR=0.00854\n",
      "  Época  15 | val_acc(ev)=0.3862 | LR=0.00250\n",
      "  Época  20 | val_acc(ev)=0.3371 | LR=0.00996\n",
      "  Época  25 | val_acc(ev)=0.3147 | LR=0.00854\n",
      "  Early stopping @ 25 (mejor val_acc(ev)=0.3862)\n",
      "[Fold 2/5] Global acc (por evento) = 0.3520\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.4128    0.3061    0.3516       147\n",
      "       Right     0.4088    0.3810    0.3944       147\n",
      "  Both Fists     0.3160    0.4966    0.3862       147\n",
      "   Both Feet     0.2973    0.2245    0.2558       147\n",
      "\n",
      "    accuracy                         0.3520       588\n",
      "   macro avg     0.3587    0.3520    0.3470       588\n",
      "weighted avg     0.3587    0.3520    0.3470       588\n",
      "\n",
      "  Fine-tuning (por sujeto, agrupado ev) acc=0.6019 | Δ=+0.2499 | sujetos=21\n",
      "\n",
      "[Fold 3/5] Entrenando (train=5712 | val=1176 | test=1764)\n",
      "  Época   1 | val_acc(ev)=0.3189 | LR=0.01000\n",
      "  Época   5 | val_acc(ev)=0.3673 | LR=0.00250\n",
      "  Época  10 | val_acc(ev)=0.3648 | LR=0.00854\n",
      "  Época  15 | val_acc(ev)=0.4235 | LR=0.00250\n",
      "  Época  20 | val_acc(ev)=0.3571 | LR=0.00996\n",
      "  Época  25 | val_acc(ev)=0.4005 | LR=0.00854\n",
      "  Early stopping @ 25 (mejor val_acc(ev)=0.4235)\n",
      "[Fold 3/5] Global acc (por evento) = 0.2619\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.1992    0.3265    0.2474       147\n",
      "       Right     0.4310    0.3401    0.3802       147\n",
      "  Both Fists     0.1863    0.2041    0.1948       147\n",
      "   Both Feet     0.3714    0.1769    0.2396       147\n",
      "\n",
      "    accuracy                         0.2619       588\n",
      "   macro avg     0.2970    0.2619    0.2655       588\n",
      "weighted avg     0.2970    0.2619    0.2655       588\n",
      "\n",
      "  Fine-tuning (por sujeto, agrupado ev) acc=0.6803 | Δ=+0.4184 | sujetos=21\n",
      "\n",
      "[Fold 4/5] Entrenando (train=5880 | val=1092 | test=1680)\n",
      "  Época   1 | val_acc(ev)=0.3379 | LR=0.01000\n",
      "  Época   5 | val_acc(ev)=0.3681 | LR=0.00250\n",
      "  Época  10 | val_acc(ev)=0.3049 | LR=0.00854\n",
      "  Época  15 | val_acc(ev)=0.3077 | LR=0.00250\n",
      "  Early stopping @ 16 (mejor val_acc(ev)=0.3791)\n",
      "[Fold 4/5] Global acc (por evento) = 0.3643\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.4685    0.3537    0.4031       147\n",
      "       Right     0.4352    0.3197    0.3686       147\n",
      "  Both Fists     0.3092    0.5500    0.3959       140\n",
      "   Both Feet     0.3043    0.2222    0.2569       126\n",
      "\n",
      "    accuracy                         0.3643       560\n",
      "   macro avg     0.3793    0.3614    0.3561       560\n",
      "weighted avg     0.3830    0.3643    0.3593       560\n",
      "\n",
      "  Fine-tuning (por sujeto, agrupado ev) acc=0.6069 | Δ=+0.2426 | sujetos=20\n",
      "\n",
      "[Fold 5/5] Entrenando (train=5796 | val=1176 | test=1680)\n",
      "  Época   1 | val_acc(ev)=0.3316 | LR=0.01000\n",
      "  Época   5 | val_acc(ev)=0.3571 | LR=0.00250\n",
      "  Época  10 | val_acc(ev)=0.3316 | LR=0.00854\n",
      "  Época  15 | val_acc(ev)=0.3112 | LR=0.00250\n",
      "  Early stopping @ 18 (mejor val_acc(ev)=0.3852)\n",
      "[Fold 5/5] Global acc (por evento) = 0.3554\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.4397    0.3469    0.3878       147\n",
      "       Right     0.4638    0.2406    0.3168       133\n",
      "  Both Fists     0.2703    0.5263    0.3571       133\n",
      "   Both Feet     0.3966    0.3129    0.3498       147\n",
      "\n",
      "    accuracy                         0.3554       560\n",
      "   macro avg     0.3926    0.3567    0.3529       560\n",
      "weighted avg     0.3938    0.3554    0.3537       560\n",
      "\n",
      "  Fine-tuning (por sujeto, agrupado ev) acc=0.6214 | Δ=+0.2661 | sujetos=20\n",
      "\n",
      "============================================================\n",
      "RESULTADOS FINALES (por EVENTO)\n",
      "============================================================\n",
      "Global folds: ['0.3214', '0.3520', '0.2619', '0.3643', '0.3554']\n",
      "Global mean: 0.3310\n",
      "Fine-tune folds: ['0.6352', '0.6019', '0.6803', '0.6069', '0.6214']\n",
      "Fine-tune mean: 0.6292\n",
      "Δ(FT-Global) mean: +0.2982\n",
      "↳ Matriz de confusión guardada: confusion_eegnet_sliding_event_eval_allfolds.png\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# EEGNet + Sliding Windows por evento + Eval agrupada por EVENTO (promedio de logits entre crops)\n",
    "# - Excluye sujetos: {38,88,89,92,100,104}\n",
    "# - Solo 8 canales: ['C3','C4','Cz','CP3','CP4','FC3','FC4','FCz']\n",
    "# - Sliding policies configurables\n",
    "# - SGDR + augments\n",
    "# - FT por sujeto (opcional)\n",
    "\n",
    "import os, re, math, random, json, itertools, copy\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit, StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# CONFIGURACIÓN GENERAL\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "FOLDS_DIR = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE); np.random.seed(RANDOM_STATE); random.seed(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "FS = 160.0\n",
    "N_FOLDS = 5\n",
    "N_CLASSES = 4\n",
    "CLASS_NAMES_4C = ['Left', 'Right', 'Both Fists', 'Both Feet']\n",
    "\n",
    "# ===== ENTRENAMIENTO GLOBAL =====\n",
    "BATCH_SIZE    = 64\n",
    "EPOCHS_GLOBAL = 100\n",
    "LR_INIT       = 1e-2\n",
    "SGDR_T0       = 6\n",
    "SGDR_Tmult    = 2\n",
    "GLOBAL_VAL_SPLIT = 0.15\n",
    "GLOBAL_PATIENCE  = 10\n",
    "LOG_EVERY        = 5\n",
    "\n",
    "# ===== FINE-TUNING POR SUJETO (opcional) =====\n",
    "DO_SUBJECT_FT = True\n",
    "CALIB_CV_FOLDS = 4\n",
    "FT_EPOCHS   = 25\n",
    "FT_BASE_LR  = 5e-5\n",
    "FT_HEAD_LR  = 1e-3\n",
    "FT_L2SP     = 1e-4\n",
    "FT_PATIENCE = 5\n",
    "FT_VAL_RATIO= 0.2\n",
    "\n",
    "# ===== Normalización por epoch (z-score por canal) =====\n",
    "NORM_EPOCH_ZSCORE = True\n",
    "\n",
    "# ===== Sujetos / Runs / Canales =====\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "MI_RUNS_LR = [4, 8, 12]\n",
    "MI_RUNS_OF = [6, 10, 14]\n",
    "BASELINE_RUNS_EO = [1]\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','FCz']\n",
    "\n",
    "# ===== Sliding windows =====\n",
    "DEBUG_WINDOWS = True  # logs de debug (resumen)\n",
    "# Elige set base: \"3s_base\" (0–3, 0.5–3.5, 1–4) o \"6s_base\" (-1–5, -0.5–5.5, 0–6)\n",
    "WINDOW_SET = \"3s_base\"\n",
    "# Política de train (agrega más ventanas para robustecer)\n",
    "# - \"standard\": usa mismo set en train y eval\n",
    "# - \"train_precue\": añade ventana con precue solo en train\n",
    "# - \"train_aggressive\": stride 0.25s solo en train (eval se queda con 3)\n",
    "WINDOW_POLICY = \"standard\"\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES CANALES / EDF\n",
    "# =========================\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = re.sub(r'([A-Za-z])0([0-9])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])Z$', r'\\1z', s)\n",
    "    s = s.replace('fp', 'Fp').replace('FP', 'Fp')\n",
    "    s = ''.join(ch.upper() if ch != 'z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {}\n",
    "    for ch in raw.ch_names:\n",
    "        lab = normalize_label(ch)\n",
    "        lab = lab[:-1] + 'z' if lab.endswith('Z') else lab\n",
    "        lab = re.sub(r'([A-Z])Z$', r'\\1z', lab)\n",
    "        mapping[ch] = lab\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired_channels=EXPECTED_8):\n",
    "    missing = [ch for ch in desired_channels if ch not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"[WARN] faltan canales {missing} en archivo {getattr(raw,'filenames', [''])[0]}\")\n",
    "        return None\n",
    "    raw.reorder_channels([ch for ch in raw.ch_names if ch in desired_channels] +\n",
    "                         [ch for ch in raw.ch_names if ch not in desired_channels])\n",
    "    raw.pick_channels(desired_channels, ordered=True)\n",
    "    return raw\n",
    "\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(run_id:int):\n",
    "    if run_id in MI_RUNS_LR: return 'LR'\n",
    "    if run_id in MI_RUNS_OF: return 'OF'\n",
    "    if run_id in BASELINE_RUNS_EO: return 'EO'\n",
    "    return None\n",
    "\n",
    "# --- Notch adaptativo opcional (si hay CSV de SNR) ---\n",
    "_SNR_TABLE = None\n",
    "def _load_snr_table():\n",
    "    global _SNR_TABLE\n",
    "    if _SNR_TABLE is not None:\n",
    "        return _SNR_TABLE\n",
    "    csv_path = PROJ / 'reports' / 'psd_mains' / 'psd_mains_summary.csv'\n",
    "    if csv_path.exists():\n",
    "        try:\n",
    "            _SNR_TABLE = pd.read_csv(csv_path)\n",
    "        except:\n",
    "            _SNR_TABLE = None\n",
    "    return _SNR_TABLE\n",
    "\n",
    "def _decide_notch(subject, run, th_db=10.0):\n",
    "    df = _load_snr_table()\n",
    "    if df is None:\n",
    "        return 60.0\n",
    "    row = df[(df['subject']==subject) & (df['run']==run)]\n",
    "    if row.empty:\n",
    "        return 60.0\n",
    "    snr50 = float(row['snr50_db'].iloc[0]); snr60 = float(row['snr60_db'].iloc[0])\n",
    "    if snr60 >= th_db and snr60 >= snr50: return 60.0\n",
    "    if snr50 >= th_db and snr50 >  snr60: return 50.0\n",
    "    return None\n",
    "\n",
    "def read_raw_edf(path: Path):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        mont = mne.channels.make_standard_montage('standard_1020')\n",
    "        raw.set_montage(mont, on_missing='ignore')\n",
    "    except Exception:\n",
    "        pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad=\"auto\")\n",
    "    raw = ensure_channels_order(raw, EXPECTED_8)\n",
    "    if raw is None:\n",
    "        return None\n",
    "    sid, rid = parse_subject_run(path)\n",
    "    notch = _decide_notch(sid, rid)\n",
    "    if notch is not None:\n",
    "        raw.notch_filter(freqs=[float(notch)], picks='eeg', method='spectrum_fit', phase='zero')\n",
    "    return raw\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    \"\"\"Extrae T1/T2 y dedup mínimo (>=0.5s).\"\"\"\n",
    "    if raw.annotations is None or len(raw.annotations) == 0:\n",
    "        return []\n",
    "    def _norm(s): return str(s).strip().upper().replace(' ', '')\n",
    "    res = []\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = _norm(desc)\n",
    "        if tag in ('T1','T2'):\n",
    "            res.append((float(onset), tag))\n",
    "    res.sort()\n",
    "    dedup = []\n",
    "    last_t1 = last_t2 = -1e9\n",
    "    for t, tag in res:\n",
    "        if tag == 'T1':\n",
    "            if (t - last_t1) >= 0.5: dedup.append((t, tag)); last_t1 = t\n",
    "        else:\n",
    "            if (t - last_t2) >= 0.5: dedup.append((t, tag)); last_t2 = t\n",
    "    return dedup\n",
    "\n",
    "# =========================\n",
    "# DISCOVERY DE SUJETOS\n",
    "# =========================\n",
    "def subjects_available():\n",
    "    subs = []\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try: sid = int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir / f\"S{sid:03d}R{r:02d}.edf\").exists() for r in (MI_RUNS_LR + MI_RUNS_OF))\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "# =========================\n",
    "# BALANCEO por EVENTOS (ANTES de crops)\n",
    "# =========================\n",
    "def _label_from_tag(kind: str, tag: str):\n",
    "    if kind == 'LR':\n",
    "        return 0 if tag == 'T1' else 1 if tag == 'T2' else None\n",
    "    elif kind == 'OF':\n",
    "        return 2 if tag == 'T1' else 3 if tag == 'T2' else None\n",
    "    return None\n",
    "\n",
    "def gather_all_events(subjects):\n",
    "    per_subj = {}\n",
    "    for s in tqdm(subjects, desc=\"Escaneando eventos (sin crops)\"):\n",
    "        sdir = DATA_RAW / f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "        per_subj[s] = {0:[], 1:[], 2:[], 3:[]}\n",
    "        for r in (MI_RUNS_LR + MI_RUNS_OF + BASELINE_RUNS_EO):\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            kind = run_kind(r)\n",
    "            if kind not in ('LR','OF'):\n",
    "                continue\n",
    "            raw = read_raw_edf(p)\n",
    "            if raw is None: \n",
    "                continue\n",
    "            events = collect_events_T1T2(raw)\n",
    "            for onset_sec, tag in events:\n",
    "                lab = _label_from_tag(kind, tag)\n",
    "                if lab is None: \n",
    "                    continue\n",
    "                per_subj[s][lab].append({\n",
    "                    \"path\": p,\n",
    "                    \"onset\": float(onset_sec),\n",
    "                    \"label\": int(lab),\n",
    "                    \"subject\": int(s),\n",
    "                    \"run\": int(r)\n",
    "                })\n",
    "    return per_subj\n",
    "\n",
    "def balance_events_per_subject(per_subj, need_per_class=21, rng_seed=RANDOM_STATE):\n",
    "    rng = check_random_state(rng_seed)\n",
    "    kept = {}\n",
    "    for s, cls_dict in per_subj.items():\n",
    "        kept[s] = {}\n",
    "        skip_subject = False\n",
    "        for c in range(4):\n",
    "            events = cls_dict.get(c, [])\n",
    "            n0 = len(events)\n",
    "            if n0 == 0:\n",
    "                skip_subject = True\n",
    "                break\n",
    "            if n0 == need_per_class:\n",
    "                sel = events\n",
    "            elif n0 > need_per_class:\n",
    "                idx = rng.choice(n0, size=need_per_class, replace=False)\n",
    "                sel = [events[i] for i in idx]\n",
    "            else:\n",
    "                idx = rng.choice(n0, size=need_per_class, replace=True)\n",
    "                sel = [events[i] for i in idx]\n",
    "            kept[s][c] = sel\n",
    "        if skip_subject:\n",
    "            kept[s] = None\n",
    "            print(f\"[SKIP] S{s:03d} (alguna clase sin eventos)\")\n",
    "    kept = {s:cls for s,cls in kept.items() if cls is not None}\n",
    "    return kept\n",
    "\n",
    "# =========================\n",
    "# SLIDING WINDOWS (train & eval)\n",
    "# =========================\n",
    "def base_windows(set_name=\"3s_base\"):\n",
    "    if set_name == \"3s_base\":\n",
    "        # 3 s cada una dentro de [0,4] (típico PhysioNet 2009)\n",
    "        return [(0.0, 3.0), (0.5, 3.5), (1.0, 4.0)]\n",
    "    elif set_name == \"6s_base\":\n",
    "        # 6 s alrededor del evento (usa con cuidado por OOB según run)\n",
    "        return [(-1.0, 5.0), (-0.5, 5.5), (0.0, 6.0)]\n",
    "    else:\n",
    "        raise ValueError(set_name)\n",
    "\n",
    "def get_crops_for_policy(window_set=\"3s_base\", policy=\"standard\"):\n",
    "    crops_eval = base_windows(window_set)\n",
    "    if policy == \"standard\":\n",
    "        crops_train = crops_eval\n",
    "    elif policy == \"train_precue\":\n",
    "        if window_set == \"3s_base\":\n",
    "            # añade -0.5 a 2.5 solo en train\n",
    "            crops_train = [(-0.5, 2.5)] + crops_eval\n",
    "        else:\n",
    "            crops_train = [(-1.5, 4.5)] + crops_eval\n",
    "    elif policy == \"train_aggressive\":\n",
    "        if window_set == \"3s_base\":\n",
    "            starts = [0.0, 0.25, 0.5, 0.75, 1.0]  # 5 ventanas (3 s con stride 0.25 s)\n",
    "            crops_train = [(s, s+3.0) for s in starts]\n",
    "        else:\n",
    "            # 6 s con stride 0.5 s (3 ventanas ya cubren, mantenemos igual para no inflar mucho)\n",
    "            crops_train = crops_eval\n",
    "    else:\n",
    "        raise ValueError(policy)\n",
    "    return crops_train, crops_eval\n",
    "\n",
    "CROPS_TRAIN, CROPS_EVAL = get_crops_for_policy(WINDOW_SET, WINDOW_POLICY)\n",
    "print(f\"🎯 Sliding windows → TRAIN={CROPS_TRAIN} | EVAL={CROPS_EVAL}\")\n",
    "\n",
    "# =========================\n",
    "# APLICAR CROPS a eventos balanceados (con event_id)\n",
    "# =========================\n",
    "def segments_from_balanced_events_with_ids(kept_balanced, crops, fs=FS):\n",
    "    X, y, groups = [], [], []\n",
    "    event_ids = []\n",
    "    ch_template = None\n",
    "    cache_raw = {}\n",
    "    # debug\n",
    "    dbg_total_events = 0\n",
    "    dbg_total_crops_ok = 0\n",
    "    dbg_skips_oob = 0\n",
    "    dbg_skips_short = 0\n",
    "    dbg_skips_nan = 0\n",
    "\n",
    "    next_event_id = 0\n",
    "    try:\n",
    "        for s, cls_dict in tqdm(kept_balanced.items(), desc=\"Generando segmentos (con crops + event_ids)\"):\n",
    "            for c in range(4):\n",
    "                for ev in cls_dict[c]:\n",
    "                    dbg_total_events += 1\n",
    "                    p = ev[\"path\"]; onset = ev[\"onset\"]; sid = ev[\"subject\"]; rid = ev[\"run\"]\n",
    "                    if p not in cache_raw:\n",
    "                        raw = read_raw_edf(p)\n",
    "                        if raw is None: \n",
    "                            continue\n",
    "                        cache_raw[p] = raw\n",
    "                    raw = cache_raw[p]\n",
    "                    data = raw.get_data()\n",
    "                    T_total = data.shape[1]\n",
    "                    if ch_template is None:\n",
    "                        ch_template = raw.ch_names\n",
    "\n",
    "                    eid = next_event_id\n",
    "                    next_event_id += 1\n",
    "\n",
    "                    for rel_start, rel_end in crops:\n",
    "                        s_samp = int(round((raw.first_time + onset + rel_start) * fs))\n",
    "                        e_samp = int(round((raw.first_time + onset + rel_end)   * fs))\n",
    "                        exp_len = int(round((rel_end - rel_start) * fs))\n",
    "                        if s_samp < 0 or e_samp > T_total:\n",
    "                            dbg_skips_oob += 1\n",
    "                            continue\n",
    "                        seg = data[:, s_samp:e_samp].T.astype(np.float32)\n",
    "                        if seg.shape[0] != exp_len or seg.shape[0] <= 0:\n",
    "                            dbg_skips_short += 1\n",
    "                            continue\n",
    "                        if not np.isfinite(seg).all():\n",
    "                            dbg_skips_nan += 1\n",
    "                            continue\n",
    "                        # normalización por epoch (opcional)\n",
    "                        if NORM_EPOCH_ZSCORE:\n",
    "                            seg = (seg - seg.mean(axis=0, keepdims=True)) / (seg.std(axis=0, keepdims=True) + 1e-6)\n",
    "\n",
    "                        X.append(seg); y.append(c); groups.append(s); event_ids.append(eid)\n",
    "                        dbg_total_crops_ok += 1\n",
    "    finally:\n",
    "        cache_raw.clear()\n",
    "\n",
    "    if DEBUG_WINDOWS:\n",
    "        print(\"=== DEBUG WINDOWS SUMMARY ===\")\n",
    "        print(f\"Eventos brutos: {dbg_total_events}\")\n",
    "        print(f\"Crops OK:      {dbg_total_crops_ok}\")\n",
    "        print(f\"Skips OOB:     {dbg_skips_oob}\")\n",
    "        print(f\"Skips LEN:     {dbg_skips_short}\")\n",
    "        print(f\"Skips NaN:     {dbg_skips_nan}\")\n",
    "\n",
    "    X = np.stack(X, axis=0) if len(X)>0 else np.zeros((0,1,1), np.float32)\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "    groups = np.asarray(groups, dtype=np.int64)\n",
    "    event_ids = np.asarray(event_ids, dtype=np.int64)\n",
    "\n",
    "    n, T, C = X.shape\n",
    "    print(f\"Dataset construido: N={n} | T={T} | C={C} | sujetos únicos={len(np.unique(groups))} | eventos únicos={len(np.unique(event_ids))}\")\n",
    "    return X, y, groups, event_ids, ch_template\n",
    "\n",
    "# =========================\n",
    "# DATASET TORCH\n",
    "# =========================\n",
    "class EEGTrials(Dataset):\n",
    "    def __init__(self, X, y, groups):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.g = groups.astype(np.int64)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]  # (T,C)\n",
    "        x = np.expand_dims(x, 0)  # (1,T,C)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx])\n",
    "\n",
    "# =========================\n",
    "# AUGMENTS (solo train)\n",
    "# =========================\n",
    "def do_time_jitter(x, max_ms=50, fs=160.0):\n",
    "    B,_,T,C = x.shape\n",
    "    max_shift = int(round(max_ms/1000.0 * fs))\n",
    "    if max_shift <= 0: return x\n",
    "    shifts = torch.randint(low=-max_shift, high=max_shift+1, size=(B,), device=x.device)\n",
    "    out = torch.empty_like(x)\n",
    "    for i,s in enumerate(shifts):\n",
    "        if s==0: out[i] = x[i]; continue\n",
    "        if s>0:\n",
    "            out[i,:,s:,:] = x[i,:,:T-s,:]\n",
    "            out[i,:,:s,:] = 0\n",
    "        else:\n",
    "            s = -s\n",
    "            out[i,:,:T-s,:] = x[i,:,s:,:]\n",
    "            out[i,:,T-s:,:] = 0\n",
    "    return out\n",
    "\n",
    "def do_gaussian_noise(x, sigma=0.01):\n",
    "    if sigma<=0: return x\n",
    "    return x + sigma*torch.randn_like(x)\n",
    "\n",
    "def do_temporal_cutout_masked(x, y, classes_mask={2,3}, min_ms=30, max_ms=60, fs=160.0):\n",
    "    B,_,T,C = x.shape\n",
    "    Lmin = int(round(min_ms/1000.0*fs))\n",
    "    Lmax = int(round(max_ms/1000.0*fs))\n",
    "    if Lmin<=0 or Lmax<=0 or Lmin>Lmax: return x\n",
    "    out = x.clone()\n",
    "    y_np = y.detach().cpu().numpy()\n",
    "    for i in range(B):\n",
    "        if int(y_np[i]) not in classes_mask:\n",
    "            continue\n",
    "        L = random.randint(Lmin, Lmax)\n",
    "        if L>=T: continue\n",
    "        s = random.randint(0, T-L)\n",
    "        out[i,:,s:s+L,:] = 0\n",
    "    return out\n",
    "\n",
    "def mixup_batch(x, y, n_classes, alpha=0.2):\n",
    "    if alpha<=0:\n",
    "        y_onehot = torch.nn.functional.one_hot(y, num_classes=n_classes).float()\n",
    "        return x, y_onehot, 1.0\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    perm = torch.randperm(x.size(0), device=x.device)\n",
    "    x_mix = lam*x + (1-lam)*x[perm]\n",
    "    y_a = torch.nn.functional.one_hot(y, num_classes=n_classes).float()\n",
    "    y_b = y_a[perm]\n",
    "    y_mix = lam*y_a + (1-lam)*y_b\n",
    "    return x_mix, y_mix, lam\n",
    "\n",
    "class WeightedSoftCrossEntropy(nn.Module):\n",
    "    def __init__(self, class_weights=None, label_smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.register_buffer('w', None if class_weights is None else class_weights.clone().float())\n",
    "        self.ls = float(label_smoothing)\n",
    "    def forward(self, logits, target_probs):\n",
    "        if self.ls > 0:\n",
    "            K = logits.size(1)\n",
    "            target_probs = (1-self.ls)*target_probs + self.ls*(1.0/K)\n",
    "        logp = torch.log_softmax(logits, dim=1)\n",
    "        loss_per_class = -(target_probs * logp)\n",
    "        if self.w is not None:\n",
    "            loss_per_class = loss_per_class * self.w.unsqueeze(0)\n",
    "        loss = loss_per_class.sum(dim=1).mean()\n",
    "        return loss\n",
    "\n",
    "# =========================\n",
    "# EEGNet (adaptado a (B,1,T,C))\n",
    "# =========================\n",
    "class ChannelDropout(nn.Module):\n",
    "    def __init__(self, p=0.1):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    def forward(self, x):\n",
    "        if not self.training or self.p<=0: return x\n",
    "        B,_,T,C = x.shape\n",
    "        mask = (torch.rand(B,1,1,C, device=x.device) > self.p).float()\n",
    "        return x * mask\n",
    "\n",
    "@torch.no_grad()\n",
    "def apply_max_norm(model, max_value=2.0, p=2.0):\n",
    "    layers = []\n",
    "    if hasattr(model, 'conv_depthwise'): layers.append(model.conv_depthwise)\n",
    "    if hasattr(model, 'conv_sep_point'): layers.append(model.conv_sep_point)\n",
    "    if hasattr(model, 'fc'):             layers.append(model.fc)\n",
    "    if hasattr(model, 'out'):            layers.append(model.out)\n",
    "    for layer in layers:\n",
    "        if hasattr(layer, 'weight') and layer.weight is not None:\n",
    "            w = layer.weight.data\n",
    "            norms = w.view(w.size(0), -1).norm(p=p, dim=1, keepdim=True)\n",
    "            desired = torch.clamp(norms, max=max_value)\n",
    "            w.view(w.size(0), -1).mul_(desired / (1e-8 + norms))\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, n_ch: int, n_classes: int,\n",
    "                 F1: int = 24, D: int = 2, kernel_t: int = 64, k_sep: int = 16,\n",
    "                 pool1_t: int = 4, pool2_t: int = 6,\n",
    "                 drop1_p: float = 0.35, drop2_p: float = 0.6,\n",
    "                 chdrop_p: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.n_ch = n_ch\n",
    "        self.n_classes = n_classes\n",
    "        self.F1 = F1\n",
    "        self.D = D\n",
    "        self.F2 = F1 * D\n",
    "        self.kernel_t = kernel_t\n",
    "        self.k_sep = k_sep\n",
    "        self.pool1_t = pool1_t\n",
    "        self.pool2_t = pool2_t\n",
    "\n",
    "        self.chdrop = ChannelDropout(p=chdrop_p)\n",
    "\n",
    "        self.conv_temporal = nn.Conv2d(1, F1, kernel_size=(kernel_t, 1),\n",
    "                                       padding=(kernel_t // 2, 0), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(F1, momentum=0.99, eps=1e-3)\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "        self.conv_depthwise = nn.Conv2d(F1, self.F2, kernel_size=(1, n_ch),\n",
    "                                        groups=F1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.F2, momentum=0.99, eps=1e-3)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=(pool1_t, 1), stride=(pool1_t, 1))\n",
    "        self.drop1 = nn.Dropout(drop1_p)\n",
    "\n",
    "        self.conv_sep_depth = nn.Conv2d(self.F2, self.F2, kernel_size=(k_sep, 1),\n",
    "                                        groups=self.F2, padding=(k_sep // 2, 0), bias=False)\n",
    "        self.conv_sep_point = nn.Conv2d(self.F2, self.F2, kernel_size=(1, 1), bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.F2, momentum=0.99, eps=1e-3)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(pool2_t, 1), stride=(pool2_t, 1))\n",
    "        self.drop2 = nn.Dropout(drop2_p)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc  = None\n",
    "        self.out = None\n",
    "        self._T_in = None\n",
    "\n",
    "    def _build_head(self, T_in: int, device: torch.device):\n",
    "        T1 = T_in // self.pool1_t\n",
    "        T2 = T1 // self.pool2_t\n",
    "        feat_dim = self.F2 * T2 * 1\n",
    "        self.fc  = nn.Linear(feat_dim, 128, bias=True).to(device)\n",
    "        self.out = nn.Linear(128, self.n_classes, bias=True).to(device)\n",
    "        self._T_in = T_in\n",
    "\n",
    "    def ensure_head(self, T_in: int, device: torch.device):\n",
    "        if (self.fc is None) or (self.out is None) or (self._T_in != T_in):\n",
    "            self._build_head(T_in, device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, _, T, C = x.shape\n",
    "        self.ensure_head(T, x.device)\n",
    "\n",
    "        x = self.chdrop(x)\n",
    "\n",
    "        z = self.conv_temporal(x)\n",
    "        z = self.bn1(z); z = self.act(z)\n",
    "\n",
    "        z = self.conv_depthwise(z)\n",
    "        z = self.bn2(z); z = self.act(z)\n",
    "        z = self.pool1(z)\n",
    "        z = self.drop1(z)\n",
    "\n",
    "        z = self.conv_sep_depth(z)\n",
    "        z = self.conv_sep_point(z)\n",
    "        z = self.bn3(z); z = self.act(z)\n",
    "        z = self.pool2(z)\n",
    "        z = self.drop2(z)\n",
    "\n",
    "        z = self.flatten(z)\n",
    "        z = self.fc(z); z = self.act(z)\n",
    "        z = self.out(z)\n",
    "        return z\n",
    "\n",
    "# =========================\n",
    "# SAMPLER / PESOS\n",
    "# =========================\n",
    "def build_weighted_sampler(y, groups):\n",
    "    y = np.asarray(y); groups = np.asarray(groups)\n",
    "    class_counts = np.bincount(y, minlength=len(np.unique(y))).astype(float)\n",
    "    class_w = 1.0 / class_counts[y]\n",
    "    subj_vals, subj_counts = np.unique(groups, return_counts=True)\n",
    "    subj_map = {s:c for s,c in zip(subj_vals, subj_counts)}\n",
    "    subj_w = np.array([1.0/subj_map[g] for g in groups], dtype=float)\n",
    "    w = class_w * subj_w\n",
    "    w = w / w.mean()\n",
    "    w_t = torch.from_numpy(w).float()\n",
    "    sampler = WeightedRandomSampler(weights=w_t, num_samples=len(w_t), replacement=True)\n",
    "    return sampler\n",
    "\n",
    "def make_class_weight_tensor(y_indices, n_classes, boost_bfists=1.20):\n",
    "    counts = np.bincount(y_indices, minlength=n_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    w = counts.sum() / counts\n",
    "    w = w / w.mean()\n",
    "    w[2] *= boost_bfists\n",
    "    w = w / w.mean()\n",
    "    return torch.tensor(w, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "# =========================\n",
    "# TRAIN / EVAL\n",
    "# =========================\n",
    "def train_epoch(model, loader, opt, criterion_soft, n_classes, fs=160.0, maxnorm=2.0):\n",
    "    model.train()\n",
    "    for xb, yb, _ in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        xb = do_time_jitter(xb, max_ms=50, fs=fs)\n",
    "        xb = do_gaussian_noise(xb, sigma=0.01)\n",
    "        xb = do_temporal_cutout_masked(xb, yb, classes_mask={2,3}, min_ms=30, max_ms=60, fs=fs)\n",
    "        xb, yt, _ = mixup_batch(xb, yb, n_classes=n_classes, alpha=0.2)\n",
    "        logits = model(xb)\n",
    "        loss = criterion_soft(logits, yt)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        apply_max_norm(model, max_value=maxnorm, p=2.0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_grouped_by_event(model, loader, event_ids_np, use_tta_jitter=True, tta_jitter_n=3):\n",
    "    model.eval()\n",
    "    all_logits, all_eids, y_all = [], [], []\n",
    "    idx_offset = 0\n",
    "    for xb, yb, _ in loader:\n",
    "        B = xb.size(0)\n",
    "        xb = xb.to(DEVICE)\n",
    "        if use_tta_jitter:\n",
    "            outs = []\n",
    "            for _ in range(tta_jitter_n):\n",
    "                outs.append(model(do_time_jitter(xb, max_ms=20, fs=FS)))\n",
    "            logits = torch.stack(outs, dim=0).mean(dim=0)\n",
    "        else:\n",
    "            logits = model(xb)\n",
    "        all_logits.append(logits.cpu().numpy())\n",
    "        all_eids.append(event_ids_np[idx_offset: idx_offset + B].copy())\n",
    "        y_all.append(yb.numpy())\n",
    "        idx_offset += B\n",
    "    all_logits = np.concatenate(all_logits, axis=0)\n",
    "    all_eids   = np.concatenate(all_eids, axis=0)\n",
    "    y_all      = np.concatenate(y_all, axis=0)\n",
    "\n",
    "    uniq_eids = np.unique(all_eids)\n",
    "    logits_ev, ytrue_ev = [], []\n",
    "    for eid in uniq_eids:\n",
    "        idx = (all_eids == eid)\n",
    "        logits_ev.append(all_logits[idx].mean(axis=0))\n",
    "        ytrue_ev.append(int(y_all[idx][0]))\n",
    "    logits_ev = np.stack(logits_ev, axis=0)\n",
    "    ytrue_ev  = np.asarray(ytrue_ev, dtype=np.int64)\n",
    "    ypred_ev  = logits_ev.argmax(axis=1)\n",
    "    acc = float((ypred_ev == ytrue_ev).mean())\n",
    "    return ytrue_ev, ypred_ev, acc\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes, title, fname):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.nan_to_num(cm_norm)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title); plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    ticks = np.arange(len(classes))\n",
    "    plt.xticks(ticks, classes, rotation=45, ha='right'); plt.yticks(ticks, classes)\n",
    "    fmt = '.2f'; thresh = cm_norm.max()/2.\n",
    "    for i,j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "        plt.text(j, i, format(cm_norm[i,j], fmt), ha=\"center\",\n",
    "                 color=\"white\" if cm_norm[i,j] > thresh else \"black\")\n",
    "    plt.ylabel('True'); plt.xlabel('Pred')\n",
    "    plt.tight_layout(); plt.savefig(fname, dpi=150, bbox_inches='tight'); plt.close()\n",
    "\n",
    "def print_report(y_true, y_pred, classes):\n",
    "    print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
    "\n",
    "# =========================\n",
    "# FT POR SUJETO (opcional, con agrupación por evento)\n",
    "# =========================\n",
    "def _freeze_for_ft(model):\n",
    "    for p in model.parameters(): p.requires_grad = False\n",
    "    for p in model.conv_depthwise.parameters(): p.requires_grad = True\n",
    "    for p in model.bn2.parameters():           p.requires_grad = True\n",
    "    for p in model.conv_sep_depth.parameters():p.requires_grad = True\n",
    "    for p in model.conv_sep_point.parameters():p.requires_grad = True\n",
    "    for p in model.bn3.parameters():           p.requires_grad = True\n",
    "    for p in model.fc.parameters():            p.requires_grad = True\n",
    "    for p in model.out.parameters():           p.requires_grad = True\n",
    "\n",
    "def subject_cv_finetune_predict_grouped(model_global, Xs, ys, eids, n_splits=CALIB_CV_FOLDS, n_classes=4):\n",
    "    \"\"\"\n",
    "    CV interno en el sujeto; evalúa agrupando por event_id del sujeto.\n",
    "    Retorna y_true_event, y_pred_event.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    y_true_all, y_pred_all = [], []\n",
    "    for tr_idx, te_idx in skf.split(Xs, ys):\n",
    "        Xcal, ycal, ecal = Xs[tr_idx], ys[tr_idx], eids[tr_idx]\n",
    "        Xho,  yho,  eho  = Xs[te_idx], ys[te_idx], eids[te_idx]\n",
    "\n",
    "        # split val\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=FT_VAL_RATIO, random_state=RANDOM_STATE)\n",
    "        (tr2, va2), = sss.split(Xcal, ycal)\n",
    "        Xtr, ytr = Xcal[tr2], ycal[tr2]\n",
    "        Xva, yva = Xcal[va2], ycal[va2]\n",
    "\n",
    "        # loaders\n",
    "        tr_ds = torch.utils.data.TensorDataset(torch.from_numpy(Xtr).float().unsqueeze(1),\n",
    "                                               torch.from_numpy(ytr).long())\n",
    "        va_ds = torch.utils.data.TensorDataset(torch.from_numpy(Xva).float().unsqueeze(1),\n",
    "                                               torch.from_numpy(yva).long())\n",
    "        te_ds = torch.utils.data.TensorDataset(torch.from_numpy(Xho).float().unsqueeze(1),\n",
    "                                               torch.from_numpy(yho).long())\n",
    "        tr_dl = DataLoader(tr_ds, batch_size=32, shuffle=True)\n",
    "        va_dl = DataLoader(va_ds, batch_size=64, shuffle=False)\n",
    "        te_dl = DataLoader(te_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "        m = copy.deepcopy(model_global).to(DEVICE)\n",
    "        _freeze_for_ft(m)\n",
    "\n",
    "        base_params = (list(m.conv_depthwise.parameters()) +\n",
    "                       list(m.bn2.parameters()) +\n",
    "                       list(m.conv_sep_depth.parameters()) +\n",
    "                       list(m.conv_sep_point.parameters()) +\n",
    "                       list(m.bn3.parameters()))\n",
    "        head_params = list(m.fc.parameters()) + list(m.out.parameters())\n",
    "\n",
    "        opt = optim.Adam([\n",
    "            {\"params\": base_params, \"lr\": FT_BASE_LR},\n",
    "            {\"params\": head_params, \"lr\": FT_HEAD_LR},\n",
    "        ])\n",
    "        ref = [p.detach().clone() for p in base_params + head_params]\n",
    "        class_w = make_class_weight_tensor(ytr, n_classes, boost_bfists=1.0)  # sin boost extra\n",
    "        crit = nn.CrossEntropyLoss(weight=class_w)\n",
    "\n",
    "        best_state = copy.deepcopy(m.state_dict())\n",
    "        best_val = float('inf'); bad = 0\n",
    "\n",
    "        for _ in range(FT_EPOCHS):\n",
    "            m.train()\n",
    "            for xb, yb in tr_dl:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                logits = m(xb)\n",
    "                loss = crit(logits, yb)\n",
    "                # L2-SP\n",
    "                tr_params = base_params + head_params\n",
    "                reg = 0.0\n",
    "                for p_cur, p_ref in zip(tr_params, ref):\n",
    "                    reg = reg + torch.sum((p_cur - p_ref)**2)\n",
    "                loss = loss + FT_L2SP * reg\n",
    "                loss.backward(); opt.step()\n",
    "                apply_max_norm(m, max_value=2.0, p=2.0)\n",
    "\n",
    "            # val\n",
    "            m.eval()\n",
    "            val_loss = 0.0; n = 0\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in va_dl:\n",
    "                    xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                    logits = m(xb)\n",
    "                    loss = crit(logits, yb)\n",
    "                    val_loss += loss.item()*xb.size(0); n += xb.size(0)\n",
    "            val_loss /= max(1,n)\n",
    "            if val_loss + 1e-7 < best_val:\n",
    "                best_val = val_loss; bad = 0\n",
    "                best_state = copy.deepcopy(m.state_dict())\n",
    "            else:\n",
    "                bad += 1\n",
    "                if bad >= FT_PATIENCE: break\n",
    "\n",
    "        m.load_state_dict(best_state)\n",
    "\n",
    "        # pred en HOLD-OUT del sujeto, agrupado por event_id\n",
    "        m.eval()\n",
    "        logits_all = []\n",
    "        with torch.no_grad():\n",
    "            for xb, _ in te_dl:\n",
    "                xb = xb.to(DEVICE)\n",
    "                logits_all.append(m(xb).cpu().numpy())\n",
    "        logits_all = np.concatenate(logits_all, axis=0)  # (Nh, C)\n",
    "        # agrupa por eid\n",
    "        uniq = np.unique(eho)\n",
    "        for eid in uniq:\n",
    "            idx = (eho == eid)\n",
    "            log_ev = logits_all[idx].mean(axis=0)\n",
    "            y_true_all.append(int(yho[idx][0]))\n",
    "            y_pred_all.append(int(np.argmax(log_ev)))\n",
    "    return np.asarray(y_true_all, dtype=np.int64), np.asarray(y_pred_all, dtype=np.int64)\n",
    "\n",
    "# =========================\n",
    "# FOLDS JSON helpers\n",
    "# =========================\n",
    "def save_group_folds_json_with_indices(subject_ids_str, groups_array, n_splits, out_json_path,\n",
    "                                       created_by=\"EEGNet_Sliding\", description=None):\n",
    "    out_json_path = Path(out_json_path)\n",
    "    unique_subjects_int = sorted(np.unique(groups_array).tolist())\n",
    "    subject_ids = [f\"S{sid:03d}\" for sid in unique_subjects_int]\n",
    "    if len(subject_ids) < n_splits:\n",
    "        raise ValueError(f\"n_splits={n_splits} mayor que número de sujetos={len(subject_ids)}\")\n",
    "    groups = np.arange(len(subject_ids))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    folds = []\n",
    "    fold_i = 0\n",
    "    for train_idx_grp, test_idx_grp in gkf.split(groups, groups, groups):\n",
    "        fold_i += 1\n",
    "        train_sids = [subject_ids[int(i)] for i in train_idx_grp]\n",
    "        test_sids  = [subject_ids[int(i)] for i in test_idx_grp]\n",
    "        train_sids_int = [int(s[1:]) for s in train_sids]\n",
    "        test_sids_int  = [int(s[1:]) for s in test_sids]\n",
    "        tr_idx = np.where(np.isin(groups_array, train_sids_int))[0].tolist()\n",
    "        te_idx = np.where(np.isin(groups_array, test_sids_int))[0].tolist()\n",
    "        folds.append({\n",
    "            \"fold\": int(fold_i),\n",
    "            \"train\": train_sids,\n",
    "            \"test\": test_sids,\n",
    "            \"tr_idx\": tr_idx,\n",
    "            \"te_idx\": te_idx\n",
    "        })\n",
    "    payload = {\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"created_by\": created_by,\n",
    "        \"description\": description if description is not None else \"\",\n",
    "        \"n_splits\": int(n_splits),\n",
    "        \"n_subjects\": len(subject_ids),\n",
    "        \"subject_ids\": subject_ids,\n",
    "        \"folds\": folds\n",
    "    }\n",
    "    out_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Folds JSON con índices guardado → {out_json_path}\")\n",
    "    return out_json_path\n",
    "\n",
    "def load_group_folds_json(path_json, expected_subject_ids=None, strict_check=True):\n",
    "    path_json = Path(path_json)\n",
    "    if not path_json.exists():\n",
    "        raise FileNotFoundError(f\"No existe {path_json}\")\n",
    "    with open(path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        payload = json.load(f)\n",
    "    subj_json = payload.get(\"subject_ids\", [])\n",
    "    if expected_subject_ids is not None:\n",
    "        expected = sorted(list(expected_subject_ids))\n",
    "        if subj_json != expected:\n",
    "            msg = (\"Los subject_ids del JSON no coinciden con expected_subject_ids.\\n\"\n",
    "                   f\"JSON has {len(subj_json)} subjects, expected {len(expected)}.\\n\"\n",
    "                   f\"First 10 JSON: {subj_json[:10]}\\nFirst 10 expected: {expected[:10]}\")\n",
    "            if strict_check:\n",
    "                raise ValueError(msg)\n",
    "            else:\n",
    "                print(\"WARNING: \" + msg)\n",
    "    return payload\n",
    "\n",
    "# =========================\n",
    "# EXPERIMENTO\n",
    "# =========================\n",
    "def run_experiment(save_folds_json=True, folds_json_path=FOLDS_DIR,\n",
    "                   folds_json_description=\"GroupKFold folds (sliding windows + event eval)\"):\n",
    "    mne.set_log_level('WARNING')\n",
    "\n",
    "    subs = subjects_available()\n",
    "    print(f\"Sujetos elegibles: {len(subs)} → {subs[:10]}{'...' if len(subs)>10 else ''}\")\n",
    "\n",
    "    # 1) Recolectar eventos (sin crops)\n",
    "    per_subj = gather_all_events(subs)\n",
    "\n",
    "    # 2) Balancear por sujeto/clase (21 eventos)\n",
    "    kept_bal = balance_events_per_subject(per_subj, need_per_class=21, rng_seed=RANDOM_STATE)\n",
    "\n",
    "    # 3) Generar segmentos con ventanas (TRAIN policy) + IDs de evento\n",
    "    X, y, groups, eids, chs = segments_from_balanced_events_with_ids(kept_bal, crops=CROPS_TRAIN, fs=FS)\n",
    "    N, T, C = X.shape\n",
    "    print(f\"Listo para entrenar: N={N} | T={T} | C={C} | clases={len(np.unique(y))} | sujetos={len(np.unique(groups))}\")\n",
    "\n",
    "    # Folds por sujetos\n",
    "    folds_json_path = Path(folds_json_path)\n",
    "    folds_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    unique_subs = sorted(np.unique(groups).tolist())\n",
    "    subject_ids_str = [f\"S{s:03d}\" for s in unique_subs]\n",
    "\n",
    "    if not folds_json_path.exists():\n",
    "        if not save_folds_json:\n",
    "            raise FileNotFoundError(f\"Folds JSON no encontrado en {folds_json_path} y save_folds_json=False.\")\n",
    "        save_group_folds_json_with_indices(subject_ids_str, groups, n_splits=N_FOLDS,\n",
    "                                           out_json_path=folds_json_path,\n",
    "                                           created_by=\"EEGNet_Sliding\",\n",
    "                                           description=folds_json_description)\n",
    "    payload = load_group_folds_json(folds_json_path, expected_subject_ids=subject_ids_str, strict_check=False)\n",
    "    folds = payload[\"folds\"]\n",
    "\n",
    "    global_folds = []\n",
    "    ft_folds = []\n",
    "    all_true_ev = []\n",
    "    all_pred_ev = []\n",
    "\n",
    "    for f in folds:\n",
    "        fold = f[\"fold\"]\n",
    "        tr_idx = np.asarray(f.get(\"tr_idx\", []), dtype=int)\n",
    "        te_idx = np.asarray(f.get(\"te_idx\", []), dtype=int)\n",
    "        if tr_idx.size == 0 or te_idx.size == 0:\n",
    "            print(f\"[WARN] fold {fold} sin índices tr/te válidos. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        # split de validación por sujetos (sobre train)\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=GLOBAL_VAL_SPLIT, random_state=RANDOM_STATE)\n",
    "        tr_subj_idx, va_subj_idx = next(gss.split(tr_idx, groups[tr_idx], groups[tr_idx]))\n",
    "        tr_sub_idx = tr_idx[tr_subj_idx]\n",
    "        va_idx     = tr_idx[va_subj_idx]\n",
    "\n",
    "        # Dataset y loaders\n",
    "        ds_tr = EEGTrials(X[tr_sub_idx], y[tr_sub_idx], groups[tr_sub_idx])\n",
    "        ds_va = EEGTrials(X[va_idx],     y[va_idx],     groups[va_idx])\n",
    "        ds_te = EEGTrials(X[te_idx],     y[te_idx],     groups[te_idx])\n",
    "\n",
    "        sampler = build_weighted_sampler(y[tr_sub_idx], groups[tr_sub_idx])\n",
    "        tr_loader = DataLoader(ds_tr, batch_size=BATCH_SIZE, sampler=sampler, drop_last=False)\n",
    "        va_loader = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "        te_loader = DataLoader(ds_te, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "        # Modelo\n",
    "        model = EEGNet(n_ch=C, n_classes=N_CLASSES,\n",
    "                       F1=24, D=2, kernel_t=64, k_sep=16,\n",
    "                       pool1_t=4, pool2_t=6, drop1_p=0.35, drop2_p=0.6,\n",
    "                       chdrop_p=0.1).to(DEVICE)\n",
    "\n",
    "        # Opt + SGDR\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_INIT)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=SGDR_T0, T_mult=SGDR_Tmult)\n",
    "\n",
    "        # Criterio suave con ponderación (+20% Both Fists)\n",
    "        class_weights = make_class_weight_tensor(y[tr_sub_idx], N_CLASSES, boost_bfists=1.20)\n",
    "        criterion_soft = WeightedSoftCrossEntropy(class_weights, label_smoothing=0.05)\n",
    "\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        best_val_ev_acc = -1.0\n",
    "        bad = 0\n",
    "\n",
    "        print(f\"\\n[Fold {fold}/{N_FOLDS}] Entrenando (train={len(tr_sub_idx)} | val={len(va_idx)} | test={len(te_idx)})\")\n",
    "        for epoch in range(1, EPOCHS_GLOBAL+1):\n",
    "            train_epoch(model, tr_loader, opt, criterion_soft, n_classes=N_CLASSES, fs=FS, maxnorm=2.0)\n",
    "            scheduler.step(epoch-1 + 1e-8)\n",
    "\n",
    "            # Eval por evento en VALIDACIÓN\n",
    "            y_true_va, y_pred_va, va_acc_ev = evaluate_grouped_by_event(\n",
    "                model, va_loader, event_ids_np=eids[va_idx], use_tta_jitter=True, tta_jitter_n=3\n",
    "            )\n",
    "\n",
    "            if (epoch % LOG_EVERY == 0) or epoch in (1, 10, 20, 50, 100):\n",
    "                cur_lr = opt.param_groups[0]['lr']\n",
    "                print(f\"  Época {epoch:3d} | val_acc(ev)={va_acc_ev:.4f} | LR={cur_lr:.5f}\")\n",
    "\n",
    "            if va_acc_ev > best_val_ev_acc + 1e-4:\n",
    "                best_val_ev_acc = va_acc_ev; bad = 0\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "            else:\n",
    "                bad += 1\n",
    "                if bad >= GLOBAL_PATIENCE:\n",
    "                    print(f\"  Early stopping @ {epoch} (mejor val_acc(ev)={best_val_ev_acc:.4f})\")\n",
    "                    break\n",
    "\n",
    "        # Cargar mejor estado\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "        # EVALUACIÓN GLOBAL por EVENTO en TEST\n",
    "        y_true_te, y_pred_te, acc_ev = evaluate_grouped_by_event(\n",
    "            model, te_loader, event_ids_np=eids[te_idx], use_tta_jitter=True, tta_jitter_n=5\n",
    "        )\n",
    "        global_folds.append(acc_ev)\n",
    "        all_true_ev.append(y_true_te); all_pred_ev.append(y_pred_te)\n",
    "        print(f\"[Fold {fold}/{N_FOLDS}] Global acc (por evento) = {acc_ev:.4f}\")\n",
    "        print_report(y_true_te, y_pred_te, CLASS_NAMES_4C)\n",
    "\n",
    "        # ---------- FT POR SUJETO (opcional) ----------\n",
    "        acc_ft = np.nan\n",
    "        if DO_SUBJECT_FT:\n",
    "            X_te, y_te, g_te, e_te = X[te_idx], y[te_idx], groups[te_idx], eids[te_idx]\n",
    "            y_true_ft_all, y_pred_ft_all = [], []\n",
    "            used_subjects = 0\n",
    "            for sid in np.unique(g_te):\n",
    "                idx = np.where(g_te == sid)[0]\n",
    "                Xs, ys, es = X_te[idx], y_te[idx], e_te[idx]\n",
    "                if len(np.unique(ys)) < 2 or len(ys) < CALIB_CV_FOLDS:\n",
    "                    continue\n",
    "                yt, yp = subject_cv_finetune_predict_grouped(model, Xs, ys, es, n_splits=CALIB_CV_FOLDS, n_classes=N_CLASSES)\n",
    "                y_true_ft_all.append(yt); y_pred_ft_all.append(yp); used_subjects += 1\n",
    "            if len(y_true_ft_all) > 0:\n",
    "                y_true_ft_all = np.concatenate(y_true_ft_all)\n",
    "                y_pred_ft_all = np.concatenate(y_pred_ft_all)\n",
    "                acc_ft = (y_true_ft_all == y_pred_ft_all).mean()\n",
    "                print(f\"  Fine-tuning (por sujeto, agrupado ev) acc={acc_ft:.4f} | Δ={acc_ft-acc_ev:+.4f} | sujetos={used_subjects}\")\n",
    "            else:\n",
    "                print(\"  Fine-tuning no ejecutado (sujeto(s) con muestras/clases insuficientes).\")\n",
    "        ft_folds.append(acc_ft)\n",
    "\n",
    "    # ---------- resultados finales ----------\n",
    "    if len(all_true_ev) > 0:\n",
    "        all_true_ev = np.concatenate(all_true_ev)\n",
    "        all_pred_ev = np.concatenate(all_pred_ev)\n",
    "    else:\n",
    "        all_true_ev = np.array([], dtype=int)\n",
    "        all_pred_ev = np.array([], dtype=int)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTADOS FINALES (por EVENTO)\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Global folds:\", [f\"{a:.4f}\" for a in global_folds])\n",
    "    if len(global_folds) > 0:\n",
    "        print(f\"Global mean: {np.mean(global_folds):.4f}\")\n",
    "\n",
    "    print(\"Fine-tune folds:\", [(\"nan\" if (a is None or np.isnan(a)) else f\"{a:.4f}\") for a in ft_folds])\n",
    "    if len(ft_folds) > 0:\n",
    "        print(f\"Fine-tune mean: {np.nanmean(ft_folds):.4f}\")\n",
    "        print(f\"Δ(FT-Global) mean: {np.nanmean(ft_folds) - np.mean(global_folds):+.4f}\")\n",
    "\n",
    "    if all_true_ev.size > 0:\n",
    "        fname = \"confusion_eegnet_sliding_event_eval_allfolds.png\"\n",
    "        plot_confusion(all_true_ev, all_pred_ev, CLASS_NAMES_4C,\n",
    "                       title=\"Confusion Matrix (Event-level, ALL FOLDS)\",\n",
    "                       fname=fname)\n",
    "        print(f\"↳ Matriz de confusión guardada: {fname}\")\n",
    "\n",
    "    return {\n",
    "        \"global_folds\": global_folds,\n",
    "        \"ft_folds\": ft_folds,\n",
    "        \"all_true_ev\": all_true_ev,\n",
    "        \"all_pred_ev\": all_pred_ev,\n",
    "        \"folds_json_path\": str(folds_json_path)\n",
    "    }\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🧠 INICIANDO EEGNet + Sliding Windows + Event-level Eval\")\n",
    "    print(f\"🔧 WINDOW_SET={WINDOW_SET} | POLICY={WINDOW_POLICY}\")\n",
    "    print(f\"⚙️  GLOBAL: epochs={EPOCHS_GLOBAL}, lr={LR_INIT}, batch={BATCH_SIZE}, SGDR(T0={SGDR_T0},Tmult={SGDR_Tmult})\")\n",
    "    print(f\"🧪 FT por sujeto: {'ON' if DO_SUBJECT_FT else 'OFF'} \"\n",
    "          f\"(epochs={FT_EPOCHS}, base_lr={FT_BASE_LR}, head_lr={FT_HEAD_LR}, L2SP={FT_L2SP})\")\n",
    "    run_experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd707fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Dispositivo: cuda\n",
      "🎯 Sliding windows → TRAIN=[(0.0, 3.0), (0.25, 3.25), (0.5, 3.5), (0.75, 3.75), (1.0, 4.0)] | EVAL=[(0.0, 3.0), (0.5, 3.5), (1.0, 4.0)]\n",
      "🧠 EEGNet + Sliding Windows (+RandomEventSampler) + Event-level Eval\n",
      "🔧 WINDOW_SET=3s_base | POLICY=train_aggressive | RANDOM_PER_EVENT=True (k=1)\n",
      "⚙️  GLOBAL: epochs=100, lr=0.01, batch=64, SGDR(T0=6,Tmult=2)\n",
      "🎛️ Augs: SpecAug=ON, Mixup=0.2, TimeJitter=50ms, Cutout(OF)\n",
      "🧪 FT por sujeto: ON (epochs=25, base_lr=5e-05, head_lr=0.001, L2SP=0.0001)\n",
      "Sujetos elegibles: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Escaneando eventos (sin crops):   1%|          | 1/103 [00:00<00:13,  7.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Escaneando eventos (sin crops): 100%|██████████| 103/103 [00:12<00:00,  8.34it/s]\n",
      "Generando segmentos (train∪eval crops): 100%|██████████| 103/103 [00:16<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUG WINDOWS SUMMARY ===\n",
      "Eventos brutos: 8652\n",
      "Crops OK:      43260\n",
      "Skips OOB:     0\n",
      "Skips LEN:     0\n",
      "Skips NaN:     0\n",
      "Dataset construido: N=43260 | T=480 | C=8 | sujetos únicos=103 | eventos únicos=8652\n",
      "Listo para entrenar: N=43260 | T=480 | C=8 | clases=4 | sujetos=103\n",
      "\n",
      "[Fold 1/5] Entrenando (train_ev≈1122 | val_ev≈272 | test_ev≈378)\n",
      "  Época   1 | train_acc(ev)=0.3182 | val_acc(ev)=0.3272 | LR=0.01000\n",
      "  Época   5 | train_acc(ev)=0.3939 | val_acc(ev)=0.4412 | LR=0.00250\n",
      "  Época  10 | train_acc(ev)=0.4100 | val_acc(ev)=0.4118 | LR=0.00854\n",
      "  Época  15 | train_acc(ev)=0.4198 | val_acc(ev)=0.4632 | LR=0.00250\n",
      "  Época  20 | train_acc(ev)=0.4242 | val_acc(ev)=0.4632 | LR=0.00996\n",
      "  Época  25 | train_acc(ev)=0.4385 | val_acc(ev)=0.4375 | LR=0.00854\n",
      "  Early stopping @ 27 (mejor val_acc(ev)=0.4743)\n",
      "↳ Curva train/val guardada: training_curve_event_fold1.png\n",
      "[Fold 1/5] Global acc (por evento) = 0.3360\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.0000    0.0000    0.0000         0\n",
      "       Right     0.6495    0.3333    0.4406       189\n",
      "  Both Fists     0.5246    0.3386    0.4116       189\n",
      "   Both Feet     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.3360       378\n",
      "   macro avg     0.2935    0.1680    0.2130       378\n",
      "weighted avg     0.5870    0.3360    0.4261       378\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fine-tuning (por sujeto, agrup. ev) acc=0.5238 | Δ=+0.1878 | sujetos=21\n",
      "\n",
      "[Fold 2/5] Entrenando (train_ev≈1123 | val_ev≈272 | test_ev≈378)\n",
      "  Época   1 | train_acc(ev)=0.3134 | val_acc(ev)=0.3162 | LR=0.01000\n",
      "  Época   5 | train_acc(ev)=0.3963 | val_acc(ev)=0.4081 | LR=0.00250\n",
      "  Época  10 | train_acc(ev)=0.4078 | val_acc(ev)=0.3897 | LR=0.00854\n",
      "  Época  15 | train_acc(ev)=0.4310 | val_acc(ev)=0.4338 | LR=0.00250\n",
      "  Época  20 | train_acc(ev)=0.4256 | val_acc(ev)=0.4228 | LR=0.00996\n",
      "  Época  25 | train_acc(ev)=0.3989 | val_acc(ev)=0.4191 | LR=0.00854\n",
      "  Early stopping @ 29 (mejor val_acc(ev)=0.4596)\n",
      "↳ Curva train/val guardada: training_curve_event_fold2.png\n",
      "[Fold 2/5] Global acc (por evento) = 0.4370\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.5893    0.3929    0.4714        84\n",
      "       Right     0.9462    0.4505    0.6104       273\n",
      "  Both Fists     0.0000    0.0000    0.0000         0\n",
      "   Both Feet     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.4370       357\n",
      "   macro avg     0.3839    0.2109    0.2705       357\n",
      "weighted avg     0.8622    0.4370    0.5777       357\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fine-tuning (por sujeto, agrup. ev) acc=0.6585 | Δ=+0.2216 | sujetos=21\n",
      "\n",
      "[Fold 3/5] Entrenando (train_ev≈1123 | val_ev≈272 | test_ev≈357)\n",
      "  Época   1 | train_acc(ev)=0.2848 | val_acc(ev)=0.2873 | LR=0.01000\n",
      "  Época   5 | train_acc(ev)=0.2414 | val_acc(ev)=0.2313 | LR=0.00250\n",
      "  Época  10 | train_acc(ev)=0.2450 | val_acc(ev)=0.2313 | LR=0.00854\n",
      "  Early stopping @ 11 (mejor val_acc(ev)=0.2873)\n",
      "↳ Curva train/val guardada: training_curve_event_fold3.png\n",
      "[Fold 3/5] Global acc (por evento) = 0.5966\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     1.0000    0.5966    0.7474       357\n",
      "       Right     0.0000    0.0000    0.0000         0\n",
      "  Both Fists     0.0000    0.0000    0.0000         0\n",
      "   Both Feet     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.5966       357\n",
      "   macro avg     0.2500    0.1492    0.1868       357\n",
      "weighted avg     1.0000    0.5966    0.7474       357\n",
      "\n",
      "  Fine-tuning no ejecutado (muestras/clases insuficientes).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold 4/5] Entrenando (train_ev≈1139 | val_ev≈272 | test_ev≈340)\n",
      "  Época   1 | train_acc(ev)=0.2467 | val_acc(ev)=0.2169 | LR=0.01000\n",
      "  Época   5 | train_acc(ev)=0.2862 | val_acc(ev)=0.2941 | LR=0.00250\n",
      "  Época  10 | train_acc(ev)=0.2888 | val_acc(ev)=0.2757 | LR=0.00854\n",
      "  Época  15 | train_acc(ev)=0.2994 | val_acc(ev)=0.3309 | LR=0.00250\n",
      "  Época  20 | train_acc(ev)=0.3310 | val_acc(ev)=0.3309 | LR=0.00996\n",
      "  Época  25 | train_acc(ev)=0.3512 | val_acc(ev)=0.3272 | LR=0.00854\n",
      "  Early stopping @ 27 (mejor val_acc(ev)=0.3566)\n",
      "↳ Curva train/val guardada: training_curve_event_fold4.png\n",
      "[Fold 4/5] Global acc (por evento) = 0.7324\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.0000    0.0000    0.0000         0\n",
      "       Right     0.0000    0.0000    0.0000         0\n",
      "  Both Fists     0.0000    0.0000    0.0000         0\n",
      "   Both Feet     1.0000    0.7324    0.8455       340\n",
      "\n",
      "    accuracy                         0.7324       340\n",
      "   macro avg     0.2500    0.1831    0.2114       340\n",
      "weighted avg     1.0000    0.7324    0.8455       340\n",
      "\n",
      "  Fine-tuning no ejecutado (muestras/clases insuficientes).\n",
      "\n",
      "[Fold 5/5] Entrenando (train_ev≈1139 | val_ev≈272 | test_ev≈360)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Época   1 | train_acc(ev)=0.2827 | val_acc(ev)=0.2794 | LR=0.01000\n",
      "  Época   5 | train_acc(ev)=0.2906 | val_acc(ev)=0.2794 | LR=0.00250\n",
      "  Época  10 | train_acc(ev)=0.3108 | val_acc(ev)=0.3640 | LR=0.00854\n",
      "  Época  15 | train_acc(ev)=0.3327 | val_acc(ev)=0.3860 | LR=0.00250\n",
      "  Época  20 | train_acc(ev)=0.4083 | val_acc(ev)=0.4412 | LR=0.00996\n",
      "  Época  25 | train_acc(ev)=0.4688 | val_acc(ev)=0.4743 | LR=0.00854\n",
      "  Época  30 | train_acc(ev)=0.4539 | val_acc(ev)=0.4706 | LR=0.00565\n",
      "  Época  35 | train_acc(ev)=0.4688 | val_acc(ev)=0.4706 | LR=0.00250\n",
      "  Early stopping @ 35 (mejor val_acc(ev)=0.4743)\n",
      "↳ Curva train/val guardada: training_curve_event_fold5.png\n",
      "[Fold 5/5] Global acc (por evento) = 0.2750\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.0000    0.0000    0.0000         0\n",
      "       Right     0.0000    0.0000    0.0000         0\n",
      "  Both Fists     0.8393    0.1808    0.2975       260\n",
      "   Both Feet     0.4262    0.5200    0.4685       100\n",
      "\n",
      "    accuracy                         0.2750       360\n",
      "   macro avg     0.3164    0.1752    0.1915       360\n",
      "weighted avg     0.7245    0.2750    0.3450       360\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fine-tuning (por sujeto, agrup. ev) acc=0.5750 | Δ=+0.3000 | sujetos=20\n",
      "\n",
      "============================================================\n",
      "RESULTADOS FINALES (por EVENTO)\n",
      "============================================================\n",
      "Global folds: ['0.3360', '0.4370', '0.5966', '0.7324', '0.2750']\n",
      "Global mean: 0.4754\n",
      "Fine-tune folds: ['0.5238', '0.6585', 'nan', 'nan', '0.5750']\n",
      "Fine-tune mean: 0.5858\n",
      "Δ(FT-Global) mean: +0.1104\n",
      "↳ Matriz de confusión guardada: confusion_eegnet_sliding_event_eval_allfolds.png\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# EEGNet + Sliding Windows por evento\n",
    "# Mejoras:\n",
    "#  - RandomEventSampler: 1 crop aleatorio por evento/época (combatir overfitting)\n",
    "#  - Curvas train(ev) vs val(ev) por época y guardado PNG\n",
    "#  - Tag de crops \"eval\" para que train-curve use SOLO los eval-crops (sin augs)\n",
    "#  - SpecAug (band masking) opcional\n",
    "#  - Hooks para consistency regularization entre ventanas del mismo evento (OFF por defecto)\n",
    "\n",
    "import os, re, math, random, json, itertools, copy\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "FOLDS_JSON = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "FIGS_DIR = PROJ / 'reports' / 'figs'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE); np.random.seed(RANDOM_STATE); random.seed(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Dispositivo: {DEVICE}\")\n",
    "\n",
    "FS = 160.0\n",
    "N_FOLDS = 5\n",
    "N_CLASSES = 4\n",
    "CLASS_NAMES_4C = ['Left', 'Right', 'Both Fists', 'Both Feet']\n",
    "\n",
    "# Entrenamiento global\n",
    "BATCH_SIZE    = 64\n",
    "EPOCHS_GLOBAL = 100\n",
    "LR_INIT       = 1e-2\n",
    "SGDR_T0       = 6\n",
    "SGDR_Tmult    = 2\n",
    "GLOBAL_VAL_SPLIT = 0.17\n",
    "GLOBAL_PATIENCE  = 10\n",
    "LOG_EVERY        = 5\n",
    "\n",
    "# Fine-tuning por sujeto\n",
    "DO_SUBJECT_FT = True\n",
    "CALIB_CV_FOLDS = 4\n",
    "FT_EPOCHS   = 25\n",
    "FT_BASE_LR  = 5e-5\n",
    "FT_HEAD_LR  = 1e-3\n",
    "FT_L2SP     = 1e-4\n",
    "FT_PATIENCE = 5\n",
    "FT_VAL_RATIO= 0.2\n",
    "\n",
    "# Normalización por epoch (z-score por canal)\n",
    "NORM_EPOCH_ZSCORE = True\n",
    "\n",
    "# Sujetos / Runs / Canales\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "MI_RUNS_LR = [4, 8, 12]\n",
    "MI_RUNS_OF = [6, 10, 14]\n",
    "BASELINE_RUNS_EO = [1]\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','FCz']\n",
    "\n",
    "# Sliding windows\n",
    "WINDOW_SET   = \"3s_base\"   # \"3s_base\" o \"6s_base\"\n",
    "WINDOW_POLICY= \"train_aggressive\"  # \"standard\" | \"train_precue\" | \"train_aggressive\"\n",
    "DEBUG_WINDOWS = True\n",
    "\n",
    "# Random per-event crop sampling\n",
    "RANDOM_PER_EVENT = True   # clave para combatir overfitting\n",
    "K_PER_EVENT      = 1      # 1 crop aleatorio por evento y época\n",
    "\n",
    "# SpecAug (band masking) opcional\n",
    "USE_SPECAUG = True\n",
    "SPECAUG_PROB = 0.35\n",
    "SPECAUG_BW_HZ = (2.0, 6.0)   # ancho de banda a enmascarar\n",
    "\n",
    "# Consistency regularization entre dos crops del mismo evento (apagado por defecto)\n",
    "USE_CONSISTENCY = False\n",
    "CONS_WEIGHT = 0.1\n",
    "\n",
    "# =========================\n",
    "# UTILIDADES CANALES / EDF\n",
    "# =========================\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = re.sub(r'([A-Za-z])0([0-9])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])Z$', r'\\1z', s)\n",
    "    s = s.replace('fp', 'Fp').replace('FP', 'Fp')\n",
    "    s = ''.join(ch.upper() if ch != 'z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {}\n",
    "    for ch in raw.ch_names:\n",
    "        lab = normalize_label(ch)\n",
    "        lab = lab[:-1] + 'z' if lab.endswith('Z') else lab\n",
    "        lab = re.sub(r'([A-Z])Z$', r'\\1z', lab)\n",
    "        mapping[ch] = lab\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired_channels=EXPECTED_8):\n",
    "    missing = [ch for ch in desired_channels if ch not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"[WARN] faltan canales {missing} en archivo {getattr(raw,'filenames', [''])[0]}\")\n",
    "        return None\n",
    "    raw.reorder_channels([ch for ch in raw.ch_names if ch in desired_channels] +\n",
    "                         [ch for ch in raw.ch_names if ch not in desired_channels])\n",
    "    raw.pick_channels(desired_channels, ordered=True)\n",
    "    return raw\n",
    "\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(run_id:int):\n",
    "    if run_id in MI_RUNS_LR: return 'LR'\n",
    "    if run_id in MI_RUNS_OF: return 'OF'\n",
    "    if run_id in BASELINE_RUNS_EO: return 'EO'\n",
    "    return None\n",
    "\n",
    "def read_raw_edf(path: Path):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        mont = mne.channels.make_standard_montage('standard_1020')\n",
    "        raw.set_montage(mont, on_missing='ignore')\n",
    "    except Exception:\n",
    "        pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad=\"auto\")\n",
    "    raw = ensure_channels_order(raw, EXPECTED_8)\n",
    "    return raw\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    if raw.annotations is None or len(raw.annotations) == 0:\n",
    "        return []\n",
    "    def _norm(s): return str(s).strip().upper().replace(' ', '')\n",
    "    res = []\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = _norm(desc)\n",
    "        if tag in ('T1','T2'):\n",
    "            res.append((float(onset), tag))\n",
    "    res.sort()\n",
    "    # dedup simple\n",
    "    out = []\n",
    "    last = {'T1': -1e9, 'T2': -1e9}\n",
    "    for t, tag in res:\n",
    "        if t - last[tag] >= 0.5:\n",
    "            out.append((t, tag))\n",
    "            last[tag] = t\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# DISCOVERY SUJETOS\n",
    "# =========================\n",
    "def subjects_available():\n",
    "    subs = []\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try: sid = int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir / f\"S{sid:03d}R{r:02d}.edf\").exists() for r in (MI_RUNS_LR + MI_RUNS_OF))\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "# =========================\n",
    "# LABELS / BALANCE por evento\n",
    "# =========================\n",
    "def _label_from_tag(kind: str, tag: str):\n",
    "    if kind == 'LR':\n",
    "        return 0 if tag == 'T1' else 1 if tag == 'T2' else None\n",
    "    elif kind == 'OF':\n",
    "        return 2 if tag == 'T1' else 3 if tag == 'T2' else None\n",
    "    return None\n",
    "\n",
    "def gather_all_events(subjects):\n",
    "    per_subj = {}\n",
    "    for s in tqdm(subjects, desc=\"Escaneando eventos (sin crops)\"):\n",
    "        sdir = DATA_RAW / f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "        per_subj[s] = {0:[], 1:[], 2:[], 3:[]}\n",
    "        for r in (MI_RUNS_LR + MI_RUNS_OF + BASELINE_RUNS_EO):\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            kind = run_kind(r)\n",
    "            if kind not in ('LR','OF'):\n",
    "                continue\n",
    "            raw = read_raw_edf(p)\n",
    "            if raw is None: \n",
    "                continue\n",
    "            events = collect_events_T1T2(raw)\n",
    "            for onset_sec, tag in events:\n",
    "                lab = _label_from_tag(kind, tag)\n",
    "                if lab is None: continue\n",
    "                per_subj[s][lab].append({\n",
    "                    \"path\": p,\n",
    "                    \"onset\": float(onset_sec),\n",
    "                    \"label\": int(lab),\n",
    "                    \"subject\": int(s),\n",
    "                    \"run\": int(r)\n",
    "                })\n",
    "    return per_subj\n",
    "\n",
    "def balance_events_per_subject(per_subj, need_per_class=21, rng_seed=RANDOM_STATE):\n",
    "    rng = check_random_state(rng_seed)\n",
    "    kept = {}\n",
    "    for s, cls_dict in per_subj.items():\n",
    "        kept[s] = {}\n",
    "        skip = False\n",
    "        for c in range(4):\n",
    "            events = cls_dict.get(c, [])\n",
    "            n0 = len(events)\n",
    "            if n0 == 0:\n",
    "                skip = True\n",
    "                break\n",
    "            if n0 == need_per_class:\n",
    "                sel = events\n",
    "            elif n0 > need_per_class:\n",
    "                idx = rng.choice(n0, size=need_per_class, replace=False)\n",
    "                sel = [events[i] for i in idx]\n",
    "            else:\n",
    "                idx = rng.choice(n0, size=need_per_class, replace=True)\n",
    "                sel = [events[i] for i in idx]\n",
    "            kept[s][c] = sel\n",
    "        if skip:\n",
    "            kept[s] = None\n",
    "            print(f\"[SKIP] S{s:03d} (alguna clase sin eventos)\")\n",
    "    return {s:cls for s,cls in kept.items() if cls is not None}\n",
    "\n",
    "# =========================\n",
    "# SLIDING WINDOWS\n",
    "# =========================\n",
    "def base_windows(set_name=\"3s_base\"):\n",
    "    if set_name == \"3s_base\":\n",
    "        return [(0.0, 3.0), (0.5, 3.5), (1.0, 4.0)]\n",
    "    elif set_name == \"6s_base\":\n",
    "        return [(-1.0, 5.0), (-0.5, 5.5), (0.0, 6.0)]\n",
    "    else:\n",
    "        raise ValueError(set_name)\n",
    "\n",
    "def get_crops_for_policy(window_set=\"3s_base\", policy=\"standard\"):\n",
    "    crops_eval = base_windows(window_set)\n",
    "    if policy == \"standard\":\n",
    "        crops_train = crops_eval\n",
    "    elif policy == \"train_precue\":\n",
    "        crops_train = [(-0.5, 2.5)] + crops_eval if window_set == \"3s_base\" else [(-1.5, 4.5)] + crops_eval\n",
    "    elif policy == \"train_aggressive\":\n",
    "        if window_set == \"3s_base\":\n",
    "            starts = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "            crops_train = [(s, s+3.0) for s in starts]\n",
    "        else:\n",
    "            crops_train = crops_eval\n",
    "    else:\n",
    "        raise ValueError(policy)\n",
    "    return crops_train, crops_eval\n",
    "\n",
    "CROPS_TRAIN, CROPS_EVAL = get_crops_for_policy(WINDOW_SET, WINDOW_POLICY)\n",
    "print(f\"🎯 Sliding windows → TRAIN={CROPS_TRAIN} | EVAL={CROPS_EVAL}\")\n",
    "\n",
    "# =========================\n",
    "# CROPS (con etiqueta is_eval) + event_id\n",
    "# =========================\n",
    "def segments_from_balanced_events_with_flags(kept_balanced, crops_train, crops_eval, fs=FS):\n",
    "    \"\"\"\n",
    "    Genera TODOS los crops de crops_train ∪ crops_eval y marca cuáles son de eval.\n",
    "    \"\"\"\n",
    "    all_crops = list({c for c in (crops_train + crops_eval)})  # unión sin duplicados\n",
    "    X, y, groups, eids, is_eval = [], [], [], [], []\n",
    "    ch_template = None\n",
    "    cache_raw = {}\n",
    "\n",
    "    dbg_total_events = 0\n",
    "    dbg_total_crops_ok = 0\n",
    "    dbg_skips_oob = dbg_skips_short = dbg_skips_nan = 0\n",
    "\n",
    "    next_event_id = 0\n",
    "    try:\n",
    "        for s, cls_dict in tqdm(kept_balanced.items(), desc=\"Generando segmentos (train∪eval crops)\"):\n",
    "            for c in range(4):\n",
    "                for ev in cls_dict[c]:\n",
    "                    dbg_total_events += 1\n",
    "                    p = ev[\"path\"]; onset = ev[\"onset\"]; sid = ev[\"subject\"]; rid = ev[\"run\"]\n",
    "                    if p not in cache_raw:\n",
    "                        raw = read_raw_edf(p)\n",
    "                        if raw is None: continue\n",
    "                        cache_raw[p] = raw\n",
    "                    raw = cache_raw[p]\n",
    "                    data = raw.get_data()\n",
    "                    T_total = data.shape[1]\n",
    "                    if ch_template is None:\n",
    "                        ch_template = raw.ch_names\n",
    "                    eid = next_event_id; next_event_id += 1\n",
    "\n",
    "                    for rel_start, rel_end in all_crops:\n",
    "                        s_samp = int(round((raw.first_time + onset + rel_start) * fs))\n",
    "                        e_samp = int(round((raw.first_time + onset + rel_end)   * fs))\n",
    "                        exp_len = int(round((rel_end - rel_start) * fs))\n",
    "                        if s_samp < 0 or e_samp > T_total:\n",
    "                            dbg_skips_oob += 1; continue\n",
    "                        seg = data[:, s_samp:e_samp].T.astype(np.float32)\n",
    "                        if seg.shape[0] != exp_len or seg.shape[0] <= 0:\n",
    "                            dbg_skips_short += 1; continue\n",
    "                        if not np.isfinite(seg).all():\n",
    "                            dbg_skips_nan += 1; continue\n",
    "\n",
    "                        if NORM_EPOCH_ZSCORE:\n",
    "                            seg = (seg - seg.mean(axis=0, keepdims=True)) / (seg.std(axis=0, keepdims=True) + 1e-6)\n",
    "\n",
    "                        X.append(seg); y.append(c); groups.append(s); eids.append(eid)\n",
    "                        is_eval.append(1 if (rel_start, rel_end) in crops_eval else 0)\n",
    "                        dbg_total_crops_ok += 1\n",
    "    finally:\n",
    "        cache_raw.clear()\n",
    "\n",
    "    if DEBUG_WINDOWS:\n",
    "        print(\"=== DEBUG WINDOWS SUMMARY ===\")\n",
    "        print(f\"Eventos brutos: {dbg_total_events}\")\n",
    "        print(f\"Crops OK:      {dbg_total_crops_ok}\")\n",
    "        print(f\"Skips OOB:     {dbg_skips_oob}\")\n",
    "        print(f\"Skips LEN:     {dbg_skips_short}\")\n",
    "        print(f\"Skips NaN:     {dbg_skips_nan}\")\n",
    "\n",
    "    X = np.stack(X, axis=0) if len(X)>0 else np.zeros((0,1,1), np.float32)\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "    groups = np.asarray(groups, dtype=np.int64)\n",
    "    eids = np.asarray(eids, dtype=np.int64)\n",
    "    is_eval = np.asarray(is_eval, dtype=np.int8)\n",
    "\n",
    "    n, T, C = X.shape\n",
    "    print(f\"Dataset construido: N={n} | T={T} | C={C} | sujetos únicos={len(np.unique(groups))} | eventos únicos={len(np.unique(eids))}\")\n",
    "    return X, y, groups, eids, is_eval, ch_template\n",
    "\n",
    "# =========================\n",
    "# DATASET / SAMPLERS\n",
    "# =========================\n",
    "class EEGTrials(Dataset):\n",
    "    def __init__(self, X, y, groups, eids, is_eval):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.g = groups.astype(np.int64)\n",
    "        self.e = eids.astype(np.int64)\n",
    "        self.is_eval = is_eval.astype(np.int8)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]  # (T,C)\n",
    "        x = np.expand_dims(x, 0)  # (1,T,C)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx]), torch.tensor(self.e[idx]), torch.tensor(self.is_eval[idx])\n",
    "\n",
    "class RandomEventSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Devuelve exactamente K_PER_EVENT índices por event_id en cada época (aleatorios).\n",
    "    Útil para no sobre-contar eventos cuando hay muchos crops por evento.\n",
    "    \"\"\"\n",
    "    def __init__(self, eids, is_train_mask, k_per_event=1, generator=None):\n",
    "        self.eids = np.asarray(eids)\n",
    "        self.is_train = np.asarray(is_train_mask).astype(bool)\n",
    "        self.k = int(k_per_event)\n",
    "        self.gen = generator\n",
    "        # agrupar indices de train por eid\n",
    "        self.by_e = {}\n",
    "        for i, (eid, flag) in enumerate(zip(self.eids, self.is_train)):\n",
    "            if not flag: continue\n",
    "            self.by_e.setdefault(int(eid), []).append(i)\n",
    "        self.events = sorted(self.by_e.keys())\n",
    "        self.num_samples = len(self.events) * self.k\n",
    "    def __len__(self): return self.num_samples\n",
    "    def __iter__(self):\n",
    "        g = torch.Generator()\n",
    "        if self.gen is not None: g = self.gen\n",
    "        for eid in self.events:\n",
    "            idxs = self.by_e[eid]\n",
    "            if len(idxs) <= self.k:\n",
    "                choice = np.random.choice(idxs, size=self.k, replace=True)\n",
    "            else:\n",
    "                choice = np.random.choice(idxs, size=self.k, replace=False)\n",
    "            for c in choice:\n",
    "                yield int(c)\n",
    "\n",
    "# =========================\n",
    "# AUGMENTS\n",
    "# =========================\n",
    "def do_time_jitter(x, max_ms=50, fs=160.0):\n",
    "    B,_,T,C = x.shape\n",
    "    max_shift = int(round(max_ms/1000.0 * fs))\n",
    "    if max_shift <= 0: return x\n",
    "    shifts = torch.randint(low=-max_shift, high=max_shift+1, size=(B,), device=x.device)\n",
    "    out = torch.empty_like(x)\n",
    "    for i,s in enumerate(shifts):\n",
    "        if s==0: out[i] = x[i]; continue\n",
    "        if s>0:\n",
    "            out[i,:,s:,:] = x[i,:,:T-s,:]\n",
    "            out[i,:,:s,:] = 0\n",
    "        else:\n",
    "            s = -s\n",
    "            out[i,:,:T-s,:] = x[i,:,s:,:]\n",
    "            out[i,:,T-s:,:] = 0\n",
    "    return out\n",
    "\n",
    "def do_gaussian_noise(x, sigma=0.01):\n",
    "    if sigma<=0: return x\n",
    "    return x + sigma*torch.randn_like(x)\n",
    "\n",
    "def do_temporal_cutout_masked(x, y, classes_mask={2,3}, min_ms=30, max_ms=60, fs=160.0):\n",
    "    B,_,T,C = x.shape\n",
    "    Lmin = int(round(min_ms/1000.0*fs))\n",
    "    Lmax = int(round(max_ms/1000.0*fs))\n",
    "    if Lmin<=0 or Lmax<=0 or Lmin>Lmax: return x\n",
    "    out = x.clone()\n",
    "    y_np = y.detach().cpu().numpy()\n",
    "    for i in range(B):\n",
    "        if int(y_np[i]) not in classes_mask:\n",
    "            continue\n",
    "        L = random.randint(Lmin, Lmax)\n",
    "        if L>=T: continue\n",
    "        s = random.randint(0, T-L)\n",
    "        out[i,:,s:s+L,:] = 0\n",
    "    return out\n",
    "\n",
    "def specaug_band_mask(x, fs=160.0, prob=0.35, bw_hz=(2.0,6.0)):\n",
    "    \"\"\"Enmascara una banda de frecuencia con filtro notch pobre (en tiempo) → aproximación barata.\"\"\"\n",
    "    if prob <= 0: return x\n",
    "    if random.random() > prob: return x\n",
    "    B,_,T,C = x.shape\n",
    "    # simula una banda bloqueada multiplicando por envolvente sinusoidal (barato, evita FFT)\n",
    "    bw = random.uniform(bw_hz[0], bw_hz[1])  # ancho\n",
    "    f0 = random.uniform(6.0, 30.0-bw)        # centro\n",
    "    t = torch.linspace(0, (T-1)/fs, T, device=x.device).view(1,1,T,1)\n",
    "    w1 = 2*math.pi*(f0 - bw/2.0); w2 = 2*math.pi*(f0 + bw/2.0)\n",
    "    mask = 1.0 - 0.35*(torch.sin(w1*t)**2 + torch.sin(w2*t)**2)/2.0\n",
    "    return x * mask\n",
    "\n",
    "def mixup_batch(x, y, n_classes, alpha=0.2):\n",
    "    if alpha<=0:\n",
    "        y_onehot = torch.nn.functional.one_hot(y, num_classes=n_classes).float()\n",
    "        return x, y_onehot, 1.0\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    perm = torch.randperm(x.size(0), device=x.device)\n",
    "    x_mix = lam*x + (1-lam)*x[perm]\n",
    "    y_a = torch.nn.functional.one_hot(y, num_classes=n_classes).float()\n",
    "    y_b = y_a[perm]\n",
    "    y_mix = lam*y_a + (1-lam)*y_b\n",
    "    return x_mix, y_mix, lam\n",
    "\n",
    "class WeightedSoftCrossEntropy(nn.Module):\n",
    "    def __init__(self, class_weights=None, label_smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.register_buffer('w', None if class_weights is None else class_weights.clone().float())\n",
    "        self.ls = float(label_smoothing)\n",
    "    def forward(self, logits, target_probs):\n",
    "        if self.ls > 0:\n",
    "            K = logits.size(1)\n",
    "            target_probs = (1-self.ls)*target_probs + self.ls*(1.0/K)\n",
    "        logp = torch.log_softmax(logits, dim=1)\n",
    "        loss_per_class = -(target_probs * logp)\n",
    "        if self.w is not None:\n",
    "            loss_per_class = loss_per_class * self.w.unsqueeze(0)\n",
    "        loss = loss_per_class.sum(dim=1).mean()\n",
    "        return loss\n",
    "\n",
    "# =========================\n",
    "# EEGNet\n",
    "# =========================\n",
    "class ChannelDropout(nn.Module):\n",
    "    def __init__(self, p=0.1): super().__init__(); self.p = p\n",
    "    def forward(self, x):\n",
    "        if not self.training or self.p<=0: return x\n",
    "        B,_,T,C = x.shape\n",
    "        mask = (torch.rand(B,1,1,C, device=x.device) > self.p).float()\n",
    "        return x * mask\n",
    "\n",
    "@torch.no_grad()\n",
    "def apply_max_norm(model, max_value=2.0, p=2.0):\n",
    "    layers = []\n",
    "    for name in ['conv_depthwise','conv_sep_point','fc','out']:\n",
    "        if hasattr(model, name): layers.append(getattr(model, name))\n",
    "    for layer in layers:\n",
    "        if hasattr(layer, 'weight') and layer.weight is not None:\n",
    "            w = layer.weight.data\n",
    "            norms = w.view(w.size(0), -1).norm(p=p, dim=1, keepdim=True)\n",
    "            desired = torch.clamp(norms, max=max_value)\n",
    "            w.view(w.size(0), -1).mul_(desired / (1e-8 + norms))\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, n_ch: int, n_classes: int,\n",
    "                 F1: int = 24, D: int = 2, kernel_t: int = 64, k_sep: int = 16,\n",
    "                 pool1_t: int = 4, pool2_t: int = 6,\n",
    "                 drop1_p: float = 0.35, drop2_p: float = 0.6,\n",
    "                 chdrop_p: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.n_ch = n_ch; self.n_classes = n_classes\n",
    "        self.F1 = F1; self.D = D; self.F2 = F1 * D\n",
    "        self.kernel_t = kernel_t; self.k_sep = k_sep\n",
    "        self.pool1_t = pool1_t; self.pool2_t = pool2_t\n",
    "        self.chdrop = ChannelDropout(p=chdrop_p)\n",
    "\n",
    "        self.conv_temporal = nn.Conv2d(1, F1, kernel_size=(kernel_t, 1),\n",
    "                                       padding=(kernel_t // 2, 0), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(F1, momentum=0.99, eps=1e-3)\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "        self.conv_depthwise = nn.Conv2d(F1, self.F2, kernel_size=(1, n_ch),\n",
    "                                        groups=F1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.F2, momentum=0.99, eps=1e-3)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=(pool1_t, 1), stride=(pool1_t, 1))\n",
    "        self.drop1 = nn.Dropout(drop1_p)\n",
    "\n",
    "        self.conv_sep_depth = nn.Conv2d(self.F2, self.F2, kernel_size=(k_sep, 1),\n",
    "                                        groups=self.F2, padding=(k_sep // 2, 0), bias=False)\n",
    "        self.conv_sep_point = nn.Conv2d(self.F2, self.F2, kernel_size=(1, 1), bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.F2, momentum=0.99, eps=1e-3)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(pool2_t, 1), stride=(pool2_t, 1))\n",
    "        self.drop2 = nn.Dropout(drop2_p)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc  = None; self.out = None; self._T_in = None\n",
    "\n",
    "    def _build_head(self, T_in: int, device: torch.device):\n",
    "        T1 = T_in // self.pool1_t\n",
    "        T2 = T1 // self.pool2_t\n",
    "        feat_dim = self.F2 * T2 * 1\n",
    "        self.fc  = nn.Linear(feat_dim, 128, bias=True).to(device)\n",
    "        self.out = nn.Linear(128, self.n_classes, bias=True).to(device)\n",
    "        self._T_in = T_in\n",
    "\n",
    "    def ensure_head(self, T_in: int, device: torch.device):\n",
    "        if (self.fc is None) or (self.out is None) or (self._T_in != T_in):\n",
    "            self._build_head(T_in, device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, _, T, C = x.shape\n",
    "        self.ensure_head(T, x.device)\n",
    "        x = self.chdrop(x)\n",
    "        z = self.conv_temporal(x); z = self.bn1(z); z = self.act(z)\n",
    "        z = self.conv_depthwise(z); z = self.bn2(z); z = self.act(z)\n",
    "        z = self.pool1(z); z = self.drop1(z)\n",
    "        z = self.conv_sep_depth(z); z = self.conv_sep_point(z)\n",
    "        z = self.bn3(z); z = self.act(z)\n",
    "        z = self.pool2(z); z = self.drop2(z)\n",
    "        z = self.flatten(z)\n",
    "        z = self.fc(z); z = self.act(z)\n",
    "        z = self.out(z)\n",
    "        return z\n",
    "\n",
    "# =========================\n",
    "# PESOS CLASE\n",
    "# =========================\n",
    "def make_class_weight_tensor(y_indices, n_classes, boost_bfists=1.20):\n",
    "    counts = np.bincount(y_indices, minlength=n_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    w = counts.sum() / counts\n",
    "    w = w / w.mean()\n",
    "    w[2] *= boost_bfists\n",
    "    w = w / w.mean()\n",
    "    return torch.tensor(w, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "# =========================\n",
    "# EVAL AGRUPADA por EVENTO\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_grouped_by_event(model, loader, eids_full, use_tta_jitter=True, tta_jitter_n=3):\n",
    "    model.eval()\n",
    "    all_logits, all_eids, y_all = [], [], []\n",
    "    idx_offset = 0\n",
    "    for xb, yb, _, _, _ in loader:\n",
    "        B = xb.size(0)\n",
    "        xb = xb.to(DEVICE)\n",
    "        if use_tta_jitter:\n",
    "            outs = []\n",
    "            for _ in range(tta_jitter_n):\n",
    "                outs.append(model(do_time_jitter(xb, max_ms=20, fs=FS)))\n",
    "            logits = torch.stack(outs, dim=0).mean(dim=0)\n",
    "        else:\n",
    "            logits = model(xb)\n",
    "        all_logits.append(logits.cpu().numpy())\n",
    "        all_eids.append(eids_full[idx_offset: idx_offset + B].copy())\n",
    "        y_all.append(yb.numpy())\n",
    "        idx_offset += B\n",
    "    all_logits = np.concatenate(all_logits, axis=0)\n",
    "    all_eids   = np.concatenate(all_eids, axis=0)\n",
    "    y_all      = np.concatenate(y_all, axis=0)\n",
    "\n",
    "    uniq_eids = np.unique(all_eids)\n",
    "    logits_ev, ytrue_ev = [], []\n",
    "    for eid in uniq_eids:\n",
    "        idx = (all_eids == eid)\n",
    "        logits_ev.append(all_logits[idx].mean(axis=0))\n",
    "        ytrue_ev.append(int(y_all[idx][0]))\n",
    "    logits_ev = np.stack(logits_ev, axis=0)\n",
    "    ytrue_ev  = np.asarray(ytrue_ev, dtype=np.int64)\n",
    "    ypred_ev  = logits_ev.argmax(axis=1)\n",
    "    acc = float((ypred_ev == ytrue_ev).mean())\n",
    "    return ytrue_ev, ypred_ev, acc\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes, title, fname):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.nan_to_num(cm_norm)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title); plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    ticks = np.arange(len(classes))\n",
    "    plt.xticks(ticks, classes, rotation=45, ha='right'); plt.yticks(ticks, classes)\n",
    "    fmt = '.2f'; thresh = cm_norm.max()/2.\n",
    "    for i in range(cm_norm.shape[0]),:\n",
    "        pass\n",
    "    for i in range(cm_norm.shape[0]):\n",
    "        for j in range(cm_norm.shape[1]):\n",
    "            plt.text(j, i, format(cm_norm[i,j], '.2f'), ha=\"center\",\n",
    "                     color=\"white\" if cm_norm[i,j] > thresh else \"black\")\n",
    "    plt.ylabel('True'); plt.xlabel('Pred')\n",
    "    plt.tight_layout(); plt.savefig(fname, dpi=150, bbox_inches='tight'); plt.close()\n",
    "\n",
    "def print_report(y_true, y_pred, classes):\n",
    "    print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
    "\n",
    "# =========================\n",
    "# TRAIN LOOP\n",
    "# =========================\n",
    "def train_epoch(model, loader, opt, criterion_soft, n_classes):\n",
    "    model.train()\n",
    "    for xb, yb, _, _, _ in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        xb = do_time_jitter(xb, max_ms=50, fs=FS)\n",
    "        xb = do_gaussian_noise(xb, sigma=0.01)\n",
    "        xb = do_temporal_cutout_masked(xb, yb, classes_mask={2,3}, min_ms=30, max_ms=60, fs=FS)\n",
    "        if USE_SPECAUG:\n",
    "            xb = specaug_band_mask(xb, fs=FS, prob=SPECAUG_PROB, bw_hz=SPECAUG_BW_HZ)\n",
    "        xb, yt, _ = mixup_batch(xb, yb, n_classes=n_classes, alpha=0.2)\n",
    "\n",
    "        logits = model(xb)\n",
    "        loss = criterion_soft(logits, yt)\n",
    "\n",
    "        # (opcional) consistencia entre dos views del mismo batch\n",
    "        if USE_CONSISTENCY:\n",
    "            with torch.no_grad():\n",
    "                xb2 = do_time_jitter(xb.clone(), max_ms=35, fs=FS)\n",
    "            logits2 = model(xb2)\n",
    "            p1 = torch.log_softmax(logits, dim=1)\n",
    "            p2 = torch.softmax(logits2, dim=1)\n",
    "            kl = torch.sum(torch.exp(p1) * (p1 - torch.log(p2 + 1e-8)), dim=1).mean()\n",
    "            loss = loss + CONS_WEIGHT * kl\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        apply_max_norm(model, max_value=2.0, p=2.0)\n",
    "\n",
    "# =========================\n",
    "# FT por sujeto\n",
    "# =========================\n",
    "def _freeze_for_ft(model):\n",
    "    for p in model.parameters(): p.requires_grad = False\n",
    "    for p in model.conv_depthwise.parameters(): p.requires_grad = True\n",
    "    for p in model.bn2.parameters():           p.requires_grad = True\n",
    "    for p in model.conv_sep_depth.parameters():p.requires_grad = True\n",
    "    for p in model.conv_sep_point.parameters():p.requires_grad = True\n",
    "    for p in model.bn3.parameters():           p.requires_grad = True\n",
    "    for p in model.fc.parameters():            p.requires_grad = True\n",
    "    for p in model.out.parameters():           p.requires_grad = True\n",
    "\n",
    "def subject_cv_finetune_predict_grouped(model_global, Xs, ys, eids, n_splits=CALIB_CV_FOLDS, n_classes=4):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    y_true_all, y_pred_all = [], []\n",
    "    for tr_idx, te_idx in skf.split(Xs, ys):\n",
    "        Xcal, ycal, ecal = Xs[tr_idx], ys[tr_idx], eids[tr_idx]\n",
    "        Xho,  yho,  eho  = Xs[te_idx], ys[te_idx], eids[te_idx]\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=FT_VAL_RATIO, random_state=RANDOM_STATE)\n",
    "        (tr2, va2), = sss.split(Xcal, ycal)\n",
    "        Xtr, ytr = Xcal[tr2], ycal[tr2]\n",
    "        Xva, yva = Xcal[va2], ycal[va2]\n",
    "\n",
    "        tr_ds = torch.utils.data.TensorDataset(torch.from_numpy(Xtr).float().unsqueeze(1),\n",
    "                                               torch.from_numpy(ytr).long())\n",
    "        va_ds = torch.utils.data.TensorDataset(torch.from_numpy(Xva).float().unsqueeze(1),\n",
    "                                               torch.from_numpy(yva).long())\n",
    "        te_ds = torch.utils.data.TensorDataset(torch.from_numpy(Xho).float().unsqueeze(1),\n",
    "                                               torch.from_numpy(yho).long())\n",
    "        tr_dl = DataLoader(tr_ds, batch_size=32, shuffle=True)\n",
    "        va_dl = DataLoader(va_ds, batch_size=64, shuffle=False)\n",
    "        te_dl = DataLoader(te_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "        m = copy.deepcopy(model_global).to(DEVICE)\n",
    "        _freeze_for_ft(m)\n",
    "\n",
    "        base_params = (list(m.conv_depthwise.parameters()) +\n",
    "                       list(m.bn2.parameters()) +\n",
    "                       list(m.conv_sep_depth.parameters()) +\n",
    "                       list(m.conv_sep_point.parameters()) +\n",
    "                       list(m.bn3.parameters()))\n",
    "        head_params = list(m.fc.parameters()) + list(m.out.parameters())\n",
    "\n",
    "        opt = optim.Adam([\n",
    "            {\"params\": base_params, \"lr\": FT_BASE_LR},\n",
    "            {\"params\": head_params, \"lr\": FT_HEAD_LR},\n",
    "        ])\n",
    "        ref = [p.detach().clone() for p in base_params + head_params]\n",
    "        class_w = make_class_weight_tensor(ytr, n_classes, boost_bfists=1.0)\n",
    "        crit = nn.CrossEntropyLoss(weight=class_w)\n",
    "\n",
    "        best_state = copy.deepcopy(m.state_dict())\n",
    "        best_val = float('inf'); bad = 0\n",
    "\n",
    "        for _ in range(FT_EPOCHS):\n",
    "            m.train()\n",
    "            for xb, yb in tr_dl:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                logits = m(xb)\n",
    "                loss = crit(logits, yb)\n",
    "                reg = 0.0\n",
    "                tr_params = base_params + head_params\n",
    "                for p_cur, p_ref in zip(tr_params, ref):\n",
    "                    reg = reg + torch.sum((p_cur - p_ref)**2)\n",
    "                loss = loss + FT_L2SP * reg\n",
    "                loss.backward(); opt.step()\n",
    "                apply_max_norm(m, max_value=2.0, p=2.0)\n",
    "\n",
    "            # val\n",
    "            m.eval()\n",
    "            val_loss = 0.0; n = 0\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in va_dl:\n",
    "                    xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                    logits = m(xb)\n",
    "                    loss = crit(logits, yb)\n",
    "                    val_loss += loss.item()*xb.size(0); n += xb.size(0)\n",
    "            val_loss /= max(1,n)\n",
    "            if val_loss + 1e-7 < best_val:\n",
    "                best_val = val_loss; bad = 0\n",
    "                best_state = copy.deepcopy(m.state_dict())\n",
    "            else:\n",
    "                bad += 1\n",
    "                if bad >= FT_PATIENCE: break\n",
    "\n",
    "        m.load_state_dict(best_state)\n",
    "\n",
    "        # pred en HOLD-OUT del sujeto, agrupado por event_id\n",
    "        m.eval()\n",
    "        logits_all = []\n",
    "        with torch.no_grad():\n",
    "            for xb, _ in te_dl:\n",
    "                xb = xb.to(DEVICE)\n",
    "                logits_all.append(m(xb).cpu().numpy())\n",
    "        logits_all = np.concatenate(logits_all, axis=0)\n",
    "        uniq = np.unique(eho)\n",
    "        for eid in uniq:\n",
    "            idx = (eho == eid)\n",
    "            log_ev = logits_all[idx].mean(axis=0)\n",
    "            y_true_all.append(int(yho[idx][0]))\n",
    "            y_pred_all.append(int(np.argmax(log_ev)))\n",
    "    return np.asarray(y_true_all, dtype=np.int64), np.asarray(y_pred_all, dtype=np.int64)\n",
    "\n",
    "# =========================\n",
    "# FOLDS JSON helpers\n",
    "# =========================\n",
    "def save_group_folds_json_with_indices(subject_ids_str, groups_array, n_splits, out_json_path,\n",
    "                                       created_by=\"EEGNet_Sliding+\", description=None):\n",
    "    out_json_path = Path(out_json_path)\n",
    "    unique_subjects_int = sorted(np.unique(groups_array).tolist())\n",
    "    subject_ids = [f\"S{sid:03d}\" for sid in unique_subjects_int]\n",
    "    if len(subject_ids) < n_splits:\n",
    "        raise ValueError(f\"n_splits={n_splits} mayor que número de sujetos={len(subject_ids)}\")\n",
    "    groups = np.arange(len(subject_ids))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    folds = []\n",
    "    fold_i = 0\n",
    "    for train_idx_grp, test_idx_grp in gkf.split(groups, groups, groups):\n",
    "        fold_i += 1\n",
    "        train_sids = [subject_ids[int(i)] for i in train_idx_grp]\n",
    "        test_sids  = [subject_ids[int(i)] for i in test_idx_grp]\n",
    "        train_sids_int = [int(s[1:]) for s in train_sids]\n",
    "        test_sids_int  = [int(s[1:]) for s in test_sids]\n",
    "        tr_idx = np.where(np.isin(groups_array, train_sids_int))[0].tolist()\n",
    "        te_idx = np.where(np.isin(groups_array, test_sids_int))[0].tolist()\n",
    "        folds.append({\n",
    "            \"fold\": int(fold_i),\n",
    "            \"train\": train_sids,\n",
    "            \"test\": test_sids,\n",
    "            \"tr_idx\": tr_idx,\n",
    "            \"te_idx\": te_idx\n",
    "        })\n",
    "    payload = {\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"created_by\": created_by,\n",
    "        \"description\": description if description is not None else \"\",\n",
    "        \"n_splits\": int(n_splits),\n",
    "        \"n_subjects\": len(subject_ids),\n",
    "        \"subject_ids\": subject_ids,\n",
    "        \"folds\": folds\n",
    "    }\n",
    "    out_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Folds JSON con índices guardado → {out_json_path}\")\n",
    "    return out_json_path\n",
    "\n",
    "def load_group_folds_json(path_json, expected_subject_ids=None, strict_check=True):\n",
    "    path_json = Path(path_json)\n",
    "    if not path_json.exists():\n",
    "        raise FileNotFoundError(f\"No existe {path_json}\")\n",
    "    with open(path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        payload = json.load(f)\n",
    "    subj_json = payload.get(\"subject_ids\", [])\n",
    "    if expected_subject_ids is not None:\n",
    "        expected = sorted(list(expected_subject_ids))\n",
    "        if subj_json != expected:\n",
    "            msg = (\"Los subject_ids del JSON no coinciden con expected_subject_ids.\\n\"\n",
    "                   f\"JSON has {len(subj_json)} subjects, expected {len(expected)}.\\n\"\n",
    "                   f\"First 10 JSON: {subj_json[:10]}\\nFirst 10 expected: {expected[:10]}\")\n",
    "            if strict_check:\n",
    "                raise ValueError(msg)\n",
    "            else:\n",
    "                print(\"WARNING: \" + msg)\n",
    "    return payload\n",
    "\n",
    "# =========================\n",
    "# EXPERIMENTO\n",
    "# =========================\n",
    "def run_experiment():\n",
    "    mne.set_log_level('WARNING')\n",
    "\n",
    "    subs = subjects_available()\n",
    "    print(f\"Sujetos elegibles: {len(subs)}\")\n",
    "\n",
    "    per_subj = gather_all_events(subs)\n",
    "    kept_bal = balance_events_per_subject(per_subj, need_per_class=21, rng_seed=RANDOM_STATE)\n",
    "\n",
    "    X, y, groups, eids, is_eval, chs = segments_from_balanced_events_with_flags(\n",
    "        kept_bal, crops_train=CROPS_TRAIN, crops_eval=CROPS_EVAL, fs=FS\n",
    "    )\n",
    "    N, T, C = X.shape\n",
    "    print(f\"Listo para entrenar: N={N} | T={T} | C={C} | clases={len(np.unique(y))} | sujetos={len(np.unique(groups))}\")\n",
    "\n",
    "    # Folds\n",
    "    FOLDS_JSON.parent.mkdir(parents=True, exist_ok=True)\n",
    "    unique_subs = sorted(np.unique(groups).tolist())\n",
    "    subject_ids_str = [f\"S{s:03d}\" for s in unique_subs]\n",
    "    if not FOLDS_JSON.exists():\n",
    "        save_group_folds_json_with_indices(subject_ids_str, groups, n_splits=N_FOLDS,\n",
    "                                           out_json_path=FOLDS_JSON,\n",
    "                                           created_by=\"EEGNet_Sliding+\",\n",
    "                                           description=\"RandomEventSampler + eval-crops\")\n",
    "    payload = load_group_folds_json(FOLDS_JSON, expected_subject_ids=subject_ids_str, strict_check=False)\n",
    "    folds = payload[\"folds\"]\n",
    "\n",
    "    global_folds = []\n",
    "    ft_folds = []\n",
    "    all_true_ev = []\n",
    "    all_pred_ev = []\n",
    "\n",
    "    for f in folds:\n",
    "        fold = f[\"fold\"]\n",
    "        tr_idx = np.asarray(f.get(\"tr_idx\", []), dtype=int)\n",
    "        te_idx = np.asarray(f.get(\"te_idx\", []), dtype=int)\n",
    "        # split val por sujeto\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=GLOBAL_VAL_SPLIT, random_state=RANDOM_STATE)\n",
    "        tr_subj_idx, va_subj_idx = next(gss.split(tr_idx, groups[tr_idx], groups[tr_idx]))\n",
    "        tr_sub_idx = tr_idx[tr_subj_idx]\n",
    "        va_idx     = tr_idx[va_subj_idx]\n",
    "\n",
    "        # Máscaras para loaders:\n",
    "        # - TRAIN: todos los crops de los sujetos de train (is_eval==0 si policy produce más crops; da igual).\n",
    "        #   pero sampler RandomEventSampler cogerá 1 crop random por eid/época\n",
    "        train_mask = np.isin(np.arange(N), tr_sub_idx)\n",
    "        val_mask   = np.isin(np.arange(N), va_idx)\n",
    "        test_mask  = np.isin(np.arange(N), te_idx)\n",
    "\n",
    "        # DataSets (comparten backing arrays)\n",
    "        ds = EEGTrials(X, y, groups, eids, is_eval)\n",
    "\n",
    "        # Sampler: uno por evento (sobre train_mask)\n",
    "        if RANDOM_PER_EVENT:\n",
    "            sampler = RandomEventSampler(eids, is_train_mask=train_mask, k_per_event=K_PER_EVENT)\n",
    "            tr_loader = DataLoader(ds, batch_size=BATCH_SIZE, sampler=sampler, drop_last=False)\n",
    "        else:\n",
    "            idx_tr = np.where(train_mask)[0]\n",
    "            tr_loader = DataLoader(torch.utils.data.Subset(ds, idx_tr),\n",
    "                                   batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "\n",
    "        # VALIDACIÓN y TEST → usar SOLO crops marcados como eval (is_eval==1)\n",
    "        idx_va = np.where(val_mask & (is_eval==1))[0]\n",
    "        idx_te = np.where(test_mask & (is_eval==1))[0]\n",
    "        va_loader = DataLoader(torch.utils.data.Subset(ds, idx_va), batch_size=BATCH_SIZE, shuffle=False)\n",
    "        te_loader = DataLoader(torch.utils.data.Subset(ds, idx_te), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        # TRAIN-EVAL (para curva): también usar SOLO eval-crops de TRAIN (sin augs)\n",
    "        idx_tr_eval = np.where(train_mask & (is_eval==1))[0]\n",
    "        tr_eval_loader = DataLoader(torch.utils.data.Subset(ds, idx_tr_eval), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        # Modelo + Opt\n",
    "        model = EEGNet(n_ch=C, n_classes=N_CLASSES,\n",
    "                       F1=24, D=2, kernel_t=64, k_sep=16,\n",
    "                       pool1_t=4, pool2_t=6, drop1_p=0.35, drop2_p=0.6,\n",
    "                       chdrop_p=0.1).to(DEVICE)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_INIT)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=SGDR_T0, T_mult=SGDR_Tmult)\n",
    "\n",
    "        class_weights = make_class_weight_tensor(y[tr_sub_idx], N_CLASSES, boost_bfists=1.20)\n",
    "        criterion_soft = WeightedSoftCrossEntropy(class_weights, label_smoothing=0.05)\n",
    "\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        best_val_ev_acc = -1.0\n",
    "        bad = 0\n",
    "\n",
    "        # Curvas\n",
    "        hist_train_ev = []\n",
    "        hist_val_ev   = []\n",
    "        hist_lr       = []\n",
    "\n",
    "        print(f\"\\n[Fold {fold}/{N_FOLDS}] Entrenando (train_ev≈{len(np.unique(eids[tr_sub_idx]))} | val_ev≈{len(np.unique(eids[va_idx]))} | test_ev≈{len(np.unique(eids[te_idx]))})\")\n",
    "        for epoch in range(1, EPOCHS_GLOBAL+1):\n",
    "            train_epoch(model, tr_loader, opt, criterion_soft, n_classes=N_CLASSES)\n",
    "            scheduler.step(epoch-1 + 1e-8)\n",
    "\n",
    "            # Train-EVAL (por evento, sin augs, usando eval-crops del split de TRAIN)\n",
    "            y_tr_ev_t, y_tr_ev_p, tr_acc_ev = evaluate_grouped_by_event(\n",
    "                model, tr_eval_loader, eids_full=eids[idx_tr_eval], use_tta_jitter=False, tta_jitter_n=1\n",
    "            )\n",
    "            # Val-EVAL (por evento)\n",
    "            y_va_ev_t, y_va_ev_p, va_acc_ev = evaluate_grouped_by_event(\n",
    "                model, va_loader, eids_full=eids[idx_va], use_tta_jitter=True, tta_jitter_n=3\n",
    "            )\n",
    "            hist_train_ev.append(tr_acc_ev)\n",
    "            hist_val_ev.append(va_acc_ev)\n",
    "            hist_lr.append(opt.param_groups[0]['lr'])\n",
    "\n",
    "            if (epoch % LOG_EVERY == 0) or epoch in (1, 10, 20, 50, 100):\n",
    "                print(f\"  Época {epoch:3d} | train_acc(ev)={tr_acc_ev:.4f} | val_acc(ev)={va_acc_ev:.4f} | LR={opt.param_groups[0]['lr']:.5f}\")\n",
    "\n",
    "            if va_acc_ev > best_val_ev_acc + 1e-4:\n",
    "                best_val_ev_acc = va_acc_ev; bad = 0\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "            else:\n",
    "                bad += 1\n",
    "                if bad >= GLOBAL_PATIENCE:\n",
    "                    print(f\"  Early stopping @ {epoch} (mejor val_acc(ev)={best_val_ev_acc:.4f})\")\n",
    "                    break\n",
    "\n",
    "        # Guardar curva\n",
    "        epochs_r = range(1, len(hist_val_ev)+1)\n",
    "        plt.figure(figsize=(6.2,4.2))\n",
    "        plt.plot(epochs_r, hist_train_ev, label='Train (event)')\n",
    "        plt.plot(epochs_r, hist_val_ev,   label='Val (event)')\n",
    "        plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title(f'Fold {fold} - Train vs Val (Event)')\n",
    "        plt.legend(); plt.grid(True, alpha=0.3)\n",
    "        fig_path = FIGS_DIR / f\"training_curve_event_fold{fold}.png\"\n",
    "        plt.tight_layout(); plt.savefig(fig_path, dpi=150); plt.close()\n",
    "        print(f\"↳ Curva train/val guardada: {fig_path.name}\")\n",
    "\n",
    "        # Cargar mejor\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "        # Test por evento\n",
    "        y_true_te, y_pred_te, acc_ev = evaluate_grouped_by_event(\n",
    "            model, te_loader, eids_full=eids[idx_te], use_tta_jitter=True, tta_jitter_n=5\n",
    "        )\n",
    "        global_folds.append(acc_ev)\n",
    "        all_true_ev.append(y_true_te); all_pred_ev.append(y_pred_te)\n",
    "        print(f\"[Fold {fold}/{N_FOLDS}] Global acc (por evento) = {acc_ev:.4f}\")\n",
    "        print_report(y_true_te, y_pred_te, CLASS_NAMES_4C)\n",
    "\n",
    "        # ---------- FT POR SUJETO ----------\n",
    "        acc_ft = np.nan\n",
    "        if DO_SUBJECT_FT:\n",
    "            # usa SOLO eval-crops del sujeto para evaluación justa\n",
    "            X_te, y_te, g_te, e_te = X[idx_te], y[idx_te], groups[idx_te], eids[idx_te]\n",
    "            y_true_ft_all, y_pred_ft_all = [], []\n",
    "            used_subjects = 0\n",
    "            for sid in np.unique(g_te):\n",
    "                idxs = np.where(g_te == sid)[0]\n",
    "                Xs, ys, es = X_te[idxs], y_te[idxs], e_te[idxs]\n",
    "                if len(np.unique(ys)) < 2 or len(ys) < CALIB_CV_FOLDS:\n",
    "                    continue\n",
    "                yt, yp = subject_cv_finetune_predict_grouped(model, Xs, ys, es, n_splits=CALIB_CV_FOLDS, n_classes=N_CLASSES)\n",
    "                y_true_ft_all.append(yt); y_pred_ft_all.append(yp); used_subjects += 1\n",
    "            if len(y_true_ft_all) > 0:\n",
    "                y_true_ft_all = np.concatenate(y_true_ft_all)\n",
    "                y_pred_ft_all = np.concatenate(y_pred_ft_all)\n",
    "                acc_ft = (y_true_ft_all == y_pred_ft_all).mean()\n",
    "                print(f\"  Fine-tuning (por sujeto, agrup. ev) acc={acc_ft:.4f} | Δ={acc_ft-acc_ev:+.4f} | sujetos={used_subjects}\")\n",
    "            else:\n",
    "                print(\"  Fine-tuning no ejecutado (muestras/clases insuficientes).\")\n",
    "        ft_folds.append(acc_ft)\n",
    "\n",
    "    # ---------- resultados finales ----------\n",
    "    if len(all_true_ev) > 0:\n",
    "        all_true_ev = np.concatenate(all_true_ev)\n",
    "        all_pred_ev = np.concatenate(all_pred_ev)\n",
    "    else:\n",
    "        all_true_ev = np.array([], dtype=int)\n",
    "        all_pred_ev = np.array([], dtype=int)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTADOS FINALES (por EVENTO)\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Global folds:\", [f\"{a:.4f}\" for a in global_folds])\n",
    "    if len(global_folds) > 0:\n",
    "        print(f\"Global mean: {np.mean(global_folds):.4f}\")\n",
    "\n",
    "    print(\"Fine-tune folds:\", [(\"nan\" if (a is None or np.isnan(a)) else f\"{a:.4f}\") for a in ft_folds])\n",
    "    if len(ft_folds) > 0:\n",
    "        print(f\"Fine-tune mean: {np.nanmean(ft_folds):.4f}\")\n",
    "        print(f\"Δ(FT-Global) mean: {np.nanmean(ft_folds) - np.mean(global_folds):+.4f}\")\n",
    "\n",
    "    if all_true_ev.size > 0:\n",
    "        fname = FIGS_DIR / \"confusion_eegnet_sliding_event_eval_allfolds.png\"\n",
    "        plot_confusion(all_true_ev, all_pred_ev, CLASS_NAMES_4C,\n",
    "                       title=\"Confusion Matrix (Event-level, ALL FOLDS)\",\n",
    "                       fname=str(fname))\n",
    "        print(f\"↳ Matriz de confusión guardada: {fname.name}\")\n",
    "\n",
    "    return {\n",
    "        \"global_folds\": global_folds,\n",
    "        \"ft_folds\": ft_folds,\n",
    "        \"all_true_ev\": all_true_ev,\n",
    "        \"all_pred_ev\": all_pred_ev,\n",
    "        \"folds_json_path\": str(FOLDS_JSON)\n",
    "    }\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🧠 EEGNet + Sliding Windows (+RandomEventSampler) + Event-level Eval\")\n",
    "    print(f\"🔧 WINDOW_SET={WINDOW_SET} | POLICY={WINDOW_POLICY} | RANDOM_PER_EVENT={RANDOM_PER_EVENT} (k={K_PER_EVENT})\")\n",
    "    print(f\"⚙️  GLOBAL: epochs={EPOCHS_GLOBAL}, lr={LR_INIT}, batch={BATCH_SIZE}, SGDR(T0={SGDR_T0},Tmult={SGDR_Tmult})\")\n",
    "    print(f\"🎛️ Augs: SpecAug={'ON' if USE_SPECAUG else 'OFF'}, Mixup=0.2, TimeJitter=50ms, Cutout(OF)\")\n",
    "    print(f\"🧪 FT por sujeto: {'ON' if DO_SUBJECT_FT else 'OFF'} \"\n",
    "          f\"(epochs={FT_EPOCHS}, base_lr={FT_BASE_LR}, head_lr={FT_HEAD_LR}, L2SP={FT_L2SP})\")\n",
    "    run_experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f588257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 DEVICE = cuda\n",
      "🧪 CNN+Transformer (sliding windows) | 8 canales | 4 clases | eval por EVENTO\n",
      "⚙️  Window: len=1.5s, stride=0.5s | TRAIN_RANGE=(-0.5, 4.0)s | EVAL_RANGE=(0.0, 3.0)s\n",
      "🔧 Augs: jitter±25ms, noise=0.01, cutout 30–60ms (OF), mixup α=0.2, LS=0.05\n",
      "🗂️  Folds=5 (GroupKFold por sujeto) | FT por sujeto=True\n",
      "Sujetos elegibles: 103  →  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Construyendo (windows):   0%|          | 0/103 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Construyendo (windows): 100%|██████████| 103/103 [00:38<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUG WINDOWS SUMMARY ===\n",
      "Ventanas totales: 64890  | sujetos: 103  | eventos: 9270\n",
      "Dataset windows: N=64890 | T=240 | C=8 | sujetos=103 | eventos=9270\n",
      "[TRAIN-EVENT] eventos≈768 dist=Counter({np.int64(0): 1386, np.int64(1): 1365, np.int64(3): 1323, np.int64(2): 1302})\n",
      "[VAL-EVENT]   eventos≈216 dist=Counter({np.int64(1): 392, np.int64(0): 385, np.int64(3): 371, np.int64(2): 364})\n",
      "[TEST-EVENT]  eventos≈252 dist=Counter({np.int64(0): 462, np.int64(3): 441, np.int64(2): 441, np.int64(1): 420})\n",
      "\n",
      "[Fold 1/5] Entrenando (train_ev≈768 | val_ev≈216 | test_ev≈252)\n",
      "  Época   1 | train_acc(win)=0.2504 | val_acc(win)=0.2500 | LR=0.00025\n",
      "  Época   5 | train_acc(win)=0.2608 | val_acc(win)=0.2407 | LR=0.01000\n",
      "  Época  10 | train_acc(win)=0.2476 | val_acc(win)=0.2407 | LR=0.00505\n",
      "  Época  15 | train_acc(win)=0.2431 | val_acc(win)=0.2407 | LR=0.01000\n",
      "  Early stopping @ 15 (mejor val_acc(win)=0.2652)\n",
      "↳ Curva train/val guardada: training_curve_event_fold1_cnntf.png\n",
      "[Fold 1/5] Global acc (por evento) = 0.2619\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left     0.6000    0.0455    0.0845        66\n",
      "       Right     0.2555    0.9667    0.4042        60\n",
      "  Both Fists     0.2500    0.0794    0.1205        63\n",
      "   Both Feet     0.0000    0.0000    0.0000        63\n",
      "\n",
      "    accuracy                         0.2619       252\n",
      "   macro avg     0.2764    0.2729    0.1523       252\n",
      "weighted avg     0.2805    0.2619    0.1485       252\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6b70b864ebda>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    896\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"🔧 Augs: jitter±25ms, noise=0.01, cutout 30–60ms (OF), mixup α={MIXUP_ALPHA}, LS={LABEL_SMOOTH}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"🗂️  Folds={N_FOLDS} (GroupKFold por sujeto) | FT por sujeto={DO_SUBJECT_FT}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m     \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-6b70b864ebda>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0mused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubj_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m                 yt, yp = finetune_subject_eventcv(model, X, y, ev_ids, groups, sid,\n\u001b[0m\u001b[1;32m    853\u001b[0m                                                   \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFT_CV_SPLITS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFT_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m                                                   lr=FT_LR, patience=FT_PATIENCE, batch=FT_BATCH)\n",
      "\u001b[0;32m<ipython-input-9-6b70b864ebda>\u001b[0m in \u001b[0;36mfinetune_subject_eventcv\u001b[0;34m(model_global, X, y, ev_ids, subj_ids, subj, n_splits, epochs, lr, patience, batch)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0mev_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mev_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0myt_ev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myp_ev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmajority_vote_per_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_te_win\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_te_win\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mev_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m         \u001b[0my_true_ev_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myt_ev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0my_pred_ev_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myp_ev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-6b70b864ebda>\u001b[0m in \u001b[0;36mmajority_vote_per_event\u001b[0;34m(y_win, p_win, ev_win, proba, n_classes)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mev_win\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mev_win\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         \u001b[0myt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_win\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ground-truth por evento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_win\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# CNN + Transformer con sliding windows + evaluación por evento\n",
    "# - Lectura EDF (PhysioNet 2009 MI), 8 canales principales, sujetos excluidos\n",
    "# - Ventanas con stride fijo (sin fuga entre eventos)\n",
    "# - Entrenamiento por ventanas; evaluación global AGRUPADA por evento (voto)\n",
    "# - Curvas train/val por fold\n",
    "# - Scheduler Cosine + warmup; mixup + label smoothing + class weights\n",
    "# - Fine-tuning por sujeto (CV por evento) tras evaluar el modelo global\n",
    "\n",
    "import os, re, math, random, json, copy, itertools\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "PROJ = Path('..').resolve().parent\n",
    "DATA_RAW = PROJ / 'data' / 'raw'\n",
    "CACHE_DIR = PROJ / 'data' / 'cache'\n",
    "FOLDS_JSON = PROJ / 'models' / 'folds' / 'Kfold5.json'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE); np.random.seed(RANDOM_STATE); random.seed(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 DEVICE = {DEVICE}\")\n",
    "\n",
    "# Datos\n",
    "FS = 160.0\n",
    "EXPECTED_8 = ['C3','C4','Cz','CP3','CP4','FC3','FC4','FCz']\n",
    "EXCLUDE_SUBJECTS = {38, 88, 89, 92, 100, 104}\n",
    "\n",
    "# Escenario\n",
    "CLASS_SCENARIO = '4c'\n",
    "CLASS_NAMES_4C = ['Left','Right','Both Fists','Both Feet']\n",
    "N_CLASSES = 4\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Runs\n",
    "MI_RUNS_LR = [4, 8, 12]   # T1/T2 → L/R\n",
    "MI_RUNS_OF = [6, 10, 14]  # T1/T2 → BothFists/BothFeet\n",
    "BASELINE_RUNS_EO = [1]\n",
    "\n",
    "# Normalización por evento (z-score canal-a-canal)\n",
    "NORM_EPOCH_ZSCORE = True\n",
    "\n",
    "# Sliding windows\n",
    "WIN_LEN_S = 1.5   # 1.5 s\n",
    "STRIDE_S  = 0.5   # 0.5 s\n",
    "# Rango donde EXTRAEMOS ventanas para entrenamiento (más largo que eval)\n",
    "TRAIN_RANGE_S = (-0.5, 4.0)  # relativo al onset (robust MI latente)\n",
    "# Rango donde EVALUAMOS por evento (ventanas solo aquí para evitar fuga)\n",
    "EVAL_RANGE_S  = ( 0.0, 3.0)\n",
    "\n",
    "# Entrenamiento\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS_GLOBAL = 80\n",
    "GLOBAL_VAL_SPLIT = 0.17\n",
    "GLOBAL_PATIENCE = 12\n",
    "LOG_EVERY = 5\n",
    "\n",
    "# Opt & Scheduler\n",
    "LR_BASE = 1e-2\n",
    "WARMUP_EPOCHS = 4   # warmup lineal a LR_BASE\n",
    "SGDR_T0 = 10\n",
    "SGDR_Tmult = 1\n",
    "\n",
    "# Pérdidas\n",
    "LABEL_SMOOTH = 0.05\n",
    "MIXUP_ALPHA  = 0.2\n",
    "BOOST_BFISTS = 1.20  # clase 2 con ligero boost\n",
    "\n",
    "# FT por sujeto (CV por evento)\n",
    "DO_SUBJECT_FT = True\n",
    "FT_EPOCHS = 20\n",
    "FT_LR = 5e-4\n",
    "FT_PATIENCE = 5\n",
    "FT_CV_SPLITS = 4\n",
    "FT_BATCH = 64\n",
    "\n",
    "# =========================\n",
    "# UTILS: canales y parsing\n",
    "# =========================\n",
    "def normalize_label(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'[^A-Za-z0-9]', '', s)\n",
    "    s = re.sub(r'([A-Za-z])0([0-9])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])Z$', r'\\1z', s)\n",
    "    s = s.replace('fp','Fp').replace('FP','Fp')\n",
    "    s = ''.join(ch.upper() if ch!='z' else 'z' for ch in s)\n",
    "    return s\n",
    "\n",
    "def rename_channels_1010(raw: mne.io.BaseRaw):\n",
    "    mapping = {}\n",
    "    for ch in raw.ch_names:\n",
    "        lab = normalize_label(ch)\n",
    "        lab = re.sub(r'([A-Z])Z$', r'\\1z', lab)\n",
    "        mapping[ch] = lab\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def ensure_channels_order(raw: mne.io.BaseRaw, desired=EXPECTED_8):\n",
    "    missing = [c for c in desired if c not in raw.ch_names]\n",
    "    if missing:\n",
    "        print(f\"[WARN] Faltan canales {missing} en {getattr(raw,'filenames',[''])[0]}\")\n",
    "        return None\n",
    "    raw.reorder_channels([c for c in raw.ch_names if c in desired] +\n",
    "                         [c for c in raw.ch_names if c not in desired])\n",
    "    raw.pick_channels(desired, ordered=True)\n",
    "    return raw\n",
    "\n",
    "_re_file = re.compile(r'[Ss](\\d{3}).*?[Rr](\\d{2})')\n",
    "\n",
    "def parse_subject_run(path: Path):\n",
    "    m = _re_file.search(str(path))\n",
    "    if not m: return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def run_kind(r):\n",
    "    if r in MI_RUNS_LR: return 'LR'\n",
    "    if r in MI_RUNS_OF: return 'OF'\n",
    "    if r in BASELINE_RUNS_EO: return 'EO'\n",
    "    return None\n",
    "\n",
    "# (opcional) notch adaptativo vía CSV SNR\n",
    "_SNR_TABLE = None\n",
    "def _load_snr_table():\n",
    "    global _SNR_TABLE\n",
    "    if _SNR_TABLE is not None: return _SNR_TABLE\n",
    "    csv_path = PROJ / 'reports' / 'psd_mains' / 'psd_mains_summary.csv'\n",
    "    if csv_path.exists():\n",
    "        try: _SNR_TABLE = pd.read_csv(csv_path)\n",
    "        except: _SNR_TABLE = None\n",
    "    return _SNR_TABLE\n",
    "\n",
    "def _decide_notch(subject, run, th_db=10.0):\n",
    "    df = _load_snr_table()\n",
    "    if df is None: return 60.0\n",
    "    row = df[(df['subject']==subject) & (df['run']==run)]\n",
    "    if row.empty: return 60.0\n",
    "    snr50 = float(row['snr50_db'].iloc[0]); snr60 = float(row['snr60_db'].iloc[0])\n",
    "    if snr60 >= th_db and snr60 >= snr50: return 60.0\n",
    "    if snr50 >= th_db and snr50 >  snr60: return 50.0\n",
    "    return None\n",
    "\n",
    "def read_raw_edf(path: Path):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    raw.pick(mne.pick_types(raw.info, eeg=True))\n",
    "    rename_channels_1010(raw)\n",
    "    try:\n",
    "        mont = mne.channels.make_standard_montage('standard_1020')\n",
    "        raw.set_montage(mont, on_missing='ignore')\n",
    "    except: pass\n",
    "    if abs(raw.info['sfreq'] - FS) > 1e-6:\n",
    "        raw.resample(FS, npad=\"auto\")\n",
    "    raw = ensure_channels_order(raw, EXPECTED_8)\n",
    "    if raw is None: return None\n",
    "    # notch (si hay SNR)\n",
    "    sid, rid = parse_subject_run(path)\n",
    "    notch = _decide_notch(sid, rid)\n",
    "    if notch is not None:\n",
    "        raw.notch_filter(freqs=[float(notch)], picks='eeg', method='spectrum_fit', phase='zero')\n",
    "    return raw\n",
    "\n",
    "def collect_events_T1T2(raw: mne.io.BaseRaw):\n",
    "    if raw.annotations is None or len(raw.annotations)==0:\n",
    "        return []\n",
    "    def _norm(s): return str(s).strip().upper().replace(' ','')\n",
    "    out = []\n",
    "    for onset, desc in zip(raw.annotations.onset, raw.annotations.description):\n",
    "        tag = _norm(desc)\n",
    "        if tag in ('T1','T2'): out.append((float(onset), tag))\n",
    "    out.sort()\n",
    "    dedup = []\n",
    "    last_t1 = last_t2 = -1e9\n",
    "    for t, tag in out:\n",
    "        if tag=='T1':\n",
    "            if (t-last_t1) >= 0.5: dedup.append((t,tag)); last_t1=t\n",
    "        else:\n",
    "            if (t-last_t2) >= 0.5: dedup.append((t,tag)); last_t2=t\n",
    "    return dedup\n",
    "\n",
    "# =========================\n",
    "# WINDOWING (train/eval)\n",
    "# =========================\n",
    "def make_windows_for_event(data_tc, fs, onset_sec, label_idx,\n",
    "                           rel_range,   # (start_s, end_s) relativo al onset\n",
    "                           win_len_s, stride_s,\n",
    "                           zscore=True):\n",
    "    \"\"\"\n",
    "    data_tc: (T, C)\n",
    "    Devuelve lista de (win_tc, label_idx) y (samp_idx_ini, samp_idx_fin) absolutos\n",
    "    \"\"\"\n",
    "    T, C = data_tc.shape\n",
    "    s_rel, e_rel = rel_range\n",
    "    s_abs = int(round((onset_sec + s_rel) * fs))\n",
    "    e_abs = int(round((onset_sec + e_rel) * fs))\n",
    "    s_abs = max(0, s_abs); e_abs = min(T, e_abs)\n",
    "    if e_abs - s_abs <= 0: return []\n",
    "\n",
    "    wl = int(round(win_len_s * fs))\n",
    "    st = int(round(stride_s * fs))\n",
    "    if wl <= 0 or st <= 0: return []\n",
    "\n",
    "    out = []\n",
    "    t = s_abs\n",
    "    while t + wl <= e_abs:\n",
    "        seg = data_tc[t:t+wl, :].astype(np.float32)\n",
    "        if zscore:\n",
    "            seg = (seg - seg.mean(axis=0, keepdims=True)) / (1e-6 + seg.std(axis=0, keepdims=True))\n",
    "        out.append((seg, label_idx, t, t+wl))\n",
    "        t += st\n",
    "    return out\n",
    "\n",
    "def label_from_tag(kind, tag):\n",
    "    if kind=='LR':\n",
    "        return 0 if tag=='T1' else 1\n",
    "    else:\n",
    "        return 2 if tag=='T1' else 3\n",
    "\n",
    "def subjects_available():\n",
    "    subs = []\n",
    "    for sdir in sorted(DATA_RAW.glob('S*')):\n",
    "        if not sdir.is_dir(): continue\n",
    "        try: sid = int(sdir.name[1:])\n",
    "        except: continue\n",
    "        if sid in EXCLUDE_SUBJECTS: continue\n",
    "        any_mi = any((sdir / f\"S{sid:03d}R{r:02d}.edf\").exists() for r in (MI_RUNS_LR+MI_RUNS_OF))\n",
    "        if any_mi: subs.append(sid)\n",
    "    return subs\n",
    "\n",
    "def build_windows_dataset(subjects):\n",
    "    X_wins, y_wins, g_wins, ev_ids, ev_meta = [], [], [], [], []  # ev_meta: (subj, run, onset_idx)\n",
    "    ch_template = None\n",
    "    event_counter = 0\n",
    "\n",
    "    for s in tqdm(subjects, desc=\"Construyendo (windows)\"):\n",
    "        sdir = DATA_RAW / f\"S{s:03d}\"\n",
    "        if not sdir.exists(): continue\n",
    "\n",
    "        for r in MI_RUNS_LR + MI_RUNS_OF:\n",
    "            p = sdir / f\"S{s:03d}R{r:02d}.edf\"\n",
    "            if not p.exists(): continue\n",
    "            kind = run_kind(r)\n",
    "            raw = read_raw_edf(p)\n",
    "            if raw is None: continue\n",
    "            if ch_template is None: ch_template = raw.ch_names\n",
    "\n",
    "            fs = raw.info['sfreq']\n",
    "            data = raw.get_data().T.astype(np.float32)  # (T, C)\n",
    "            events = collect_events_T1T2(raw)\n",
    "\n",
    "            for onset, tag in events:\n",
    "                lab = label_from_tag(kind, tag)\n",
    "                # ventanas de TRAIN (amplio)\n",
    "                wins_tr = make_windows_for_event(data, fs, raw.first_time+onset, lab,\n",
    "                                                 rel_range=TRAIN_RANGE_S,\n",
    "                                                 win_len_s=WIN_LEN_S, stride_s=STRIDE_S,\n",
    "                                                 zscore=NORM_EPOCH_ZSCORE)\n",
    "                # ventanas de EVAL (estricto)\n",
    "                wins_ev = make_windows_for_event(data, fs, raw.first_time+onset, lab,\n",
    "                                                 rel_range=EVAL_RANGE_S,\n",
    "                                                 win_len_s=WIN_LEN_S, stride_s=STRIDE_S,\n",
    "                                                 zscore=NORM_EPOCH_ZSCORE)\n",
    "\n",
    "                # asignamos un id único por evento (para agrupar en eval/FT)\n",
    "                ev_id = event_counter; event_counter += 1\n",
    "\n",
    "                for seg, yk, si, ei in wins_tr:\n",
    "                    X_wins.append(seg); y_wins.append(yk); g_wins.append(s); ev_ids.append(ev_id)\n",
    "                # guardamos meta de eval (cantidad/índices por evento)\n",
    "                ev_meta.append({\n",
    "                    'event_id': ev_id, 'subject': s, 'run': r, 'label': lab,\n",
    "                    'n_train_wins': len(wins_tr), 'n_eval_wins': len(wins_ev)\n",
    "                })\n",
    "\n",
    "    X = np.stack(X_wins, axis=0) if len(X_wins)>0 else np.zeros((0,int(WIN_LEN_S*FS),len(EXPECTED_8)), np.float32)\n",
    "    y = np.asarray(y_wins, dtype=np.int64)\n",
    "    g = np.asarray(g_wins, dtype=np.int64)\n",
    "    ev_ids = np.asarray(ev_ids, dtype=np.int64)\n",
    "\n",
    "    print(\"=== DEBUG WINDOWS SUMMARY ===\")\n",
    "    print(f\"Ventanas totales: {len(X_wins)}  | sujetos: {len(set(g_wins))}  | eventos: {event_counter}\")\n",
    "    return X, y, g, ev_ids, ch_template, pd.DataFrame(ev_meta)\n",
    "\n",
    "# =========================\n",
    "# DATASET torch\n",
    "# =========================\n",
    "class EEGWindows(Dataset):\n",
    "    def __init__(self, X, y, groups, ev_ids):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.g = groups.astype(np.int64)\n",
    "        self.e = ev_ids.astype(np.int64)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]   # (T, C)\n",
    "        x = np.expand_dims(x, 0)  # (1, T, C)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx]), torch.tensor(self.g[idx]), torch.tensor(self.e[idx])\n",
    "\n",
    "# =========================\n",
    "# AUGMENTS\n",
    "# =========================\n",
    "def do_time_jitter(x, max_ms=25, fs=160.0):\n",
    "    max_shift = int(round(max_ms/1000.0*fs))\n",
    "    if max_shift<=0: return x\n",
    "    B,_,T,C = x.shape\n",
    "    shifts = torch.randint(-max_shift, max_shift+1, (B,), device=x.device)\n",
    "    out = torch.zeros_like(x)\n",
    "    for i,s in enumerate(shifts):\n",
    "        if s>=0:\n",
    "            out[i,:,s:,:] = x[i,:,:T-s,:]\n",
    "        else:\n",
    "            s=-s\n",
    "            out[i,:,:T-s,:] = x[i,:,s:,:]\n",
    "    return out\n",
    "\n",
    "def do_gaussian_noise(x, sigma=0.01):\n",
    "    if sigma<=0: return x\n",
    "    return x + sigma*torch.randn_like(x)\n",
    "\n",
    "def do_temporal_cutout_masked(x, y, classes_mask={2,3}, min_ms=30, max_ms=60, fs=160.0):\n",
    "    B,_,T,C = x.shape\n",
    "    Lmin = int(round(min_ms/1000.0*fs)); Lmax = int(round(max_ms/1000.0*fs))\n",
    "    if Lmin<=0 or Lmax<=0 or Lmin>Lmax: return x\n",
    "    out = x.clone()\n",
    "    y_np = y.detach().cpu().numpy()\n",
    "    for i in range(B):\n",
    "        if int(y_np[i]) not in classes_mask: continue\n",
    "        L = random.randint(Lmin, Lmax); \n",
    "        if L>=T: continue\n",
    "        s = random.randint(0, T-L)\n",
    "        out[i,:,s:s+L,:] = 0\n",
    "    return out\n",
    "\n",
    "def mixup_batch(x, y, n_classes, alpha=MIXUP_ALPHA):\n",
    "    if alpha<=0:\n",
    "        y_one = torch.nn.functional.one_hot(y, num_classes=n_classes).float()\n",
    "        return x, y_one, 1.0\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    perm = torch.randperm(x.size(0), device=x.device)\n",
    "    x_mix = lam*x + (1-lam)*x[perm]\n",
    "    ya = torch.nn.functional.one_hot(y, num_classes=n_classes).float()\n",
    "    yb = ya[perm]\n",
    "    y_mix = lam*ya + (1-lam)*yb\n",
    "    return x_mix, y_mix, lam\n",
    "\n",
    "class WeightedSoftCrossEntropy(nn.Module):\n",
    "    def __init__(self, class_weights=None, label_smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.register_buffer('w', None if class_weights is None else class_weights.clone().float())\n",
    "        self.ls = float(label_smoothing)\n",
    "    def forward(self, logits, target_probs):\n",
    "        if self.ls>0:\n",
    "            K = logits.size(1)\n",
    "            target_probs = (1-self.ls)*target_probs + self.ls*(1.0/K)\n",
    "        logp = torch.log_softmax(logits, dim=1)\n",
    "        loss_per_class = -(target_probs * logp)\n",
    "        if self.w is not None:\n",
    "            loss_per_class = loss_per_class * self.w.unsqueeze(0)\n",
    "        return loss_per_class.sum(dim=1).mean()\n",
    "\n",
    "def class_weight_tensor(y_indices, n_classes, boost_bfists=1.2):\n",
    "    cnt = np.bincount(y_indices, minlength=n_classes).astype(np.float32)\n",
    "    cnt[cnt==0]=1.0\n",
    "    w = cnt.sum()/cnt\n",
    "    w = w/w.mean()\n",
    "    if n_classes>=3:\n",
    "        w[2] *= boost_bfists\n",
    "        w = w/w.mean()\n",
    "    return torch.tensor(w, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "# =========================\n",
    "# MODELO: CNN + Transformer\n",
    "# =========================\n",
    "class ConvFrontEnd(nn.Module):\n",
    "    \"\"\"Conv temporal + depthwise espacial (tipo EEGNet lite)\"\"\"\n",
    "    def __init__(self, n_ch, feat_dim=64, k_t=64, pool_t=4, drop_p=0.25):\n",
    "        super().__init__()\n",
    "        self.conv_t = nn.Conv2d(1, feat_dim, kernel_size=(k_t,1), padding=(k_t//2,0), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(feat_dim)\n",
    "        self.act = nn.ELU()\n",
    "        self.conv_dw = nn.Conv2d(feat_dim, feat_dim, kernel_size=(1,n_ch), groups=feat_dim, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(feat_dim)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=(pool_t,1), stride=(pool_t,1))\n",
    "        self.drop = nn.Dropout(drop_p)\n",
    "    def forward(self, x):  # x: (B,1,T,C)\n",
    "        z = self.conv_t(x); z = self.bn1(z); z = self.act(z)\n",
    "        z = self.conv_dw(z); z = self.bn2(z); z = self.act(z)\n",
    "        z = self.pool(z); z = self.drop(z)      # (B, F, T', 1)\n",
    "        z = z.squeeze(-1).permute(0,2,1)        # (B, T', F)\n",
    "        return z\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=4000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0)/d_model))\n",
    "        pe[:,0::2] = torch.sin(pos*div)\n",
    "        pe[:,1::2] = torch.cos(pos*div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # (1, max_len, d_model)\n",
    "    def forward(self, x):  # x: (B, T, D)\n",
    "        T = x.size(1)\n",
    "        return x + self.pe[:,:T,:]\n",
    "\n",
    "class CNNTransformer(nn.Module):\n",
    "    def __init__(self, n_ch, n_classes, cnn_feat=64, k_t=64, pool_t=4,\n",
    "                 tf_d_model=96, nhead=4, tf_dim_ff=192, tf_layers=3, tf_dropout=0.1,\n",
    "                 proj_dim=128, drop_p=0.3):\n",
    "        super().__init__()\n",
    "        self.front = ConvFrontEnd(n_ch, feat_dim=cnn_feat, k_t=k_t, pool_t=pool_t, drop_p=0.2)\n",
    "        self.proj = nn.Linear(cnn_feat, tf_d_model)\n",
    "        self.pe = PositionalEncoding(tf_d_model)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=tf_d_model, nhead=nhead,\n",
    "            dim_feedforward=tf_dim_ff, dropout=tf_dropout,\n",
    "            activation='gelu', batch_first=True, norm_first=True\n",
    "        )\n",
    "        # 🔇 evitar warning nested_tensor\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=tf_layers, enable_nested_tensor=False)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(tf_d_model),\n",
    "            nn.Linear(tf_d_model, proj_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(proj_dim, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # (B,1,T,C)\n",
    "        z = self.front(x)              # (B, T', F)\n",
    "        z = self.proj(z)               # (B, T', D)\n",
    "        z = self.pe(z)\n",
    "        z = self.encoder(z)            # (B, T', D)\n",
    "        z = z.mean(dim=1)              # pool temporal\n",
    "        return self.head(z)\n",
    "\n",
    "# =========================\n",
    "# TRAIN / EVAL helpers\n",
    "# =========================\n",
    "def build_weighted_sampler(y, groups):\n",
    "    y = np.asarray(y); groups = np.asarray(groups)\n",
    "    cls_counts = np.bincount(y, minlength=len(np.unique(y))).astype(float)\n",
    "    cls_w = 1.0 / cls_counts[y]\n",
    "    subj_vals, subj_counts = np.unique(groups, return_counts=True)\n",
    "    s_map = {s:c for s,c in zip(subj_vals, subj_counts)}\n",
    "    subj_w = np.array([1.0/s_map[g] for g in groups], float)\n",
    "    w = cls_w * subj_w; w = w / w.mean()\n",
    "    return WeightedRandomSampler(torch.tensor(w, dtype=torch.float32), num_samples=len(w), replacement=True)\n",
    "\n",
    "def train_one_epoch(model, loader, opt, criterion_soft, n_classes, fs=160.0, maxnorm=None):\n",
    "    model.train()\n",
    "    total = 0; correct = 0\n",
    "    for xb, yb, _, _ in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        # augments\n",
    "        xb = do_time_jitter(xb, max_ms=25, fs=fs)\n",
    "        xb = do_gaussian_noise(xb, sigma=0.01)\n",
    "        xb = do_temporal_cutout_masked(xb, yb, classes_mask={2,3}, min_ms=30, max_ms=60, fs=fs)\n",
    "        xb, ymix, _ = mixup_batch(xb, yb, n_classes=n_classes, alpha=MIXUP_ALPHA)\n",
    "\n",
    "        logits = model(xb)\n",
    "        loss = criterion_soft(logits, ymix)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward(); opt.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(1)\n",
    "            # para precisión aproximada con mixup, comparamos con argmax de ymix (solo para logging)\n",
    "            tgt = ymix.argmax(1)\n",
    "            correct += (pred==tgt).sum().item(); total += xb.size(0)\n",
    "        if maxnorm is not None:\n",
    "            # aplica max-norm a capas densas/pointwise si lo deseas (opcional)\n",
    "            for m in model.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    w = m.weight.data\n",
    "                    norms = w.view(w.size(0), -1).norm(p=2, dim=1, keepdim=True)\n",
    "                    desired = torch.clamp(norms, max=maxnorm)\n",
    "                    w.view(w.size(0), -1).mul_(desired / (1e-8 + norms))\n",
    "    return correct/max(1,total)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_accuracy_windows(model, loader):\n",
    "    model.eval()\n",
    "    total = 0; correct = 0\n",
    "    all_y, all_p = [], []\n",
    "    for xb, yb, _, _ in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        pred = logits.argmax(1).cpu()\n",
    "        all_y.extend(yb.numpy().tolist())\n",
    "        all_p.extend(pred.numpy().tolist())\n",
    "        correct += (pred==yb).sum().item(); total += yb.size(0)\n",
    "    return correct/max(1,total), np.asarray(all_y, int), np.asarray(all_p, int)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_grouped_by_event(model, loader):\n",
    "    \"\"\"\n",
    "    Devuelve y_true_event, y_pred_event, acc_event\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_win, p_win, ev_win = [], [], []\n",
    "    for xb, yb, _, ev in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        pred = logits.argmax(1).cpu().numpy()\n",
    "        y_win.extend(yb.numpy().tolist())\n",
    "        p_win.extend(pred.tolist())\n",
    "        ev_win.extend(ev.numpy().tolist())\n",
    "\n",
    "    y_win = np.asarray(y_win, int)\n",
    "    p_win = np.asarray(p_win, int)\n",
    "    ev_win = np.asarray(ev_win, int)\n",
    "\n",
    "    # voto mayoritario + desempate por frecuencia\n",
    "    y_true_ev, y_pred_ev = majority_vote_per_event(y_win, p_win, ev_win, proba=None, n_classes=N_CLASSES)\n",
    "    acc_ev = (y_true_ev == y_pred_ev).mean() if len(y_true_ev)>0 else 0.0\n",
    "    return y_true_ev, y_pred_ev, acc_ev\n",
    "\n",
    "def majority_vote_per_event(y_win, p_win, ev_win, proba=None, n_classes=4):\n",
    "    y_true_ev, y_pred_ev = [], []\n",
    "    for e in np.unique(ev_win):\n",
    "        idx = np.where(ev_win==e)[0]\n",
    "        yt = int(np.bincount(y_win[idx]).argmax())  # ground-truth por evento\n",
    "        counts = np.bincount(p_win[idx], minlength=n_classes)\n",
    "        top = np.where(counts == counts.max())[0]\n",
    "        if len(top)==1:\n",
    "            yhat = int(top[0])\n",
    "        else:\n",
    "            # desempate por suma de probas si las tuvieras; aquí por índice mínimo\n",
    "            yhat = int(top.min())\n",
    "        y_true_ev.append(yt); y_pred_ev.append(yhat)\n",
    "    return np.asarray(y_true_ev, int), np.asarray(y_pred_ev, int)\n",
    "\n",
    "def plot_training_curves(history, fname):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(history['train_acc'], label='train_acc(win)')\n",
    "    plt.plot(history['val_acc'], label='val_acc(win)')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
    "    plt.title('Training curve (windows)')\n",
    "    plt.legend(); plt.tight_layout(); plt.savefig(fname, dpi=150); plt.close()\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes, title, fname):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cmn = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
    "    cmn = np.nan_to_num(cmn)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.imshow(cmn, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title); plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    ticks = np.arange(len(classes))\n",
    "    plt.xticks(ticks, classes, rotation=45, ha='right'); plt.yticks(ticks, classes)\n",
    "    fmt = '.2f'; thresh = cmn.max()/2\n",
    "    for i,j in itertools.product(range(cmn.shape[0]), range(cmn.shape[1])):\n",
    "        plt.text(j, i, format(cmn[i,j], fmt),\n",
    "                 ha='center', va='center',\n",
    "                 color='white' if cmn[i,j]>thresh else 'black')\n",
    "    plt.ylabel('True'); plt.xlabel('Pred'); plt.tight_layout()\n",
    "    plt.savefig(fname, dpi=150, bbox_inches='tight'); plt.close()\n",
    "\n",
    "# =========================\n",
    "# SCHEDULER (warmup + Cosine)\n",
    "# =========================\n",
    "class WarmupThenCosine:\n",
    "    def __init__(self, opt, base_lr, warmup_epochs, T0, Tmult=1, min_lr=1e-5):\n",
    "        self.opt = opt\n",
    "        self.base_lr = base_lr\n",
    "        self.warm = max(1, warmup_epochs)\n",
    "        self.cos = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=T0, T_mult=Tmult, eta_min=min_lr)\n",
    "        self._epoch = 0\n",
    "        # arranca en lr pequeño\n",
    "        for pg in self.opt.param_groups: pg['lr'] = base_lr / (self.warm*10.0)\n",
    "    def step(self):\n",
    "        self._epoch += 1\n",
    "        if self._epoch <= self.warm:\n",
    "            lr = (self.base_lr * self._epoch) / float(self.warm)\n",
    "            for pg in self.opt.param_groups: pg['lr'] = lr\n",
    "        else:\n",
    "            self.cos.step(self._epoch - self.warm)\n",
    "\n",
    "# =========================\n",
    "# FT por sujeto (CV por evento)\n",
    "# =========================\n",
    "def _events_from_indices(idx, ev_ids, y):\n",
    "    idx = np.asarray(idx, int)\n",
    "    ev = ev_ids[idx]; yw = y[idx]\n",
    "    events = {}\n",
    "    first = {}\n",
    "    for k,i in enumerate(idx):\n",
    "        e = ev[k]\n",
    "        events.setdefault(e, []).append(i)\n",
    "        if e not in first: first[e] = int(yw[k])\n",
    "    ev_list = np.array(sorted(events.keys()), dtype=int)\n",
    "    y_ev = np.array([first[e] for e in ev_list], dtype=int)\n",
    "    win_lists = [events[e] for e in ev_list]\n",
    "    return ev_list, y_ev, win_lists\n",
    "\n",
    "def finetune_subject_eventcv(model_global, X, y, ev_ids, subj_ids, subj,\n",
    "                             n_splits=FT_CV_SPLITS, epochs=FT_EPOCHS, lr=FT_LR, patience=FT_PATIENCE, batch=FT_BATCH):\n",
    "    device = next(model_global.parameters()).device\n",
    "    idx_subj = np.where(subj_ids==subj)[0]\n",
    "    if idx_subj.size==0: return np.array([], int), np.array([], int)\n",
    "\n",
    "    ev_list, y_events, win_lists = _events_from_indices(idx_subj, ev_ids, y)\n",
    "    if len(np.unique(y_events))<2 or len(y_events)<n_splits:\n",
    "        return np.array([], int), np.array([], int)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    y_true_ev_all, y_pred_ev_all = [], []\n",
    "\n",
    "    for tr_e, te_e in skf.split(ev_list, y_events):\n",
    "        tr_idx = np.concatenate([win_lists[i] for i in tr_e])\n",
    "        te_idx = np.concatenate([win_lists[i] for i in te_e])\n",
    "\n",
    "        ds_tr = EEGWindows(X[tr_idx], y[tr_idx], subj_ids[tr_idx], ev_ids[tr_idx])\n",
    "        ds_te = EEGWindows(X[te_idx], y[te_idx], subj_ids[te_idx], ev_ids[te_idx])\n",
    "        dl_tr = DataLoader(ds_tr, batch_size=batch, shuffle=True)\n",
    "        dl_te = DataLoader(ds_te, batch_size=batch, shuffle=False)\n",
    "\n",
    "        m = copy.deepcopy(model_global)\n",
    "        for p in m.parameters(): p.requires_grad = True  # FT completo; si quieres, puedes hacer etapas\n",
    "        opt = optim.Adam(m.parameters(), lr=lr)\n",
    "        crit = nn.CrossEntropyLoss()\n",
    "\n",
    "        best = copy.deepcopy(m.state_dict()); best_val = 1e9; bad=0\n",
    "        # validación rápida: pérdida media en train (proxy) + early stop\n",
    "        for ep in range(1, epochs+1):\n",
    "            m.train()\n",
    "            for xb, yb, _, _ in dl_tr:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                loss = crit(m(xb), yb)\n",
    "                loss.backward(); opt.step()\n",
    "            # proxy val\n",
    "            with torch.no_grad():\n",
    "                m.eval()\n",
    "                tot, cor = 0, 0\n",
    "                for xb, yb, _, _ in dl_tr:\n",
    "                    xb, yb = xb.to(device), yb.to(device)\n",
    "                    pred = m(xb).argmax(1)\n",
    "                    cor += (pred==yb).sum().item(); tot += yb.size(0)\n",
    "                val = 1.0 - cor/max(1,tot)\n",
    "                if val<best_val: best_val, bad, best = val, 0, copy.deepcopy(m.state_dict())\n",
    "                else:\n",
    "                    bad += 1\n",
    "                    if bad>=patience: break\n",
    "        m.load_state_dict(best)\n",
    "\n",
    "        # pred por evento\n",
    "        y_te_win, p_te_win, _ = eval_accuracy_windows(m, dl_te)\n",
    "        # reconstruimos ev ids de te\n",
    "        ev_te = []\n",
    "        for _, _, _, e in dl_te:\n",
    "            ev_te.extend(e.numpy().tolist())\n",
    "        ev_te = np.asarray(ev_te, int)\n",
    "\n",
    "        yt_ev, yp_ev = majority_vote_per_event(y_te_win, p_te_win, ev_te, proba=None, n_classes=N_CLASSES)\n",
    "        y_true_ev_all.append(yt_ev); y_pred_ev_all.append(yp_ev)\n",
    "\n",
    "    return np.concatenate(y_true_ev_all), np.concatenate(y_pred_ev_all)\n",
    "\n",
    "# =========================\n",
    "# Folds helpers\n",
    "# =========================\n",
    "def save_group_folds_json_with_indices(subject_ids_str, groups_array, n_splits, out_json_path,\n",
    "                                       created_by=\"cnn_tf_event\", description=None):\n",
    "    out_json_path = Path(out_json_path)\n",
    "    unique_subjects_int = sorted(np.unique(groups_array).tolist())\n",
    "    subject_ids = [f\"S{sid:03d}\" for sid in unique_subjects_int]\n",
    "    if len(subject_ids) < n_splits:\n",
    "        raise ValueError(f\"n_splits={n_splits} > num_subjects={len(subject_ids)}\")\n",
    "    groups = np.arange(len(subject_ids))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    folds = []; fold_i = 0\n",
    "    for tr_grp, te_grp in gkf.split(groups, groups, groups):\n",
    "        fold_i += 1\n",
    "        tr_sids = [subject_ids[int(i)] for i in tr_grp]\n",
    "        te_sids = [subject_ids[int(i)] for i in te_grp]\n",
    "        tr_int = [int(s[1:]) for s in tr_sids]; te_int = [int(s[1:]) for s in te_sids]\n",
    "        tr_idx = np.where(np.isin(groups_array, tr_int))[0].tolist()\n",
    "        te_idx = np.where(np.isin(groups_array, te_int))[0].tolist()\n",
    "        folds.append({\"fold\": fold_i, \"train\": tr_sids, \"test\": te_sids, \"tr_idx\": tr_idx, \"te_idx\": te_idx})\n",
    "\n",
    "    payload = {\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"created_by\": created_by,\n",
    "        \"description\": description or \"\",\n",
    "        \"n_splits\": int(n_splits),\n",
    "        \"n_subjects\": len(subject_ids),\n",
    "        \"subject_ids\": subject_ids,\n",
    "        \"folds\": folds\n",
    "    }\n",
    "    out_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Folds JSON guardado → {out_json_path}\")\n",
    "    return out_json_path\n",
    "\n",
    "def load_group_folds_json(path_json, expected_subject_ids=None, strict_check=True):\n",
    "    path_json = Path(path_json)\n",
    "    if not path_json.exists():\n",
    "        raise FileNotFoundError(str(path_json))\n",
    "    with open(path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        payload = json.load(f)\n",
    "    subj_json = payload.get(\"subject_ids\", [])\n",
    "    if expected_subject_ids is not None:\n",
    "        exp = sorted(list(expected_subject_ids))\n",
    "        if subj_json != exp:\n",
    "            msg = (\"Subject IDs del JSON no coinciden con los esperados.\\n\"\n",
    "                   f\"JSON={len(subj_json)} vs EXP={len(exp)}\\n\"\n",
    "                   f\"JSON first10={subj_json[:10]} | EXP first10={exp[:10]}\")\n",
    "            if strict_check: raise ValueError(msg)\n",
    "            else: print(\"[WARN] \" + msg)\n",
    "    return payload\n",
    "\n",
    "# =========================\n",
    "# MAIN EXP\n",
    "# =========================\n",
    "def run_experiment():\n",
    "    mne.set_log_level('WARNING')\n",
    "    subs = subjects_available()\n",
    "    print(f\"Sujetos elegibles: {len(subs)}  →  {subs[:12]}{'...' if len(subs)>12 else ''}\")\n",
    "\n",
    "    # Construir ventanas (train-range), y ev_ids (por-evento consistente)\n",
    "    X, y, groups, ev_ids, chs, ev_meta = build_windows_dataset(subs)\n",
    "    N, T, C = X.shape\n",
    "    print(f\"Dataset windows: N={N} | T={T} | C={C} | sujetos={len(np.unique(groups))} | eventos={len(np.unique(ev_ids))}\")\n",
    "\n",
    "    ds = EEGWindows(X, y, groups, ev_ids)\n",
    "\n",
    "    # Folds por sujetos\n",
    "    unique_subs = sorted(np.unique(groups).tolist())\n",
    "    subject_ids_str = [f\"S{s:03d}\" for s in unique_subs]\n",
    "    folds_json = Path(FOLDS_JSON)\n",
    "    folds_json.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not folds_json.exists():\n",
    "        save_group_folds_json_with_indices(subject_ids_str, groups, N_FOLDS, folds_json,\n",
    "                                           created_by=\"CNNTF_windows\",\n",
    "                                           description=\"GroupKFold por sujeto (windows)\")\n",
    "\n",
    "    payload = load_group_folds_json(folds_json, expected_subject_ids=subject_ids_str, strict_check=False)\n",
    "    folds = payload[\"folds\"]\n",
    "\n",
    "    global_folds = []\n",
    "    all_true_ev = []\n",
    "    all_pred_ev = []\n",
    "\n",
    "    for f in folds:\n",
    "        fold = f[\"fold\"]\n",
    "        tr_idx = np.asarray(f[\"tr_idx\"], int)\n",
    "        te_idx = np.asarray(f[\"te_idx\"], int)\n",
    "\n",
    "        # validación por sujeto (split de sujetos dentro de train)\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=GLOBAL_VAL_SPLIT, random_state=RANDOM_STATE)\n",
    "        tr_subj_idx, va_subj_idx = next(gss.split(tr_idx, groups[tr_idx], groups[tr_idx]))\n",
    "        tr_sub_idx = tr_idx[tr_subj_idx]\n",
    "        va_idx     = tr_idx[va_subj_idx]\n",
    "\n",
    "        # Log distribuciones por evento (aprox)\n",
    "        print(f\"[TRAIN-EVENT] eventos≈{len(np.unique(ev_ids[tr_sub_idx]))} dist={Counter(y[tr_sub_idx])}\")\n",
    "        print(f\"[VAL-EVENT]   eventos≈{len(np.unique(ev_ids[va_idx]))} dist={Counter(y[va_idx])}\")\n",
    "        print(f\"[TEST-EVENT]  eventos≈{len(np.unique(ev_ids[te_idx]))} dist={Counter(y[te_idx])}\")\n",
    "\n",
    "        sampler = build_weighted_sampler(y[tr_sub_idx], groups[tr_sub_idx])\n",
    "        tr_loader = DataLoader(Subset(ds, tr_sub_idx), batch_size=BATCH_SIZE, sampler=sampler, drop_last=False)\n",
    "        va_loader = DataLoader(Subset(ds, va_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "        te_loader = DataLoader(Subset(ds, te_idx),     batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "        # Modelo\n",
    "        model = CNNTransformer(n_ch=C, n_classes=N_CLASSES,\n",
    "                               cnn_feat=64, k_t=64, pool_t=4,\n",
    "                               tf_d_model=96, nhead=4, tf_dim_ff=192, tf_layers=3, tf_dropout=0.1,\n",
    "                               proj_dim=128, drop_p=0.3).to(DEVICE)\n",
    "\n",
    "        # Opt + scheduler\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_BASE, weight_decay=0.0)\n",
    "        sched = WarmupThenCosine(opt, base_lr=LR_BASE, warmup_epochs=WARMUP_EPOCHS, T0=SGDR_T0, Tmult=SGDR_Tmult, min_lr=1e-4)\n",
    "\n",
    "        # Criterio suave + pesos de clase\n",
    "        w = class_weight_tensor(y[tr_sub_idx], N_CLASSES, boost_bfists=BOOST_BFISTS)\n",
    "        criterion_soft = WeightedSoftCrossEntropy(class_weights=w, label_smoothing=LABEL_SMOOTH)\n",
    "\n",
    "        history = {'train_acc': [], 'val_acc': []}\n",
    "        best_model = copy.deepcopy(model.state_dict()); best_val = -1.0; bad=0\n",
    "\n",
    "        print(f\"\\n[Fold {fold}/{N_FOLDS}] Entrenando (train_ev≈{len(np.unique(ev_ids[tr_sub_idx]))} | val_ev≈{len(np.unique(ev_ids[va_idx]))} | test_ev≈{len(np.unique(ev_ids[te_idx]))})\")\n",
    "        for ep in range(1, EPOCHS_GLOBAL+1):\n",
    "            tr_acc = train_one_epoch(model, tr_loader, opt, criterion_soft, n_classes=N_CLASSES, fs=FS, maxnorm=2.0)\n",
    "            va_acc_win, _, _ = eval_accuracy_windows(model, va_loader)\n",
    "            history['train_acc'].append(tr_acc)\n",
    "            history['val_acc'].append(va_acc_win)\n",
    "\n",
    "            if ep % LOG_EVERY == 0 or ep in (1, 5, 10, 15, 20, 25, 30):\n",
    "                cur_lr = opt.param_groups[0]['lr']\n",
    "                print(f\"  Época {ep:3d} | train_acc(win)={tr_acc:.4f} | val_acc(win)={va_acc_win:.4f} | LR={cur_lr:.5f}\")\n",
    "\n",
    "            # early stop por val (windows)\n",
    "            if va_acc_win > best_val + 1e-4:\n",
    "                best_val = va_acc_win; best_model = copy.deepcopy(model.state_dict()); bad=0\n",
    "            else:\n",
    "                bad += 1\n",
    "                if bad >= GLOBAL_PATIENCE:\n",
    "                    print(f\"  Early stopping @ {ep} (mejor val_acc(win)={best_val:.4f})\")\n",
    "                    break\n",
    "\n",
    "            sched.step()\n",
    "\n",
    "        curve_path = f\"training_curve_event_fold{fold}_cnntf.png\"\n",
    "        plot_training_curves(history, curve_path)\n",
    "        print(f\"↳ Curva train/val guardada: {curve_path}\")\n",
    "\n",
    "        model.load_state_dict(best_model)\n",
    "\n",
    "        # ===== Evaluación global por EVENTO =====\n",
    "        y_true_te, y_pred_te, acc_ev = evaluate_grouped_by_event(model, te_loader)\n",
    "        print(f\"[Fold {fold}/{N_FOLDS}] Global acc (por evento) = {acc_ev:.4f}\")\n",
    "        print(classification_report(y_true_te, y_pred_te, target_names=CLASS_NAMES_4C, digits=4))\n",
    "\n",
    "        global_folds.append(acc_ev)\n",
    "        all_true_ev.append(y_true_te); all_pred_ev.append(y_pred_te)\n",
    "\n",
    "        # ===== Fine-tuning por sujeto (CV por evento) =====\n",
    "        if DO_SUBJECT_FT:\n",
    "            subj_te = groups[te_idx]\n",
    "            y_true_ft_all, y_pred_ft_all = [], []\n",
    "            used = 0\n",
    "            for sid in np.unique(subj_te):\n",
    "                yt, yp = finetune_subject_eventcv(model, X, y, ev_ids, groups, sid,\n",
    "                                                  n_splits=FT_CV_SPLITS, epochs=FT_EPOCHS,\n",
    "                                                  lr=FT_LR, patience=FT_PATIENCE, batch=FT_BATCH)\n",
    "                if yt.size==0: continue\n",
    "                y_true_ft_all.append(yt); y_pred_ft_all.append(yp); used += 1\n",
    "            if y_true_ft_all:\n",
    "                y_true_ft_all = np.concatenate(y_true_ft_all)\n",
    "                y_pred_ft_all = np.concatenate(y_pred_ft_all)\n",
    "                acc_ft = (y_true_ft_all==y_pred_ft_all).mean()\n",
    "                print(f\"  Fine-tuning (por sujeto, agrup. ev) acc={acc_ft:.4f} | Δ={acc_ft-acc_ev:+.4f} | sujetos={used}\")\n",
    "            else:\n",
    "                print(\"  Fine-tuning no ejecutado (eventos/clases insuficientes).\")\n",
    "\n",
    "    # ===== Resultados finales =====\n",
    "    if all_true_ev:\n",
    "        all_true_ev = np.concatenate(all_true_ev)\n",
    "        all_pred_ev = np.concatenate(all_pred_ev)\n",
    "    else:\n",
    "        all_true_ev = np.array([], int); all_pred_ev = np.array([], int)\n",
    "\n",
    "    print(\"\\n============================================================\")\n",
    "    print(\"RESULTADOS FINALES (por EVENTO) - CNN+Transformer\")\n",
    "    print(\"============================================================\")\n",
    "    print(\"Global folds:\", [f\"{a:.4f}\" for a in global_folds])\n",
    "    if global_folds:\n",
    "        print(f\"Global mean: {np.mean(global_folds):.4f}\")\n",
    "\n",
    "    if all_true_ev.size>0:\n",
    "        plot_confusion(all_true_ev, all_pred_ev, CLASS_NAMES_4C,\n",
    "                       title=\"Confusion Matrix - Global (event-level, all folds)\",\n",
    "                       fname=\"confusion_cnntf_sliding_event_eval_allfolds.png\")\n",
    "        print(\"↳ Matriz de confusión guardada: confusion_cnntf_sliding_event_eval_allfolds.png\")\n",
    "\n",
    "    return {\n",
    "        \"global_folds\": global_folds,\n",
    "        \"all_true_ev\": all_true_ev,\n",
    "        \"all_pred_ev\": all_pred_ev,\n",
    "        \"folds_json\": str(FOLDS_JSON)\n",
    "    }\n",
    "\n",
    "# =============== MAIN ===============\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🧪 CNN+Transformer (sliding windows) | 8 canales | 4 clases | eval por EVENTO\")\n",
    "    print(f\"⚙️  Window: len={WIN_LEN_S}s, stride={STRIDE_S}s | TRAIN_RANGE={TRAIN_RANGE_S}s | EVAL_RANGE={EVAL_RANGE_S}s\")\n",
    "    print(f\"🔧 Augs: jitter±25ms, noise=0.01, cutout 30–60ms (OF), mixup α={MIXUP_ALPHA}, LS={LABEL_SMOOTH}\")\n",
    "    print(f\"🗂️  Folds={N_FOLDS} (GroupKFold por sujeto) | FT por sujeto={DO_SUBJECT_FT}\")\n",
    "    run_experiment()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
