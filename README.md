# Proyecto EEG MI â€” Primer Avance

Este repositorio contiene el trabajo inicial de anÃ¡lisis y preprocesamiento de seÃ±ales EEG de **imaginaciÃ³n motora (MI)**, asÃ­ como la implementaciÃ³n del primer modelo base **FBCSP + LDA**.  

A continuaciÃ³n se describen las fases clave desarrolladas hasta el momento.

---

## 1. EDA de datos RAW

Antes de cualquier limpieza se realizÃ³ un **anÃ¡lisis exploratorio de los datos crudos** (`data/raw`) con los siguientes objetivos:

- **Inventario de archivos:** verificaciÃ³n de sujetos disponibles y runs por sujeto.
- **Conteo de eventos por clase:** Left, Right, Both Fists, Both Feet, Rest.
- **Amplitud extrema:** cÃ¡lculo de percentil 99 (`p99_uV`) y desviaciÃ³n estÃ¡ndar por canal (`std_uV`) para detectar outliers.
- **Artefactos musculares (EMG):** estimados mediante la **relaciÃ³n de potencia 20â€“40 Hz** en los canales motores.
- **PSD (densidad espectral de potencia):** inspecciÃ³n en C3, Cz, C4.
- **Separabilidad inicial:** anÃ¡lisis con **t-SNE** sobre log-varianza de las Ã©pocas.

**Hallazgos importantes en RAW:**
- Gran variabilidad inter-sujetos en amplitud (50â€“300 Î¼V).
- Presencia de ruido y artefactos musculares en varios sujetos.
- La mayorÃ­a de sujetos muestran **silhouette score negativo** â†’ baja separabilidad entre clases en el estado crudo.
- Dataset heterogÃ©neo, requiere un pipeline de preprocesamiento robusto.

---

## 2. Pipeline de Preprocesamiento

El preprocesamiento busca limpiar los EEG y asegurar que las features extraÃ­das representen la actividad neuronal y no artefactos.


### ExplicaciÃ³n paso a paso

1. **NormalizaciÃ³n de nombres y montaje**  
   - EstandarizaciÃ³n de canales y asignaciÃ³n al sistema 10â€“20.  
   - Permite localizar fÃ¡cilmente C3, Cz y C4, fundamentales en MI.

2. **Filtro Notch**  
   - Remueve interferencia elÃ©ctrica de 50/60 Hz.  
   - Evita que el ruido de la red contamine las bandas mu y beta.

3. **Clipping de amplitud**  
   - **Softclip:** atenÃºa valores extremos sin descartarlos.  
   - **Hardclip:** elimina Ã©pocas con amplitudes fuera de rango.  
   - Previene que outliers extremos dominen el entrenamiento.

4. **Filtro Bandpass (8â€“30 Hz)**  
   - AÃ­sla las bandas mu (8â€“12 Hz) y beta (13â€“30 Hz).  
   - Son las mÃ¡s asociadas a imaginaciÃ³n motora.

5. **ICA (FastICA / Picard)**  
   - Separa fuentes independientes y elimina componentes de artefactos:  
     - Oculares (EOG),  
     - Musculares (EMG),  
     - Cardiacos (ECG).  
   - Esencial para mejorar la pureza de las seÃ±ales.

6. **SegmentaciÃ³n en epochs**  
   - Ventana de **0.5â€“4.5 s post-estÃ­mulo**.  
   - Captura la actividad cortical durante la tarea, evitando ruido al inicio/final.

7. **Rechazo automÃ¡tico de Ã©pocas**  
   - Se eliminan ensayos con amplitudes excesivas (peak-to-peak).  
   - Garantiza un set final mÃ¡s balanceado y limpio.

8. **ExportaciÃ³n en formato FIF**  
   - Archivos por sujeto (`Sxxx_MI-epo.fif`).  
   - Estandariza y permite reutilizar fÃ¡cilmente con MNE y modelos posteriores.

**Resultados del preprocesamiento:**
- ReducciÃ³n significativa de amplitudes extremas.  
- Menor presencia de artefactos EMG.  
- Balance de clases mÃ¡s uniforme por sujeto.  
- Mejora en separabilidad en varios sujetos (Î” silhouette > 0).  

---

## 3. EDA de datos POST

Una vez aplicado el pipeline, se evaluaron nuevamente los datos procesados (`data/processed`) con mÃ©tricas y visualizaciones:

- **Conteo de Ã©pocas por sujeto y clase** â†’ para verificar balance.
- **PSD en C3, Cz, C4 por clase** â†’ confirmaciÃ³n de la actividad en bandas mu/beta.
- **Topomapas de potencia** en bandas mu y beta â†’ patrones espaciales de activaciÃ³n.
- **QA automÃ¡tico:** detecciÃ³n de sujetos problemÃ¡ticos (muy pocas Ã©pocas, EMG residual alto, silhouette muy negativo).
- **ComparaciÃ³n RAW vs POST**:  
  - Î” silhouette â†’ mejora en separabilidad de clases.  
  - Î” amplitud extrema (`p99_uV`) â†’ reducciÃ³n de outliers.  
  - EMG ratio â†’ caÃ­da en la mayorÃ­a de sujetos.

**Hallazgos importantes en POST:**
- DisminuciÃ³n clara en amplitudes extremas y ruido.  
- Aumento en la calidad de Ã©pocas disponibles.  
- Mejor diferenciaciÃ³n de clases en varios sujetos.  
- Sin embargo, algunos sujetos siguen presentando problemas de EMG o baja separabilidad, que deberÃ¡n manejarse con **flags de QA** en futuras iteraciones.

---

## ConclusiÃ³n preliminar

El **pipeline de preprocesamiento** aplicado transforma un dataset crudo, heterogÃ©neo y ruidoso, en un conjunto mÃ¡s limpio y balanceado. Esto sienta las bases para los experimentos con modelos como **FBCSP + LDA** y, en fases posteriores, comparaciones con **SVM, Riemannianos y redes profundas**.

--- 

## 4. FBCSP + LDA

**Objetivo:** clasificar EEG de *Motor Imagery* (Î¼/Î²) usando **Filter-Bank CSP (FBCSP)** + **LDA con shrinkage**.  
Se evalÃºa en dos configuraciones: **INTRA-subject** (K-Fold por ensayos) y **INTER-subject** (folds predefinidos por sujetos desde JSON, con **validaciÃ³n interna por sujetos** y opciÃ³n de **calibraciÃ³n**).

### Pipeline
1. **Carga y recorte** de Ã©pocas MNE (`.fif`) con ventana fija `crop_window` (p. ej., 0.5â€“3.5 s).
2. **SelecciÃ³n de canales motores** (opcional): `C3, CZ, C4, FC3, FC4, CP3, CPZ, CP4`; alineaciÃ³n de canales en VAL/TEST con `reorder_channels`.
3. **Banco de filtros** (Î¼/Î²): subbandas densas de 2 Hz entre 8â€“30 Hz (11 bandas).
4. **CSP por subbanda**: `n_csp` componentes, `reg='ledoit_wolf'`, `log=True` (devuelve vectores de varianzas proyectadas).  
   - **Fit** con **solo TRAIN** del fold; **transform** en VAL/TEST.
5. **ConcatenaciÃ³n de features** de todas las subbandas.
6. **EstandarizaciÃ³n** (z-score de features) con `StandardScaler` (fit en TRAIN).
7. **Clasificador**: `LDA(solver='lsqr', shrinkage='auto')`.

### FBCSP + LDA (features)
Para cada sub-banda (8â€“30 Hz, pasos de 2 Hz):
1) Filtramos; 
2) Ajustamos **CSP** en TRAIN y transformamos VAL/TEST;  
3) Calculamos **log-varianzas** de las `n_csp` componentes;  
4) **Concatenamos** features de todas las sub-bandas.  
Luego estandarizamos y clasificamos con **LDA (shrinkage)**.

### Evaluaciones
- **INTRA-subject** (`run_intra_all`):
  - `StratifiedKFold(k)` por sujeto (split por ensayos).
  - MÃ©tricas: Accuracy y F1-macro por fold; promedio Â± DE por sujeto; fila **GLOBAL**.
  - Artefactos: CSV, TXT, figuras de matrices de confusiÃ³n (mosaicos).

- **INTER-subject** (`run_inter_subject_cv_from_json`):
  - Folds desde JSON (`train/test` por sujetos).
  - **ValidaciÃ³n interna por sujetos**: fracciÃ³n `val_ratio_subjects` dentro de TRAIN para ajuste/selecciÃ³n.
  - CalibraciÃ³n per-subject (k-shots): Para cada sujeto de test, tomamos **k=5** Ã©pocas por clase como **calibraciÃ³n** y evaluamos en el resto de sus Ã©pocas. Durante la calibraciÃ³n se re-ajustan **FBCSP** (fit con TRAIN + k-shots del propio sujeto) y **LDA** (tras estandarizaciÃ³n). Este esquema refleja el uso real de BCI: una **breve sesiÃ³n inicial** de calibraciÃ³n por usuario mejora sustancialmente la transferencia inter-sujeto.
  - MÃ©tricas: VAL (acc, F1-macro) y TEST (acc, F1-macro) por fold + **GLOBAL**.
  - Artefactos: CSV consolidado, TXT de mÃ©tricas, TXT con `classification_report` por fold, figuras de confusiÃ³n por fold y **GLOBAL**.

### Antileakage y reproducibilidad
- CSP, scaler y LDA se ajustan **exclusivamente** con TRAIN del fold (o TRAIN+CALIB si la calibraciÃ³n estÃ¡ activa).  
- VAL/TEST sÃ³lo se **transforman**.  
- Canales de VAL/TEST se **reordenan** para coincidir con TRAIN.  
- Se generan logs con timestamp y parÃ¡metros para auditorÃ­a.

### Principales hiperparÃ¡metros
- `crop_window=(0.5, 3.5)` s  
- `motor_only=True | False`  
- `zscore_epoch=True | False` (z-score por Ã©poca previo a CSP)  
- `fb_bands`: denso (2 Hz) de 8â€“30 Hz  
- `n_csp`: tÃ­picamente 4â€“8 (p. ej., 4 Ã³ 6)  
- `val_ratio_subjectsâ‰ˆ0.16`, `calibrate_n` (sÃ³lo INTER)

### Salidas
- **Tablas** (`/models/fbcsp_lda/tables`): CSV de mÃ©tricas con fila GLOBAL.
- **Logs** (`/models/fbcsp_lda/logs`): TXT de mÃ©tricas y `classification_reports_by_fold_*.txt`.
- **Figuras** (`/models/fbcsp_lda/figures`): matrices de confusiÃ³n por sujeto/fold y GLOBAL.

> **Nota:** El mapeo de clases usa `LabelEncoder` para asegurar etiquetas consistentes. No influye en la seÃ±al ni en parÃ¡metros del modelo.

## 5. Modelo Riemanniano para MI-EEG (MDM / FgMDM)

**Resumen.** Cada Ã©poca se representa por su **matriz de covarianza SPD** (SPD significa Symmetric Positive Definite) por sub-banda (8â€“30 Hz), y se clasifica por **distancia geodÃ©sica** a las medias de clase en la geometrÃ­a Riemanniana (pyRiemann). Usamos **OAS** como estimador de covarianza y **normalizaciÃ³n por traza** para estabilizar escala. Dos variantes:
- **MDM**: Minimum Distance to Mean sobre un **bloque-diagonal** que apila las covariancias de todas las bandas.
- **FgMDM**: Filter-geodesic MDM, que **agrega en el manifold** la informaciÃ³n multi-banda.

**Preprocesado.**
- Ventana temporal: `crop_window=(0.5, 3.5)` (configurable).
- Canales: `motor_only=True` (C3, Cz, C4, FC3/4, CP3/z/4).
- Banco de bandas: denso 8â€“30 Hz, paso 2 Hz.
- Covarianza: `Covariances(estimator='oas')` + normalizaciÃ³n por traza.

**Validaciones.**
- **INTRA-sujeto (k-fold)**: 5 folds estratificados dentro de cada sujeto; mÃ©tricas por sujeto + GLOBAL.
- **INTER-sujeto (folds JSON)**: split de **validaciÃ³n por sujetos** dentro de TRAIN; ajuste del espacio **solo con TRAIN**; mÃ©tricas en VALID y TEST; **matriz de confusiÃ³n global** y `classification_report` por fold.

**CalibraciÃ³n per-subject (k-shots, recomendada).**
Para cada sujeto de TEST, tomamos **k=5** Ã©pocas por clase como **calibraciÃ³n**, recomputamos el espacio con `TRAIN + CALIB_del_sujeto` y **evaluamos en el resto** de sus Ã©pocas. Refleja el uso real de BCI con una **breve sesiÃ³n inicial** de calibraciÃ³n por usuario. (Alternativamente, se puede calibrar con **n sujetos completos** del TEST si el escenario lo requiere.)

**Features (Riemann).** Cada Ã©poca se representa por **matrices de covarianza SPD** por sub-banda (estimador OAS + normalizaciÃ³n por traza).  
**GeometrÃ­a.** Las SPD viven en un manifold; usamos **distancia geodÃ©sica Riemanniana** (afin-invariante) para comparar.  
**Clasificadores.**  
- **MDM**: calcula la **media Riemanniana** por clase y predice por **distancia al centroide**. Multi-banda vÃ­a **bloque diagonal**.  
- **FgMDM**: mantiene **una SPD por banda** y **agrega geodÃ©sicamente** la info multi-banda; suele rendir mejor.  
**En este repo:** `model='fgmdm'` (por defecto en inter-sujeto) â‡’ el clasificador activo es **FgMDM**.


---

# ðŸ§  EEGNet (4 clases, 8 canales) con Augmentaciones, TTA y Fine-Tuning Progresivo

Este proyecto implementa un clasificador EEGNet en PyTorch para seÃ±ales EEG de imaginaciÃ³n motora (MI-EEG), con:
- Entrenamiento global (5 folds) sobre datos sin balancear por sujeto/clase.
- **Augmentaciones (jitter, ruido, channel-drop)** inspiradas en CNN+Transformer.
- **SGDR (CosineAnnealingWarmRestarts)** para ajustar la tasa de aprendizaje.
- **Label smoothing**, **pesos por clase** y **max-norm** como regularizaciones.
- **Test-Time Augmentation (TTA)** por desplazamientos temporales.
- **Fine-Tuning progresivo** por sujeto con penalizaciÃ³n L2SP.

---

## ðŸ“¦ Estructura general

| Bloque | DescripciÃ³n |
|:--|:--|
| **Carga de datos** | Lectura de archivos EDF, extracciÃ³n de epochs `[-1,5]s`, selecciÃ³n de 8 canales, z-score por Ã©poca. |
| **NormalizaciÃ³n** | EstandarizaciÃ³n por canal usando estadÃ­sticas del conjunto de entrenamiento. |
| **Modelo** | EEGNet con filtros temporales y espaciales separables, salida lineal para 4 clases. |
| **Entrenamiento global** | Augments + SGDR + label smoothing + pesos por clase + max-norm. |
| **EvaluaciÃ³n (Test)** | TTA por *time-shifts* y mÃ©tricas (ACC, F1, matriz de confusiÃ³n). |
| **Fine-tuning progresivo** | AdaptaciÃ³n por sujeto con L2SP y congelamiento progresivo de capas. |

---

## âš™ï¸ ConfiguraciÃ³n global (hiperparÃ¡metros)

| CategorÃ­a | ParÃ¡metro | Valor | ExplicaciÃ³n |
|:--|:--|:--|:--|
| Datos | FS | 160 Hz | Frecuencia de muestreo objetivo |
| | TMIN/TMAX | -1.0 / 5.0 s | Ventana temporal de cada ensayo |
| | EXPECTED_8 | C3,C4,Cz,CP3,CP4,FC3,FC4,FCz | Canales usados |
| | NORM_EPOCH_ZSCORE | True | Z-score por Ã©poca y canal |
| Split | VAL_SUBJECT_FRAC | 0.18 | % de sujetos usados como validaciÃ³n |
| | VAL_STRAT_SUBJECT | True | ValidaciÃ³n estratificada por etiqueta dominante |
| Train | EPOCHS_GLOBAL | 100 | Ã‰pocas mÃ¡ximas |
| | BATCH_SIZE | 64 | TamaÃ±o de batch |
| | LR_INIT | 1e-2 | Tasa inicial de aprendizaje |
| | SGDR_T0 / SGDR_Tmult | 6 / 2 | Ciclos coseno: 6, 12, 24â€¦ |
| | GLOBAL_PATIENCE | 10 | Early stopping |
| EEGNet | F1 / D | 24 / 2 | Filtros temporales y multiplicador depthwise |
| | kernel_t / k_sep | 64 / 16 | Kernels temporal y separable |
| | pool1_t / pool2_t | 4 / 6 | ReducciÃ³n temporal por bloque |
| | drop1_p / drop2_p | 0.35 / 0.6 | Dropout |
| | chdrop_p | 0.10 | Channel dropout |
| Loss | label_smoothing | 0.05 | Suavizado de etiquetas |
| | boost (clase 2/3) | 1.25 / 1.05 | Pesos extra para clases raras |
| Augments | p_jitter / p_noise / p_chdrop | 0.35 / 0.35 / 0.15 | Probabilidad de aplicar cada tipo |
| | max_jitter_frac / noise_std | 0.03 / 0.03 | Magnitud del jitter y ruido |
| TTA | shifts_s | Â±0.075,â€¦,0 s | Desplazamientos en inferencia |
| FT | CALIB_CV_FOLDS | 4 | Folds internos por sujeto |
| | FT_EPOCHS | 30 | Ã‰pocas por modo |
| | FT_HEAD_LR / FT_BASE_LR | 1e-3 / 5e-5 | LR para cabeza y base |
| | FT_L2SP | 1e-4 | PenalizaciÃ³n de alejamiento de pesos globales |
| | FT_PATIENCE / FT_VAL_RATIO | 5 / 0.2 | Early stopping y validaciÃ³n interna |

---

## ðŸ§  Arquitectura EEGNet

### Estructura general
Entrada: `(B, 1, T, C)` â†’ Salida: `(B, 4)`  
(`B`: batch, `T`: tiempo â‰ˆ 960, `C`: canales=8)

1. **ChannelDropout (p=0.10)**  
   Apaga canales completos aleatoriamente (simula fallos de electrodos).

2. **Bloque temporal**
   ```python
   Conv2d(1 â†’ F1, kernel=(64,1)) â†’ BN â†’ ELU
   ```
   Extrae patrones de oscilaciÃ³n y filtros pasa-banda temporales.

3. **Bloque espacial (depthwise)**
   ```python
   Conv2d(F1 â†’ F1*D, kernel=(1, n_ch), groups=F1)
   AvgPool2d(4,1) â†’ Dropout(0.35)
   ```
   Aprende combinaciones espaciales (proyecciones de canales) por cada filtro temporal.

4. **Bloque separable temporal**

   ```python
   Conv2d(F2 â†’ F2, kernel=(16,1), groups=F2)
   Conv2d(F2 â†’ F2, kernel=(1,1))
   AvgPool2d(6,1) â†’ Dropout(0.6)
   ```
   Refina dinÃ¡micas temporales especÃ­ficas con pocos parÃ¡metros.

5. **Head**

   ```python
   Flatten â†’ Linear(1920â†’128) â†’ ELU â†’ Linear(128â†’4)
   ```
   ProyecciÃ³n final a clases.

## ðŸ§© Entrenamiento global

- **Optimizador:** Adam (`LR inicial = 1e-2`)
- **Scheduler:** CosineAnnealingWarmRestarts (SGDR)  
  - Ciclos: 6, 12, 24, 48 Ã©pocasâ€¦  
  - Cada reinicio restablece el LR alto para explorar nuevos mÃ­nimos.
- **Augmentaciones:** jitter temporal, ruido gaussiano, channel-drop
- **PÃ©rdida:** Weighted Soft CrossEntropy con *label smoothing* (0.05)
- **Regularizaciones:**
  - Max-norm = 2.0 (filtros espaciales y FC)
  - Dropout + ChannelDropout
  - Label smoothing + pesos por clase

## ðŸ”„ CosineAnnealingWarmRestarts (SGDR)

Controla la **tasa de aprendizaje (LR)** de forma cÃ­clica, alternando fases de exploraciÃ³n y refinamiento.

### ðŸ§  IntuiciÃ³n
- **Grandes saltos** â†’ explora nuevos mÃ­nimos.  
- **LR pequeÃ±o** â†’ refina soluciones locales.  
- **Reinicios** â†’ permite escapar de mÃ­nimos subÃ³ptimos.

### âš™ï¸ ParÃ¡metros
- **Tâ‚€ = 6** â†’ duraciÃ³n del primer ciclo (Ã©pocas).  
- **T_mult = 2** â†’ cada ciclo siguiente dura el doble: 6, 12, 24, 48, â€¦

El LR se reinicia al valor inicial en cada ciclo, generando una curva coseno decreciente dentro de cada fase.

### ðŸ’¡ Consejos prÃ¡cticos
| SituaciÃ³n | Ajuste recomendado |
|:--|:--|
| `val_acc` oscila mucho | Aumenta **Tâ‚€** o reduce **LR_INIT** |
| Entrenamiento estancado | Sube **LR_INIT** o acorta **Tâ‚€** |

âœ¨ **Idea clave:** equilibrar exploraciÃ³n (LR alto) y refinamiento (LR bajo) para alcanzar mejores mÃ­nimos sin sobreentrenar.

## ðŸŒˆ Augmentaciones (entrenamiento)

Tres transformaciones principales se aplican por batch (B, 1, T, C):

### 1ï¸âƒ£ Jitter temporal
Desplaza la seÃ±al unos milisegundos (roll en el eje temporal).  
**max_jitter_frac = 0.03**  â†’ Â±180 ms para 6 s (Tâ‰ˆ960)  

**Motivo:** simula pequeÃ±as desincronizaciones del inicio del ensayo (onset).  
**Rango tÃ­pico:** 0.02â€“0.05 (Â±120â€“300 ms).  

---

### 2ï¸âƒ£ Ruido gaussiano
**noise_std = 0.03**  

**Motivo:** mejora la robustez frente a ruido fisiolÃ³gico y electrÃ³nico.  
**Rango recomendado:** Ïƒ = 0.01â€“0.05 para seÃ±ales EEG normalizadas (z-scoreadas).  

---

### 3ï¸âƒ£ Channel-drop
**p_chdrop = 0.15**, **max_chdrop = 1**  

**Motivo:** incrementa la robustez espacial ante la pÃ©rdida o mal contacto de electrodos.  
**RecomendaciÃ³n:** apagar 1â€“2 canales cuando se trabaja con 8 canales totales.  

## ðŸ§± Regularizaciones

### ðŸ§© Label Smoothing (Îµ=0.05)
Suaviza las etiquetas para evitar sobreconfianza:

**FÃ³rmula:**
\[
\tilde{y} = (1 - \varepsilon) \cdot \text{one\_hot} + \frac{\varepsilon}{K}
\]

Esto reduce la probabilidad de que el modelo se vuelva demasiado confiado en una sola clase, mejorando la **calibraciÃ³n** de las predicciones.

---

### âš–ï¸ Pesos por clase
Para corregir el desbalance de clases en el entrenamiento se asignan pesos distintos a las clases minoritarias:

- **Clase 2 (Both Fists):** Ã—1.25  
- **Clase 3 (Both Feet):** Ã—1.05  

Esto aumenta la contribuciÃ³n de las clases menos frecuentes en la funciÃ³n de pÃ©rdida y ayuda a equilibrar el rendimiento entre categorÃ­as.

---

### ðŸ§® Max-Norm
Se aplica un lÃ­mite **L2 mÃ¡ximo (2.0)** sobre los pesos de los filtros.  
Este mÃ©todo evita la explosiÃ³n de pesos y mejora la **estabilidad del entrenamiento**.  
Si el modelo sobreajusta, se puede reducir el lÃ­mite a 1.8; si no aprende bien, puede aumentarse a 2.5â€“3.0.

---

## ðŸ”® Inferencia: Test-Time Augmentation (TTA)

Durante la fase de inferencia, cada seÃ±al EEG se **desplaza ligeramente en el tiempo** y se promedian las predicciones (logits) para mejorar la robustez.

**Desplazamientos usados (en segundos):**  
[-0.075, -0.05, -0.025, 0, +0.025, +0.05, +0.075]

**PropÃ³sito:**  
Aumentar la resistencia a pequeÃ±as desalineaciones temporales entre ensayos.

**Costo:**  
Proporcional al nÃºmero de desplazamientos (mÃ¡s TTA = mÃ¡s tiempo de inferencia).

**Recomendaciones:**
- MÃ¡ximo rendimiento: usar 5â€“7 desplazamientos.  
- Cuando el tiempo sea crÃ­tico: usar 3 desplazamientos ([-0.05, 0, +0.05]).

---

## ðŸ”§ Fine-Tuning progresivo por sujeto

Cada sujeto de test se adapta mediante **fine-tuning** interno (4 folds por sujeto), ajustando partes especÃ­ficas del modelo sin olvidar el conocimiento global.

**Modos de ajuste:**
- **out:** solo la capa de salida  
- **head:** capa totalmente conectada + salida  
- **spatial+head:** bloque espacial + cabeza del modelo  

**ParÃ¡metros clave:**
- LR base: 5e-5  
- LR cabeza: 1e-3  
- L2SP: 1e-4  
- Patience (early stopping): 5  
- ValidaciÃ³n interna: 20%  

El objetivo es **personalizar el modelo a cada sujeto** preservando las representaciones generales aprendidas.

---

## ðŸ“Š MÃ©tricas y salidas

- **Accuracy global** y **F1 macro** promedio.  
- Curvas de entrenamiento: `training_curve_foldX.png`  
- Matrices de confusiÃ³n: `confusion_global_foldX.png`  
- Resultados del fine-tuning: `acc_global`, `acc_ft` y diferencia `Î”(FT-Global)`.

---

## ðŸ’¡ Cheatsheet de ajuste

| Problema | Causa posible | SoluciÃ³n sugerida |
|:--|:--|:--|
| **Overfitting (Trainâ†‘, Valâ†“)** | Modelo muy complejo o augmentaciones suaves | Subir dropout, aumentar Îµ de Label Smoothing, reducir F1 |
| **Underfitting (Trainâ†“, Valâ†“)** | LR demasiado bajo o augmentaciones muy fuertes | Aumentar LR, reducir ruido/jitter |
| **Oscilaciones en validaciÃ³n** | LR alto o ciclos SGDR muy cortos | Aumentar Tâ‚€ o reducir LR inicial |
| **Mala precisiÃ³n en clases 2/3** | Pocas muestras o sin boost | Subir pesos (1.4 / 1.2) |
| **Modelo inestable** | Max-norm demasiado alto | Bajar a 1.8â€“2.0 |

---


## ðŸ§® Flujo resumido del pipeline
```bash
raw EDF â”€â–º selecciÃ³n de 8 canales
        â””â–º epoch [-1, 5] s @160 Hz
            â””â–º z-score por Ã©poca
                â””â–º split Kfold5 (train/val/test)
                    â””â–º estandarizaciÃ³n canal-fit(train)
                        â””â–º EEGNet + Augments
                            â””â–º Train con SGDR
                                â””â–º Test con TTA
                                    â””â–º Fine-tuning por sujeto (L2SP)
```


# ðŸ§  CNN+Transformer para MI-EEG (4 clases, 8 canales)

Con sampler balanceado, Focal Loss, Warmup+Cosine, EMA, TTA y Fine-Tuning Progresivo.

Este modelo combina **CNNs** (para aprender patrones locales) y **Transformers** (para dependencias globales) en la clasificaciÃ³n de seÃ±ales EEG de imaginaciÃ³n motora. Usa 8 canales seleccionados y una ventana de 6 s, con tÃ©cnicas modernas de regularizaciÃ³n y calibraciÃ³n por sujeto.

---

## ðŸ“¦ Datos y preprocesamiento

- **Ventana temporal:** [-1, 5] s (6 s totales)  
- **Canales usados (8):** C3, C4, Cz, CP3, CP4, FC3, FC4, FCz  
- **Runs utilizados:**
  - R04, R08, R12 â†’ T1/T2 = left/right  
  - R06, R10, R14 â†’ T1/T2 = both fists/both feet

**Preprocesamiento configurable:**
- Filtro notch 60 Hz (`DO_NOTCH=True`)  
- Band-pass opcional 4â€“38 Hz (`DO_BANDPASS=False`)  
- Referencia promedio (CAR) opcional  
- Resample opcional (`RESAMPLE_HZ=None`)  
- `ZSCORE_PER_EPOCH=False` â†’ usa estandarizaciÃ³n por canal  
- **NormalizaciÃ³n:** por canal (fit en train, aplicado a val/test)

---

## ðŸ”€ ParticiÃ³n por sujetos

- **K-fold (5)** definido en `models/folds/Kfold5.json`  
- Cada fold usa ~18 % de los sujetos de entrenamiento como validaciÃ³n  
- **EstratificaciÃ³n:** por etiqueta dominante de cada sujeto para asegurar balance

---

## âš–ï¸ Sampler balanceado (templado)

Se usa un `WeightedRandomSampler` con pesos:

\[
w = (1 / w_s)^{0.8} \cdot (1 / w_{(s,y)})^{1.0}
\]

donde:
- \(w_s\): nÃºmero de ensayos del sujeto *s*  
- \(w_{(s,y)}\): nÃºmero de ensayos de la clase *y* dentro del sujeto *s*

Esto equilibra tanto el nÃºmero de sujetos como el de clases, evitando dominancia por participantes o categorÃ­as mÃ¡s frecuentes.

---

## ðŸ—ï¸ Arquitectura: CNN + Transformer

### CNN (bloque temporal)

La **CNN** extrae patrones locales de la seÃ±al temporal (oscilaciones, desincronizaciones o transientes) y los combina jerÃ¡rquicamente.

| Capa | Tipo | PropÃ³sito | Salida (T=960) |
|:--|:--|:--|:--|
| 1 | Conv 1D (8â†’32, k=129, stride=2) + GN + ELU + Drop 0.2 | Detecta patrones largos (~0.8 s) | (B, 32, 480) |
| 2 | Depthwise Sep Conv (32â†’64, k=31, stride=2) | Refina patrones / mezcla espacial | (B, 64, 240) |
| 3 | Depthwise Sep Conv (64â†’128, k=15, stride=2) | Captura interacciones amplias | (B, 128, 120) |

**CaracterÃ­sticas clave:**  
- **Depthwise separable convolutions:** separan â€œquÃ© patrÃ³n por canalâ€ de â€œcÃ³mo combinar canalesâ€.  
- **GroupNorm:** estabilidad con batches pequeÃ±os.  
- **ELU:** activaciÃ³n suave sin saturaciÃ³n.  
- **Dropout:** regularizaciÃ³n temporal y espacial.  

La CNN produce una secuencia comprimida (B, 128, â‰ˆ120) que resume la dinÃ¡mica temporal.

---

### Transformer (bloque global)

El **Transformer Encoder** aprende dependencias globales entre las caracterÃ­sticas producidas por la CNN.  
Cada posiciÃ³n â€œobservaâ€ todas las demÃ¡s mediante **auto-atenciÃ³n**.

$$
\mathrm{Attention}(Q,K,V)=\mathrm{softmax}\!\left(\frac{QK^{\top}}{\sqrt{d_k}}\right)V
$$

- **Q (Query):** quÃ© busca  
- **K (Key):** dÃ³nde buscar  
- **V (Value):** informaciÃ³n aportada

AsÃ­, cada instante combina informaciÃ³n de otros momentos relevantes, capturando **relaciones a larga distancia** (p. ej., desincronizaciÃ³n temprana y rebote tardÃ­o).

**Estructura del Transformer:**
- ProyecciÃ³n conv 1Ã—1 (128â†’128)  
- Positional encoding (senos/cosenos fijos)  
- Token [CLS] entrenable que resume la secuencia  
- Encoder: 2 capas, 4 cabezas de atenciÃ³n, GELU, dropout 0.1  
- Head final: LayerNorm â†’ Linear â†’ 4 clases

**QuÃ© aprende:**
- Dependencias temporales no locales  
- Relaciones entre fases del ensayo  
- Evidencias dispersas â†’ representaciÃ³n global

---

## ðŸ”— Sinergia CNN + Transformer

| Componente | Aporta | Ejemplo EEG |
|:--|:--|:--|
| **CNN** | Patrones locales robustos, filtrado y reducciÃ³n temporal | Ritmos Î¼, Î², transientes |
| **Transformer** | Relaciones globales, sincronÃ­as, dependencias largas | ConexiÃ³n entre desincronizaciÃ³n y rebote |
| **Combinados** | Robustez + contexto global | DecisiÃ³n estable por ensayo |

---

## ðŸŽ¯ PÃ©rdida y optimizaciÃ³n

- **Focal Loss (Î³ = 1.5):** enfatiza ejemplos difÃ­ciles  
- **Î± por clase:** inversa de frecuencia con boosts  
  - both fists Ã— 1.25  
  - both feet Ã— 1.05  
- **Optimizador:** AdamW (lr = 1e-3, weight_decay = 1e-2)  
- **Scheduler:** Warmup (8 Ã©pocas) + Cosine decay (0.1Ã—lr mÃ­nimo)  
- **EMA:** promedio exponencial de pesos (decay = 0.9995)

---

## ðŸŒˆ Augmentaciones

| Tipo | ParÃ¡metros globales | PropÃ³sito |
|:--|:--|:--|
| **Jitter temporal** | Â±3 % T (~Â±180 ms) | Robustez a desalineaciÃ³n del onset |
| **Ruido gaussiano** | Ïƒ = 0.03 | Simula ruido fisiolÃ³gico/electrÃ³nico |
| **Channel dropout** | 1 canal (p = 0.15) | Robustez ante fallos de electrodos |

En **fine-tuning**, se suavizan: jitter 2 %, Ïƒ 0.02, p 0.10.

---

## ðŸ”® Inferencia (TTA / Subwindows)

- **TTA:** desplaza la seÃ±al Â±75 ms y promedia los logits  
- **Subwindows:** evalÃºa tramos de 4.5 s cada 1.5 s â†’ promedio  
- **Modo combinado:** mezcla ambos promedios  

â†’ Invarianza ante errores de tiempo y mayor estabilidad de predicciÃ³n.

---

## ðŸ”§ Fine-Tuning progresivo por sujeto

Ajuste personalizado en dos etapas:

| Etapa | Capas entrenadas | Ã‰pocas | PropÃ³sito |
|:--|:--|:--|:--|
| 1 | Solo head | 8 | Calibrar salida sin alterar features |
| 2 | Todo modelo (unfreeze) | 8 | Adaptar representaciÃ³n al sujeto |

**CV interna:** 4 folds  
**Optimizador FT:** AdamW (backbone lr 2e-4, head lr 1e-3)  
**PÃ©rdida:** Focal Loss re-estimada por sujeto (con boosts)  
**Augmentaciones:** suaves  

---

## ðŸ“Š MÃ©tricas y resultados

- Early stopping por F1 macro (validaciÃ³n de sujetos)  
- Curvas: `training_curve_foldX.png`  
- Matrices: `confusion_global_foldX.png`, `confusion_ft_foldX.png`  
- Resumen: Accuracy global, F1 macro, FT accuracy y Î” (FT âˆ’ Global)

---

## âš™ï¸ DiagnÃ³stico y ajuste rÃ¡pido

| Problema | Posible causa | Ajuste sugerido |
|:--|:--|:--|
| **F1 inestable** | LR alto / warmup corto | Bajar LR (7e-4 â€“ 1e-3), subir warmup (10â€“12) |
| **Underfit** | Augment fuerte / modelo pequeÃ±o | Reducir ruido/jitter, aumentar d_model a 160â€“192 |
| **Overfit** | RegularizaciÃ³n dÃ©bil | Aumentar dropout (0.25â€“0.4), WD 2e-2, Î³ 1.7â€“2.0 |
| **Recall bajo clase 2/3** | Desbalance residual | Subir Î±[2]/Î±[3], boost FT, ajustar sampler (b > 1.0) |
| **FT no mejora** | Ajuste agresivo | Reducir augment FT, usar solo etapa 1, bajar lr head |

---

## ðŸ§© Flujo de datos

EEG (B, 8, T = 960)  
â†’ Conv1d 8â†’32 (k129, s2) â†’ GN â†’ ELU â†’ Drop  
â†’ SepConv 32â†’64 (k31, s2)  
â†’ SepConv 64â†’128 (k15, s2)  
â†’ Conv 1Ã—1 128â†’128 â†’ Drop  
â†’ Transpose (B, L â‰ˆ 120, D = 128)  
â†’ + PosEnc â†’ concat CLS  
â†’ Transformer Encoder Ã— 2  
â†’ CLS â†’ LayerNorm â†’ Linear â†’ logits (4)

---

## ðŸ§  CÃ³mo funciona el CNN y el Transformer

### CNN â€” ExtracciÃ³n local de patrones

La CNN aprende filtros que responden a **patrones temporales especÃ­ficos** (oscilaciones, desincronizaciones, transientes).  
Las convoluciones con *stride* reducen resoluciÃ³n temporal y amplÃ­an el contexto.  
Las **depthwise separable convolutions** separan la detecciÃ³n temporal (por canal) de la mezcla espacial, generando mapas de activaciÃ³n compactos y expresivos.

### Transformer â€” Aprendizaje global de dependencias

El Transformer usa **auto-atenciÃ³n** para que cada instante observe todos los demÃ¡s, asignando pesos segÃºn relevancia.  
AsÃ­ capta **relaciones de largo alcance** sin necesidad de mÃ¡s capas convolucionales.  
El token [CLS] sintetiza toda la secuencia en un vector representativo.  
Cada cabeza de atenciÃ³n analiza las relaciones en un subespacio distinto.

### Sinergia

- **CNN:** aprende *quÃ©* patrones existen  
- **Transformer:** aprende *cÃ³mo* se combinan en el tiempo  
- **CombinaciÃ³n:** robustez al ruido + comprensiÃ³n de contextos largos y complejos
