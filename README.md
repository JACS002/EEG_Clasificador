# Classification of Motor Imagery EEG Signals Using CNNTransformer Hybrid Models
### Autor: Joel Cuascota

Clasificaci贸n de se帽ales EEG de imaginaci贸n motora (motor imagery) utilizando un modelo h铆brido **CNN+Transformer** para el reconocimiento de movimientos imaginados de manos (izquierda/derecha).

## Descripci贸n del Proyecto

Este proyecto implementa un sistema de clasificaci贸n de se帽ales electroencefalogr谩ficas (EEG) basado en imaginaci贸n motora, utilizando un modelo h铆brido que combina redes neuronales convolucionales (CNN) con arquitectura Transformer. El objetivo principal es clasificar se帽ales EEG correspondientes a la imaginaci贸n de movimientos de la mano izquierda o derecha.

### Caracter铆sticas Principales

- **Modelo H铆brido CNN+Transformer**: Combina extracci贸n de caracter铆sticas espaciotemporales (CNN) con modelado de dependencias temporales largas (Transformer)
- **Clasificaci贸n Binaria**: Distinci贸n entre imaginaci贸n motora de mano izquierda vs derecha
- **Dataset**: PhysioNet EEG Motor Movement/Imagery Dataset
- **Canales Motores**: 8 canales espec铆ficos del 谩rea motora (C3, C4, Cz, CP3, CP4, FC3, FC4, FCz)
- **Validaci贸n Robusta**: K-Fold cross-validation por sujeto (5 folds)
- **T茅cnicas Avanzadas**:
  - Test-Time Augmentation (TTA)
  - Exponential Moving Average (EMA) de pesos
  - Focal Loss para manejo de desbalance de clases
  - Fine-tuning por sujeto
  - Interpretabilidad mediante mapas de atenci贸n

## Arquitectura del Modelo

### EEGCNNTransformer

El modelo se compone de tres etapas principales:

1. **Backbone Convolucional**:
   - Stem inicial con conv1d (129 kernels)
   - Bloques depthwise separable para reducci贸n de par谩metros
   - GroupNorm y ELU como activaci贸n

2. **Encoder Transformer**:
   - Multi-head self-attention para modelar dependencias temporales
   - Codificaci贸n posicional sinusoidal
   - Token CLS para clasificaci贸n

3. **Head de Clasificaci贸n**:
   - LayerNorm + Linear layer
   - Salida de 2 clases (left/right)

### Hiperpar谩metros Principales

```python
D_MODEL = 144              # Dimensi贸n del embedding
N_HEADS = 4                # Cabezas de atenci贸n
N_LAYERS = 1               # Capas del Transformer
BATCH_SIZE = 64            # Tama帽o de batch
BASE_LR = 5e-4            # Learning rate
EPOCHS = 60                # pocas de entrenamiento
TMIN, TMAX = -1.0, 5.0    # Ventana temporal (6s)
```

## Instalaci贸n

### Requisitos

- Python >= 3.8
- CUDA compatible GPU (recomendado)

### Instalaci贸n de Dependencias

```bash
git clone https://github.com/JACS002/EEG_Clasificador.git
cd EEG_Clasificador
pip install -r requirements.txt
```

### Dependencias Principales

- **PyTorch** >= 2.0 - Framework de deep learning
- **MNE** >= 1.5 - Procesamiento de se帽ales EEG
- **NumPy** >= 1.24 - Computaci贸n num茅rica
- **scikit-learn** >= 1.3 - M茅tricas y validaci贸n
- **matplotlib** >= 3.7 - Visualizaci贸n

## Dataset

El proyecto utiliza el **PhysioNet EEG Motor Movement/Imagery Dataset**, que contiene:

- 109 sujetos
- 64 canales de EEG
- Tareas de imaginaci贸n motora de manos y pies
- Frecuencia de muestreo: 160 Hz

### Preprocesamiento

1. **Selecci贸n de canales**: 8 canales del 谩rea motora
2. **Filtrado**: Notch filter en 60 Hz (ruido el茅ctrico)
3. **Epoching**: Ventanas de 6 segundos (-1s a 5s respecto al evento)
4. **Normalizaci贸n**: Z-score por canal usando estad铆sticas del train set
5. **Runs utilizados**: 4, 8, 12 (imaginaci贸n motora L/R)

## Uso

### Entrenamiento del Modelo

El notebook principal se encuentra en:

```
models/04_hybrid/cnntransformer2c.ipynb
```

**Flujo de entrenamiento:**

1. **Entrenamiento Global** (5-fold cross-validation por sujeto)
   - Entrenamiento en m煤ltiples sujetos
   - Validaci贸n por sujetos no vistos
   - EMA de pesos activado
   - TTA en evaluaci贸n

2. **Fine-tuning por Sujeto** (opcional)
   - Adaptaci贸n a caracter铆sticas individuales
   - Dos etapas: congelado + descongelado del backbone
   - Data augmentation (jitter, ruido, channel dropout)

### Pipeline de Evaluaci贸n

```python
# El notebook implementa:
1. Carga de datos por sujeto
2. Divisi贸n en folds seg煤n Kfold5.json
3. Entrenamiento con early stopping
4. Evaluaci贸n con TTA
5. M茅tricas: Accuracy, F1, Precision, Recall
6. Visualizaciones: matrices de confusi贸n, curvas de aprendizaje
```

## Resultados

El modelo logra:

- **Validaci贸n Cruzada**: Accuracy y Macro F1 ~82% en evaluaci贸n inter-sujeto (5-fold CV)

### M茅tricas Reportadas

- Accuracy por fold y promedio
- F1-score macro/weighted
- Matrices de confusi贸n


## Modelos Adicionales

### Extensi贸n a 4 Clases (cnntransformer4c.ipynb)

El notebook `cnntransformer4c.ipynb` extiende el modelo para clasificar **4 tipos de imaginaci贸n motora**:
- Mano izquierda
- Mano derecha
- Ambos pu帽os
- Ambos pies

**Nota**: Este es un problema m谩s desafiante con menor accuracy (~54%), 煤til para evaluar la capacidad del modelo en escenarios multi-clase m谩s complejos.

## Caracter铆sticas T茅cnicas

### Reproducibilidad

Configuraci贸n completa de semillas para garantizar experimentos reproducibles:
```python
RANDOM_STATE = 42
torch.use_deterministic_algorithms(True)
torch.backends.cudnn.deterministic = True
```

## Citaci贸n

Este proyecto es parte del trabajo de tesis para el grado de **Ingenier铆a en Ciencias de la Computaci贸n** en la **Universidad San Francisco de Quito**.

Si utilizas este c贸digo, modelos o metodolog铆a en tu investigaci贸n, por favor cita este repositorio:

```bibtex
@misc{cuascota2025eeg_cnn_transformer,
  author = {Cuascota, Joel},
  title = {Classification of Motor Imagery EEG Signals Using CNNTransformer Hybrid Models},
  year = {2025},
  month = {12},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/JACS002/EEG_Clasificador}},
  note = {Trabajo de tesis, Universidad San Francisco de Quito}
}
```

**Formato APA:**
> Cuascota, J. (2025). *Classification of Motor Imagery EEG Signals Using CNNTransformer Hybrid Models* [Repositorio de GitHub]. Universidad San Francisco de Quito. https://github.com/JACS002/EEG_Clasificador

##  Licencia

Este proyecto es de c贸digo abierto y est谩 disponible bajo la licencia MIT.

